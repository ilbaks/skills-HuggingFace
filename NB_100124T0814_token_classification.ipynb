{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB_100124T0814_token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fine-tune and use LLM for token classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Steps\n",
    "    \n",
    "    - load dataset in cache memory\n",
    "    - EDA of the dataset\n",
    "    - tokenizer\n",
    "    - processing the dataset according to the tokenizer shift\n",
    "    - datacollector\n",
    "    - metrics\n",
    "    - defining the model\n",
    "    - fine-tuning the model through Trainer\n",
    "    - fine-tuning the model through custom training loop\n",
    "    - using the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Initializing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional framework used to evaluate token classification prediction is [seqeval](https://github.com/chakki-works/seqeval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/43.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.6/43.6 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.14.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from seqeval) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from seqeval) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (pyproject.toml): started\n",
      "  Building wheel for seqeval (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16185 sha256=0ad3e3123bf309b588e73e1b8be5f1e24e9e0edaba38f1b41fb24d098c1e3534\n",
      "  Stored in directory: c:\\users\\baksa\\appdata\\local\\pip\\cache\\wheels\\e2\\a5\\92\\2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"conll2003\"\n",
    "MODEL_CHECKPOINT = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Load data set in cache memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset very bit, it can be loaded through iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. EDA of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "    num_rows: 14041\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9, 9, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_datasets[\"train\"][0][\"tokens\"]), len(\n",
    "    raw_datasets[\"train\"][0][\"pos_tags\"]\n",
    "), len(raw_datasets[\"train\"][0][\"chunk_tags\"]), len(\n",
    "    raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features[\"pos_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abbreviations you've listed are part-of-speech (POS) tags commonly used in natural language processing. They are used to classify words in a sentence according to their grammatical role. Here's what each abbreviation stands for:\n",
    "\n",
    "1. **CC**: Coordinating conjunction (e.g., and, but, or)\n",
    "2. **CD**: Cardinal number (e.g., one, two, 3)\n",
    "3. **DT**: Determiner (e.g., the, a, these)\n",
    "4. **EX**: Existential there (e.g., there is, there were)\n",
    "5. **FW**: Foreign word (a word from another language)\n",
    "6. **IN**: Preposition or subordinating conjunction (e.g., in, of, like, because)\n",
    "7. **JJ**: Adjective (e.g., big, happy)\n",
    "8. **JJR**: Adjective, comparative (e.g., bigger, happier)\n",
    "9. **JJS**: Adjective, superlative (e.g., biggest, happiest)\n",
    "10. **LS**: List item marker (used in lists)\n",
    "11. **MD**: Modal (e.g., can, should, would)\n",
    "12. **NN**: Noun, singular or mass (e.g., cat, tree)\n",
    "13. **NNP**: Proper noun, singular (e.g., Alice, London)\n",
    "14. **NNPS**: Proper noun, plural (e.g., Americans, Carolinas)\n",
    "15. **NNS**: Noun, plural (e.g., cats, trees)\n",
    "16. **NN|SYM**: This seems to be a non-standard tag, possibly denoting either a noun or a symbol.\n",
    "17. **PDT**: Predeterminer (e.g., all, both, half)\n",
    "18. **POS**: Possessive ending ('s)\n",
    "19. **PRP**: Personal pronoun (e.g., I, you, he)\n",
    "20. **PRP$**: Possessive pronoun (e.g., my, your, his)\n",
    "21. **RB**: Adverb (e.g., quickly, not, very)\n",
    "22. **RBR**: Adverb, comparative (e.g., faster)\n",
    "23. **RBS**: Adverb, superlative (e.g., fastest)\n",
    "24. **RP**: Particle (e.g., up, off, out)\n",
    "25. **SYM**: Symbol (e.g., +, %, &)\n",
    "26. **TO**: The word \"to\" (used before a verb, e.g., to run, to play)\n",
    "27. **UH**: Interjection (e.g., uh, wow, oops)\n",
    "28. **VB**: Verb, base form (e.g., run, play)\n",
    "29. **VBD**: Verb, past tense (e.g., ran, played)\n",
    "30. **VBG**: Verb, gerund or present participle (e.g., running, playing)\n",
    "31. **VBN**: Verb, past participle (e.g., run, played)\n",
    "32. **VBP**: Verb, non-3rd person singular present (e.g., run, play)\n",
    "33. **VBZ**: Verb, 3rd person singular present (e.g., runs, plays)\n",
    "34. **WDT**: Wh-determiner (e.g., which, whatever, whichever)\n",
    "35. **WP**: Wh-pronoun (e.g., who, whom, which)\n",
    "36. **WP$**: Possessive wh-pronoun (e.g., whose)\n",
    "37. **WRB**: Wh-adverb (e.g., where, when, why)\n",
    "\n",
    "These tags are part of a standard set known as the Penn Treebank POS tags, widely used in computational linguistics and natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features[\"chunk_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This abbreviation relates to a specific way of annotating text for chunking or shallow parsing in natural language processing (NLP). Let's break it down:\n",
    "\n",
    "1. **`Sequence(feature=ClassLabel(names=[...]), length=-1, id=None)`**:\n",
    "    - **`Sequence`**: This indicates that the data structure is a sequence, typically a list or array of elements in a specific order. In NLP, this often refers to sequences of words or tokens in a sentence.\n",
    "    - **`feature=ClassLabel(names=[...])`**: This specifies that each element in the sequence is a class label from a predefined set of labels. These labels represent different types of chunks in text.\n",
    "    - **`length=-1`**: This likely means that the sequences can be of variable length.\n",
    "    - **`id=None`**: This suggests that there is no specific identifier associated with each sequence.\n",
    "\n",
    "2. **Abbreviations in `names=[...]`**:\n",
    "    - **`O`**: Outside of any chunk.\n",
    "    - **`B-`** and **`I-`** prefixes: These are common in BIO tagging, a method used in NLP. `B-` stands for the beginning of a chunk, and `I-` stands for inside a chunk. These prefixes are followed by the type of chunk.\n",
    "        - **`ADJP`**: Adjective Phrase.\n",
    "        - **`ADVP`**: Adverb Phrase.\n",
    "        - **`CONJP`**: Conjunction Phrase.\n",
    "        - **`INTJ`**: Interjection.\n",
    "        - **`LST`**: List marker.\n",
    "        - **`NP`**: Noun Phrase.\n",
    "        - **`PP`**: Prepositional Phrase.\n",
    "        - **`PRT`**: Particle.\n",
    "        - **`SBAR`**: Clause introduced by a (subordinating) conjunction.\n",
    "        - **`UCP`**: Unlike Coordinated Phrase.\n",
    "        - **`VP`**: Verb Phrase.\n",
    "\n",
    "Each of these tags is used to annotate specific parts of a sentence. For example, in the sentence \"The quick brown fox\", \"The quick brown\" could be tagged as `B-NP I-NP I-NP` (beginning of a noun phrase, inside, inside), indicating that these words form a noun phrase. This type of tagging is crucial for tasks like information extraction, where understanding the structure of sentences is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abbreviation you're referring to, `ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'])`, is used in the context of named entity recognition (NER), a common task in natural language processing (NLP). This task involves identifying and classifying key information (entities) in text into predefined categories. Let's explain each component:\n",
    "\n",
    "1. **`ClassLabel(names=[...])`**: This indicates a classification task where each entity in the text is labeled as one of the predefined classes.\n",
    "\n",
    "2. **The abbreviations in `names=[...]`**:\n",
    "   - **`O`**: Stands for \"Outside\" any named entity. It's used for tokens that are not part of any named entity.\n",
    "   - **`B-`** and **`I-`** prefixes: These are used in the BIO tagging scheme. `B-` indicates the beginning of a named entity, and `I-` indicates that the token is inside a named entity. These prefixes are followed by the type of entity.\n",
    "      - **`PER`**: Person. For names of people.\n",
    "      - **`ORG`**: Organization. For names of companies, governmental organizations, etc.\n",
    "      - **`LOC`**: Location. For names of geographical locations, like cities, countries, rivers, etc.\n",
    "      - **`MISC`**: Miscellaneous. For named entities that don't fall into the above categories, like events, nationalities, languages, etc.\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "1. **Sentence**: \"Steve Jobs co-founded Apple in Cupertino.\"\n",
    "   - **`Steve`**: B-PER (Beginning of a Person entity)\n",
    "   - **`Jobs`**: I-PER (Inside a Person entity)\n",
    "   - **`co-founded`**: O (Outside any entity)\n",
    "   - **`Apple`**: B-ORG (Beginning of an Organization entity)\n",
    "   - **`in`**: O (Outside any entity)\n",
    "   - **`Cupertino`**: B-LOC (Beginning of a Location entity)\n",
    "\n",
    "2. **Sentence**: \"The United Nations was established after World War II.\"\n",
    "   - **`The`**: O\n",
    "   - **`United`**: B-ORG\n",
    "   - **`Nations`**: I-ORG\n",
    "   - **`was`**: O\n",
    "   - **`established`**: O\n",
    "   - **`after`**: O\n",
    "   - **`World`**: B-MISC (Beginning of a Miscellaneous entity)\n",
    "   - **`War`**: I-MISC\n",
    "   - **`II`**: I-MISC\n",
    "\n",
    "In these examples, each word in the sentence is labeled according to whether it is part of a named entity and what type of entity it is. This labeling is essential for extracting structured information from unstructured text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_with_labels(dataset: datasets.arrow_dataset.Dataset, i: int):\n",
    "    words = dataset[i][\"tokens\"]\n",
    "    pos_labels = dataset[i][\"pos_tags\"]\n",
    "    chunk_labels = dataset[i][\"chunk_tags\"]\n",
    "    ner_labels = dataset[i][\"ner_tags\"]\n",
    "    pos_label_names = dataset.features[\"pos_tags\"].feature.names\n",
    "    chunk_label_names = dataset.features[\"chunk_tags\"].feature.names\n",
    "    ner_label_names = dataset.features[\"ner_tags\"].feature.names\n",
    "\n",
    "    line1, line2, line3, line4 = \"\", \"\", \"\", \"\"\n",
    "    for word, pos, chunk, ner in zip(\n",
    "        words, pos_labels, chunk_labels, ner_labels\n",
    "    ):\n",
    "        # Assuming pos_label_names is a list of POS tag names\n",
    "        full_pos = pos_label_names[pos]\n",
    "        # Assuming chunk_label_names is a list of chunk tag names\n",
    "        full_chunk = chunk_label_names[chunk]\n",
    "        # Assuming ner_label_names is a list of NER tag names\n",
    "        full_ner = ner_label_names[ner]\n",
    "\n",
    "        max_length = max(\n",
    "            len(word), len(full_pos), len(full_chunk), len(full_ner)\n",
    "        )\n",
    "        space_padding = max_length - len(word) + 1\n",
    "\n",
    "        line1 += word + \" \" * space_padding\n",
    "        line2 += full_pos + \" \" * (max_length - len(full_pos) + 1)\n",
    "        line3 += full_chunk + \" \" * (max_length - len(full_chunk) + 1)\n",
    "        line4 += full_ner + \" \" * (max_length - len(full_ner) + 1)\n",
    "\n",
    "    return line1, line2, line3, line4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('EU    rejects German call to   boycott British lamb . ',\n",
       " 'NNP   VBZ     JJ     NN   TO   VB      JJ      NN   . ',\n",
       " 'B-NP  B-VP    B-NP   I-NP B-VP I-VP    B-NP    I-NP O ',\n",
       " 'B-ORG O       B-MISC O    O    O       B-MISC  O    O ')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_with_labels(raw_datasets[\"train\"], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\" We   do   n\\'t  support any  such recommendation because we   do   n\\'t  see  any  grounds for  it   , \" the  Commission \\'s   chief spokesman Nikolaus van   der   Pas   told a    news briefing . ',\n",
       " '\" PRP  VBP  RB   VB      DT   JJ   NN             IN      PRP  VBP  RB   VB   DT   NNS     IN   PRP  , \" DT   NNP        POS  JJ    NN        NNP      NNP   FW    NNP   VBD  DT   NN   NN       . ',\n",
       " 'O B-NP B-VP I-VP I-VP    B-NP I-NP I-NP           B-SBAR  B-NP B-VP I-VP I-VP B-NP I-NP    B-PP B-NP O O B-NP I-NP       B-NP I-NP  I-NP      I-NP     I-NP  I-NP  I-NP  B-VP B-NP I-NP I-NP     O ',\n",
       " 'O O    O    O    O       O    O    O              O       O    O    O    O    O    O       O    O    O O O    B-ORG      O    O     O         B-PER    I-PER I-PER I-PER O    O    O    O        O ')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_with_labels(raw_datasets[\"train\"], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True\n",
    ")\n",
    "print(inputs.tokens())\n",
    "print(inputs.word_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.Processing the dataset according to the tokenizer shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that after appling the tokenizer, our labels do not match to the tokens, as a rule the number of tokens more than the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 9 9 {'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(inputs.tokens()),\n",
    "    len(raw_datasets[\"train\"][0][\"tokens\"]),\n",
    "    len(raw_datasets[\"train\"][0][\"ner_tags\"]),\n",
    "    inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to align labels with tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(inputs.tokens())=12 len(raw_datasets[\"train\"][0][\"tokens\"])=9 len(raw_datasets[\"train\"][0][\"ner_tags\"])=9\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"{len(inputs.tokens())=}\",\n",
    "    f'{len(raw_datasets[\"train\"][0][\"tokens\"])=}',\n",
    "    f'{len(raw_datasets[\"train\"][0][\"ner_tags\"])=}',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.tokens()=['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']\n",
      " raw_datasets[\"train\"][0][\"tokens\"]=['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      " raw_datasets[\"train\"][0][\"ner_tags\"]=[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"{inputs.tokens()=}\\n\",\n",
    "    f'{raw_datasets[\"train\"][0][\"tokens\"]=}\\n',\n",
    "    f'{raw_datasets[\"train\"][0][\"ner_tags\"]=}\\n',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(inputs.tokens())=12\n",
      " len(alligned_labels)=12\n",
      " alligned_labels=[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True\n",
    ")\n",
    "\n",
    "alligned_labels = align_labels_with_tokens(\n",
    "    raw_datasets[\"train\"][0][\"ner_tags\"], inputs.word_ids()\n",
    ")\n",
    "print(\n",
    "    f\"{len(inputs.tokens())=}\\n\",\n",
    "    f\"{len(alligned_labels)=}\\n\",\n",
    "    f\"{alligned_labels=}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to tokenize and allign labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(\n",
    "    dataset: datasets.arrow_dataset.Dataset,\n",
    "    token_name: str = \"tokens\",\n",
    "    name_tags: str = \"ner_tags\",\n",
    "):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        dataset[token_name], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = dataset[name_tags]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34a8a5405774bffb5c8d19911b1bf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958b04f44f3f48ebafd8deeb8eaf0212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2fa84f3ed64efe9fc972da3f64df67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102],\n",
       " [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][\"input_ids\"][0], tokenized_datasets[\"train\"][\n",
    "    \"labels\"\n",
    "][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.Datacollector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datacollector make alignment according to the longest string in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n",
      "[-100, 1, 2, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.Metrcis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: A library for easily evaluating machine learning models and datasets. \n",
    "- [github for eveluate](https://github.com/huggingface/evaluate)\n",
    "- [hugging_face_page for eveluate](https://huggingface.co/evaluate-metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e45d9fa18f48d0b0565f421fcdb7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric does not behave like the standard accuracy: it will actually take the lists of labels as strings, not integers, so we will need to fully decode the predictions and labels before passing them to the metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels=['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "predictions=['B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 2},\n",
       " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.6666666666666666,\n",
       " 'overall_f1': 0.8,\n",
       " 'overall_accuracy': 0.8888888888888888}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the list of named entity recognition (NER) tag names from the dataset's features\n",
    "label_names = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "# Extract the NER labels for the first entry in the training dataset\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "\n",
    "# Convert numeric NER labels to their corresponding string representations\n",
    "labels = [label_names[i] for i in labels]\n",
    "\n",
    "# Copy the true labels to create a set of 'predicted' labels (for demonstration)\n",
    "predictions = labels.copy()\n",
    "\n",
    "# Modify one of the predicted labels to simulate a prediction error\n",
    "predictions[2] = \"O\"\n",
    "\n",
    "# Print the true labels and the modified predictions\n",
    "print(f\"{labels=}\")\n",
    "print(f\"{predictions=}\")\n",
    "\n",
    "# Compute a metric (like accuracy, F1-score, etc.) by comparing predictions with true labels\n",
    "# The 'metric' object should be previously defined or imported from an appropriate library\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define compute metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    eval_preds: Tuple[np.ndarray, np.ndarray]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute precision, recall, F1 score, and accuracy for NER predictions.\n",
    "\n",
    "    This function processes the output of a model's predictions and the true labels,\n",
    "    computes the NER metrics using a predefined metrics object, and returns the\n",
    "    calculated precision, recall, F1 score, and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    eval_preds (Tuple[np.ndarray, np.ndarray]): A tuple containing two elements:\n",
    "        - logits: A numpy array of model logits (predictions before applying activation function).\n",
    "        - labels: A numpy array of true labels.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, float]: A dictionary containing the computed metrics:\n",
    "        - 'precision': The overall precision of the model.\n",
    "        - 'recall': The overall recall of the model.\n",
    "        - 'f1': The overall F1 score of the model.\n",
    "        - 'accuracy': The overall accuracy of the model.\n",
    "\n",
    "    Note:\n",
    "    The function assumes the existence of a global 'metric' object for computing NER metrics\n",
    "    and a 'label_names' list mapping label indices to their string representations.\n",
    "    It also assumes that '-100' is used as the label for special tokens that should be ignored\n",
    "    in the evaluation.\n",
    "\n",
    "    Example:\n",
    "    >>> eval_preds = (model_logits, true_labels)\n",
    "    >>> metrics = compute_metrics(eval_preds)\n",
    "    >>> print(metrics)\n",
    "    \"\"\"\n",
    "\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [\n",
    "        [label_names[l] for l in label if l != -100] for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(\n",
    "        predictions=true_predictions, references=true_labels\n",
    "    )\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7.Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working on a token classification problem, we will use the AutoModelForTokenClassification class. The main thing to remember when defining this model is to pass along some information on the number of labels we have. The easiest way to do this is to pass that number with the num_labels argument, but if we want a nice inference widget working like the one we saw at the beginning of this section, it’s better to set the correct label correspondences instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label={0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}\n",
      "label2id={'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n"
     ]
    }
   ],
   "source": [
    "label_names = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(f\"{id2label=}\")\n",
    "print(f\"{label2id=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb7b704305943b481a37678754c42c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\toy\\skills-HuggingFace\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\baksa\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8.Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- login in hugging face account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f14d36c11b24c11a83f27185c6a2742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set arguments for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create Trainer and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9074b5da4bfe461480b25efbdc01e17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2635, 'learning_rate': 1.810174639331815e-05, 'epoch': 0.28}\n",
      "{'loss': 0.104, 'learning_rate': 1.6203492786636296e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0762, 'learning_rate': 1.4305239179954442e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5192c0a1e694ee1beb54a0ff6e79b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06335246562957764, 'eval_precision': 0.904916965157929, 'eval_recall': 0.9353752945136318, 'eval_f1': 0.9198940748096657, 'eval_accuracy': 0.9822658503561547, 'eval_runtime': 3.2464, 'eval_samples_per_second': 1001.096, 'eval_steps_per_second': 125.368, 'epoch': 1.0}\n",
      "{'loss': 0.0626, 'learning_rate': 1.240698557327259e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0419, 'learning_rate': 1.0508731966590738e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0405, 'learning_rate': 8.610478359908885e-06, 'epoch': 1.71}\n",
      "{'loss': 0.0361, 'learning_rate': 6.712224753227031e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4327507fee4756934290ae22029b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06697254627943039, 'eval_precision': 0.9293379560838699, 'eval_recall': 0.9473241332884551, 'eval_f1': 0.9382448537378115, 'eval_accuracy': 0.9851945605463001, 'eval_runtime': 3.2, 'eval_samples_per_second': 1015.635, 'eval_steps_per_second': 127.189, 'epoch': 2.0}\n",
      "{'loss': 0.0233, 'learning_rate': 4.8139711465451785e-06, 'epoch': 2.28}\n",
      "{'loss': 0.0202, 'learning_rate': 2.9157175398633257e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0243, 'learning_rate': 1.0174639331814731e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a07d33f6534ed3979273056c707e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06140302121639252, 'eval_precision': 0.9370444002650762, 'eval_recall': 0.9518680578929654, 'eval_f1': 0.9443980631157121, 'eval_accuracy': 0.9861658915641373, 'eval_runtime': 3.2968, 'eval_samples_per_second': 985.791, 'eval_steps_per_second': 123.451, 'epoch': 3.0}\n",
      "{'train_runtime': 181.1464, 'train_samples_per_second': 232.536, 'train_steps_per_second': 29.081, 'train_loss': 0.06650318163493048, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.06650318163493048, metrics={'train_runtime': 181.1464, 'train_samples_per_second': 232.536, 'train_steps_per_second': 29.081, 'train_loss': 0.06650318163493048, 'epoch': 3.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- push to the hugging face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab58264f68a548738c2d3a6132a00d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ilbaks/bert-finetuned-ner/commit/88131286c1bd2c81a2d8df054f069e3fb1ee6c7b', commit_message='Training complete', commit_description='', oid='88131286c1bd2c81a2d8df054f069e3fb1ee6c7b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9.A custom training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.1.Build dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    dataset=tokenized_datasets[\"validation\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.2.Reinstantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.3.Initialize optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.4.Accelerate prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.5.Set learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.6.Set a repository object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ilbaks/bert-finetuned-ner-accelerate'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - clone that repository in a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/ilbaks/bert-finetuned-ner-accelerate into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"bert-finetuned-ner-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.10.Define postprocess function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- postprocess() function that takes predictions and labels and converts them to lists of strings, like our metric object expects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [\n",
    "        [label_names[l] for l in label if l != -100] for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.11.Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The training in itself, which is the classic iteration over the train_dataloader, forward pass through the model, then backward pass and optimizer step.\n",
    "\n",
    "- The evaluation, in which there is a novelty after getting the outputs of our model on a batch: since two processes may have padded the inputs and labels to different shapes, we need to use accelerator.pad_across_processes() to make the predictions and labels the same shape before calling the gather() method. If we don’t do this, the evaluation will either error out or hang forever. Then we send the results to metric.add_batch() and call metric.compute() once the evaluation loop is over.\n",
    "\n",
    "- Saving and uploading, where we first save the model and the tokenizer, then call repo.push_to_hub(). Notice that we use the argument blocking=False to tell the 🤗 Hub library to push in an asynchronous process. This way, training continues normally and this (long) instruction is executed in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e786079c164a8e99bf9010c550a941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.9400875126220128, 'recall': 0.9231531978185424, 'f1': 0.9315434003168516, 'accuracy': 0.9840024724789544}\n",
      "epoch 1: {'precision': 0.947997307303938, 'recall': 0.9286185295087372, 'f1': 0.9382078614257161, 'accuracy': 0.9861806087007712}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'precision': 0.947997307303938, 'recall': 0.9286185295087372, 'f1': 0.9382078614257161, 'accuracy': 0.9861806087007712}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "# Initialize a progress bar for tracking training steps\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# Loop over each epoch (one pass over the entire dataset)\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training Phase\n",
    "    model.train()  # Set the model to training mode (enables dropout, batch normalization etc.)\n",
    "\n",
    "    # Loop over each batch in the training data loader\n",
    "    for batch in train_dataloader:\n",
    "        # Forward pass: compute outputs by passing the batch through the model\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss  # Extract the loss from the model's outputs\n",
    "\n",
    "        # Perform backpropagation to compute gradients\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()  # Update model parameters based on gradients\n",
    "        lr_scheduler.step()  # Update learning rate\n",
    "        optimizer.zero_grad()  # Reset gradients to zero for the next iteration\n",
    "        progress_bar.update(1)  # Update the progress bar\n",
    "\n",
    "    # Evaluation Phase\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, batch normalization etc.)\n",
    "\n",
    "    # Loop over each batch in the evaluation data loader\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():  # Disable gradient calculations for efficiency\n",
    "            outputs = model(**batch)  # Forward pass: compute outputs\n",
    "\n",
    "        # Post-process the outputs to extract predictions and labels\n",
    "        # Get the predicted labels (class with highest logit)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Pad predictions and labels for consistent shape across all distributed processes\n",
    "        predictions = accelerator.pad_across_processes(\n",
    "            predictions, dim=1, pad_index=-100\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(\n",
    "            labels, dim=1, pad_index=-100\n",
    "        )\n",
    "\n",
    "        # Gather predictions and labels from all processes\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        # Further processing of gathered predictions and labels (e.g., removing padding)\n",
    "        true_predictions, true_labels = postprocess(\n",
    "            predictions_gathered, labels_gathered\n",
    "        )\n",
    "\n",
    "        # Add the results of this batch to the metric for later calculation\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Compute and print the evaluation metrics\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            # Print precision, recall, F1 score, and accuracy for the epoch\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save the model and tokenizer, and upload to a model hub if needed\n",
    "    accelerator.wait_for_everyone()  # Ensure all processes are synchronized\n",
    "    unwrapped_model = accelerator.unwrap_model(\n",
    "        model\n",
    "    )  # Unwrap the model from the accelerator\n",
    "    unwrapped_model.save_pretrained(\n",
    "        output_dir, save_function=accelerator.save\n",
    "    )  # Save the model\n",
    "    if (\n",
    "        accelerator.is_main_process\n",
    "    ):  # Check if it's the main process to avoid redundant saves/uploads\n",
    "        tokenizer.save_pretrained(output_dir)  # Save the tokenizer\n",
    "        repo.push_to_hub(\n",
    "            # Push model to the hub\n",
    "            commit_message=f\"Training in progress epoch {epoch}\",\n",
    "            blocking=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10.Using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa2365311f747c9bd74ff4f1866010a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b96a956b2054d22bcb8ae44a37b888e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9264e416f94a389c6217387d9db724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ca92a79bdd43fd9d0e8ed7866399c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4045bd2956c54979991f84a22b44619c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9988506,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9647624,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9986118,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "MODEL_CHECKPOINT_FINE_TUNED = \"huggingface-course/bert-finetuned-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=MODEL_CHECKPOINT_FINE_TUNED,\n",
    "    aggregation_strategy=\"simple\",\n",
    ")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
