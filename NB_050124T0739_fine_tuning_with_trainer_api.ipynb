{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB_050124T0739_fine_tuning_with_trainer_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine-tuning model \"bert-base-uncased\" on  dataset  \"glue mrpc\" using trainer API in hugging face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Trainer class we must define:\n",
    "\n",
    "    - tokenizer\n",
    "    - datacollector\n",
    "    - model\n",
    "    - training_args\n",
    "    - compute_metrics\n",
    "    - train_dataset\n",
    "    - eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "     -------------------------------------- 507.1/507.1 KB 1.9 MB/s eta 0:00:00\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 84.1/84.1 KB 2.4 MB/s eta 0:00:00\n",
      "Collecting transformers[sentencepiece]\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "     ---------------------------------------- 8.2/8.2 MB 2.8 MB/s eta 0:00:00\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.9.1-cp39-cp39-win_amd64.whl (365 kB)\n",
      "     -------------------------------------- 365.5/365.5 KB 3.8 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.8,>=0.3.0\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "     -------------------------------------- 115.3/115.3 KB 3.4 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "     -------------------------------------- 166.4/166.4 KB 2.5 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.3/78.3 KB 2.2 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-14.0.2-cp39-cp39-win_amd64.whl (24.6 MB)\n",
      "     ---------------------------------------- 24.6/24.6 MB 2.1 MB/s eta 0:00:00\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.3/133.3 KB 1.6 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.26.3-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "     ---------------------------------------- 15.8/15.8 MB 1.5 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.19.4\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl (330 kB)\n",
      "     -------------------------------------- 330.1/330.1 KB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from datasets) (23.2)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "     -------------------------------------- 152.8/152.8 KB 1.3 MB/s eta 0:00:00\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting requests>=2.19.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.6/62.6 KB 1.1 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.1.4-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "     ---------------------------------------- 10.8/10.8 MB 1.6 MB/s eta 0:00:00\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "     ------------------------------------ 277.8/277.8 KB 952.3 kB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.12.25-cp39-cp39-win_amd64.whl (269 kB)\n",
      "     ------------------------------------ 269.5/269.5 KB 974.8 kB/s eta 0:00:00\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 1.2 MB/s eta 0:00:00\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "     -------------------------------------- 977.6/977.6 KB 1.5 MB/s eta 0:00:00\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "     -------------------------------------- 413.4/413.4 KB 1.6 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-win_amd64.whl (50 kB)\n",
      "     ---------------------------------------- 50.7/50.7 KB ? eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp39-cp39-win_amd64.whl (76 kB)\n",
      "     ---------------------------------------- 76.9/76.9 KB 2.2 MB/s eta 0:00:00\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "     ---------------------------------------- 60.8/60.8 KB 1.6 MB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
      "     -------------------------------------- 100.4/100.4 KB 1.5 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "     ------------------------------------ 162.5/162.5 KB 750.2 kB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "     -------------------------------------- 61.6/61.6 KB 658.0 kB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "     ------------------------------------ 104.6/104.6 KB 754.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "     ------------------------------------ 502.5/502.5 KB 716.5 kB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "     ------------------------------------ 346.6/346.6 KB 597.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: sentencepiece, pytz, xxhash, urllib3, tzdata, tqdm, safetensors, regex, pyyaml, pyarrow-hotfix, protobuf, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, pyarrow, pandas, multiprocess, aiosignal, responses, huggingface-hub, aiohttp, tokenizers, transformers, datasets, evaluate\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 certifi-2023.11.17 charset-normalizer-3.3.2 datasets-2.16.1 dill-0.3.7 evaluate-0.4.1 filelock-3.13.1 frozenlist-1.4.1 fsspec-2023.10.0 huggingface-hub-0.20.1 idna-3.6 multidict-6.0.4 multiprocess-0.70.15 numpy-1.26.3 pandas-2.1.4 protobuf-4.25.1 pyarrow-14.0.2 pyarrow-hotfix-0.6 pytz-2023.3.post1 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 responses-0.18.0 safetensors-0.4.1 sentencepiece-0.1.99 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.36.2 tzdata-2023.4 urllib3-2.1.0 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'D:\\Projects\\toy\\skills-HuggingFace\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#!pip install datasets evaluate transformers[sentencepiece] install transformers[torch] scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\toy\\skills-HuggingFace\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.Initializing raw dataset and checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Loading the MRPC (Microsoft Research Paraphrase Corpus) dataset from the GLUE benchmark.\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"  # model name in huggingFace hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Loading the tokenizer corresponding to the 'bert-base-uncased' checkpoint.\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Datacollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Creating a data collator that will dynamically pad the inputs received, to the maximum length in a batch.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test-trainer\", evaluation_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.Train and evalution datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3668/3668 [00:00<00:00, 18095.89 examples/s]\n",
      "Map: 100%|██████████| 408/408 [00:00<00:00, 21248.85 examples/s]\n",
      "Map: 100%|██████████| 1725/1725 [00:00<00:00, 31090.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Defining a function to tokenize a pair of sentences.\n",
    "def tokenize_function(example):\n",
    "    # Tokenizes a pair of sentences and ensures truncation to the maximum length the model can handle.\n",
    "    return tokenizer(\n",
    "        example[\"sentence1\"], example[\"sentence2\"], truncation=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Applying the tokenize_function to all examples in the dataset using map, processing in batches for efficiency.\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.Fine-tune with Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1377 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "                                                  \n",
      " 34%|███▎      | 464/1377 [00:17<01:35,  9.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41969034075737, 'eval_accuracy': 0.8284313725490197, 'eval_f1': 0.8841059602649006, 'eval_runtime': 1.3522, 'eval_samples_per_second': 301.723, 'eval_steps_per_second': 37.715, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 500/1377 [00:18<00:30, 28.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5557, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 67%|██████▋   | 924/1377 [00:33<00:45,  9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45415228605270386, 'eval_accuracy': 0.8676470588235294, 'eval_f1': 0.9052631578947367, 'eval_runtime': 1.3194, 'eval_samples_per_second': 309.224, 'eval_steps_per_second': 38.653, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1000/1377 [00:36<00:11, 31.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3358, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1375/1377 [00:49<00:00, 29.46it/s]\n",
      "100%|██████████| 1377/1377 [00:50<00:00, 27.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5870423913002014, 'eval_accuracy': 0.8578431372549019, 'eval_f1': 0.8993055555555555, 'eval_runtime': 1.3582, 'eval_samples_per_second': 300.405, 'eval_steps_per_second': 37.551, 'epoch': 3.0}\n",
      "{'train_runtime': 50.9817, 'train_samples_per_second': 215.842, 'train_steps_per_second': 27.01, 'train_loss': 0.37521212154657146, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.37521212154657146, metrics={'train_runtime': 50.9817, 'train_samples_per_second': 215.842, 'train_steps_per_second': 27.01, 'train_loss': 0.37521212154657146, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
