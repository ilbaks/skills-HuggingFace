{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB_080124T0722_create_my_own_dataset_semantic_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create my own dataset [link](https://huggingface.co/learn/nlp-course/chapter5/5#creating-your-own-dataset)\n",
    "- semantic search [link]https://huggingface.co/learn/nlp-course/chapter5/6#semantic-search-with-faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the dataset that you need to build an NLP application doesn’t exist, so you’ll need to create it yourself. In this section we’ll show you how to create a corpus of GitHub issues, which are commonly used to track bugs or features in GitHub repositories. This corpus could be used for various purposes, including:\n",
    "- Exploring how long it takes to close open issues or pull requests\n",
    "- Training a multilabel classifier that can tag issues with metadata based on the issue’s description (e.g., “bug,” “enhancement,” or “question”)\n",
    "- Creating a semantic search engine to find which issues match a user’s query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - worworking with dataset\n",
    "      - fetch issues\n",
    "      - load locally as HF dataset\n",
    "      - cleaning up the data\n",
    "      - augmenting the dataset\n",
    "      - uploading the dataset to the HF hub\n",
    "      - creating a dataset card \n",
    "    - semantic search\n",
    "      - loading and prepating dataset\n",
    "        - load\n",
    "        - filter only for issues not pull request\n",
    "        - remain only the requires columns\n",
    "        - convert to pandas and explode\n",
    "        - convert back to dataset HF\n",
    "        - filter comments longer 15 sympbols length\n",
    "        - concatenate  issue title, description, and comments together\n",
    "      - creating text embeddings\n",
    "        - download model for tokenization\n",
    "        - Create embedding  using CLSpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipywidgets) (5.14.1)\n",
      "Collecting widgetsnbextension~=4.0.9 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.9-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.9 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: colorama in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.1-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.4 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 30.7/139.4 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 112.6/139.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 139.4/139.4 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading jupyterlab_widgets-3.0.9-py3-none-any.whl (214 kB)\n",
      "   ---------------------------------------- 0.0/214.9 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 174.1/214.9 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 214.9/214.9 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading widgetsnbextension-4.0.9-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.2/2.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.3 MB 5.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.9/2.3 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.3 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.2/2.3 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.3 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.3 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.3 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.9/2.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.2/2.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 3.7 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.1 jupyterlab-widgets-3.0.9 widgetsnbextension-4.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter\n",
      "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: ipywidgets in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (8.1.1)\n",
      "Collecting notebook (from jupyter)\n",
      "  Downloading notebook-7.0.6-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting qtconsole (from jupyter)\n",
      "  Downloading qtconsole-5.5.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jupyter-console (from jupyter)\n",
      "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Collecting nbconvert (from jupyter)\n",
      "  Downloading nbconvert-7.14.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: ipykernel in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from jupyter) (6.28.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: decorator in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: colorama in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipykernel->jupyter) (1.8.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipykernel->jupyter) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipykernel->jupyter) (5.7.0)\n",
      "Requirement already satisfied: nest-asyncio in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipykernel->jupyter) (1.5.8)\n",
      "Requirement already satisfied: packaging in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipykernel->jupyter) (23.2)\n",
      "Requirement already satisfied: psutil in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipykernel->jupyter) (5.9.7)\n",
      "Requirement already satisfied: pyzmq>=24 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipykernel->jupyter) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from ipykernel->jupyter) (6.4)\n",
      "Collecting beautifulsoup4 (from nbconvert->jupyter)\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting bleach!=5.0.0 (from nbconvert->jupyter)\n",
      "  Downloading bleach-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert->jupyter)\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from nbconvert->jupyter) (7.0.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from nbconvert->jupyter) (3.1.2)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter)\n",
      "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: markupsafe>=2.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from nbconvert->jupyter) (2.1.3)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter)\n",
      "  Downloading mistune-3.0.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert->jupyter)\n",
      "  Downloading nbclient-0.9.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting nbformat>=5.7 (from nbconvert->jupyter)\n",
      "  Downloading nbformat-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter)\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting tinycss2 (from nbconvert->jupyter)\n",
      "  Using cached tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from notebook->jupyter)\n",
      "  Downloading jupyter_server-2.12.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting jupyterlab-server<3,>=2.22.1 (from notebook->jupyter)\n",
      "  Downloading jupyterlab_server-2.25.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jupyterlab<5,>=4.0.2 (from notebook->jupyter)\n",
      "  Downloading jupyterlab-4.0.10-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting notebook-shim<0.3,>=0.2 (from notebook->jupyter)\n",
      "  Downloading notebook_shim-0.2.3-py3-none-any.whl (13 kB)\n",
      "Collecting qtpy>=2.4.0 (from qtconsole->jupyter)\n",
      "  Downloading QtPy-2.4.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.16.0)\n",
      "Collecting webencodings (from bleach!=5.0.0->nbconvert->jupyter)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from importlib-metadata>=3.6->nbconvert->jupyter) (3.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.1.0)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (306)\n",
      "Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting argon2-cffi (from jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jupyter-events>=0.9.0 (from jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading jupyter_events-0.9.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jupyter-server-terminals (from jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading jupyter_server_terminals-0.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting overrides (from jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading overrides-7.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting prometheus-client (from jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading prometheus_client-0.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pywinpty (from jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading pywinpty-2.0.12-cp39-none-win_amd64.whl.metadata (5.2 kB)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading terminado-0.18.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting websocket-client (from jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading websocket_client-1.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab<5,>=4.0.2->notebook->jupyter)\n",
      "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab<5,>=4.0.2->notebook->jupyter)\n",
      "  Downloading jupyter_lsp-2.2.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tomli (from jupyterlab<5,>=4.0.2->notebook->jupyter)\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.22.1->notebook->jupyter)\n",
      "  Downloading Babel-2.14.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.22.1->notebook->jupyter)\n",
      "  Downloading json5-0.9.14-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.22.1->notebook->jupyter)\n",
      "  Downloading jsonschema-4.20.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: requests>=2.31 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2.31.0)\n",
      "Collecting fastjsonschema (from nbformat>=5.7->nbconvert->jupyter)\n",
      "  Downloading fastjsonschema-2.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: wcwidth in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (3.6)\n",
      "Collecting sniffio>=1.1 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (23.2.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter)\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter)\n",
      "  Downloading referencing-0.32.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook->jupyter)\n",
      "  Downloading rpds_py-0.16.2-cp39-none-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter) (6.0.1)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook->jupyter) (2023.11.17)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=1.11 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading webcolors-1.13-py3-none-any.whl (14 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading cffi-1.16.0-cp39-cp39-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "     ---------------------------------------- 0.0/118.7 kB ? eta -:--:--\n",
      "     -------------------- ------------------ 61.4/118.7 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 118.7/118.7 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter)\n",
      "  Downloading types_python_dateutil-2.8.19.20240106-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading nbconvert-7.14.0-py3-none-any.whl (256 kB)\n",
      "   ---------------------------------------- 0.0/256.4 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 163.8/256.4 kB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 163.8/256.4 kB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 245.8/256.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 256.4/256.4 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading notebook-7.0.6-py3-none-any.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/4.0 MB 3.5 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.3/4.0 MB 3.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.4/4.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.6/4.0 MB 2.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.7/4.0 MB 3.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.9/4.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.0/4.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.1/4.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.3/4.0 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.4/4.0 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.5/4.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.7/4.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.9/4.0 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.0/4.0 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.1/4.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.3/4.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.3/4.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.3/4.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.6/4.0 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.7/4.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.8/4.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.9/4.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.0/4.0 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.1/4.0 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.2/4.0 MB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.3/4.0 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.4/4.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.5/4.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.6/4.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.7/4.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.8/4.0 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.9/4.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.0/4.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading qtconsole-5.5.1-py3-none-any.whl (123 kB)\n",
      "   ---------------------------------------- 0.0/123.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 123.4/123.4 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading bleach-6.1.0-py3-none-any.whl (162 kB)\n",
      "   ---------------------------------------- 0.0/162.8 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 112.6/162.8 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 162.8/162.8 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading jupyter_server-2.12.2-py3-none-any.whl (380 kB)\n",
      "   ---------------------------------------- 0.0/380.3 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 112.6/380.3 kB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 225.3/380.3 kB 2.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 317.4/380.3 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 380.3/380.3 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading jupyterlab-4.0.10-py3-none-any.whl (9.2 MB)\n",
      "   ---------------------------------------- 0.0/9.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.2 MB 3.3 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/9.2 MB 2.0 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.3/9.2 MB 2.2 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.4/9.2 MB 2.1 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.5/9.2 MB 2.4 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/9.2 MB 2.3 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.7/9.2 MB 2.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.8/9.2 MB 2.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/9.2 MB 2.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.0/9.2 MB 2.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.1/9.2 MB 2.3 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.2/9.2 MB 2.2 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.3/9.2 MB 2.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.4/9.2 MB 2.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.5/9.2 MB 2.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.6/9.2 MB 2.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/9.2 MB 2.1 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/9.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.7/9.2 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/9.2 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.9/9.2 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.0/9.2 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.1/9.2 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.1/9.2 MB 1.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.2/9.2 MB 1.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.2/9.2 MB 1.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.3/9.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.4/9.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.5/9.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.5/9.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.6/9.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.7/9.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.8/9.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.9/9.2 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.0/9.2 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.1/9.2 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.2/9.2 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.3/9.2 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.4/9.2 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.4/9.2 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.4/9.2 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.4/9.2 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.4/9.2 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.5/9.2 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.6/9.2 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 3.6/9.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.7/9.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.8/9.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 3.8/9.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.9/9.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.0/9.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.1/9.2 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 4.1/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.2/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.3/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.4/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.4/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.5/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.6/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.7/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.8/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.8/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.9/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 5.0/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.1/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.1/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.2/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 5.3/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 5.4/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 5.5/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 5.5/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 5.6/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 5.7/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.8/9.2 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 5.9/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.9/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.0/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.1/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.2/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.3/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.3/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.5/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.6/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.6/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.7/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.8/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.9/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.0/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.1/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.1/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.2/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.3/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 7.4/9.2 MB 1.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 7.5/9.2 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.6/9.2 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.7/9.2 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.8/9.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.9/9.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.0/9.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.1/9.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.2/9.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.3/9.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.4/9.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.4/9.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.6/9.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.6/9.2 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.7/9.2 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.8/9.2 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.9/9.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.0/9.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.2 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.2/9.2 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading jupyterlab_server-2.25.2-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.9/58.9 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/48.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 48.0/48.0 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading nbclient-0.9.0-py3-none-any.whl (24 kB)\n",
      "Downloading nbformat-5.9.2-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.6/77.6 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading QtPy-2.4.1-py3-none-any.whl (93 kB)\n",
      "   ---------------------------------------- 0.0/93.5 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 41.0/93.5 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 61.4/93.5 kB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 93.5/93.5 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 85.5/85.5 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
      "Downloading Babel-2.14.0-py3-none-any.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.0 MB 1.7 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.2/11.0 MB 1.7 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.2/11.0 MB 1.7 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.3/11.0 MB 1.7 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.4/11.0 MB 1.7 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.5/11.0 MB 1.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/11.0 MB 1.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.7/11.0 MB 1.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.8/11.0 MB 1.8 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/11.0 MB 1.7 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.9/11.0 MB 1.7 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.0/11.0 MB 1.7 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.1/11.0 MB 1.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.1/11.0 MB 1.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.2/11.0 MB 1.7 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 1.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.4/11.0 MB 1.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.5/11.0 MB 1.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 1.8 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.7/11.0 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.8/11.0 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.9/11.0 MB 1.8 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.0/11.0 MB 1.8 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.1/11.0 MB 1.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.2/11.0 MB 1.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.3/11.0 MB 1.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.4/11.0 MB 1.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.5/11.0 MB 1.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.6/11.0 MB 1.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.7/11.0 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.8/11.0 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 1.9 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 1.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 1.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 1.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.2/11.0 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.3/11.0 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.4/11.0 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.5/11.0 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.5/11.0 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.6/11.0 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.7/11.0 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 3.7/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.8/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.9/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.0/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.0/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.1/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.2/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.3/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.4/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.6/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.6/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.8/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.9/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 4.9/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.0/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.1/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.2/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.3/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.3/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.4/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.5/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.6/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.7/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 5.7/11.0 MB 1.7 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 5.8/11.0 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 5.9/11.0 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.0/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.1/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.2/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.2/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.4/11.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.5/11.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 6.6/11.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.6/11.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.7/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 6.8/11.0 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 6.9/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.0/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.1/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.2/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.2/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.3/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.4/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 7.5/11.0 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 7.6/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.6/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.7/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.8/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.0/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.2/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.2/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.3/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.4/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.5/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.6/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.7/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.8/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.8/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.9/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.0/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.0/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.1/11.0 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.2/11.0 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.3/11.0 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.3/11.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.4/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.5/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.6/11.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.8/11.0 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.9/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.0/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.0/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.1/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.1/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.2/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.3/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.3/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.4/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.6/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.7/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.7/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.8/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.9/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.9/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading json5-0.9.14-py2.py3-none-any.whl (19 kB)\n",
      "Downloading jsonschema-4.20.0-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 30.7/84.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 84.7/84.7 kB 942.1 kB/s eta 0:00:00\n",
      "Downloading jupyter_events-0.9.0-py3-none-any.whl (18 kB)\n",
      "Downloading jupyter_lsp-2.2.1-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.0 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 41.0/66.0 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 66.0/66.0 kB 1.8 MB/s eta 0:00:00\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Downloading terminado-0.18.0-py3-none-any.whl (14 kB)\n",
      "Downloading pywinpty-2.0.12-cp39-none-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/1.4 MB 1.6 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.1/1.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.2/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.3/1.4 MB 1.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.3/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.5/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.6/1.4 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.6/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.4 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.8/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.4 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.0/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.1/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.1/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.2/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.3/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.3/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.3/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.4/1.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading fastjsonschema-2.19.1-py3-none-any.whl (23 kB)\n",
      "Downloading jupyter_server_terminals-0.5.1-py3-none-any.whl (13 kB)\n",
      "Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading prometheus_client-0.19.0-py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/54.2 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/54.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 54.2/54.2 kB 709.5 kB/s eta 0:00:00\n",
      "Downloading websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.5 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.2/58.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.5/58.5 kB 618.2 kB/s eta 0:00:00\n",
      "Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Downloading referencing-0.32.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.16.2-cp39-none-win_amd64.whl (195 kB)\n",
      "   ---------------------------------------- 0.0/195.7 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/195.7 kB ? eta -:--:--\n",
      "   -------- ------------------------------ 41.0/195.7 kB 495.5 kB/s eta 0:00:01\n",
      "   ---------------- ---------------------- 81.9/195.7 kB 657.6 kB/s eta 0:00:01\n",
      "   ------------------ -------------------- 92.2/195.7 kB 585.1 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 122.9/195.7 kB 602.4 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 163.8/195.7 kB 656.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 195.7/195.7 kB 659.9 kB/s eta 0:00:00\n",
      "Downloading cffi-1.16.0-cp39-cp39-win_amd64.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 30.7/181.6 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------- ------------------------- 61.4/181.6 kB 812.7 kB/s eta 0:00:01\n",
      "   ------------------- ------------------- 92.2/181.6 kB 744.7 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 122.9/181.6 kB 722.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 153.6/181.6 kB 706.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 181.6/181.6 kB 782.6 kB/s eta 0:00:00\n",
      "Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.4 kB ? eta -:--:--\n",
      "   ------------------ --------------------- 30.7/66.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 66.4/66.4 kB 891.7 kB/s eta 0:00:00\n",
      "Downloading types_python_dateutil-2.8.19.20240106-py3-none-any.whl (9.7 kB)\n",
      "Installing collected packages: webencodings, json5, fastjsonschema, websocket-client, webcolors, uri-template, types-python-dateutil, tomli, tinycss2, soupsieve, sniffio, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, qtpy, pywinpty, python-json-logger, pycparser, prometheus-client, pandocfilters, overrides, mistune, jupyterlab-pygments, jsonpointer, fqdn, defusedxml, bleach, babel, async-lru, terminado, referencing, cffi, beautifulsoup4, arrow, anyio, jupyter-server-terminals, jsonschema-specifications, isoduration, argon2-cffi-bindings, jsonschema, argon2-cffi, qtconsole, nbformat, jupyter-console, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter\n",
      "Successfully installed anyio-4.2.0 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 async-lru-2.0.4 babel-2.14.0 beautifulsoup4-4.12.2 bleach-6.1.0 cffi-1.16.0 defusedxml-0.7.1 fastjsonschema-2.19.1 fqdn-1.5.1 isoduration-20.11.0 json5-0.9.14 jsonpointer-2.4 jsonschema-4.20.0 jsonschema-specifications-2023.12.1 jupyter-1.0.0 jupyter-console-6.6.3 jupyter-events-0.9.0 jupyter-lsp-2.2.1 jupyter-server-2.12.2 jupyter-server-terminals-0.5.1 jupyterlab-4.0.10 jupyterlab-pygments-0.3.0 jupyterlab-server-2.25.2 mistune-3.0.2 nbclient-0.9.0 nbconvert-7.14.0 nbformat-5.9.2 notebook-7.0.6 notebook-shim-0.2.3 overrides-7.4.0 pandocfilters-1.5.0 prometheus-client-0.19.0 pycparser-2.21 python-json-logger-2.0.7 pywinpty-2.0.12 qtconsole-5.5.1 qtpy-2.4.1 referencing-0.32.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.16.2 send2trash-1.8.2 sniffio-1.3.0 soupsieve-2.5 terminado-0.18.0 tinycss2-1.2.1 tomli-2.0.1 types-python-dateutil-2.8.19.20240106 uri-template-1.3.0 webcolors-1.13 webencodings-0.5.1 websocket-client-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade jupyter ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in d:\\projects\\toy\\skills-huggingface\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\n",
      "ERROR: No matching distribution found for faiss\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0.Inititalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your GitHub token here\n",
    "GITHUB_TOKEN = \"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\"\n",
    "HEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.Fetch issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " requests.models.Response,\n",
       " [{'url': 'https://api.github.com/repos/huggingface/datasets/issues/6566',\n",
       "   'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
       "   'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/6566/labels{/name}',\n",
       "   'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/6566/comments',\n",
       "   'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/6566/events',\n",
       "   'html_url': 'https://github.com/huggingface/datasets/issues/6566',\n",
       "   'id': 2069495429,\n",
       "   'node_id': 'I_kwDODunzps57Wf6F',\n",
       "   'number': 6566,\n",
       "   'title': 'I train controlnet_sdxl in bf16 datatype, got unsupported ERROR in datasets',\n",
       "   'user': {'login': 'HelloWorldBeginner',\n",
       "    'id': 25008090,\n",
       "    'node_id': 'MDQ6VXNlcjI1MDA4MDkw',\n",
       "    'avatar_url': 'https://avatars.githubusercontent.com/u/25008090?v=4',\n",
       "    'gravatar_id': '',\n",
       "    'url': 'https://api.github.com/users/HelloWorldBeginner',\n",
       "    'html_url': 'https://github.com/HelloWorldBeginner',\n",
       "    'followers_url': 'https://api.github.com/users/HelloWorldBeginner/followers',\n",
       "    'following_url': 'https://api.github.com/users/HelloWorldBeginner/following{/other_user}',\n",
       "    'gists_url': 'https://api.github.com/users/HelloWorldBeginner/gists{/gist_id}',\n",
       "    'starred_url': 'https://api.github.com/users/HelloWorldBeginner/starred{/owner}{/repo}',\n",
       "    'subscriptions_url': 'https://api.github.com/users/HelloWorldBeginner/subscriptions',\n",
       "    'organizations_url': 'https://api.github.com/users/HelloWorldBeginner/orgs',\n",
       "    'repos_url': 'https://api.github.com/users/HelloWorldBeginner/repos',\n",
       "    'events_url': 'https://api.github.com/users/HelloWorldBeginner/events{/privacy}',\n",
       "    'received_events_url': 'https://api.github.com/users/HelloWorldBeginner/received_events',\n",
       "    'type': 'User',\n",
       "    'site_admin': False},\n",
       "   'labels': [],\n",
       "   'state': 'open',\n",
       "   'locked': False,\n",
       "   'assignee': None,\n",
       "   'assignees': [],\n",
       "   'milestone': None,\n",
       "   'comments': 0,\n",
       "   'created_at': '2024-01-08T02:37:03Z',\n",
       "   'updated_at': '2024-01-08T02:37:03Z',\n",
       "   'closed_at': None,\n",
       "   'author_association': 'NONE',\n",
       "   'active_lock_reason': None,\n",
       "   'body': '### Describe the bug\\n\\n```\\r\\nTraceback (most recent call last):\\r\\n  File \"train_controlnet_sdxl.py\", line 1252, in <module>\\r\\n    main(args)\\r\\n  File \"train_controlnet_sdxl.py\", line 1013, in main\\r\\n    train_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)\\r\\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 592, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 557, in wrapper\\r\\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\\r\\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3093, in map\\r\\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\\r\\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3489, in _map_single\\r\\n    writer.write_batch(batch)\\r\\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 557, in write_batch\\r\\n    arrays.append(pa.array(typed_sequence))\\r\\n  File \"pyarrow/array.pxi\", line 248, in pyarrow.lib.array\\r\\n  File \"pyarrow/array.pxi\", line 113, in pyarrow.lib._handle_arrow_array_protocol\\r\\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 191, in __arrow_array__\\r\\n    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\\r\\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/features/features.py\", line 447, in cast_to_python_objects\\r\\n    return _cast_to_python_objects(\\r\\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/features/features.py\", line 324, in _cast_to_python_objects\\r\\n    for x in obj.detach().cpu().numpy()\\r\\nTypeError: Got unsupported ScalarType BFloat16\\r\\n```\\n\\n### Steps to reproduce the bug\\n\\nHere is my train script I use BF16 type，I use diffusers train my model\\r\\n```\\r\\nexport MODEL_DIR=\"/home/mhh/sd_models/stable-diffusion-xl-base-1.0\"\\r\\nexport OUTPUT_DIR=\"./control_net\"\\r\\nexport VAE_NAME=\"/home/mhh/sd_models/sdxl-vae-fp16-fix\"\\r\\n\\r\\naccelerate launch train_controlnet_sdxl.py \\\\\\r\\n --pretrained_model_name_or_path=$MODEL_DIR \\\\\\r\\n --output_dir=$OUTPUT_DIR \\\\\\r\\n --pretrained_vae_model_name_or_path=$VAE_NAME \\\\\\r\\n --dataset_name=/home/mhh/sd_datasets/fusing/fill50k \\\\\\r\\n --mixed_precision=\"bf16\" \\\\\\r\\n --resolution=1024 \\\\\\r\\n --learning_rate=1e-5 \\\\\\r\\n --max_train_steps=200 \\\\\\r\\n --validation_image \"/home/mhh/sd_datasets/controlnet_image/conditioning_image_1.png\" \"/home/mhh/sd_datasets/controlnet_image/conditioning_image_2.png\" \\\\\\r\\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\\\\r\\n --validation_steps=50 \\\\\\r\\n --train_batch_size=1 \\\\\\r\\n --gradient_accumulation_steps=4 \\\\\\r\\n --report_to=\"wandb\" \\\\\\r\\n --seed=42 \\\\\\r\\n```\\n\\n### Expected behavior\\n\\nWhen I changed the data type to fp16, it worked.\\n\\n### Environment info\\n\\ndatasets                2.16.1\\r\\nnumpy                   1.24.4',\n",
       "   'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/6566/reactions',\n",
       "    'total_count': 0,\n",
       "    '+1': 0,\n",
       "    '-1': 0,\n",
       "    'laugh': 0,\n",
       "    'hooray': 0,\n",
       "    'confused': 0,\n",
       "    'heart': 0,\n",
       "    'rocket': 0,\n",
       "    'eyes': 0},\n",
       "   'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/6566/timeline',\n",
       "   'performed_via_github_app': None,\n",
       "   'state_reason': None}])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
    "response = requests.get(url)\n",
    "response.status_code, type(response), response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fetch issues implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "\n",
    "\n",
    "def fetch_issues(\n",
    "    owner=\"huggingface\",\n",
    "    repo=\"datasets\",\n",
    "    # num_issues=10_000,\n",
    "    num_issues=1_000,\n",
    "    rate_limit=5_000,\n",
    "    issues_path=Path(\".\"),\n",
    "):\n",
    "    if not issues_path.is_dir():\n",
    "        issues_path.mkdir(exist_ok=True)\n",
    "\n",
    "    batch = []\n",
    "    all_issues = []\n",
    "    per_page = 100  # Number of issues to return per page\n",
    "    num_pages = math.ceil(num_issues / per_page)\n",
    "    base_url = \"https://api.github.com/repos\"\n",
    "\n",
    "    for page in tqdm(range(num_pages)):\n",
    "        # Query with state=all to get both open and closed issues\n",
    "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
    "        issues = requests.get(\n",
    "            f\"{base_url}/{owner}/{repo}/{query}\", headers=HEADERS\n",
    "        )\n",
    "        batch.extend(issues.json())\n",
    "\n",
    "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
    "            all_issues.extend(batch)\n",
    "            batch = []  # Flush batch for next time period\n",
    "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
    "            time.sleep(60 * 60 + 1)\n",
    "\n",
    "    all_issues.extend(batch)\n",
    "    df = pd.DataFrame.from_records(all_issues)\n",
    "    df.to_json(\n",
    "        f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True\n",
    "    )\n",
    "    print(\n",
    "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7f96dc2f9f465bad9fbbf58a00511f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded all the issues for datasets! Dataset stored at ./datasets-issues.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Depending on your internet connection, this can take several minutes to run...\n",
    "fetch_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.Load locally as HF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecd5822e65147859ca2c7bb36478e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = load_dataset(\n",
    "    \"json\", data_files=\"datasets-issues.jsonl\", split=\"train\"\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.Cleaning up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> URL: https://github.com/huggingface/datasets/pull/6509\n",
      ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/6509', 'html_url': 'https://github.com/huggingface/datasets/pull/6509', 'diff_url': 'https://github.com/huggingface/datasets/pull/6509.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/6509.patch', 'merged_at': datetime.datetime(2023, 12, 19, 9, 31, 3)}\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/6540\n",
      ">> Pull request: None\n",
      "\n",
      ">> URL: https://github.com/huggingface/datasets/issues/5768\n",
      ">> Pull request: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = issues_dataset.shuffle(seed=666).select(range(3))\n",
    "\n",
    "# Print out the URL and pull request entries\n",
    "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
    "    print(f\">> URL: {url}\")\n",
    "    print(f\">> Pull request: {pr}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ps:  \n",
    "Here we can see that each pull request is associated with various URLs, while ordinary issues have a None entry. We can use this distinction to create a new is_pull_request column that checks whether the pull_request field is None or not:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a062c34b6fb4b2b986ca4dda196cf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.map(\n",
    "    lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.Augmenting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-897594128',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
       "  'id': 897594128,\n",
       "  'node_id': 'IC_kwDODunzps41gDMQ',\n",
       "  'user': {'login': 'bhavitvyamalik',\n",
       "   'id': 19718818,\n",
       "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
       "   'html_url': 'https://github.com/bhavitvyamalik',\n",
       "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
       "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
       "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2021-08-12T12:21:52Z',\n",
       "  'updated_at': '2021-08-12T12:31:17Z',\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'body': \"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None},\n",
       " {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889',\n",
       "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-898644889',\n",
       "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
       "  'id': 898644889,\n",
       "  'node_id': 'IC_kwDODunzps41kDuZ',\n",
       "  'user': {'login': 'bhavitvyamalik',\n",
       "   'id': 19718818,\n",
       "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
       "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
       "   'gravatar_id': '',\n",
       "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
       "   'html_url': 'https://github.com/bhavitvyamalik',\n",
       "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
       "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
       "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
       "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
       "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
       "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
       "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
       "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
       "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
       "   'type': 'User',\n",
       "   'site_admin': False},\n",
       "  'created_at': '2021-08-13T18:28:27Z',\n",
       "  'updated_at': '2021-08-13T18:28:27Z',\n",
       "  'author_association': 'CONTRIBUTOR',\n",
       "  'body': 'Thanks for the help, @albertvillanova! All tests are passing now.',\n",
       "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889/reactions',\n",
       "   'total_count': 0,\n",
       "   '+1': 0,\n",
       "   '-1': 0,\n",
       "   'laugh': 0,\n",
       "   'hooray': 0,\n",
       "   'confused': 0,\n",
       "   'heart': 0,\n",
       "   'rocket': 0,\n",
       "   'eyes': 0},\n",
       "  'performed_via_github_app': None}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_number = 2792\n",
    "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "response = requests.get(url, headers=HEADERS)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
       " 'Thanks for the help, @albertvillanova! All tests are passing now.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_comments(issue_number):\n",
    "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    return [r[\"body\"] for r in response.json()]\n",
    "\n",
    "\n",
    "# Test our function works as expected\n",
    "get_comments(2792)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae0001b7acf48df8d8b6d98886e42c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Depending on your internet connection, this can take a few minutes...\n",
    "issues_with_comments_dataset = issues_dataset.map(\n",
    "    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_with_comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5.Uploading the dataset to the HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b668656d0d9490a9ee0040dbd1855d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "# huggingface-cli login  # for terminal through cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13275ee0b94407c865188cf451d61b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07cb54993c0463db0dde9f28f7ae2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957e097997304a60ab1234c6273d6753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\toy\\skills-HuggingFace\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\baksa\\.cache\\huggingface\\hub\\datasets--ilbaks--github-issues. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ilbaks/github-issues/commit/4c21e09eab5c5480d71567689323c6b557261c1b', commit_message='Upload dataset', commit_description='', oid='4c21e09eab5c5480d71567689323c6b557261c1b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_with_comments_dataset.push_to_hub(\"ilbaks/github-issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Semantic search with FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.Loading and preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1.load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8c2d463e9e4407b646b3576d927673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0d80506f274ade866029f8e4dbe61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.47M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252b6bc2f5ac45459899a6ea0f54d2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"ilbaks/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2.filter only for issues not pull request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8560c675063448a191e6dd0e1b2e1935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'state_reason', 'draft', 'pull_request', 'is_pull_request'],\n",
       "    num_rows: 449\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3.remain only the requires columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 449\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4.convert to pandas and explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format of the dataset to 'pandas' for easier manipulation\n",
    "issues_dataset.set_format(\"pandas\")\n",
    "\n",
    "# Convert the entire dataset to a Pandas DataFrame\n",
    "df = issues_dataset[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['html_url', 'title', 'comments', 'body'],\n",
       "     num_rows: 449\n",
       " }),\n",
       "                                               html_url  \\\n",
       " 0    https://github.com/huggingface/datasets/issues...   \n",
       " 1    https://github.com/huggingface/datasets/issues...   \n",
       " 2    https://github.com/huggingface/datasets/issues...   \n",
       " 3    https://github.com/huggingface/datasets/issues...   \n",
       " 4    https://github.com/huggingface/datasets/issues...   \n",
       " ..                                                 ...   \n",
       " 444  https://github.com/huggingface/datasets/issues...   \n",
       " 445  https://github.com/huggingface/datasets/issues...   \n",
       " 446  https://github.com/huggingface/datasets/issues...   \n",
       " 447  https://github.com/huggingface/datasets/issues...   \n",
       " 448  https://github.com/huggingface/datasets/issues...   \n",
       " \n",
       "                                                  title  \\\n",
       " 0     `drop_last_batch=True` for IterableDataset ma...   \n",
       " 1    `ImportError`: cannot import name 'insecure_ha...   \n",
       " 2          Document YAML configuration with \"data_dir\"   \n",
       " 3    Latest version 2.16.1, when load dataset error...   \n",
       " 4    Parquet exports are used even if revision is p...   \n",
       " ..                                                 ...   \n",
       " 444                 Make all print statements optional   \n",
       " 445  Datasets map and select(range()) is giving dil...   \n",
       " 446         xPath to implement all operations for Path   \n",
       " 447  IterableDataset with_format does not support '...   \n",
       " 448  Not all progress bars are showing up when they...   \n",
       " \n",
       "                                               comments  \\\n",
       " 0    [My current workaround this issue is to return...   \n",
       " 1    [@Wauplin Do you happen to know what's up?, <d...   \n",
       " 2    [In particular, I would like to have an exampl...   \n",
       " 3    [Hi ! The \"allenai--c4\" config doesn't exist (...   \n",
       " 4    [I don't think this bug is a thing ? Do you ha...   \n",
       " ..                                                 ...   \n",
       " 444  [related to #5444 , We now log these messages ...   \n",
       " 445  [It looks like an error that we observed once ...   \n",
       " 446  [ I think https://github.com/fsspec/universal_...   \n",
       " 447  [Hi! Yes, only `torch` is currently supported....   \n",
       " 448  [Hi! \\r\\n\\r\\nBy default, tqdm has `leave=True`...   \n",
       " \n",
       "                                                   body  \n",
       " 0    ### Describe the bug\\r\\n\\r\\nScenario:\\r\\n- Int...  \n",
       " 1    ### Describe the bug\\n\\nYep its not [there](ht...  \n",
       " 2    See https://huggingface.co/datasets/uonlp/Cult...  \n",
       " 3    ### Describe the bug\\n\\npython script is:\\r\\n`...  \n",
       " 4    We should not used Parquet exports if `revisio...  \n",
       " ..                                                 ...  \n",
       " 444  ### Feature request\\n\\nMake all print statemen...  \n",
       " 445  ### Describe the bug\\n\\nI'm using Huggingface ...  \n",
       " 446  ### Feature request\\n\\nCurrent xPath implement...  \n",
       " 447  ### Describe the bug\\r\\n\\r\\nAs seen here: http...  \n",
       " 448  ### Describe the bug\\n\\nDuring downloading the...  \n",
       " \n",
       " [449 rows x 4 columns])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My current workaround this issue is to return `None` in the second element and then filter out samples which have `None` in  them.\\r\\n\\r\\n```python\\r\\ndef merge_samples(batch):\\r\\n    if len(batch['a']) == 1:\\r\\n        batch['c'] = [batch['a'][0]]\\r\\n        batch['d'] = [None]\\r\\n    else:\\r\\n        batch['c'] = [batch['a'][0]]\\r\\n        batch['d'] = [batch['a'][1]]\\r\\n    return batch\\r\\n    \\r\\ndef filter_fn(x):\\r\\n    return x['d'] is not None\\r\\n\\r\\n# other code...\\r\\nmapped = mapped.filter(filter_fn)\\r\\n```\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments\"][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>`drop_last_batch=True` for IterableDataset ma...</td>\n",
       "      <td>My current workaround this issue is to return ...</td>\n",
       "      <td>### Describe the bug\\r\\n\\r\\nScenario:\\r\\n- Int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>`ImportError`: cannot import name 'insecure_ha...</td>\n",
       "      <td>@Wauplin Do you happen to know what's up?</td>\n",
       "      <td>### Describe the bug\\n\\nYep its not [there](ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>`ImportError`: cannot import name 'insecure_ha...</td>\n",
       "      <td>&lt;del&gt;Installing `datasets` from `main` did the...</td>\n",
       "      <td>### Describe the bug\\n\\nYep its not [there](ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>`ImportError`: cannot import name 'insecure_ha...</td>\n",
       "      <td>@wasertech upgrading `huggingface_hub` to a ne...</td>\n",
       "      <td>### Describe the bug\\n\\nYep its not [there](ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0   `drop_last_batch=True` for IterableDataset ma...   \n",
       "1  `ImportError`: cannot import name 'insecure_ha...   \n",
       "2  `ImportError`: cannot import name 'insecure_ha...   \n",
       "3  `ImportError`: cannot import name 'insecure_ha...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  My current workaround this issue is to return ...   \n",
       "1          @Wauplin Do you happen to know what's up?   \n",
       "2  <del>Installing `datasets` from `main` did the...   \n",
       "3  @wasertech upgrading `huggingface_hub` to a ne...   \n",
       "\n",
       "                                                body  \n",
       "0  ### Describe the bug\\r\\n\\r\\nScenario:\\r\\n- Int...  \n",
       "1  ### Describe the bug\\n\\nYep its not [there](ht...  \n",
       "2  ### Describe the bug\\n\\nYep its not [there](ht...  \n",
       "3  ### Describe the bug\\n\\nYep its not [there](ht...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform each element of those lists into a separate row\n",
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5.convert back to dataset HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 1392\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.6.filter comments longer 15 sympbols length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b45c2d9e7342ed9f72c99c465e6061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1392 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a7cb06062d469b9df4565aedaec93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1392 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 1019\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.7.concatenate  issue title, description, and comments together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26ce2b4c6cb4bd7a074a03c48cc68fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 1019\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.Creating text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1.Download model for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f953c5a4037c4583b2824a2bf42a66c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\toy\\skills-HuggingFace\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\baksa\\.cache\\huggingface\\hub\\models--sentence-transformers--multi-qa-mpnet-base-dot-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1f13e0bd6c47b18cf605ad42714fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e9b67734b943a5bf196719e234ec6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae772e41a4f0433988f869e467754d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613758bcdcfc4e638293222eff74c455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e6c79c6ed94db4a844c95e4a5ef792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2.Create embeddings  using CLSpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " represent each entry in our GitHub issues corpus as a single vector, so we need to “pool” or average our token embeddings in some way. One popular approach is to perform CLS pooling on our model’s outputs, where we simply collect the last hidden state for the special [CLS] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs [explanation with gpt](https://chat.openai.com/share/6c58d2f2-4798-459f-bde9-e689491337be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' `drop_last_batch=True` for IterableDataset map function is ignored with multiprocessing DataLoader  \\n ### Describe the bug\\r\\n\\r\\nScenario:\\r\\n- Interleaving two iterable datasets of unequal lengths (`all_exhausted`), followed by a batch mapping with batch size 2 to effectively merge the two datasets and get a sample from each dataset in a single batch, with `drop_last_batch=True` to skip the last batch in case it doesn\\'t have two samples.\\r\\n\\r\\nWhat works:\\r\\n- Using DataLoader with `num_workers=0`\\r\\n\\r\\nWhat does not work:\\r\\n- Using DataLoader with `num_workers=1`, errors in the last batch.\\r\\n\\r\\nBasically, `drop_last_batch=True` is ignored when using multiple dataloading workers.\\r\\n\\r\\nPlease take a look at the minimal repro script below.\\r\\n\\r\\n### Steps to reproduce the bug\\r\\n\\r\\n```python\\r\\nfrom datasets import Dataset, interleave_datasets\\r\\nfrom torch.utils.data import DataLoader\\r\\n\\r\\n\\r\\ndef merge_samples(batch):\\r\\n    assert len(batch[\\'a\\']) == 2, \"Batch size must be 2\"\\r\\n    batch[\\'c\\'] = [batch[\\'a\\'][0]]\\r\\n    batch[\\'d\\'] = [batch[\\'a\\'][1]]\\r\\n    return batch\\r\\n\\r\\n\\r\\ndef gen1():\\r\\n    for ii in range(1, 8385):\\r\\n        yield {\"a\": ii}\\r\\n\\r\\n\\r\\ndef gen2():\\r\\n    for ii in range(1, 5302):\\r\\n        yield {\"a\": ii}\\r\\n\\r\\n\\r\\nif __name__ == \\'__main__\\':\\r\\n\\r\\n    dataset1 = Dataset.from_generator(gen1).to_iterable_dataset(num_shards=1024)\\r\\n    dataset2 = Dataset.from_generator(gen2).to_iterable_dataset(num_shards=1024)\\r\\n\\r\\n    interleaved = interleave_datasets([dataset1, dataset2], stopping_strategy=\"all_exhausted\")\\r\\n    mapped = interleaved.map(merge_samples, batched=True, batch_size=2, remove_columns=interleaved.column_names,\\r\\n                             drop_last_batch=True)\\r\\n\\r\\n    # Works\\r\\n    loader = DataLoader(mapped, batch_size=32, num_workers=0)\\r\\n    i = 0\\r\\n    for b in loader:\\r\\n        print(i, b[\\'c\\'].shape, b[\\'d\\'].shape)\\r\\n        i += 1\\r\\n\\r\\n    print(\"DataLoader with num_workers=0 works\")\\r\\n\\r\\n    # Doesn\\'t work\\r\\n    loader = DataLoader(mapped, batch_size=32, num_workers=1)\\r\\n    i = 0\\r\\n    for b in loader:\\r\\n        print(i, b[\\'c\\'].shape, b[\\'d\\'].shape)\\r\\n        i += 1\\r\\n\\r\\n\\r\\n```\\r\\n\\r\\n### Expected behavior\\r\\n\\r\\n `drop_last_batch=True` should have same behaviour for `num_workers=0` and `num_workers>=1`\\r\\n\\r\\n### Environment info\\r\\n\\r\\n- `datasets` version: 2.16.1\\r\\n- Platform: macOS-10.16-x86_64-i386-64bit\\r\\n- Python version: 3.10.12\\r\\n- `huggingface_hub` version: 0.20.2\\r\\n- PyArrow version: 12.0.1\\r\\n- Pandas version: 2.0.3\\r\\n- `fsspec` version: 2023.6.0\\r\\n\\r\\nI have also tested on Linux and got the same behavior. \\n My current workaround this issue is to return `None` in the second element and then filter out samples which have `None` in  them.\\r\\n\\r\\n```python\\r\\ndef merge_samples(batch):\\r\\n    if len(batch[\\'a\\']) == 1:\\r\\n        batch[\\'c\\'] = [batch[\\'a\\'][0]]\\r\\n        batch[\\'d\\'] = [None]\\r\\n    else:\\r\\n        batch[\\'c\\'] = [batch[\\'a\\'][0]]\\r\\n        batch[\\'d\\'] = [batch[\\'a\\'][1]]\\r\\n    return batch\\r\\n    \\r\\ndef filter_fn(x):\\r\\n    return x[\\'d\\'] is not None\\r\\n\\r\\n# other code...\\r\\nmapped = mapped.filter(filter_fn)\\r\\n```'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4506709632144cfd84c18a6243e7feeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\n",
    "        \"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 1019\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dataset[\"embeddings\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.2956484258174896,\n",
       " -0.06441400945186615,\n",
       " -0.03195429593324661,\n",
       " 0.1042032539844513,\n",
       " -0.08406780660152435,\n",
       " -0.0683007761836052,\n",
       " 0.6586068868637085,\n",
       " 0.20464758574962616,\n",
       " 0.23949621617794037,\n",
       " 0.4104593098163605]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset[\"embeddings\"][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.Using FAISS for efficient similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset of embeddings, we need some way to search over them. To do this, we’ll use a special data structure in 🤗 Datasets called a [FAISS index](https://faiss.ai/). FAISS (short for Facebook AI Similarity Search) is a library that provides efficient algorithms to quickly search and cluster embedding vectors.\n",
    "\n",
    "The basic idea behind FAISS is to create a special data structure called an index that allows one to find which embeddings are similar to an input embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. A community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. Note that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43membeddings_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_faiss_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\toy\\skills-HuggingFace\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:5673\u001b[0m, in \u001b[0;36mDataset.add_faiss_index\u001b[1;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose, dtype)\u001b[0m\n\u001b[0;32m   5619\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a dense index using Faiss for fast retrieval.\u001b[39;00m\n\u001b[0;32m   5620\u001b[0m \u001b[38;5;124;03mBy default the index is done over the vectors of the specified column.\u001b[39;00m\n\u001b[0;32m   5621\u001b[0m \u001b[38;5;124;03mYou can specify `device` if you want to run it on GPU (`device` must be the GPU index).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5670\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m   5671\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5672\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatted_as(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[column], dtype\u001b[38;5;241m=\u001b[39mdtype):\n\u001b[1;32m-> 5673\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_faiss_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstring_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaiss_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfaiss_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5683\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5684\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Projects\\toy\\skills-HuggingFace\\venv\\lib\\site-packages\\datasets\\search.py:481\u001b[0m, in \u001b[0;36mIndexableMixin.add_faiss_index\u001b[1;34m(self, column, index_name, device, string_factory, metric_type, custom_index, batch_size, train_size, faiss_verbose)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a dense index using Faiss for fast retrieval.\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;124;03mThe index is created using the vectors of the specified column.\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;124;03mYou can specify `device` if you want to run it on GPU (`device` must be the GPU index, see more below).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03m    faiss_verbose (`bool`, defaults to False): Enable the verbosity of the Faiss index.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    480\u001b[0m index_name \u001b[38;5;241m=\u001b[39m index_name \u001b[38;5;28;01mif\u001b[39;00m index_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m--> 481\u001b[0m faiss_index \u001b[38;5;241m=\u001b[39m \u001b[43mFaissIndex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstring_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_index\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m faiss_index\u001b[38;5;241m.\u001b[39madd_vectors(\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28mself\u001b[39m, column\u001b[38;5;241m=\u001b[39mcolumn, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, train_size\u001b[38;5;241m=\u001b[39mtrain_size, faiss_verbose\u001b[38;5;241m=\u001b[39mfaiss_verbose\n\u001b[0;32m    486\u001b[0m )\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexes[index_name] \u001b[38;5;241m=\u001b[39m faiss_index\n",
      "File \u001b[1;32md:\\Projects\\toy\\skills-HuggingFace\\venv\\lib\\site-packages\\datasets\\search.py:248\u001b[0m, in \u001b[0;36mFaissIndex.__init__\u001b[1;34m(self, device, string_factory, metric_type, custom_index)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfaiss_index \u001b[38;5;241m=\u001b[39m custom_index\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_faiss:\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: You must install Faiss to use FaissIndex. To do so you can run `conda install -c pytorch faiss-cpu` or `conda install -c pytorch faiss-gpu`. A community supported package is also available on pypi: `pip install faiss-cpu` or `pip install faiss-gpu`. Note that pip may not have the latest version of FAISS, and thus, some of the latest features and bug fixes may not be available."
     ]
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform queries on this index by doing a nearest neighbor lookup with the Dataset.get_nearest_examples() function. Let’s test this out by first embedding a question as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset.get_nearest_examples() function returns a tuple of scores that rank the overlap between the query and the document, and a corresponding set of samples (here, the 5 best matches). Let’s collect these in a pandas.DataFrame so we can easily sort them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate over the first few rows to see how well our query matched the available comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
    "\n",
    "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
    "SCORE: 25.505046844482422\n",
    "TITLE: Discussion using datasets in offline mode\n",
    "URL: https://github.com/huggingface/datasets/issues/824\n",
    "==================================================\n",
    "\n",
    "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
    "You can now use them offline\n",
    "\\`\\`\\`python\n",
    "datasets = load_dataset(\"text\", data_files=data_files)\n",
    "\\`\\`\\`\n",
    "\n",
    "We'll do a new release soon\n",
    "SCORE: 24.555509567260742\n",
    "TITLE: Discussion using datasets in offline mode\n",
    "URL: https://github.com/huggingface/datasets/issues/824\n",
    "==================================================\n",
    "\n",
    "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
    "\n",
    "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)\n",
    "\n",
    "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
    "\n",
    "----------\n",
    "\n",
    "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
    "\n",
    "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
    "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
    "\\`\\`\\`python\n",
    "load_dataset(\"./my_dataset\")\n",
    "\\`\\`\\`\n",
    "and the dataset script will generate your dataset once and for all.\n",
    "\n",
    "----------\n",
    "\n",
    "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
    "cf #1724\n",
    "SCORE: 24.14896583557129\n",
    "TITLE: Discussion using datasets in offline mode\n",
    "URL: https://github.com/huggingface/datasets/issues/824\n",
    "==================================================\n",
    "\n",
    "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
    ">\n",
    "> 1. (online machine)\n",
    ">\n",
    "> ```\n",
    ">\n",
    "> import datasets\n",
    ">\n",
    "> data = datasets.load_dataset(...)\n",
    ">\n",
    "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
    ">\n",
    "> ```\n",
    ">\n",
    "> 2. copy the dir from online to the offline machine\n",
    ">\n",
    "> 3. (offline machine)\n",
    ">\n",
    "> ```\n",
    ">\n",
    "> import datasets\n",
    ">\n",
    "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
    ">\n",
    "> ```\n",
    ">\n",
    ">\n",
    ">\n",
    "> HTH.\n",
    "\n",
    "\n",
    "SCORE: 22.893993377685547\n",
    "TITLE: Discussion using datasets in offline mode\n",
    "URL: https://github.com/huggingface/datasets/issues/824\n",
    "==================================================\n",
    "\n",
    "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
    "1. (online machine)\n",
    "\\`\\`\\`\n",
    "import datasets\n",
    "data = datasets.load_dataset(...)\n",
    "data.save_to_disk(/YOUR/DATASET/DIR)\n",
    "\\`\\`\\`\n",
    "2. copy the dir from online to the offline machine\n",
    "3. (offline machine)\n",
    "\\`\\`\\`\n",
    "import datasets\n",
    "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
    "\\`\\`\\`\n",
    "\n",
    "HTH.\n",
    "SCORE: 22.406635284423828\n",
    "TITLE: Discussion using datasets in offline mode\n",
    "URL: https://github.com/huggingface/datasets/issues/824\n",
    "==================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
