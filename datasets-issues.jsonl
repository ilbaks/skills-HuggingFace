{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6566","id":2069495429,"node_id":"I_kwDODunzps57Wf6F","number":6566,"title":"I train controlnet_sdxl in bf16 datatype, got unsupported ERROR in datasets","user":{"login":"HelloWorldBeginner","id":25008090,"node_id":"MDQ6VXNlcjI1MDA4MDkw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/25008090?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/HelloWorldBeginner","html_url":"https:\/\/github.com\/HelloWorldBeginner","followers_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/followers","following_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/orgs","repos_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/repos","events_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2024-01-08T02:37:03Z","updated_at":"2024-01-08T02:37:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n```\r\nTraceback (most recent call last):\r\n  File \"train_controlnet_sdxl.py\", line 1252, in <module>\r\n    main(args)\r\n  File \"train_controlnet_sdxl.py\", line 1013, in main\r\n    train_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 592, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 3093, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 3489, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_writer.py\", line 557, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow\/array.pxi\", line 248, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 113, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_writer.py\", line 191, in __arrow_array__\r\n    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/features\/features.py\", line 447, in cast_to_python_objects\r\n    return _cast_to_python_objects(\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/features\/features.py\", line 324, in _cast_to_python_objects\r\n    for x in obj.detach().cpu().numpy()\r\nTypeError: Got unsupported ScalarType BFloat16\r\n```\n\n### Steps to reproduce the bug\n\nHere is my train script I use BF16 type\uff0cI use diffusers train my model\r\n```\r\nexport MODEL_DIR=\"\/home\/mhh\/sd_models\/stable-diffusion-xl-base-1.0\"\r\nexport OUTPUT_DIR=\".\/control_net\"\r\nexport VAE_NAME=\"\/home\/mhh\/sd_models\/sdxl-vae-fp16-fix\"\r\n\r\naccelerate launch train_controlnet_sdxl.py \\\r\n --pretrained_model_name_or_path=$MODEL_DIR \\\r\n --output_dir=$OUTPUT_DIR \\\r\n --pretrained_vae_model_name_or_path=$VAE_NAME \\\r\n --dataset_name=\/home\/mhh\/sd_datasets\/fusing\/fill50k \\\r\n --mixed_precision=\"bf16\" \\\r\n --resolution=1024 \\\r\n --learning_rate=1e-5 \\\r\n --max_train_steps=200 \\\r\n --validation_image \"\/home\/mhh\/sd_datasets\/controlnet_image\/conditioning_image_1.png\" \"\/home\/mhh\/sd_datasets\/controlnet_image\/conditioning_image_2.png\" \\\r\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\r\n --validation_steps=50 \\\r\n --train_batch_size=1 \\\r\n --gradient_accumulation_steps=4 \\\r\n --report_to=\"wandb\" \\\r\n --seed=42 \\\r\n```\n\n### Expected behavior\n\nWhen I changed the data type to fp16, it worked.\n\n### Environment info\n\ndatasets                2.16.1\r\nnumpy                   1.24.4","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6565","id":2068939670,"node_id":"I_kwDODunzps57UYOW","number":6565,"title":" `drop_last_batch=True` for IterableDataset map function is ignored with multiprocessing DataLoader ","user":{"login":"naba89","id":12119806,"node_id":"MDQ6VXNlcjEyMTE5ODA2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12119806?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/naba89","html_url":"https:\/\/github.com\/naba89","followers_url":"https:\/\/api.github.com\/users\/naba89\/followers","following_url":"https:\/\/api.github.com\/users\/naba89\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/naba89\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/naba89\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/naba89\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/naba89\/orgs","repos_url":"https:\/\/api.github.com\/users\/naba89\/repos","events_url":"https:\/\/api.github.com\/users\/naba89\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/naba89\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-07T02:46:50Z","updated_at":"2024-01-07T03:03:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nScenario:\r\n- Interleaving two iterable datasets of unequal lengths (`all_exhausted`), followed by a batch mapping with batch size 2 to effectively merge the two datasets and get a sample from each dataset in a single batch, with `drop_last_batch=True` to skip the last batch in case it doesn't have two samples.\r\n\r\nWhat works:\r\n- Using DataLoader with `num_workers=0`\r\n\r\nWhat does not work:\r\n- Using DataLoader with `num_workers=1`, errors in the last batch.\r\n\r\nBasically, `drop_last_batch=True` is ignored when using multiple dataloading workers.\r\n\r\nPlease take a look at the minimal repro script below.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import Dataset, interleave_datasets\r\nfrom torch.utils.data import DataLoader\r\n\r\n\r\ndef merge_samples(batch):\r\n    assert len(batch['a']) == 2, \"Batch size must be 2\"\r\n    batch['c'] = [batch['a'][0]]\r\n    batch['d'] = [batch['a'][1]]\r\n    return batch\r\n\r\n\r\ndef gen1():\r\n    for ii in range(1, 8385):\r\n        yield {\"a\": ii}\r\n\r\n\r\ndef gen2():\r\n    for ii in range(1, 5302):\r\n        yield {\"a\": ii}\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    dataset1 = Dataset.from_generator(gen1).to_iterable_dataset(num_shards=1024)\r\n    dataset2 = Dataset.from_generator(gen2).to_iterable_dataset(num_shards=1024)\r\n\r\n    interleaved = interleave_datasets([dataset1, dataset2], stopping_strategy=\"all_exhausted\")\r\n    mapped = interleaved.map(merge_samples, batched=True, batch_size=2, remove_columns=interleaved.column_names,\r\n                             drop_last_batch=True)\r\n\r\n    # Works\r\n    loader = DataLoader(mapped, batch_size=32, num_workers=0)\r\n    i = 0\r\n    for b in loader:\r\n        print(i, b['c'].shape, b['d'].shape)\r\n        i += 1\r\n\r\n    print(\"DataLoader with num_workers=0 works\")\r\n\r\n    # Doesn't work\r\n    loader = DataLoader(mapped, batch_size=32, num_workers=1)\r\n    i = 0\r\n    for b in loader:\r\n        print(i, b['c'].shape, b['d'].shape)\r\n        i += 1\r\n\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\n `drop_last_batch=True` should have same behaviour for `num_workers=0` and `num_workers>=1`\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.16.1\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.6.0\r\n\r\nI have also tested on Linux and got the same behavior.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6564","id":2068893194,"node_id":"I_kwDODunzps57UM4K","number":6564,"title":"Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method","user":{"login":"kopyl","id":17604849,"node_id":"MDQ6VXNlcjE3NjA0ODQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17604849?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kopyl","html_url":"https:\/\/github.com\/kopyl","followers_url":"https:\/\/api.github.com\/users\/kopyl\/followers","following_url":"https:\/\/api.github.com\/users\/kopyl\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kopyl\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kopyl\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kopyl\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kopyl\/orgs","repos_url":"https:\/\/api.github.com\/users\/kopyl\/repos","events_url":"https:\/\/api.github.com\/users\/kopyl\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kopyl\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2024-01-06T23:48:13Z","updated_at":"2024-01-06T23:48:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThe issue shall be open: https:\/\/github.com\/huggingface\/datasets\/issues\/6435\r\n\r\nWhen i try to pass `with_rank` to `Dataset.filter()`, i get this:\r\n\r\n`Dataset.filter() got an unexpected keyword argument 'with_rank'`\n\n### Steps to reproduce the bug\n\nRun notebook:\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/1WUNKph8BdP0on5ve3gQnh_PE0cFLQqTn?usp=sharing\n\n### Expected behavior\n\nShould work?\n\n### Environment info\n\nNVIDIA RTX 4090","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6563","id":2068302402,"node_id":"I_kwDODunzps57R8pC","number":6563,"title":"`ImportError`: cannot import name 'insecure_hashlib' from 'huggingface_hub.utils' (...\/huggingface_hub\/utils\/__init__.py)","user":{"login":"wasertech","id":79070834,"node_id":"MDQ6VXNlcjc5MDcwODM0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/79070834?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wasertech","html_url":"https:\/\/github.com\/wasertech","followers_url":"https:\/\/api.github.com\/users\/wasertech\/followers","following_url":"https:\/\/api.github.com\/users\/wasertech\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wasertech\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wasertech\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wasertech\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wasertech\/orgs","repos_url":"https:\/\/api.github.com\/users\/wasertech\/repos","events_url":"https:\/\/api.github.com\/users\/wasertech\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wasertech\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2024-01-06T02:28:54Z","updated_at":"2024-01-06T21:05:37Z","closed_at":"2024-01-06T16:13:27Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nYep its not [there](https:\/\/github.com\/huggingface\/huggingface_hub\/blob\/main\/src\/huggingface_hub\/utils\/__init__.py) anymore.\r\n\r\n```text\r\n+ python \/home\/trainer\/sft_train.py --model_name cognitivecomputations\/dolphin-2.2.1-mistral-7b --dataset_name wasertech\/OneOS --load_in_4bit --use_peft --batch_size 4 --num_train_epochs 1 --learning_rate 1.41e-5 --gradient_accumulation_steps 8 --seq_length 4096 --output_dir output --log_with wandb\r\nTraceback (most recent call last):\r\n  File \"\/home\/trainer\/sft_train.py\", line 22, in <module>\r\n    from datasets import load_dataset\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/__init__.py\", line 22, in <module>\r\n    from .arrow_dataset import Dataset\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 66, in <module>\r\n    from .arrow_reader import ArrowReader\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/arrow_reader.py\", line 30, in <module>\r\n    from .download.download_config import DownloadConfig\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/download\/__init__.py\", line 9, in <module>\r\n    from .download_manager import DownloadManager, DownloadMode\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/download\/download_manager.py\", line 31, in <module>\r\n    from ..utils import tqdm as hf_tqdm\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/utils\/__init__.py\", line 19, in <module>\r\n    from .info_utils import VerificationMode\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/utils\/info_utils.py\", line 5, in <module>\r\n    from huggingface_hub.utils import insecure_hashlib\r\nImportError: cannot import name 'insecure_hashlib' from 'huggingface_hub.utils' (\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/huggingface_hub\/utils\/__init__.py)\r\n```\n\n### Steps to reproduce the bug\n\nUsing `datasets==2.16.1` and `huggingface_hub== 0.17.3`, load a dataset with `load_dataset`.\n\n### Expected behavior\n\nThe dataset should be (downloaded - if needed - and) returned.\n\n### Environment info\n\n```text\r\ntrainer@a311ae86939e:\/mnt$ pip show datasets\r\nName: datasets\r\nVersion: 2.16.1\r\nSummary: HuggingFace community-driven open-source library of datasets\r\nHome-page: https:\/\/github.com\/huggingface\/datasets\r\nAuthor: HuggingFace Inc.\r\nAuthor-email: thomas@huggingface.co\r\nLicense: Apache 2.0\r\nLocation: \/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\r\nRequires: packaging, pyyaml, multiprocess, pyarrow-hotfix, pandas, pyarrow, xxhash, dill, numpy, aiohttp, tqdm, fsspec, requests, filelock, huggingface-hub\r\nRequired-by: trl, lm-eval, evaluate\r\n\r\ntrainer@a311ae86939e:\/mnt$ pip show huggingface_hub\r\nName: huggingface-hub\r\nVersion: 0.17.3\r\nSummary: Client library to download and publish models, datasets and other repos on the huggingface.co hub\r\nHome-page: https:\/\/github.com\/huggingface\/huggingface_hub\r\nAuthor: Hugging Face, Inc.\r\nAuthor-email: julien@huggingface.co\r\nLicense: Apache\r\nLocation: \/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\r\nRequires: requests, pyyaml, packaging, typing-extensions, tqdm, filelock, fsspec\r\nRequired-by: transformers, tokenizers, peft, evaluate, datasets, accelerate\r\n\r\ntrainer@a311ae86939e:\/mnt$ huggingface-cli env\r\n\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- huggingface_hub version: 0.17.3\r\n- Platform: Linux-6.5.13-7-MANJARO-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Running in iPython ?: No\r\n- Running in notebook ?: No\r\n- Running in Google Colab ?: No\r\n- Token path ?: \/home\/trainer\/.cache\/huggingface\/token\r\n- Has saved token ?: True\r\n- Who am I ?: wasertech\r\n- Configured git credential helpers: \r\n- FastAI: N\/A\r\n- Tensorflow: N\/A\r\n- Torch: 2.1.2\r\n- Jinja2: 3.1.2\r\n- Graphviz: N\/A\r\n- Pydot: N\/A\r\n- Pillow: 10.2.0\r\n- hf_transfer: N\/A\r\n- gradio: N\/A\r\n- tensorboard: N\/A\r\n- numpy: 1.24.4\r\n- pydantic: N\/A\r\n- aiohttp: 3.9.1\r\n- ENDPOINT: https:\/\/huggingface.co\r\n- HUGGINGFACE_HUB_CACHE: \/home\/trainer\/.cache\/huggingface\/hub\r\n- HUGGINGFACE_ASSETS_CACHE: \/home\/trainer\/.cache\/huggingface\/assets\r\n- HF_TOKEN_PATH: \/home\/trainer\/.cache\/huggingface\/token\r\n- HF_HUB_OFFLINE: False\r\n- HF_HUB_DISABLE_TELEMETRY: False\r\n- HF_HUB_DISABLE_PROGRESS_BARS: None\r\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\r\n- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\r\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\r\n- HF_HUB_ENABLE_HF_TRANSFER: False\r\n\r\n\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6562","id":2067904504,"node_id":"I_kwDODunzps57Qbf4","number":6562,"title":"datasets.DownloadMode.FORCE_REDOWNLOAD use cache to download dataset features with load_dataset function","user":{"login":"LsTam91","id":73234162,"node_id":"MDQ6VXNlcjczMjM0MTYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/73234162?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/LsTam91","html_url":"https:\/\/github.com\/LsTam91","followers_url":"https:\/\/api.github.com\/users\/LsTam91\/followers","following_url":"https:\/\/api.github.com\/users\/LsTam91\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/LsTam91\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/LsTam91\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/LsTam91\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/LsTam91\/orgs","repos_url":"https:\/\/api.github.com\/users\/LsTam91\/repos","events_url":"https:\/\/api.github.com\/users\/LsTam91\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/LsTam91\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2024-01-05T19:10:25Z","updated_at":"2024-01-05T19:10:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI have updated my dataset by adding a new feature, and push it to the hub. When I want to download it on my machine which contain the old version by using `datasets.load_dataset(\"your_dataset_name\", download_mode=datasets.DownloadMode.FORCE_REDOWNLOAD)` I get an error (paste bellow).\r\n\r\nSeems that the load_dataset function still use the old features schema instead of downloading everything new from the HUB.\r\n\r\nI find a way to go around this issue by manually deleting the old dataset cache. But from my understanding of `datasets.DownloadMode.FORCE_REDOWNLOAD` option, the dataset cache should be ignored.\n\n### Steps to reproduce the bug\n\n1. Download your dataset in your machine using `datasets.load_dataset`\r\n2. Create a new feature in your dataset and push it to the hub\r\n3. On the same machine redownload your dataset using `datasets.load_dataset(\"your_dataset_name\", download_mode=datasets.DownloadMode.FORCE_REDOWNLOAD)`\n\n### Expected behavior\n\n`\r\nValueError: Couldn't cast\r\nid: string\r\nlevel: string\r\ncontext: list<element: string>\r\n  child 0, element: string\r\ntype: string\r\nanswer: string\r\nquestion: string\r\nsupporting_facts: list<element: string>\r\n  child 0, element: string\r\nfra_answer: string\r\nfra_question: string\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"id\": {\"dtype\": \"string\", \"_type\": \"' + 490\r\nto\r\n{'id': Value(dtype='string', id=None), 'level': Value(dtype='string', id=None), 'context': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'type': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'supporting_facts': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError\r\n...\r\nDatasetGenerationError: An error occurred while generating the dataset`\n\n### Environment info\n\ndatasets-2.16.1 huggingface-hub-0.20.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6561","id":2067404951,"node_id":"I_kwDODunzps57OhiX","number":6561,"title":"Document YAML configuration with \"data_dir\"","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-05T14:03:33Z","updated_at":"2024-01-05T14:06:18Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"See https:\/\/huggingface.co\/datasets\/uonlp\/CulturaX\/discussions\/15#6597e83f185db94370d6bf50 for reference","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6560","id":2065637625,"node_id":"I_kwDODunzps57HyD5","number":6560,"title":"Support Video ","user":{"login":"yuvalkirstain","id":57996478,"node_id":"MDQ6VXNlcjU3OTk2NDc4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57996478?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yuvalkirstain","html_url":"https:\/\/github.com\/yuvalkirstain","followers_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/followers","following_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/orgs","repos_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/repos","events_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2024-01-04T13:10:58Z","updated_at":"2024-01-04T13:10:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nHF datasets are awesome in supporting text and images. Will be great to see such a support in videos :) \n\n### Motivation\n\nVideo generation :) \n\n### Your contribution\n\nWill probably be limited to raising this feature request ;)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6559","id":2065118332,"node_id":"I_kwDODunzps57FzR8","number":6559,"title":"Latest version 2.16.1, when load dataset error occurs. ValueError: BuilderConfig 'allenai--c4' not found. Available: ['default']","user":{"login":"zhulinJulia24","id":145004780,"node_id":"U_kgDOCKSY7A","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/145004780?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/zhulinJulia24","html_url":"https:\/\/github.com\/zhulinJulia24","followers_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/followers","following_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/orgs","repos_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/repos","events_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2024-01-04T07:04:48Z","updated_at":"2024-01-05T01:26:26Z","closed_at":"2024-01-05T01:26:25Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\npython script is:\r\n```\r\n\r\nfrom datasets import load_dataset\r\ncache_dir = 'path\/to\/your\/cache\/directory'\r\ndataset = load_dataset('allenai\/c4','allenai--c4', data_files={'train': 'en\/c4-train.00000-of-01024.json.gz'}, split='train',  use_auth_token=False, cache_dir=cache_dir)\r\n```\r\n\r\n\r\nthe script success when datasets version is 2.14.7. \r\nwhen using 2.16.1, error occurs\r\n`\r\nValueError: BuilderConfig 'allenai--c4' not found. Available: ['default']`\n\n### Steps to reproduce the bug\n\n1. pip install datasets==2.16.1\r\n2. run python script:\r\n```\r\n\r\nfrom datasets import load_dataset\r\ncache_dir = 'path\/to\/your\/cache\/directory'\r\ndataset = load_dataset('allenai\/c4','allenai--c4', data_files={'train': 'en\/c4-train.00000-of-01024.json.gz'}, split='train',  use_auth_token=False, cache_dir=cache_dir)\r\n```\n\n### Expected behavior\n\nthe dataset should be loaded successful in the latest version. \n\n### Environment info\n\ndatasets 2.16.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6558","id":2064885984,"node_id":"I_kwDODunzps57E6jg","number":6558,"title":"OSError: image file is truncated (1 bytes not processed) #28323","user":{"login":"andysingal","id":20493493,"node_id":"MDQ6VXNlcjIwNDkzNDkz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20493493?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/andysingal","html_url":"https:\/\/github.com\/andysingal","followers_url":"https:\/\/api.github.com\/users\/andysingal\/followers","following_url":"https:\/\/api.github.com\/users\/andysingal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/andysingal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/andysingal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/andysingal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/andysingal\/orgs","repos_url":"https:\/\/api.github.com\/users\/andysingal\/repos","events_url":"https:\/\/api.github.com\/users\/andysingal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/andysingal\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2024-01-04T02:15:13Z","updated_at":"2024-01-04T02:15:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\nCell In[24], line 28\r\n     23     return example\r\n     25 # Filter the dataset\r\n     26 # filtered_dataset = dataset.filter(contains_number)\r\n     27 # Add the 'label' field in the dataset\r\n---> 28 labeled_dataset = dataset.filter(contains_number).map(add_label)\r\n     29 # View the structure of the updated dataset\r\n     30 print(labeled_dataset)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/dataset_dict.py:975, in DatasetDict.filter(self, function, with_indices, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, fn_kwargs, num_proc, desc)\r\n    972 if cache_file_names is None:\r\n    973     cache_file_names = {k: None for k in self}\r\n    974 return DatasetDict(\r\n--> 975     {\r\n    976         k: dataset.filter(\r\n    977             function=function,\r\n    978             with_indices=with_indices,\r\n    979             input_columns=input_columns,\r\n    980             batched=batched,\r\n    981             batch_size=batch_size,\r\n    982             keep_in_memory=keep_in_memory,\r\n    983             load_from_cache_file=load_from_cache_file,\r\n    984             cache_file_name=cache_file_names[k],\r\n    985             writer_batch_size=writer_batch_size,\r\n    986             fn_kwargs=fn_kwargs,\r\n    987             num_proc=num_proc,\r\n    988             desc=desc,\r\n    989         )\r\n    990         for k, dataset in self.items()\r\n    991     }\r\n    992 )\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/dataset_dict.py:976, in <dictcomp>(.0)\r\n    972 if cache_file_names is None:\r\n    973     cache_file_names = {k: None for k in self}\r\n    974 return DatasetDict(\r\n    975     {\r\n--> 976         k: dataset.filter(\r\n    977             function=function,\r\n    978             with_indices=with_indices,\r\n    979             input_columns=input_columns,\r\n    980             batched=batched,\r\n    981             batch_size=batch_size,\r\n    982             keep_in_memory=keep_in_memory,\r\n    983             load_from_cache_file=load_from_cache_file,\r\n    984             cache_file_name=cache_file_names[k],\r\n    985             writer_batch_size=writer_batch_size,\r\n    986             fn_kwargs=fn_kwargs,\r\n    987             num_proc=num_proc,\r\n    988             desc=desc,\r\n    989         )\r\n    990         for k, dataset in self.items()\r\n    991     }\r\n    992 )\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:557, in transmit_format.<locals>.wrapper(*args, **kwargs)\r\n    550 self_format = {\r\n    551     \"type\": self._format_type,\r\n    552     \"format_kwargs\": self._format_kwargs,\r\n    553     \"columns\": self._format_columns,\r\n    554     \"output_all_columns\": self._output_all_columns,\r\n    555 }\r\n    556 # apply actual function\r\n--> 557 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    558 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    559 # re-apply format to the output\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/fingerprint.py:481, in fingerprint_transform.<locals>._fingerprint.<locals>.wrapper(*args, **kwargs)\r\n    477             validate_fingerprint(kwargs[fingerprint_name])\r\n    479 # Call actual function\r\n--> 481 out = func(dataset, *args, **kwargs)\r\n    483 # Update fingerprint of in-place transforms + update in-place history of transforms\r\n    485 if inplace:  # update after calling func so that the fingerprint doesn't change if the function fails\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:3623, in Dataset.filter(self, function, with_indices, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   3620 if len(self) == 0:\r\n   3621     return self\r\n-> 3623 indices = self.map(\r\n   3624     function=partial(\r\n   3625         get_indices_from_mask_function, function, batched, with_indices, input_columns, self._indices\r\n   3626     ),\r\n   3627     with_indices=True,\r\n   3628     features=Features({\"indices\": Value(\"uint64\")}),\r\n   3629     batched=True,\r\n   3630     batch_size=batch_size,\r\n   3631     remove_columns=self.column_names,\r\n   3632     keep_in_memory=keep_in_memory,\r\n   3633     load_from_cache_file=load_from_cache_file,\r\n   3634     cache_file_name=cache_file_name,\r\n   3635     writer_batch_size=writer_batch_size,\r\n   3636     fn_kwargs=fn_kwargs,\r\n   3637     num_proc=num_proc,\r\n   3638     suffix_template=suffix_template,\r\n   3639     new_fingerprint=new_fingerprint,\r\n   3640     input_columns=input_columns,\r\n   3641     desc=desc or \"Filter\",\r\n   3642 )\r\n   3643 new_dataset = copy.deepcopy(self)\r\n   3644 new_dataset._indices = indices.data\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:592, in transmit_tasks.<locals>.wrapper(*args, **kwargs)\r\n    590     self: \"Dataset\" = kwargs.pop(\"self\")\r\n    591 # apply actual function\r\n--> 592 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    593 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    594 for dataset in datasets:\r\n    595     # Remove task templates if a column mapping of the template is no longer valid\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:557, in transmit_format.<locals>.wrapper(*args, **kwargs)\r\n    550 self_format = {\r\n    551     \"type\": self._format_type,\r\n    552     \"format_kwargs\": self._format_kwargs,\r\n    553     \"columns\": self._format_columns,\r\n    554     \"output_all_columns\": self._output_all_columns,\r\n    555 }\r\n    556 # apply actual function\r\n--> 557 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    558 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    559 # re-apply format to the output\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:3093, in Dataset.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   3087 if transformed_dataset is None:\r\n   3088     with hf_tqdm(\r\n   3089         unit=\" examples\",\r\n   3090         total=pbar_total,\r\n   3091         desc=desc or \"Map\",\r\n   3092     ) as pbar:\r\n-> 3093         for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n   3094             if done:\r\n   3095                 shards_done += 1\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:3470, in Dataset._map_single(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\r\n   3466 indices = list(\r\n   3467     range(*(slice(i, i + batch_size).indices(shard.num_rows)))\r\n   3468 )  # Something simpler?\r\n   3469 try:\r\n-> 3470     batch = apply_function_on_filtered_inputs(\r\n   3471         batch,\r\n   3472         indices,\r\n   3473         check_same_num_examples=len(shard.list_indexes()) > 0,\r\n   3474         offset=offset,\r\n   3475     )\r\n   3476 except NumExamplesMismatchError:\r\n   3477     raise DatasetTransformationNotAllowedError(\r\n   3478         \"Using `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn't create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\"\r\n   3479     ) from None\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:3349, in Dataset._map_single.<locals>.apply_function_on_filtered_inputs(pa_inputs, indices, check_same_num_examples, offset)\r\n   3347 if with_rank:\r\n   3348     additional_args += (rank,)\r\n-> 3349 processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n   3350 if isinstance(processed_inputs, LazyDict):\r\n   3351     processed_inputs = {\r\n   3352         k: v for k, v in processed_inputs.data.items() if k not in processed_inputs.keys_to_format\r\n   3353     }\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:6212, in get_indices_from_mask_function(function, batched, with_indices, input_columns, indices_mapping, *args, **fn_kwargs)\r\n   6209 if input_columns is None:\r\n   6210     # inputs only contains a batch of examples\r\n   6211     batch: dict = inputs[0]\r\n-> 6212     num_examples = len(batch[next(iter(batch.keys()))])\r\n   6213     for i in range(num_examples):\r\n   6214         example = {key: batch[key][i] for key in batch}\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py:272, in LazyDict.__getitem__(self, key)\r\n    270 value = self.data[key]\r\n    271 if key in self.keys_to_format:\r\n--> 272     value = self.format(key)\r\n    273     self.data[key] = value\r\n    274     self.keys_to_format.remove(key)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py:375, in LazyBatch.format(self, key)\r\n    374 def format(self, key):\r\n--> 375     return self.formatter.format_column(self.pa_table.select([key]))\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py:442, in PythonFormatter.format_column(self, pa_table)\r\n    440 def format_column(self, pa_table: pa.Table) -> list:\r\n    441     column = self.python_arrow_extractor().extract_column(pa_table)\r\n--> 442     column = self.python_features_decoder.decode_column(column, pa_table.column_names[0])\r\n    443     return column\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py:218, in PythonFeaturesDecoder.decode_column(self, column, column_name)\r\n    217 def decode_column(self, column: list, column_name: str) -> list:\r\n--> 218     return self.features.decode_column(column, column_name) if self.features else column\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/features\/features.py:1951, in Features.decode_column(self, column, column_name)\r\n   1938 def decode_column(self, column: list, column_name: str):\r\n   1939     \"\"\"Decode column with custom feature decoding.\r\n   1940 \r\n   1941     Args:\r\n   (...)\r\n   1948         `list[Any]`\r\n   1949     \"\"\"\r\n   1950     return (\r\n-> 1951         [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n   1952         if self._column_requires_decoding[column_name]\r\n   1953         else column\r\n   1954     )\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/features\/features.py:1951, in <listcomp>(.0)\r\n   1938 def decode_column(self, column: list, column_name: str):\r\n   1939     \"\"\"Decode column with custom feature decoding.\r\n   1940 \r\n   1941     Args:\r\n   (...)\r\n   1948         `list[Any]`\r\n   1949     \"\"\"\r\n   1950     return (\r\n-> 1951         [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n   1952         if self._column_requires_decoding[column_name]\r\n   1953         else column\r\n   1954     )\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/features\/features.py:1339, in decode_nested_example(schema, obj, token_per_repo_id)\r\n   1336 elif isinstance(schema, (Audio, Image)):\r\n   1337     # we pass the token to read and decode files from private repositories in streaming mode\r\n   1338     if obj is not None and schema.decode:\r\n-> 1339         return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n   1340 return obj\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/features\/image.py:185, in Image.decode_example(self, value, token_per_repo_id)\r\n    183 else:\r\n    184     image = PIL.Image.open(BytesIO(bytes_))\r\n--> 185 image.load()  # to avoid \"Too many open files\" errors\r\n    186 return image\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/PIL\/ImageFile.py:254, in ImageFile.load(self)\r\n    252         break\r\n    253     else:\r\n--> 254         raise OSError(\r\n    255             \"image file is truncated \"\r\n    256             f\"({len(b)} bytes not processed)\"\r\n    257         )\r\n    259 b = b + s\r\n    260 n, err_code = decoder.decode(b)\r\n\r\nOSError: image file is truncated (1 bytes not processed)\r\n```\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"mehul7\/captioned_military_aircraft\")\r\n\r\nfrom transformers import AutoImageProcessor\r\n\r\ncheckpoint = \"microsoft\/resnet-50\"\r\nimage_processor = AutoImageProcessor.from_pretrained(checkpoint)\r\n\r\nimport re\r\nfrom PIL import Image\r\nimport io\r\n\r\ndef contains_number(example):\r\n    try:\r\n        image = Image.open(io.BytesIO(example[\"image\"]['bytes']))\r\n        t = image_processor(images=image, return_tensors=\"pt\")['pixel_values']\r\n    except Exception as e:\r\n        print(f\"Error processing image\uff1a{example['text']}\")\r\n        return False\r\n    return bool(re.search(r'\\d', example['text']))\r\n\r\n# Define a function to add the 'label' field\r\ndef add_label(example):\r\n    lab = example['text'].split()\r\n    temp = 'NOT'\r\n    for item in lab:\r\n        if str(item[-1]).isdigit():\r\n            temp = item\r\n            break\r\n    example['label'] = temp\r\n    return example\r\n\r\n# Filter the dataset\r\n# filtered_dataset = dataset.filter(contains_number)\r\n# Add the 'label' field in the dataset\r\nlabeled_dataset = dataset.filter(contains_number).map(add_label)\r\n# View the structure of the updated dataset\r\nprint(labeled_dataset)\r\n```\r\n\r\n\n\n### Expected behavior\n\nneeds to form labels\r\nsame as : https:\/\/www.kaggle.com\/code\/jiabaowangts\/dataset-air\/notebook\n\n### Environment info\n\nKaggle notebook P100 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6557","id":2064341965,"node_id":"PR_kwDODunzps5jJ63z","number":6557,"title":"Support standalone yaml","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-03T16:47:35Z","updated_at":"2024-01-03T17:45:47Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"see (internal) https:\/\/huggingface.slack.com\/archives\/C02V51Q3800\/p1703885853581679","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6557","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6557","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6557.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6557.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6556","id":2064018208,"node_id":"PR_kwDODunzps5jI0nN","number":6556,"title":"Fix imagefolder with one image","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-03T13:13:02Z","updated_at":"2024-01-03T18:55:41Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"A dataset repository with one image and one metadata file was considered a JSON dataset instead of an ImageFolder dataset. This is because we pick the dataset type with the most compatible data file extensions present in the repository and it results in a tie in this case.\r\n\r\ne.g. for https:\/\/huggingface.co\/datasets\/multimodalart\/repro_1_image\r\n\r\nI fixed this by deprioritizing metadata files in the count.\r\n\r\nfix https:\/\/github.com\/huggingface\/datasets\/issues\/6545","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6556","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6556","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6556.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6556.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6555","id":2063841286,"node_id":"PR_kwDODunzps5jIM79","number":6555,"title":"Do not use Parquet exports if revision is passed","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2024-01-03T11:33:10Z","updated_at":"2024-01-03T12:44:05Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Fix #6554.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6555","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6555","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6555.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6555.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6554","id":2063839916,"node_id":"I_kwDODunzps57A7Ks","number":6554,"title":"Parquet exports are used even if revision is passed","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2024-01-03T11:32:26Z","updated_at":"2024-01-03T13:23:55Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"We should not used Parquet exports if `revision` is passed.\r\n\r\nI think this is a regression.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6553","id":2063474183,"node_id":"I_kwDODunzps56_h4H","number":6553,"title":"Cannot import name 'load_dataset' from .... module \u2018datasets\u2019","user":{"login":"ciaoyizhen","id":83450192,"node_id":"MDQ6VXNlcjgzNDUwMTky","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/83450192?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ciaoyizhen","html_url":"https:\/\/github.com\/ciaoyizhen","followers_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/followers","following_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/orgs","repos_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/repos","events_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-03T08:18:21Z","updated_at":"2024-01-03T08:25:19Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n\r\nuse python -m pip install datasets to install\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\n\n### Expected behavior\n\nit doesn't work\n\n### Environment info\n\ndatasets version==2.15.0\r\npython == 3.10.12\r\nlinux  version I don't know??","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6552","id":2063157187,"node_id":"I_kwDODunzps56-UfD","number":6552,"title":"Loading a dataset from Google Colab hangs at \"Resolving data files\".","user":{"login":"KelSolaar","id":99779,"node_id":"MDQ6VXNlcjk5Nzc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/99779?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/KelSolaar","html_url":"https:\/\/github.com\/KelSolaar","followers_url":"https:\/\/api.github.com\/users\/KelSolaar\/followers","following_url":"https:\/\/api.github.com\/users\/KelSolaar\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/KelSolaar\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/KelSolaar\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/KelSolaar\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/KelSolaar\/orgs","repos_url":"https:\/\/api.github.com\/users\/KelSolaar\/repos","events_url":"https:\/\/api.github.com\/users\/KelSolaar\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/KelSolaar\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2024-01-03T02:18:17Z","updated_at":"2024-01-06T05:04:36Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHello,\r\n\r\nI'm trying to load a dataset from Google Colab but the process hangs at `Resolving data files`:\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/99779\/7175ad85-e571-46ed-9f87-92653985777d)\r\n\r\nIt is happening when the `_get_origin_metadata` definition is invoked:\r\n\r\n```python\r\ndef _get_origin_metadata(\r\n    data_files: List[str],\r\n    max_workers=64,\r\n    download_config: Optional[DownloadConfig] = None,\r\n) -> Tuple[str]:\r\n    return thread_map(\r\n        partial(_get_single_origin_metadata, download_config=download_config),\r\n        data_files,\r\n        max_workers=max_workers,\r\n        tqdm_class=hf_tqdm,\r\n        desc=\"Resolving data files\",\r\n        disable=len(data_files) <= 16,\r\n```\r\n\r\nThe thread is then stuck at `waiter.acquire()` in the builtin `threading.py` file.\r\n\r\nI can load the dataset just fine on my machine.\r\n\r\nCheers,\r\n\r\nThomas\r\n\n\n### Steps to reproduce the bug\n\nIn Google Colab:\r\n\r\n```python\r\n!pip install datasets\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"colour-science\/color-checker-detection-dataset\")\r\n```\n\n### Expected behavior\n\nThe dataset should be loaded.\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.1\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6551","id":2062768400,"node_id":"PR_kwDODunzps5jEi1C","number":6551,"title":"Fix parallel downloads for datasets without scripts","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2024-01-02T18:06:18Z","updated_at":"2024-01-06T20:14:57Z","closed_at":"2024-01-03T13:19:48Z","author_association":"MEMBER","active_lock_reason":null,"body":"Enable parallel downloads using multiprocessing when `num_proc` is passed to `load_dataset`.\r\n\r\nIt was enabled for datasets with scripts already (if they passed lists to `dl_manager.download`) but not for no-script datasets (we pass dicts {split: [list of files]} to `dl_manager.download` for those ones).\r\n\r\nI fixed this by parallelising on the lists contained in the data files dicts when possible.\r\n\r\nI also added a context manager `stack_multiprocessing_download_progress_bars` in `DownloadManager` to stack the progress bard of the downloads (from `cached_path(...)` calls). Otherwise the progress bars overlap each other with an annoying flickering effect.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551\/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":2,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6551","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6551","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6551.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6551.patch","merged_at":"2024-01-03T13:19:47Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6550","id":2062556493,"node_id":"PR_kwDODunzps5jD1OL","number":6550,"title":"Multi gpu docs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-02T15:11:58Z","updated_at":"2024-01-02T15:16:56Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"after discussions in https:\/\/github.com\/huggingface\/datasets\/pull\/6415","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6550","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6550","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6550.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6550.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6549","id":2062420259,"node_id":"I_kwDODunzps567gkj","number":6549,"title":"Loading from hf hub with clearer error message","user":{"login":"thomwolf","id":7353373,"node_id":"MDQ6VXNlcjczNTMzNzM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7353373?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomwolf","html_url":"https:\/\/github.com\/thomwolf","followers_url":"https:\/\/api.github.com\/users\/thomwolf\/followers","following_url":"https:\/\/api.github.com\/users\/thomwolf\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomwolf\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomwolf\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomwolf\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomwolf\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomwolf\/repos","events_url":"https:\/\/api.github.com\/users\/thomwolf\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomwolf\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-02T13:26:34Z","updated_at":"2024-01-02T14:06:49Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"### Feature request\n\nShouldn't this kinda work ?\r\n```\r\nDataset.from_json(\"hf:\/\/datasets\/HuggingFaceTB\/eval_data\/resolve\/main\/eval_data_context_and_answers.json\")\r\n```\r\nI got an error\r\n```\r\nFile ~\/miniconda3\/envs\/datatrove\/lib\/python3.10\/site-packages\/datasets\/data_files.py:380, in resolve_pattern(pattern, base_path, allowed_extensions, download_config)\r\n    378     if allowed_extensions is not None:\r\n    379         error_msg += f\" with any supported extension {list(allowed_extensions)}\"\r\n--> 380     raise FileNotFoundError(error_msg)\r\n    381 return out\r\n\r\nFileNotFoundError: Unable to find 'hf:\/\/datasets\/HuggingFaceTB\/eval_data\/resolve\/main\/eval_data_context_and_answers.json'\r\n(I'm logged in)\r\n```\r\n\r\nFix: the correct path is\r\n```\r\nhf:\/\/datasets\/HuggingFaceTB\/eval_data\/eval_data_context_and_answers.json\r\n```\r\n\r\nProposal: raise a clearer error\n\n### Motivation\n\nClearer error message\n\n### Your contribution\n\nCan open a PR","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6548","id":2061047984,"node_id":"I_kwDODunzps562Riw","number":6548,"title":"Skip if a dataset has issues","user":{"login":"hadianasliwa","id":143214684,"node_id":"U_kgDOCIlIXA","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/143214684?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/hadianasliwa","html_url":"https:\/\/github.com\/hadianasliwa","followers_url":"https:\/\/api.github.com\/users\/hadianasliwa\/followers","following_url":"https:\/\/api.github.com\/users\/hadianasliwa\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/hadianasliwa\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/hadianasliwa\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/hadianasliwa\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/hadianasliwa\/orgs","repos_url":"https:\/\/api.github.com\/users\/hadianasliwa\/repos","events_url":"https:\/\/api.github.com\/users\/hadianasliwa\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/hadianasliwa\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-31T12:41:26Z","updated_at":"2024-01-02T10:33:17Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHello everyone,\r\nI'm using **load_datasets** from **huggingface** to download the datasets and I'm facing an issue, the download starts but it reaches some state and then  fails with the following error:\r\nCouldn't reach https:\/\/huggingface.co\/datasets\/wikimedia\/wikipedia\/resolve\/4cb9b0d719291f1a10f96f67d609c5d442980dc9\/20231101.ext\/train-00000-of-00001.parquet\r\n\r\nFailed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))')))\r\n\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/143214684\/8847d9cb-529e-4eda-9c76-282713dfa3af)\r\n\r\nso I was wondering is there a parameter to be passed to load_dataset() to skip files that can't be downloaded??\n\n### Steps to reproduce the bug\n\nParameter to be passed to load_dataset() of huggingface to skip files that can't be downloaded??\n\n### Expected behavior\n\nload_dataset() finishes without error\n\n### Environment info\n\nNone","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6547","id":2060796927,"node_id":"PR_kwDODunzps5i-Jni","number":6547,"title":"set dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-30T16:47:17Z","updated_at":"2023-12-30T16:53:38Z","closed_at":"2023-12-30T16:47:27Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6547","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6547","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6547.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6547.patch","merged_at":"2023-12-30T16:47:27Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6546","id":2060796369,"node_id":"PR_kwDODunzps5i-Jgv","number":6546,"title":"Release: 2.16.1","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-30T16:44:51Z","updated_at":"2023-12-30T16:52:07Z","closed_at":"2023-12-30T16:45:52Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6546","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6546","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6546.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6546.patch","merged_at":"2023-12-30T16:45:52Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6545","id":2060789507,"node_id":"I_kwDODunzps561ScD","number":6545,"title":"`image` column not automatically inferred if image dataset only contains 1 image","user":{"login":"apolinario","id":788417,"node_id":"MDQ6VXNlcjc4ODQxNw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/788417?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/apolinario","html_url":"https:\/\/github.com\/apolinario","followers_url":"https:\/\/api.github.com\/users\/apolinario\/followers","following_url":"https:\/\/api.github.com\/users\/apolinario\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/apolinario\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/apolinario\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/apolinario\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/apolinario\/orgs","repos_url":"https:\/\/api.github.com\/users\/apolinario\/repos","events_url":"https:\/\/api.github.com\/users\/apolinario\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/apolinario\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-30T16:17:29Z","updated_at":"2024-01-01T11:49:48Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nBy default, the standard Image Dataset maps out `file_name` to `image` when loading an Image Dataset. \r\n\r\nHowever, if the dataset contains only 1 image, this does not take place \r\n\r\n### Steps to reproduce the bug\r\n\r\nInput\r\n(dataset with one image `multimodalart\/repro_1_image`) \r\n```py\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"multimodalart\/repro_1_image\")\r\ndataset\r\n```\r\n\r\nOutput: \r\n```py\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['file_name', 'prompt'],\r\n        num_rows: 1\r\n    })\r\n})\r\n```\r\n\r\nInput \r\n(dataset with 2+ images `multimodalart\/repro_2_image`) \r\n```py\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"multimodalart\/repro_2_image\")\r\ndataset\r\n```\r\n\r\nOutput:\r\n```py\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'prompt'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\n### Expected behavior\r\n\r\nExpected to map `file_name`  \u2192 `image` for all dataset sizes, including 1. \r\n\r\n### Environment info\r\n\r\nBoth latest main and 2.16.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6544","id":2060782594,"node_id":"PR_kwDODunzps5i-G4_","number":6544,"title":"Fix custom configs from script","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-30T15:51:25Z","updated_at":"2024-01-02T11:02:39Z","closed_at":"2023-12-30T16:09:49Z","author_association":"MEMBER","active_lock_reason":null,"body":"We should not use the parquet export when the user is passing config_kwargs\r\n\r\nI also fixed a regression that would disallow creating a custom config when a dataset has multiple predefined configs\r\n\r\nfix https:\/\/github.com\/huggingface\/datasets\/issues\/6533","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6544","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6544","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6544.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6544.patch","merged_at":"2023-12-30T16:09:49Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6543","id":2060776174,"node_id":"PR_kwDODunzps5i-Frx","number":6543,"title":"Fix dl_manager.extract returning FileNotFoundError","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-30T15:24:50Z","updated_at":"2023-12-30T16:00:06Z","closed_at":"2023-12-30T15:53:59Z","author_association":"MEMBER","active_lock_reason":null,"body":"The dl_manager base path is remote (e.g. a hf:\/\/ path), so local cached paths should be passed as absolute paths.\r\nThis could happen if users provide a relative path as `cache_dir`\r\n\r\nfix https:\/\/github.com\/huggingface\/datasets\/issues\/6536","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6543","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6543","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6543.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6543.patch","merged_at":"2023-12-30T15:53:59Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6542","id":2059198575,"node_id":"I_kwDODunzps56vOBv","number":6542,"title":"Datasets : wikipedia 20220301.en error ","user":{"login":"ppx666","id":53203620,"node_id":"MDQ6VXNlcjUzMjAzNjIw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/53203620?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ppx666","html_url":"https:\/\/github.com\/ppx666","followers_url":"https:\/\/api.github.com\/users\/ppx666\/followers","following_url":"https:\/\/api.github.com\/users\/ppx666\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ppx666\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ppx666\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ppx666\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ppx666\/orgs","repos_url":"https:\/\/api.github.com\/users\/ppx666\/repos","events_url":"https:\/\/api.github.com\/users\/ppx666\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ppx666\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-29T08:34:51Z","updated_at":"2024-01-02T13:21:06Z","closed_at":"2024-01-02T13:20:30Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I used load_dataset to download this data set, the following error occurred. The main problem was that the target data did not exist.\n\n### Steps to reproduce the bug\n\n1.I tried downloading directly.\r\n```python\r\nwiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\")\r\n```\r\nAn exception occurred\r\n```\r\nMissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https:\/\/beam.apache.org\/documentation\/runners\/capability-matrix\/\r\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \r\nExample of usage: \r\n\t`load_dataset('wikipedia', '20220301.en', beam_runner='DirectRunner')`\r\n```\r\n2.I modified the code as prompted.\r\n```python\r\nwiki_dataset = load_dataset('wikipedia', '20220301.en', beam_runner='DirectRunner')\r\n```\r\nAn exception occurred:\r\n```\r\nFileNotFoundError: Couldn't find file at https:\/\/dumps.wikimedia.org\/enwiki\/20220301\/dumpstatus.json\r\n```\r\n\n\n### Expected behavior\n\nI searched in the parent directory of the corresponding URL, but there was no corresponding \"20220301\" directory.\r\nI really need this data set and hope to provide a download method.\n\n### Environment info\n\npython 3.8\r\ndatasets 2.16.0\r\napache-beam 2.52.0\r\ndill 0.3.7\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6541","id":2058983826,"node_id":"I_kwDODunzps56uZmS","number":6541,"title":"Dataset not loading successfully.","user":{"login":"hi-sushanta","id":93595990,"node_id":"U_kgDOBZQpVg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93595990?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/hi-sushanta","html_url":"https:\/\/github.com\/hi-sushanta","followers_url":"https:\/\/api.github.com\/users\/hi-sushanta\/followers","following_url":"https:\/\/api.github.com\/users\/hi-sushanta\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/hi-sushanta\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/hi-sushanta\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/hi-sushanta\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/hi-sushanta\/orgs","repos_url":"https:\/\/api.github.com\/users\/hi-sushanta\/repos","events_url":"https:\/\/api.github.com\/users\/hi-sushanta\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/hi-sushanta\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-29T01:35:47Z","updated_at":"2023-12-29T01:35:47Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I run down the below code shows this error: AttributeError: module 'numpy' has no attribute '_no_nep50_warning'\r\nI also added this issue in transformers library please check out: [link](https:\/\/github.com\/huggingface\/transformers\/issues\/28099)\n\n### Steps to reproduce the bug\n\n## Reproduction\r\nHi, please check this line of code, when I run Show attribute error.\r\n\r\n```\r\nfrom datasets import load_dataset\r\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\r\n\r\n# Select an audio file and read it:\r\nds = load_dataset(\"hf-internal-testing\/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\naudio_sample = ds[0][\"audio\"]\r\nwaveform = audio_sample[\"array\"]\r\nsampling_rate = audio_sample[\"sampling_rate\"]\r\n\r\n# Load the Whisper model in Hugging Face format:\r\nprocessor = WhisperProcessor.from_pretrained(\"openai\/whisper-tiny.en\")\r\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai\/whisper-tiny.en\")\r\n\r\n# Use the model and processor to transcribe the audio:\r\ninput_features = processor(\r\n    waveform, sampling_rate=sampling_rate, return_tensors=\"pt\"\r\n).input_features\r\n\r\n# Generate token ids\r\npredicted_ids = model.generate(input_features)\r\n\r\n# Decode token ids to text\r\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\r\n\r\ntranscription[0]\r\n\r\n```\r\n\r\n**Attribute Error**\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[9], line 6\r\n      4 # Select an audio file and read it:\r\n      5 ds = load_dataset(\"hf-internal-testing\/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\n----> 6 audio_sample = ds[0][\"audio\"]\r\n      7 waveform = audio_sample[\"array\"]\r\n      8 sampling_rate = audio_sample[\"sampling_rate\"]\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py:2795, in Dataset.__getitem__(self, key)\r\n   2793 def __getitem__(self, key):  # noqa: F811\r\n   2794     \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 2795     return self._getitem(key)\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py:2780, in Dataset._getitem(self, key, **kwargs)\r\n   2778 formatter = get_formatter(format_type, features=self._info.features, **format_kwargs)\r\n   2779 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n-> 2780 formatted_output = format_table(\r\n   2781     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n   2782 )\r\n   2783 return formatted_output\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py:629, in format_table(table, key, formatter, format_columns, output_all_columns)\r\n    627 python_formatter = PythonFormatter(features=formatter.features)\r\n    628 if format_columns is None:\r\n--> 629     return formatter(pa_table, query_type=query_type)\r\n    630 elif query_type == \"column\":\r\n    631     if key in format_columns:\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py:396, in Formatter.__call__(self, pa_table, query_type)\r\n    394 def __call__(self, pa_table: pa.Table, query_type: str) -> Union[RowFormat, ColumnFormat, BatchFormat]:\r\n    395     if query_type == \"row\":\r\n--> 396         return self.format_row(pa_table)\r\n    397     elif query_type == \"column\":\r\n    398         return self.format_column(pa_table)\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py:437, in PythonFormatter.format_row(self, pa_table)\r\n    435     return LazyRow(pa_table, self)\r\n    436 row = self.python_arrow_extractor().extract_row(pa_table)\r\n--> 437 row = self.python_features_decoder.decode_row(row)\r\n    438 return row\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py:215, in PythonFeaturesDecoder.decode_row(self, row)\r\n    214 def decode_row(self, row: dict) -> dict:\r\n--> 215     return self.features.decode_example(row) if self.features else row\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/features\/features.py:1917, in Features.decode_example(self, example, token_per_repo_id)\r\n   1903 def decode_example(self, example: dict, token_per_repo_id: Optional[Dict[str, Union[str, bool, None]]] = None):\r\n   1904     \"\"\"Decode example with custom feature decoding.\r\n   1905 \r\n   1906     Args:\r\n   (...)\r\n   1914         `dict[str, Any]`\r\n   1915     \"\"\"\r\n-> 1917     return {\r\n   1918         column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\r\n   1919         if self._column_requires_decoding[column_name]\r\n   1920         else value\r\n   1921         for column_name, (feature, value) in zip_dict(\r\n   1922             {key: value for key, value in self.items() if key in example}, example\r\n   1923         )\r\n   1924     }\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/features\/features.py:1918, in <dictcomp>(.0)\r\n   1903 def decode_example(self, example: dict, token_per_repo_id: Optional[Dict[str, Union[str, bool, None]]] = None):\r\n   1904     \"\"\"Decode example with custom feature decoding.\r\n   1905 \r\n   1906     Args:\r\n   (...)\r\n   1914         `dict[str, Any]`\r\n   1915     \"\"\"\r\n   1917     return {\r\n-> 1918         column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\r\n   1919         if self._column_requires_decoding[column_name]\r\n   1920         else value\r\n   1921         for column_name, (feature, value) in zip_dict(\r\n   1922             {key: value for key, value in self.items() if key in example}, example\r\n   1923         )\r\n   1924     }\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/features\/features.py:1339, in decode_nested_example(schema, obj, token_per_repo_id)\r\n   1336 elif isinstance(schema, (Audio, Image)):\r\n   1337     # we pass the token to read and decode files from private repositories in streaming mode\r\n   1338     if obj is not None and schema.decode:\r\n-> 1339         return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n   1340 return obj\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/features\/audio.py:191, in Audio.decode_example(self, value, token_per_repo_id)\r\n    189 array = array.T\r\n    190 if self.mono:\r\n--> 191     array = librosa.to_mono(array)\r\n    192 if self.sampling_rate and self.sampling_rate != sampling_rate:\r\n    193     array = librosa.resample(array, orig_sr=sampling_rate, target_sr=self.sampling_rate)\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/lazy_loader\/__init__.py:78, in attach.<locals>.__getattr__(name)\r\n     76 submod_path = f\"{package_name}.{attr_to_modules[name]}\"\r\n     77 submod = importlib.import_module(submod_path)\r\n---> 78 attr = getattr(submod, name)\r\n     80 # If the attribute lives in a file (module) with the same\r\n     81 # name as the attribute, ensure that the attribute and *not*\r\n     82 # the module is accessible on the package.\r\n     83 if name == attr_to_modules[name]:\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/lazy_loader\/__init__.py:77, in attach.<locals>.__getattr__(name)\r\n     75 elif name in attr_to_modules:\r\n     76     submod_path = f\"{package_name}.{attr_to_modules[name]}\"\r\n---> 77     submod = importlib.import_module(submod_path)\r\n     78     attr = getattr(submod, name)\r\n     80     # If the attribute lives in a file (module) with the same\r\n     81     # name as the attribute, ensure that the attribute and *not*\r\n     82     # the module is accessible on the package.\r\n\r\nFile \/usr\/lib\/python3.8\/importlib\/__init__.py:127, in import_module(name, package)\r\n    125             break\r\n    126         level += 1\r\n--> 127 return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\nFile <frozen importlib._bootstrap>:1014, in _gcd_import(name, package, level)\r\n\r\nFile <frozen importlib._bootstrap>:991, in _find_and_load(name, import_)\r\n\r\nFile <frozen importlib._bootstrap>:975, in _find_and_load_unlocked(name, import_)\r\n\r\nFile <frozen importlib._bootstrap>:671, in _load_unlocked(spec)\r\n\r\nFile <frozen importlib._bootstrap_external>:848, in exec_module(self, module)\r\n\r\nFile <frozen importlib._bootstrap>:219, in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/librosa\/core\/audio.py:13\r\n     11 import audioread\r\n     12 import numpy as np\r\n---> 13 import scipy.signal\r\n     14 import soxr\r\n     15 import lazy_loader as lazy\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/signal\/__init__.py:323\r\n    314 from ._spline import (  # noqa: F401\r\n    315     cspline2d,\r\n    316     qspline2d,\r\n   (...)\r\n    319     symiirorder2,\r\n    320 )\r\n    322 from ._bsplines import *\r\n--> 323 from ._filter_design import *\r\n    324 from ._fir_filter_design import *\r\n    325 from ._ltisys import *\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/signal\/_filter_design.py:16\r\n     13 from numpy.polynomial.polynomial import polyval as npp_polyval\r\n     14 from numpy.polynomial.polynomial import polyvalfromroots\r\n---> 16 from scipy import special, optimize, fft as sp_fft\r\n     17 from scipy.special import comb\r\n     18 from scipy._lib._util import float_factorial\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/optimize\/__init__.py:405\r\n      1 \"\"\"\r\n      2 =====================================================\r\n      3 Optimization and root finding (:mod:`scipy.optimize`)\r\n   (...)\r\n    401 \r\n    402 \"\"\"\r\n    404 from ._optimize import *\r\n--> 405 from ._minimize import *\r\n    406 from ._root import *\r\n    407 from ._root_scalar import *\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/optimize\/_minimize.py:26\r\n     24 from ._trustregion_krylov import _minimize_trust_krylov\r\n     25 from ._trustregion_exact import _minimize_trustregion_exact\r\n---> 26 from ._trustregion_constr import _minimize_trustregion_constr\r\n     28 # constrained minimization\r\n     29 from ._lbfgsb_py import _minimize_lbfgsb\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/optimize\/_trustregion_constr\/__init__.py:4\r\n      1 \"\"\"This module contains the equality constrained SQP solver.\"\"\"\r\n----> 4 from .minimize_trustregion_constr import _minimize_trustregion_constr\r\n      6 __all__ = ['_minimize_trustregion_constr']\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/optimize\/_trustregion_constr\/minimize_trustregion_constr.py:5\r\n      3 from scipy.sparse.linalg import LinearOperator\r\n      4 from .._differentiable_functions import VectorFunction\r\n----> 5 from .._constraints import (\r\n      6     NonlinearConstraint, LinearConstraint, PreparedConstraint, strict_bounds)\r\n      7 from .._hessian_update_strategy import BFGS\r\n      8 from .._optimize import OptimizeResult\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/optimize\/_constraints.py:8\r\n      6 from ._optimize import OptimizeWarning\r\n      7 from warnings import warn, catch_warnings, simplefilter\r\n----> 8 from numpy.testing import suppress_warnings\r\n      9 from scipy.sparse import issparse\r\n     12 def _arr_to_scalar(x):\r\n     13     # If x is a numpy array, return x.item().  This will\r\n     14     # fail if the array has more than one element.\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/numpy\/testing\/__init__.py:11\r\n      8 from unittest import TestCase\r\n     10 from . import _private\r\n---> 11 from ._private.utils import *\r\n     12 from ._private.utils import (_assert_valid_refcount, _gen_alignment_data)\r\n     13 from ._private import extbuild, decorators as dec\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/numpy\/testing\/_private\/utils.py:480\r\n    476         pprint.pprint(desired, msg)\r\n    477         raise AssertionError(msg.getvalue())\r\n--> 480 @np._no_nep50_warning()\r\n    481 def assert_almost_equal(actual,desired,decimal=7,err_msg='',verbose=True):\r\n    482     \"\"\"\r\n    483     Raises an AssertionError if two items are not equal up to desired\r\n    484     precision.\r\n   (...)\r\n    548 \r\n    549     \"\"\"\r\n    550     __tracebackhide__ = True  # Hide traceback for py.test\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/numpy\/__init__.py:313, in __getattr__(attr)\r\n    305     raise AttributeError(__former_attrs__[attr])\r\n    307 # Importing Tester requires importing all of UnitTest which is not a\r\n    308 # cheap import Since it is mainly used in test suits, we lazy import it\r\n    309 # here to save on the order of 10 ms of import time for most users\r\n    310 #\r\n    311 # The previous way Tester was imported also had a side effect of adding\r\n    312 # the full `numpy.testing` namespace\r\n--> 313 if attr == 'testing':\r\n    314     import numpy.testing as testing\r\n    315     return testing\r\n\r\nAttributeError: module 'numpy' has no attribute '_no_nep50_warning'\r\n\r\n```\n\n### Expected behavior\n\n``` ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.' ```\r\n\r\nAlso, make sure this script is provided for your official website so please update:\r\n[script](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/whisper)\n\n### Environment info\n\n**System Info**\r\n* transformers -> 4.36.1\r\n* datasets -> 2.15.0\r\n* huggingface_hub -> 0.19.4\r\n* python -> 3.8.10\r\n* accelerate -> 0.25.0\r\n* pytorch -> 2.0.1+cpu\r\n* Using GPU in Script -> No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6540","id":2058965157,"node_id":"I_kwDODunzps56uVCl","number":6540,"title":"Extreme inefficiency for `save_to_disk` when merging datasets","user":{"login":"KatarinaYuan","id":43512683,"node_id":"MDQ6VXNlcjQzNTEyNjgz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/43512683?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/KatarinaYuan","html_url":"https:\/\/github.com\/KatarinaYuan","followers_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/followers","following_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/orgs","repos_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/repos","events_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-29T00:44:35Z","updated_at":"2023-12-30T15:05:48Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi, I tried to merge in total 22M sequences of data, where each sequence is of maximum length 2000. I found that merging these datasets and then `save_to_disk` is extremely slow because of flattening the indices. Wondering if you have any suggestions or guidance on this. Thank you very much! \n\n### Steps to reproduce the bug\n\nThe source data is too big to demonstrate\n\n### Expected behavior\n\nThe source data is too big to demonstrate\n\n### Environment info\n\npython                     3.9.0\r\ndatasets                  2.7.0\r\npytorch                   2.0.0 \r\ntokenizers                0.13.1\r\ntransformers              4.31.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6539","id":2058493960,"node_id":"I_kwDODunzps56siAI","number":6539,"title":"'Repo card metadata block was not found' when loading a pragmeval dataset","user":{"login":"lambdaofgod","id":3647577,"node_id":"MDQ6VXNlcjM2NDc1Nzc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3647577?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lambdaofgod","html_url":"https:\/\/github.com\/lambdaofgod","followers_url":"https:\/\/api.github.com\/users\/lambdaofgod\/followers","following_url":"https:\/\/api.github.com\/users\/lambdaofgod\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lambdaofgod\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lambdaofgod\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lambdaofgod\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lambdaofgod\/orgs","repos_url":"https:\/\/api.github.com\/users\/lambdaofgod\/repos","events_url":"https:\/\/api.github.com\/users\/lambdaofgod\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lambdaofgod\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-28T14:18:25Z","updated_at":"2023-12-28T14:18:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI can't load dataset subsets of 'pragmeval'.\r\n\r\nThe funny thing is I ran the dataset author's [colab notebook](https:\/\/colab.research.google.com\/drive\/1sg--LF4z7XR1wxAOfp0-3d4J6kQ9nj_A?usp=sharing) and it works just fine. I tried to install exactly the same packages that are installed on colab using poetry, so my environment info only differs from the one from colab in linux version - I still get the same bug outside colab.\n\n### Steps to reproduce the bug\n\nInstall dependencies with poetry\r\n\r\npyproject.toml\r\n```\r\n[tool.poetry]\r\nname = \"project\"\r\nversion = \"0.1.0\"\r\ndescription = \"\"\r\nauthors = []\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.10\"\r\ndatasets = \"2.16.0\"\r\npandas = \"1.5.3\"\r\npyarrow = \"10.0.1\"\r\nhuggingface-hub = \"0.19.4\"\r\nfsspec = \"2023.6.0\"\r\n\r\n\r\n[build-system]\r\nrequires = [\"poetry-core\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n```\r\n\r\n`poetry run python -c \"import datasets; print(datasets.get_dataset_config_names('pragmeval'))`\r\n\r\nprints ['default']\n\n### Expected behavior\n\nThe command should print\r\n\r\n```\r\n['emergent',\r\n 'emobank-arousal',\r\n 'emobank-dominance',\r\n 'emobank-valence',\r\n 'gum',\r\n 'mrda',\r\n 'pdtb',\r\n 'persuasiveness-claimtype',\r\n 'persuasiveness-eloquence',\r\n 'persuasiveness-premisetype',\r\n 'persuasiveness-relevance',\r\n 'persuasiveness-specificity',\r\n 'persuasiveness-strength',\r\n 'sarcasm',\r\n 'squinky-formality',\r\n 'squinky-implicature',\r\n 'squinky-informativeness',\r\n 'stac',\r\n 'switchboard',\r\n 'verifiability']\r\n```\n\n### Environment info\n\n- `datasets` version: 2.16.0\r\n- Platform: Linux-6.2.0-37-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6538","id":2057377630,"node_id":"I_kwDODunzps56oRde","number":6538,"title":"ImportError: cannot import name 'SchemaInferenceError' from 'datasets.arrow_writer' (\/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py)","user":{"login":"Sonali-Behera-TRT","id":131662185,"node_id":"U_kgDOB9kBaQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/131662185?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT","html_url":"https:\/\/github.com\/Sonali-Behera-TRT","followers_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/followers","following_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/orgs","repos_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/repos","events_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":15,"created_at":"2023-12-27T13:31:16Z","updated_at":"2024-01-03T10:06:47Z","closed_at":"2024-01-03T10:04:58Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhile importing from packages getting the error\r\nCode:\r\n\r\n```\r\nimport os\r\nimport torch\r\nfrom datasets import load_dataset, Dataset\r\nfrom transformers import (\r\n    AutoModelForCausalLM,\r\n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n    HfArgumentParser,\r\n    TrainingArguments,\r\n    pipeline,\r\n    logging\r\n)\r\nfrom peft import LoraConfig, PeftModel\r\nfrom trl import SFTTrainer\r\nfrom huggingface_hub import login\r\nimport pandas as pd\r\n```\r\n\r\nError:\r\n````\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nCell In[5], line 14\r\n      4 from transformers import (\r\n      5     AutoModelForCausalLM,\r\n      6     AutoTokenizer,\r\n   (...)\r\n     11     logging\r\n     12 )\r\n     13 from peft import LoraConfig, PeftModel\r\n---> 14 from trl import SFTTrainer\r\n     15 from huggingface_hub import login\r\n     16 import pandas as pd\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/trl\/__init__.py:21\r\n      8 from .import_utils import (\r\n      9     is_diffusers_available,\r\n     10     is_npu_available,\r\n   (...)\r\n     13     is_xpu_available,\r\n     14 )\r\n     15 from .models import (\r\n     16     AutoModelForCausalLMWithValueHead,\r\n     17     AutoModelForSeq2SeqLMWithValueHead,\r\n     18     PreTrainedModelWrapper,\r\n     19     create_reference_model,\r\n     20 )\r\n---> 21 from .trainer import (\r\n     22     DataCollatorForCompletionOnlyLM,\r\n     23     DPOTrainer,\r\n     24     IterativeSFTTrainer,\r\n     25     PPOConfig,\r\n     26     PPOTrainer,\r\n     27     RewardConfig,\r\n     28     RewardTrainer,\r\n     29     SFTTrainer,\r\n     30 )\r\n     33 if is_diffusers_available():\r\n     34     from .models import (\r\n     35         DDPOPipelineOutput,\r\n     36         DDPOSchedulerOutput,\r\n     37         DDPOStableDiffusionPipeline,\r\n     38         DefaultDDPOStableDiffusionPipeline,\r\n     39     )\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/trl\/trainer\/__init__.py:44\r\n     42 from .ppo_trainer import PPOTrainer\r\n     43 from .reward_trainer import RewardTrainer, compute_accuracy\r\n---> 44 from .sft_trainer import SFTTrainer\r\n     45 from .training_configs import RewardConfig\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/trl\/trainer\/sft_trainer.py:23\r\n     21 import torch.nn as nn\r\n     22 from datasets import Dataset\r\n---> 23 from datasets.arrow_writer import SchemaInferenceError\r\n     24 from datasets.builder import DatasetGenerationError\r\n     25 from transformers import (\r\n     26     AutoModelForCausalLM,\r\n     27     AutoTokenizer,\r\n   (...)\r\n     33     TrainingArguments,\r\n     34 )\r\n\r\nImportError: cannot import name 'SchemaInferenceError' from 'datasets.arrow_writer' (\/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\r\n\r\n````\r\n\r\ntransformers version: 4.36.2\r\npython version: 3.10.12\r\ndatasets version: 2.16.1\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Install packages\r\n```\r\n!pip install -U datasets trl accelerate peft bitsandbytes transformers trl huggingface_hub\r\n```\r\n\r\n2. import packages\r\n```\r\nimport os\r\nimport torch\r\nfrom datasets import load_dataset, Dataset\r\nfrom transformers import (\r\n    AutoModelForCausalLM,\r\n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n    HfArgumentParser,\r\n    TrainingArguments,\r\n    pipeline,\r\n    logging\r\n)\r\nfrom peft import LoraConfig, PeftModel\r\nfrom trl import SFTTrainer\r\nfrom huggingface_hub import login\r\nimport pandas as pd\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo error while importing\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.16.0\r\n- Platform: Linux-5.15.133+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6537","id":2057132173,"node_id":"I_kwDODunzps56nViN","number":6537,"title":"Adding support for netCDF (*.nc) files","user":{"login":"shermansiu","id":12627125,"node_id":"MDQ6VXNlcjEyNjI3MTI1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12627125?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shermansiu","html_url":"https:\/\/github.com\/shermansiu","followers_url":"https:\/\/api.github.com\/users\/shermansiu\/followers","following_url":"https:\/\/api.github.com\/users\/shermansiu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shermansiu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shermansiu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shermansiu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shermansiu\/orgs","repos_url":"https:\/\/api.github.com\/users\/shermansiu\/repos","events_url":"https:\/\/api.github.com\/users\/shermansiu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shermansiu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-27T09:27:29Z","updated_at":"2023-12-27T20:46:53Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nnetCDF (*.nc) is a file format for storing multidimensional scientific data, which is used by packages like `xarray` (labelled multi-dimensional arrays in Python). It would be nice to have native support for netCDF in `datasets`.\n\n### Motivation\n\nWhen uploading *.nc files onto Huggingface Hub through the `datasets` API, I would like to be able to preview the dataset without converting it to another format.\n\n### Your contribution\n\nI can submit a PR, provided I have the time.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6536","id":2056863239,"node_id":"I_kwDODunzps56mT4H","number":6536,"title":"datasets.load_dataset raises FileNotFoundError for datasets==2.16.0","user":{"login":"ArvinZhuang","id":46237844,"node_id":"MDQ6VXNlcjQ2MjM3ODQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/46237844?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ArvinZhuang","html_url":"https:\/\/github.com\/ArvinZhuang","followers_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/followers","following_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/orgs","repos_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/repos","events_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-12-27T03:15:48Z","updated_at":"2023-12-30T18:58:04Z","closed_at":"2023-12-30T15:54:00Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nSeems `datasets.load_dataset` raises FileNotFoundError for some hub datasets with the latest `datasets==2.16.0`\n\n### Steps to reproduce the bug\n\nFor example `pip install datasets==2.16.0`\r\nthen\r\n\r\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset(\"wentingzhao\/anthropic-hh-first-prompt\", cache_dir='cache1')[\"train\"]\r\n\r\n```\r\n\r\nThis will raise:\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 2545, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 1003, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 1076, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/packaged_modules\/parquet\/parquet.py\", line 43, in _split_generators\r\n    data_files = dl_manager.download_and_extract(self.config.data_files)\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/download\/download_manager.py\", line 566, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/download\/download_manager.py\", line 539, in extract\r\n    extracted_paths = map_nested(\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 466, in map_nested\r\n    mapped = [\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 467, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True, None))\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 387, in _single_map_nested\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 387, in <listcomp>\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 370, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/download\/download_manager.py\", line 451, in _download\r\n    out = cached_path(url_or_filename, download_config=download_config)\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/file_utils.py\", line 188, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/file_utils.py\", line 570, in get_from_cache\r\n    raise FileNotFoundError(f\"Couldn't find file at {url}\")\r\nFileNotFoundError: Couldn't find file at https:\/\/huggingface.co\/datasets\/wentingzhao\/anthropic-hh-first-prompt\/resolve\/11b393a5545f706a357ebcd4a5285d93db176715\/cache1\/downloads\/87d66c365626feca116cba323c4856c9aae056e4503f09f23e34aa085eb9de15\r\n```\r\n\r\nHowever, seems it works fine for some datasets, for example, if works fine for `datasets.load_dataset(\"ag_news\", cache_dir='cache2')[\"test\"]`\r\n\r\nBut the dataset works fine for datasets==2.15.0, for example `pip install datasets==2.15.0`,\r\nthen\r\n\r\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset(\"wentingzhao\/anthropic-hh-first-prompt\", cache_dir='cache3')[\"train\"]\r\n\r\nDataset({\r\n    features: ['user', 'system', 'source'],\r\n    num_rows: 8552\r\n})\r\n```\r\n\r\n\r\n\n\n### Expected behavior\n\n2.16.0 should work as same as 2.15.0 for all datasets\n\n### Environment info\n\npython3.9\r\nconda env\r\ntested on MacOS and Linux","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6535","id":2056264339,"node_id":"I_kwDODunzps56kBqT","number":6535,"title":"IndexError: Invalid key: 47682 is out of bounds for size 0 while using PEFT","user":{"login":"MahavirDabas18","id":57484266,"node_id":"MDQ6VXNlcjU3NDg0MjY2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57484266?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/MahavirDabas18","html_url":"https:\/\/github.com\/MahavirDabas18","followers_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/followers","following_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/orgs","repos_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/repos","events_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-26T10:14:33Z","updated_at":"2024-01-02T13:26:45Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am trying to fine-tune the t5 model on the paraphrasing task. While running the same code without-\r\n\r\nmodel = get_peft_model(model, config)\r\n\r\nthe model trains without any issues. However, using the model returned from get_peft_model raises the following error due to datasets-\r\n\r\nIndexError: Invalid key: 47682 is out of bounds for size 0.\r\n\r\nI had raised this in https:\/\/github.com\/huggingface\/peft\/issues\/1299#issue-2056173386 and they suggested that I raise it here.\r\n\r\nHere is the complete error-\r\n\r\nIndexError Traceback (most recent call last)\r\nin <cell line: 1>()\r\n----> 1 trainer.train()\r\n\r\n11 frames\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/trainer.py](https:\/\/localhost:8080\/#) in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\r\n1553 hf_hub_utils.enable_progress_bars()\r\n1554 else:\r\n-> 1555 return inner_training_loop(\r\n1556 args=args,\r\n1557 resume_from_checkpoint=resume_from_checkpoint,\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/trainer.py](https:\/\/localhost:8080\/#) in _inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\r\n1836\r\n1837 step = -1\r\n-> 1838 for step, inputs in enumerate(epoch_iterator):\r\n1839 total_batched_samples += 1\r\n1840 if rng_to_sync:\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/accelerate\/data_loader.py](https:\/\/localhost:8080\/#) in iter(self)\r\n446 # We iterate one batch ahead to check when we are at the end\r\n447 try:\r\n--> 448 current_batch = next(dataloader_iter)\r\n449 except StopIteration:\r\n450 yield\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/utils\/data\/dataloader.py](https:\/\/localhost:8080\/#) in next(self)\r\n628 # TODO(https:\/\/github.com\/pytorch\/pytorch\/issues\/76750)\r\n629 self._reset() # type: ignore[call-arg]\r\n--> 630 data = self._next_data()\r\n631 self._num_yielded += 1\r\n632 if self._dataset_kind == _DatasetKind.Iterable and \\\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/utils\/data\/dataloader.py](https:\/\/localhost:8080\/#) in _next_data(self)\r\n672 def _next_data(self):\r\n673 index = self._next_index() # may raise StopIteration\r\n--> 674 data = self._dataset_fetcher.fetch(index) # may raise StopIteration\r\n675 if self._pin_memory:\r\n676 data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/utils\/data\/_utils\/fetch.py](https:\/\/localhost:8080\/#) in fetch(self, possibly_batched_index)\r\n47 if self.auto_collation:\r\n48 if hasattr(self.dataset, \"getitems\") and self.dataset.getitems:\r\n---> 49 data = self.dataset.getitems(possibly_batched_index)\r\n50 else:\r\n51 data = [self.dataset[idx] for idx in possibly_batched_index]\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in getitems(self, keys)\r\n2802 def getitems(self, keys: List) -> List:\r\n2803 \"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\r\n-> 2804 batch = self.getitem(keys)\r\n2805 n_examples = len(batch[next(iter(batch))])\r\n2806 return [{col: array[i] for col, array in batch.items()} for i in range(n_examples)]\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in getitem(self, key)\r\n2798 def getitem(self, key): # noqa: F811\r\n2799 \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 2800 return self._getitem(key)\r\n2801\r\n2802 def getitems(self, keys: List) -> List:\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in _getitem(self, key, **kwargs)\r\n2782 format_kwargs = format_kwargs if format_kwargs is not None else {}\r\n2783 formatter = get_formatter(format_type, features=self._info.features, **format_kwargs)\r\n-> 2784 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n2785 formatted_output = format_table(\r\n2786 pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py](https:\/\/localhost:8080\/#) in query_table(table, key, indices)\r\n581 else:\r\n582 size = indices.num_rows if indices is not None else table.num_rows\r\n--> 583 _check_valid_index_key(key, size)\r\n584 # Query the main table\r\n585 if indices is None:\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py](https:\/\/localhost:8080\/#) in _check_valid_index_key(key, size)\r\n534 elif isinstance(key, Iterable):\r\n535 if len(key) > 0:\r\n--> 536 _check_valid_index_key(int(max(key)), size=size)\r\n537 _check_valid_index_key(int(min(key)), size=size)\r\n538 else:\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py](https:\/\/localhost:8080\/#) in _check_valid_index_key(key, size)\r\n524 if isinstance(key, int):\r\n525 if (key < 0 and key + size < 0) or (key >= size):\r\n--> 526 raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\r\n527 return\r\n528 elif isinstance(key, slice):\r\n\r\nIndexError: Invalid key: 47682 is out of bounds for size 0\n\n### Steps to reproduce the bug\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n\r\n#defining model name for tokenizer and model loading\r\nmodel_name= \"t5-small\"\r\n#loading the tokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\ndef preprocess_function(data, tokenizer):\r\ninputs = [f\"Paraphrase this sentence: {doc}\" for doc in data[\"text\"]]\r\nmodel_inputs = tokenizer(inputs, max_length=150, truncation=True)\r\nlabels = [ast.literal_eval(i)[0] for i in data['paraphrases']]\r\nlabels = tokenizer(labels, max_length=150, truncation=True)\r\nmodel_inputs[\"labels\"] = labels[\"input_ids\"]\r\nreturn model_inputs\r\n\r\ntrain_dataset = load_dataset(\"humarin\/chatgpt-paraphrases\", split=\"train\").shuffle(seed=42).select(range(50000))\r\nval_dataset = load_dataset(\"humarin\/chatgpt-paraphrases\", split=\"train\").shuffle(seed=42).select(range(50000,55000))\r\n\r\ntokenized_train = train_dataset.map(lambda batch: preprocess_function(batch, tokenizer), batched=True)\r\ntokenized_val = val_dataset.map(lambda batch: preprocess_function(batch, tokenizer), batched=True)\r\n\r\ndef print_trainable_parameters(model):\r\n\"\"\"\r\nPrints the number of trainable parameters in the model.\r\n\"\"\"\r\ntrainable_params = 0\r\nall_param = 0\r\nfor _, param in model.named_parameters():\r\nall_param += param.numel()\r\nif param.requires_grad:\r\ntrainable_params += param.numel()\r\nprint(\r\nf\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params \/ all_param}\"\r\n)\r\n\r\nconfig = LoraConfig(\r\nr=16, #attention heads\r\nlora_alpha=32, #alpha scaling\r\nlora_dropout=0.05,\r\nbias=\"none\",\r\ntask_type=\"Seq2Seq\"\r\n)\r\n\r\n#loading the model\r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\r\n\r\nmodel = get_peft_model(model, config)\r\nprint_trainable_parameters(model)\r\n\r\n#loading the data collator\r\ndata_collator = DataCollatorForSeq2Seq(\r\ntokenizer=tokenizer,\r\nmodel=model,\r\nlabel_pad_token_id=-100,\r\npadding=\"longest\"\r\n)\r\n\r\n#defining the training arguments\r\ntraining_args = Seq2SeqTrainingArguments(\r\noutput_dir=os.getcwd(),\r\nevaluation_strategy=\"epoch\",\r\nsave_strategy=\"epoch\",\r\nlearning_rate=2e-5,\r\nper_device_train_batch_size=16,\r\nper_device_eval_batch_size=16,\r\nweight_decay=1e-3,\r\nsave_total_limit=3,\r\nload_best_model_at_end=True,\r\nnum_train_epochs=1,\r\npredict_with_generate=True\r\n)\r\n\r\ndef compute_metric_with_extra(tokenizer):\r\ndef compute_metrics(eval_preds):\r\nmetric = evaluate.load('rouge')\r\npreds, labels = eval_preds\r\n\r\n    # decode preds and labels\r\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\r\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\r\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\r\n\r\n    # rougeLSum expects newline after each sentence\r\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\r\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\r\n\r\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\r\n    return result\r\n\r\nreturn compute_metrics\r\ntrainer = Seq2SeqTrainer(\r\nmodel=model,\r\nargs=training_args,\r\ntrain_dataset=tokenized_train,\r\neval_dataset=tokenized_val,\r\ntokenizer=tokenizer,\r\ndata_collator=data_collator,\r\ncompute_metrics= compute_metric_with_extra(tokenizer)\r\n)\r\n\r\ntrainer.train()\n\n### Expected behavior\n\nI would want the trainer to train normally as it was before I used-\r\n\r\nmodel = get_peft_model(model, config)\r\n\r\n\n\n### Environment info\n\ndatasets version- 2.16.0\r\npeft version- 0.7.1\r\ntransformers version- 4.35.2\r\naccelerate version- 0.25.0\r\npython- 3.10.12\r\nenviroment- google colab","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6534","id":2056002548,"node_id":"I_kwDODunzps56jBv0","number":6534,"title":"How to configure multiple folders in the same zip package","user":{"login":"d710055071","id":12895488,"node_id":"MDQ6VXNlcjEyODk1NDg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12895488?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/d710055071","html_url":"https:\/\/github.com\/d710055071","followers_url":"https:\/\/api.github.com\/users\/d710055071\/followers","following_url":"https:\/\/api.github.com\/users\/d710055071\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/d710055071\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/d710055071\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/d710055071\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/d710055071\/orgs","repos_url":"https:\/\/api.github.com\/users\/d710055071\/repos","events_url":"https:\/\/api.github.com\/users\/d710055071\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/d710055071\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-26T03:56:20Z","updated_at":"2023-12-26T06:31:16Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"How should I write \"config\" in readme when all the data, such as train test, is in a zip file\r\n\r\ntrain floder and test floder in data.zip","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6533","id":2055929101,"node_id":"I_kwDODunzps56iv0N","number":6533,"title":"ted_talks_iwslt | Error: Config name is missing","user":{"login":"rayliuca","id":35850903,"node_id":"MDQ6VXNlcjM1ODUwOTAz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35850903?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rayliuca","html_url":"https:\/\/github.com\/rayliuca","followers_url":"https:\/\/api.github.com\/users\/rayliuca\/followers","following_url":"https:\/\/api.github.com\/users\/rayliuca\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rayliuca\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rayliuca\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rayliuca\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rayliuca\/orgs","repos_url":"https:\/\/api.github.com\/users\/rayliuca\/repos","events_url":"https:\/\/api.github.com\/users\/rayliuca\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rayliuca\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-12-26T00:38:18Z","updated_at":"2023-12-30T18:58:21Z","closed_at":"2023-12-30T16:09:50Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nRunning load_dataset using the newest `datasets` library like below on the ted_talks_iwslt using year pair data will throw an error \"Config name is missing\"\r\n\r\nsee also:\r\nhttps:\/\/huggingface.co\/datasets\/ted_talks_iwslt\/discussions\/3\r\n\r\nlikely caused by #6493, where the `and not config_kwargs` part in the if logic was removed \r\n https:\/\/github.com\/huggingface\/datasets\/blob\/ef3b5dd3633995c95d77f35fb17f89ff44990bc4\/src\/datasets\/builder.py#L512\n\n### Steps to reproduce the bug\n\nrun: \r\n```python\r\nload_dataset(\"ted_talks_iwslt\", language_pair=(\"ja\", \"en\"), year=\"2015\")\r\n```\n\n### Expected behavior\n\nLoad the data without error\n\n### Environment info\n\ndatasets 2.16.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6532","id":2055631201,"node_id":"I_kwDODunzps56hnFh","number":6532,"title":"[Feature request] Indexing datasets by a customly-defined id field to enable random access dataset items via the id","user":{"login":"Yu-Shi","id":3377221,"node_id":"MDQ6VXNlcjMzNzcyMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3377221?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Yu-Shi","html_url":"https:\/\/github.com\/Yu-Shi","followers_url":"https:\/\/api.github.com\/users\/Yu-Shi\/followers","following_url":"https:\/\/api.github.com\/users\/Yu-Shi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Yu-Shi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Yu-Shi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Yu-Shi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Yu-Shi\/orgs","repos_url":"https:\/\/api.github.com\/users\/Yu-Shi\/repos","events_url":"https:\/\/api.github.com\/users\/Yu-Shi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Yu-Shi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-25T11:37:10Z","updated_at":"2024-01-02T13:52:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nSome datasets may contain an id-like field, for example the `id` field in [wikimedia\/wikipedia](https:\/\/huggingface.co\/datasets\/wikimedia\/wikipedia) and the `_id` field in [BeIR\/dbpedia-entity](https:\/\/huggingface.co\/datasets\/BeIR\/dbpedia-entity). HF datasets support efficient random access via row, but not via this kinds of id fields. I wonder if it is possible to add support for indexing by a custom \"id-like\" field to enable random access via such ids. The ids may be numbers or strings.\r\n\r\n### Motivation\r\n\r\nIn some cases, especially during inference\/evaluation, I may want to find out the item that has a specified id, defined by the dataset itself.\r\n\r\nFor example, in a typical re-ranking setting in information retrieval, the user may want to re-rank the set of candidate documents of each query. The input is usually presented in a TREC-style run file, with the following format:\r\n\r\n```\r\n<qid> Q0 <docno> <rank> <score> <tag>\r\n```\r\n\r\nThe re-ranking program should be able to fetch the queries and documents according to the `<qid>` and `<docno>`, which are the original id defined in the query\/document datasets. To accomplish this, I have to iterate over the whole HF dataset to get the mapping from real ids to row ids every time I start the program, which is time-consuming. Thus I want HF dataset to provide options for users to index by a custom id column, not by row.\r\n\r\n### Your contribution\r\n\r\nI'm not an expert in this project and I'm afraid that I'm not able to make contributions on the code.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6531","id":2055201605,"node_id":"PR_kwDODunzps5it5Sm","number":6531,"title":"Add polars compatibility","user":{"login":"psmyth94","id":11325244,"node_id":"MDQ6VXNlcjExMzI1MjQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11325244?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/psmyth94","html_url":"https:\/\/github.com\/psmyth94","followers_url":"https:\/\/api.github.com\/users\/psmyth94\/followers","following_url":"https:\/\/api.github.com\/users\/psmyth94\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/psmyth94\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/psmyth94\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/psmyth94\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/psmyth94\/orgs","repos_url":"https:\/\/api.github.com\/users\/psmyth94\/repos","events_url":"https:\/\/api.github.com\/users\/psmyth94\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/psmyth94\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-24T20:03:23Z","updated_at":"2023-12-24T20:03:23Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hey there,\r\n\r\nI've just finished adding support to convert and format to `polars.DataFrame`. This was in response to the open issue about integrating Polars [#3334](https:\/\/github.com\/huggingface\/datasets\/issues\/3334). Datasets can be switched to Polars format via `Dataset.set_format(\"polars\")`. I've also included `to_polars` and `from_polars`. All polars functions are checked via config.POLARS_AVAILABLE. \r\n\r\nA few notes:\r\nThis only supports `DataFrames` and not `LazyFrames`. This probably could be integrated fairly easily via `is_lazy` args in `set_format`, and `to_polars`.\r\n\r\nLet me know your feedbacks.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6531","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6531","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6531.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6531.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6530","id":2054817609,"node_id":"I_kwDODunzps56egdJ","number":6530,"title":"Impossible to save a mapped dataset to disk","user":{"login":"kopyl","id":17604849,"node_id":"MDQ6VXNlcjE3NjA0ODQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17604849?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kopyl","html_url":"https:\/\/github.com\/kopyl","followers_url":"https:\/\/api.github.com\/users\/kopyl\/followers","following_url":"https:\/\/api.github.com\/users\/kopyl\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kopyl\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kopyl\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kopyl\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kopyl\/orgs","repos_url":"https:\/\/api.github.com\/users\/kopyl\/repos","events_url":"https:\/\/api.github.com\/users\/kopyl\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kopyl\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-23T15:18:27Z","updated_at":"2023-12-24T09:40:30Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI want to play around with different hyperparameters when training but don't want to re-map my dataset with 3 million samples each time for tens of hours when I [fully fine-tune SDXL](https:\/\/github.com\/huggingface\/diffusers\/blob\/main\/examples\/text_to_image\/train_text_to_image_sdxl.py).\r\n\r\nAfter I do the mapping like this:\r\n```\r\ntrain_dataset = train_dataset.map(compute_embeddings_fn, batched=True)\r\ntrain_dataset = train_dataset.map(\r\n    compute_vae_encodings_fn,\r\n    batched=True,\r\n    batch_size=16,\r\n)\r\n```\r\nand try to save it like this:\r\n`train_dataset.save_to_disk(\"test\")`\r\ni get this error ([full traceback](https:\/\/pastebin.com\/kq3vt739)):\r\n```\r\nTypeError: Object of type function is not JSON serializable\r\nThe format kwargs must be JSON serializable, but key 'transform' isn't.\r\n```\r\n\r\nBut what is interesting is that pushing to hub works like that:\r\n`train_dataset.push_to_hub(\"kopyl\/mapped-833-icons-sdxl-1024-dataset\", token=True)`\r\nHere is the link of the pushed dataset: https:\/\/huggingface.co\/datasets\/kopyl\/mapped-833-icons-sdxl-1024-dataset\r\n\r\n### Steps to reproduce the bug\r\n\r\nHere is the self-contained notebook:\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/1RtCsEMVcwWcMwlWURk_cj_9xUBHz065M?usp=sharing\r\n\r\n### Expected behavior\r\n\r\nIt should be easily saved to disk\r\n\r\n### Environment info\r\n\r\nNVIDIA A100, Linux (NC24ads A100 v4 from Azure), CUDA 12.2.\r\n\r\n[pip freeze](https:\/\/pastebin.com\/QTNb6iru)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6529","id":2054209449,"node_id":"I_kwDODunzps56cL-p","number":6529,"title":"Impossible to only download a test split","user":{"login":"ysig","id":28439529,"node_id":"MDQ6VXNlcjI4NDM5NTI5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/28439529?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ysig","html_url":"https:\/\/github.com\/ysig","followers_url":"https:\/\/api.github.com\/users\/ysig\/followers","following_url":"https:\/\/api.github.com\/users\/ysig\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ysig\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ysig\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ysig\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ysig\/orgs","repos_url":"https:\/\/api.github.com\/users\/ysig\/repos","events_url":"https:\/\/api.github.com\/users\/ysig\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ysig\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-22T16:56:32Z","updated_at":"2023-12-22T20:46:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I've spent a significant amount of time trying to locate the split object inside my _split_generators() custom function.\r\nThen after diving [in the code](https:\/\/github.com\/huggingface\/datasets\/blob\/5ff3670c18ed34fa8ddfa70a9aa403ae6cc9ad54\/src\/datasets\/load.py#L2558) I realized that `download_and_prepare` is executed before! split is passed to the dataset builder in `as_dataset`.\r\n\r\nIf I'm not missing something, this seems like bad design, for the following use case:\r\n\r\n> Imagine there is a huge dataset that has an evaluation test set and you want to just download and run just to compare your method.\r\n\r\nIs there a current workaround that can help me achieve the same result?\r\n\r\nThank you,","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6528","id":2053996494,"node_id":"PR_kwDODunzps5ip9JH","number":6528,"title":"set dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-22T14:23:18Z","updated_at":"2023-12-22T14:31:42Z","closed_at":"2023-12-22T14:25:34Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6528","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6528","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6528.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6528.patch","merged_at":"2023-12-22T14:25:34Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6527","id":2053966748,"node_id":"PR_kwDODunzps5ip2vd","number":6527,"title":"Release: 2.16.0","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-22T13:59:56Z","updated_at":"2023-12-22T14:24:12Z","closed_at":"2023-12-22T14:17:55Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6527","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6527","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6527.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6527.patch","merged_at":"2023-12-22T14:17:55Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6526","id":2053726451,"node_id":"PR_kwDODunzps5ipB5v","number":6526,"title":"Preserve order of configs and splits when using Parquet exports","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-22T10:35:56Z","updated_at":"2023-12-22T11:42:22Z","closed_at":"2023-12-22T11:36:14Z","author_association":"MEMBER","active_lock_reason":null,"body":"Preserve order of configs and splits, as defined in dataset infos.\r\n\r\nFix #6521.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6526","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6526","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6526.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6526.patch","merged_at":"2023-12-22T11:36:14Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6525","id":2053119357,"node_id":"PR_kwDODunzps5im-lL","number":6525,"title":"BBox type","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-21T22:13:27Z","updated_at":"2023-12-21T22:39:28Z","closed_at":"2023-12-21T22:39:27Z","author_association":"MEMBER","active_lock_reason":null,"body":"see [internal discussion](https:\/\/huggingface.slack.com\/archives\/C02EK7C3SHW\/p1703097195609209)\r\n\r\nDraft to get some feedback on a possible `BBox` feature type that can be used to get object detection bounding boxes data in one format or another.\r\n\r\n```python\r\n>>> from datasets import load_dataset, BBox\r\n>>> ds = load_dataset(\"svhn\", \"full_numbers\", split=\"train\")\r\n>>> ds[0]\r\n{\r\n  'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=107x46 at 0x126409BE0>,\r\n  'digits': {'bbox': [[38, 1, 21, 40], [57, 3, 16, 40]], 'label': [4, 6]}\r\n}\r\n>>> ds = ds.rename_column(\"digits\", \"annotations\").cast_column(\"annotations\", BBox(format=\"coco\"))\r\n>>> ds[0]\r\n{\r\n  'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=107x46 at 0x147730070>,\r\n  'annotations': [{'bbox': [38, 1, 21, 40], 'category_id': 4}, {'bbox': [57, 3, 16, 40], 'category_id': 6}]\r\n}\r\n```\r\n\r\nnote that it's a type for a list of bounding boxes, not just one - which would be needed to switch from a format to another using type casting.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6525","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6525","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6525.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6525.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6524","id":2053076311,"node_id":"I_kwDODunzps56X3VX","number":6524,"title":"Streaming the Pile: Missing Files","user":{"login":"FelixLabelle","id":23347756,"node_id":"MDQ6VXNlcjIzMzQ3NzU2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23347756?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/FelixLabelle","html_url":"https:\/\/github.com\/FelixLabelle","followers_url":"https:\/\/api.github.com\/users\/FelixLabelle\/followers","following_url":"https:\/\/api.github.com\/users\/FelixLabelle\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/FelixLabelle\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/FelixLabelle\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/FelixLabelle\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/FelixLabelle\/orgs","repos_url":"https:\/\/api.github.com\/users\/FelixLabelle\/repos","events_url":"https:\/\/api.github.com\/users\/FelixLabelle\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/FelixLabelle\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-12-21T21:25:09Z","updated_at":"2023-12-22T09:17:05Z","closed_at":"2023-12-22T09:17:05Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThe pile does not stream, a \"File not Found error\" is returned. It looks like the Pile's files have been moved.\n\n### Steps to reproduce the bug\n\nTo reproduce run the following code:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('EleutherAI\/pile', 'en', split='train', streaming=True)\r\nnext(iter(dataset))\r\n```\r\n\r\nI get the following error:\r\n`FileNotFoundError: https:\/\/the-eye.eu\/public\/AI\/pile\/train\/00.jsonl.zst`\n\n### Expected behavior\n\nReturn the data in a stream.\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.11.5\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6523","id":2052643484,"node_id":"PR_kwDODunzps5ilV6d","number":6523,"title":"fix tests","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-21T15:36:21Z","updated_at":"2023-12-21T15:56:54Z","closed_at":"2023-12-21T15:50:38Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6523","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6523","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6523.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6523.patch","merged_at":"2023-12-21T15:50:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6522","id":2052332528,"node_id":"I_kwDODunzps56VBvw","number":6522,"title":"Loading HF Hub Dataset (private org repo) fails to load all features","user":{"login":"versipellis","id":6579034,"node_id":"MDQ6VXNlcjY1NzkwMzQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6579034?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/versipellis","html_url":"https:\/\/github.com\/versipellis","followers_url":"https:\/\/api.github.com\/users\/versipellis\/followers","following_url":"https:\/\/api.github.com\/users\/versipellis\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/versipellis\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/versipellis\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/versipellis\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/versipellis\/orgs","repos_url":"https:\/\/api.github.com\/users\/versipellis\/repos","events_url":"https:\/\/api.github.com\/users\/versipellis\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/versipellis\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-21T12:26:35Z","updated_at":"2023-12-21T13:24:31Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen pushing a `Dataset` with multiple `Features` (`input`, `output`, `tags`) to Huggingface Hub (private org repo), and later downloading the `Dataset`, only `input` and `output` load - I believe the expected behavior is for all `Features` to be loaded by default?\r\n\r\n### Steps to reproduce the bug\r\n\r\nPushing the data. `data_concat` is a `list` of `dict`s.\r\n```python\r\nfor datum in data_concat:\r\n    datum_tags = {d[\"key\"]: d[\"value\"] for d in datum[\"tags\"]}\r\n    split_fraction = # some logic that generates a train\/test split number\r\n    if split_faction < test_fraction:\r\n        data_test.append(datum)\r\n    else:\r\n        data_train.append(datum)\r\n\r\ndataset = DatasetDict(\r\n    {\r\n        \"train\": Dataset.from_list(data_train),\r\n        \"test\": Dataset.from_list(data_test),\r\n        \"full\": Dataset.from_list(data_concat),\r\n    },\r\n)\r\n\r\ndataset_shuffled = dataset.shuffle(seed=shuffle_seed)\r\n\r\ndataset_shuffled.push_to_hub(\r\n    repo_id=hf_repo_id,\r\n    private=True,\r\n    config_name=m,\r\n    revision=revision,\r\n    token=hf_token,\r\n)\r\n```\r\n\r\nLoading it later:\r\n```python\r\n        dataset = datasets.load_dataset(\r\n            path=hf_repo_id,\r\n            name=name,\r\n            token=hf_token,\r\n        )\r\n```\r\n\r\nProduces:\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    test: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    full: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: <obfuscated>\r\n    })\r\n})\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe expected result is below:\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['input', 'output', 'tags'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    test: Dataset({\r\n        features: ['input', 'output', 'tags'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    full: Dataset({\r\n        features: ['input', 'output', 'tags'],\r\n        num_rows: <obfuscated>\r\n    })\r\n})\r\n```\r\n\r\nMy workaround is as follows:\r\n```python\r\ndsinfo = datasets.get_dataset_config_info(\r\n    path=data_files,\r\n    config_name=data_config,\r\n    token=hf_token,\r\n)\r\n\r\nallfeatures = dsinfo.features.copy()\r\nif \"tags\" not in allfeatures:\r\n    allfeatures[\"tags\"] = [{\"key\": Value(dtype=\"string\", id=None), \"value\": Value(dtype=\"string\", id=None)}]\r\n\r\ndataset = datasets.load_dataset(\r\n    path=data_files,\r\n    name=data_config,\r\n    features=allfeatures,\r\n    token=hf_token,\r\n)\r\n```\r\n\r\nInterestingly enough (and perhaps a related bug?), if I don't add the `tags` to `allfeatures` above (i.e. only loading `input` and `output`), it throws an error when executing `load_dataset`:\r\n\r\n```\r\nValueError: Couldn't cast\r\ntags: list<element: struct<key: string, value: string>>\r\n  child 0, element: struct<key: string, value: string>\r\n      child 0, key: string\r\n      child 1, value: string\r\ninput: <obfuscated>\r\noutput: <obfuscated>\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"tags\": [{\"key\": {\"dtype\": \"string\",' + 532\r\nto\r\n{'input': <obfuscated>, 'output': <obfuscated>\r\nbecause column names don't match\r\n```\r\n\r\nTraceback for this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/Users\/bt\/github\/core\/.venv\/lib\/python3.11\/site-packages\/datasets\/load.py\", line 2152, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/Users\/bt\/github\/core\/.venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 948, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/Users\/bt\/github\/core\/.venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1043, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/Users\/bt\/github\/core\/.venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1805, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"\/Users\/bt\/github\/core\/.venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1950, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: macOS-14.0-arm64-arm-64bit\r\n- Python version: 3.11.5\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6521","id":2052229538,"node_id":"I_kwDODunzps56Uomi","number":6521,"title":"The order of the splits is not preserved","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-12-21T11:17:27Z","updated_at":"2023-12-22T11:36:15Z","closed_at":"2023-12-22T11:36:15Z","author_association":"MEMBER","active_lock_reason":null,"body":"We had a regression and the order of the splits is not preserved. They are alphabetically sorted, instead of preserving original \"train\", \"validation\", \"test\" order.\r\n\r\nCheck: In branch \"main\"\r\n```python\r\nIn [9]: dataset = load_dataset(\"adversarial_qa\", '\"adversarialQA\")\r\n\r\nIn [10]: dataset\r\nOut[10]: \r\nDatasetDict({\r\n    test: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n    train: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 30000\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n})\r\n```\r\n\r\nBefore (2.15.0) it was:\r\n```python\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 30000\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n    test: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n})\r\n```\r\n\r\nSee issues: \r\n- https:\/\/huggingface.co\/datasets\/adversarial_qa\/discussions\/3\r\n- https:\/\/huggingface.co\/datasets\/beans\/discussions\/4\r\n\r\nThis is a regression because it was previously fixed. See:\r\n- #6196\r\n- #5728","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6520","id":2052059078,"node_id":"PR_kwDODunzps5ijUiw","number":6520,"title":"Support commit_description parameter in push_to_hub","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-21T09:36:11Z","updated_at":"2023-12-21T14:49:47Z","closed_at":"2023-12-21T14:43:35Z","author_association":"MEMBER","active_lock_reason":null,"body":"Support `commit_description` parameter in `push_to_hub`.\r\n\r\nCC: @Wauplin ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6520","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6520","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6520.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6520.patch","merged_at":"2023-12-21T14:43:35Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6519","id":2050759824,"node_id":"PR_kwDODunzps5ie4MA","number":6519,"title":"Support push_to_hub canonical datasets","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-12-20T15:16:45Z","updated_at":"2023-12-21T14:48:20Z","closed_at":"2023-12-21T14:40:57Z","author_association":"MEMBER","active_lock_reason":null,"body":"Support `push_to_hub` canonical datasets.\r\n\r\nThis is necessary in the Space to convert script-datasets to Parquet: https:\/\/huggingface.co\/spaces\/albertvillanova\/convert-dataset-to-parquet\r\n\r\nNote that before this PR, the `repo_id` \"dataset_name\" was transformed to \"user\/dataset_name\". This behavior was introduced by:\r\n- #6269","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6519","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6519","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6519.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6519.patch","merged_at":"2023-12-21T14:40:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6518","id":2050137038,"node_id":"PR_kwDODunzps5icu-W","number":6518,"title":"fix get_metadata_patterns function args error","user":{"login":"d710055071","id":12895488,"node_id":"MDQ6VXNlcjEyODk1NDg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12895488?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/d710055071","html_url":"https:\/\/github.com\/d710055071","followers_url":"https:\/\/api.github.com\/users\/d710055071\/followers","following_url":"https:\/\/api.github.com\/users\/d710055071\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/d710055071\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/d710055071\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/d710055071\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/d710055071\/orgs","repos_url":"https:\/\/api.github.com\/users\/d710055071\/repos","events_url":"https:\/\/api.github.com\/users\/d710055071\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/d710055071\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-20T09:06:22Z","updated_at":"2023-12-21T15:14:17Z","closed_at":"2023-12-21T15:07:57Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Bug get_metadata_patterns arg error https:\/\/github.com\/huggingface\/datasets\/issues\/6517","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6518","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6518","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6518.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6518.patch","merged_at":"2023-12-21T15:07:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6517","id":2050121588,"node_id":"I_kwDODunzps56Ml90","number":6517,"title":"Bug get_metadata_patterns arg error","user":{"login":"d710055071","id":12895488,"node_id":"MDQ6VXNlcjEyODk1NDg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12895488?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/d710055071","html_url":"https:\/\/github.com\/d710055071","followers_url":"https:\/\/api.github.com\/users\/d710055071\/followers","following_url":"https:\/\/api.github.com\/users\/d710055071\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/d710055071\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/d710055071\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/d710055071\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/d710055071\/orgs","repos_url":"https:\/\/api.github.com\/users\/d710055071\/repos","events_url":"https:\/\/api.github.com\/users\/d710055071\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/d710055071\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-20T08:56:44Z","updated_at":"2023-12-22T00:24:23Z","closed_at":"2023-12-22T00:24:23Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"https:\/\/github.com\/huggingface\/datasets\/blob\/3f149204a2a5948287adcade5e90707aa5207a92\/src\/datasets\/load.py#L1240C1-L1240C69\r\nmetadata_patterns = get_metadata_patterns(base_path, download_config=self.download_config)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6516","id":2050033322,"node_id":"PR_kwDODunzps5icYX0","number":6516,"title":"Support huggingface-hub pre-releases","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-20T07:52:29Z","updated_at":"2023-12-20T08:51:34Z","closed_at":"2023-12-20T08:44:44Z","author_association":"MEMBER","active_lock_reason":null,"body":"Support `huggingface-hub` pre-releases.\r\n\r\nThis way we will have our CI green when testing `huggingface-hub` release candidates. See: https:\/\/github.com\/huggingface\/datasets\/tree\/ci-test-huggingface-hub-v0.20.0.rc1\r\n\r\nClose #6513.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6516","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6516","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6516.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6516.patch","merged_at":"2023-12-20T08:44:44Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6515","id":2049724251,"node_id":"I_kwDODunzps56LE9b","number":6515,"title":"Why call http_head() when fsspec_head()  succeeds","user":{"login":"d710055071","id":12895488,"node_id":"MDQ6VXNlcjEyODk1NDg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12895488?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/d710055071","html_url":"https:\/\/github.com\/d710055071","followers_url":"https:\/\/api.github.com\/users\/d710055071\/followers","following_url":"https:\/\/api.github.com\/users\/d710055071\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/d710055071\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/d710055071\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/d710055071\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/d710055071\/orgs","repos_url":"https:\/\/api.github.com\/users\/d710055071\/repos","events_url":"https:\/\/api.github.com\/users\/d710055071\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/d710055071\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-20T02:25:51Z","updated_at":"2023-12-26T05:35:46Z","closed_at":"2023-12-26T05:35:46Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"https:\/\/github.com\/huggingface\/datasets\/blob\/a91582de288d98e94bcb5ab634ca1cfeeff544c5\/src\/datasets\/utils\/file_utils.py#L510C1-L523C14","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6514","id":2049600663,"node_id":"PR_kwDODunzps5ia6Os","number":6514,"title":"Cache backward compatibility with 2.15.0","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-12-19T23:52:25Z","updated_at":"2023-12-21T21:14:11Z","closed_at":"2023-12-21T21:07:55Z","author_association":"MEMBER","active_lock_reason":null,"body":"...for datasets without scripts\r\n\r\nIt takes into account the changes in cache from\r\n- https:\/\/github.com\/huggingface\/datasets\/pull\/6493: switch to `config\/version\/commit_sha` schema\r\n- https:\/\/github.com\/huggingface\/datasets\/pull\/6454: fix `DataFilesDict` keys ordering when hashing\r\n\r\nrequires https:\/\/github.com\/huggingface\/datasets\/pull\/6493 to be merged","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6514","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6514","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6514.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6514.patch","merged_at":"2023-12-21T21:07:55Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6513","id":2048869151,"node_id":"I_kwDODunzps56H0Mf","number":6513,"title":"Support huggingface-hub 0.20.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-19T15:15:46Z","updated_at":"2023-12-20T08:44:45Z","closed_at":"2023-12-20T08:44:45Z","author_association":"MEMBER","active_lock_reason":null,"body":"CI to test the support of `huggingface-hub` 0.20.0: https:\/\/github.com\/huggingface\/datasets\/compare\/main...ci-test-huggingface-hub-v0.20.0.rc1\r\n\r\nWe need to merge:\r\n- #6510 \r\n- #6512\r\n- #6516","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6512","id":2048795819,"node_id":"PR_kwDODunzps5iYI5z","number":6512,"title":"Remove deprecated HfFolder","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-19T14:40:49Z","updated_at":"2023-12-19T20:21:13Z","closed_at":"2023-12-19T20:14:30Z","author_association":"MEMBER","active_lock_reason":null,"body":"...and use `huggingface_hub.get_token()` instead","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6512","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6512","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6512.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6512.patch","merged_at":"2023-12-19T20:14:30Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6511","id":2048465958,"node_id":"PR_kwDODunzps5iXAXR","number":6511,"title":"Implement get dataset default config name","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-19T11:26:19Z","updated_at":"2023-12-21T14:48:57Z","closed_at":"2023-12-21T14:42:41Z","author_association":"MEMBER","active_lock_reason":null,"body":"Implement `get_dataset_default_config_name`.\r\n\r\nNow that we support setting a configuration as default in `push_to_hub` (see #6500), we need a programmatically way to know in advance which is the default configuration. This will be used in the Space to convert script-datasets to Parquet: https:\/\/huggingface.co\/spaces\/albertvillanova\/convert-dataset-to-parquet\r\n\r\nFollow-up of:\r\n- #6500\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6511","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6511","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6511.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6511.patch","merged_at":"2023-12-21T14:42:40Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6510","id":2046928742,"node_id":"PR_kwDODunzps5iRyiV","number":6510,"title":"Replace `list_files_info` with `list_repo_tree` in `push_to_hub`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-18T15:34:19Z","updated_at":"2023-12-19T18:05:47Z","closed_at":"2023-12-19T17:58:34Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Starting from `huggingface_hub` 0.20.0, `list_files_info` will be deprecated in favor of `list_repo_tree` (see https:\/\/github.com\/huggingface\/huggingface_hub\/pull\/1910)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6510","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6510","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6510.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6510.patch","merged_at":"2023-12-19T17:58:34Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6509","id":2046720869,"node_id":"PR_kwDODunzps5iREyE","number":6509,"title":"Better cast error when generating dataset","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-18T13:57:24Z","updated_at":"2023-12-19T09:37:12Z","closed_at":"2023-12-19T09:31:03Z","author_association":"MEMBER","active_lock_reason":null,"body":"I want to improve the error message for datasets like https:\/\/huggingface.co\/datasets\/m-a-p\/COIG-CQIA\r\n\r\nCc @albertvillanova @severo is this new error ok ? Or should I use a dedicated error class ?\r\n\r\nNew:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1920, in _prepare_split_single\r\n    writer.write_table(table)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/arrow_writer.py\", line 574, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/table.py\", line 2322, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/table.py\", line 2276, in cast_table_to_schema\r\n    raise CastError(\r\ndatasets.table.CastError: Couldn't cast\r\ninstruction: string\r\nother: string\r\nindex: string\r\ndomain: list<item: string>\r\n  child 0, item: string\r\noutput: string\r\ntask_type: struct<major: list<item: string>, minor: list<item: string>>\r\n  child 0, major: list<item: string>\r\n      child 0, item: string\r\n  child 1, minor: list<item: string>\r\n      child 0, item: string\r\ntask_name_in_eng: string\r\ninput: string\r\nto\r\n{'answer_from': Value(dtype='string', id=None), 'instruction': Value(dtype='string', id=None), 'human_verified': Value(dtype='bool', id=None), 'domain': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'output': Value(dtype='string', id=None), 'task_type': {'major': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'minor': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'copyright': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None)}\r\nbecause column names don't match\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/playground\/ttest.py\", line 74, in <module>\r\n    load_dataset(\"m-a-p\/COIG-CQIA\")\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/load.py\", line 2529, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 936, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1031, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1791, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1922, in _prepare_split_single\r\n    raise DatasetGenerationCastError.from_cast_error(\r\ndatasets.exceptions.DatasetGenerationCastError: An error occurred while generating the dataset\r\n\r\nAll the data files must have the same columns, but at some point there are 3 new columns (other, index, task_name_in_eng) and 3 missing columns (answer_from, copyright, human_verified).\r\n\r\nThis happened while the json dataset builder was generating data using\r\n\r\nhf:\/\/datasets\/m-a-p\/COIG-CQIA\/coig_pc\/coig_pc_core_sample.json (at revision b7b7ecf290f6515036c7c04bd8537228ac2eb474)\r\n\r\nPlease either edit the data files to have matching columns, or separate them into different configurations (see docs at https:\/\/hf.co\/docs\/hub\/datasets-manual-configuration#multiple-configurations)\r\n```\r\n\r\nPreviously:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1931, in _prepare_split_single\r\n    writer.write_table(table)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/arrow_writer.py\", line 574, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/table.py\", line 2295, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/table.py\", line 2253, in cast_table_to_schema\r\n    raise ValueError(f\"Couldn't cast\\n{table.schema}\\nto\\n{features}\\nbecause column names don't match\")\r\nValueError: Couldn't cast\r\ntask_type: struct<major: list<item: string>, minor: list<item: string>>\r\n  child 0, major: list<item: string>\r\n      child 0, item: string\r\n  child 1, minor: list<item: string>\r\n      child 0, item: string\r\nother: string\r\ninstruction: string\r\ntask_name_in_eng: string\r\ndomain: list<item: string>\r\n  child 0, item: string\r\nindex: string\r\noutput: string\r\ninput: string\r\nto\r\n{'human_verified': Value(dtype='bool', id=None), 'task_type': {'major': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'minor': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'answer_from': Value(dtype='string', id=None), 'copyright': Value(dtype='string', id=None), 'instruction': Value(dtype='string', id=None), 'domain': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'output': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/playground\/ttest.py\", line 74, in <module>\r\n    load_dataset(\"m-a-p\/COIG-CQIA\")\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/load.py\", line 2529, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 949, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1044, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1804, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1949, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6509","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6509","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6509.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6509.patch","merged_at":"2023-12-19T09:31:03Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6508","id":2045733273,"node_id":"PR_kwDODunzps5iNvAu","number":6508,"title":"Read GeoParquet files using parquet reader","user":{"login":"weiji14","id":23487320,"node_id":"MDQ6VXNlcjIzNDg3MzIw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23487320?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/weiji14","html_url":"https:\/\/github.com\/weiji14","followers_url":"https:\/\/api.github.com\/users\/weiji14\/followers","following_url":"https:\/\/api.github.com\/users\/weiji14\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/weiji14\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/weiji14\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/weiji14\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/weiji14\/orgs","repos_url":"https:\/\/api.github.com\/users\/weiji14\/repos","events_url":"https:\/\/api.github.com\/users\/weiji14\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/weiji14\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-12-18T04:50:37Z","updated_at":"2024-01-02T15:40:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Let GeoParquet files with the file extension `*.geoparquet` or `*.gpq` be readable by the default parquet reader.\r\n\r\nThose two file extensions are the ones most commonly used for GeoParquet files, and is included in the `gpq` validator tool at https:\/\/github.com\/planetlabs\/gpq\/blob\/e5576b4ee7306b4d2259d56c879465a9364dab90\/cmd\/gpq\/command\/convert.go#L73-L75\r\n\r\nAddresses https:\/\/github.com\/huggingface\/datasets\/issues\/6438","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6508","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6508","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6508.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6508.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6507","id":2045152928,"node_id":"I_kwDODunzps555o6g","number":6507,"title":"where is glue_metric.py> @Frankie123421 what was the resolution to this?","user":{"login":"Mcccccc1024","id":119146162,"node_id":"U_kgDOBxoGsg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/119146162?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Mcccccc1024","html_url":"https:\/\/github.com\/Mcccccc1024","followers_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/followers","following_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/orgs","repos_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/repos","events_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-17T09:58:25Z","updated_at":"2023-12-18T11:42:49Z","closed_at":"2023-12-18T11:42:49Z","author_association":"NONE","active_lock_reason":null,"body":"              > @Frankie123421 what was the resolution to this?\r\n\r\nuse glue_metric.py instead of glue.py in load_metric\r\n\r\n_Originally posted by @Frankie123421 in https:\/\/github.com\/huggingface\/datasets\/issues\/2117#issuecomment-905093763_\r\n            ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507\/timeline","performed_via_github_app":null,"state_reason":"not_planned","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6506","id":2044975038,"node_id":"I_kwDODunzps5549e-","number":6506,"title":"Incorrect test set labels for RTE and CoLA datasets via load_dataset","user":{"login":"emreonal11","id":73316684,"node_id":"MDQ6VXNlcjczMzE2Njg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/73316684?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emreonal11","html_url":"https:\/\/github.com\/emreonal11","followers_url":"https:\/\/api.github.com\/users\/emreonal11\/followers","following_url":"https:\/\/api.github.com\/users\/emreonal11\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emreonal11\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emreonal11\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emreonal11\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emreonal11\/orgs","repos_url":"https:\/\/api.github.com\/users\/emreonal11\/repos","events_url":"https:\/\/api.github.com\/users\/emreonal11\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emreonal11\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-16T22:06:08Z","updated_at":"2023-12-21T09:57:57Z","closed_at":"2023-12-21T09:57:57Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nThe test set labels for the RTE and CoLA datasets when loading via datasets load_dataset are all -1.\r\nEdit: It appears this is also the case for every other dataset except for MRPC (stsb, sst2, qqp, mnli (both matched and mismatched), qnli, wnli, ax). Is this intended behavior to safeguard the test set for evaluation purposes?\r\n\r\n### Steps to reproduce the bug\r\n\r\n!pip install datasets\r\nfrom datasets import load_dataset\r\n\r\nrte_data = load_dataset('glue', 'rte')\r\ncola_data = load_dataset('glue', 'cola')\r\nprint(rte_data['test'][0:30]['label'])\r\nprint(cola_data['test'][0:30]['label'])\r\n\r\nOutput:\r\n[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\r\n[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\r\n\r\nThe non-label test data seems to be fine:\r\ne.g. rte_data['test'][1] is:\r\n{'sentence1': \"Authorities in Brazil say that more than 200 people are being held hostage in a prison in the country's remote, Amazonian-jungle state of Rondonia.\",\r\n 'sentence2': 'Authorities in Brazil hold 200 people as hostage.',\r\n 'label': -1,\r\n 'idx': 1}\r\nTraining and validation data are also fine:\r\ne.g. rte_data['train][0] is:\r\n{'sentence1': 'No Weapons of Mass Destruction Found in Iraq Yet.',\r\n 'sentence2': 'Weapons of Mass Destruction Found in Iraq.',\r\n 'label': 1,\r\n 'idx': 0}\r\n\r\n\r\n### Expected behavior\r\n\r\nExpected the labels to be binary 0\/1 values; Got all -1s instead\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6505","id":2044721288,"node_id":"I_kwDODunzps553_iI","number":6505,"title":"Got stuck when I trying to load a dataset","user":{"login":"yirenpingsheng","id":18232551,"node_id":"MDQ6VXNlcjE4MjMyNTUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/18232551?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yirenpingsheng","html_url":"https:\/\/github.com\/yirenpingsheng","followers_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/followers","following_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/orgs","repos_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/repos","events_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-16T11:51:07Z","updated_at":"2024-01-06T15:28:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHello, everyone. I met a problem when I am trying to load a data file using load_dataset method on a Debian 10 system. The data file is not very large, only 1.63MB with 600 records. \r\nHere is my code:\r\n\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('json', data_files='mypath\/oaast_rm_zh.json')\r\n\r\nI waited it for 20 minutes. It still no response. I cannot using Ctrl+C to cancel the command. I have to use Ctrl+Z to kill it. I also try it with a txt file, it still no response in a long time. \r\n\r\nI can load the same file successfully using my laptop (windows 10, python 3.8.5, datasets==2.14.5). I can also make it on another computer (Ubuntu 20.04.5 LTS, python 3.10.13, datasets 2.14.7). It only takes me 1-2 miniutes.\r\n\r\nCould you give me some suggestions? Thank you.\r\n\r\n\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('json', data_files='mypath\/oaast_rm_zh.json')\n\n### Expected behavior\n\nI hope it can load the file successfully.\n\n### Environment info\n\nOS: Debian GNU\/Linux 10\r\nPython: Python 3.10.13\r\nPip list:\r\nPackage                   Version\r\n------------------------- ------------\r\naccelerate                0.25.0\r\naddict                    2.4.0\r\naiofiles                  23.2.1\r\naiohttp                   3.9.1\r\naiosignal                 1.3.1\r\naliyun-python-sdk-core    2.14.0\r\naliyun-python-sdk-kms     2.16.2\r\naltair                    5.2.0\r\nannotated-types           0.6.0\r\nanyio                     3.7.1\r\nasync-timeout             4.0.3\r\nattrs                     23.1.0\r\ncertifi                   2023.11.17\r\ncffi                      1.16.0\r\ncharset-normalizer        3.3.2\r\nclick                     8.1.7\r\ncontourpy                 1.2.0\r\ncrcmod                    1.7\r\ncryptography              41.0.7\r\ncycler                    0.12.1\r\ndatasets                  2.14.7\r\ndill                      0.3.7\r\ndocstring-parser          0.15\r\neinops                    0.7.0\r\nexceptiongroup            1.2.0\r\nfastapi                   0.105.0\r\nffmpy                     0.3.1\r\nfilelock                  3.13.1\r\nfonttools                 4.46.0\r\nfrozenlist                1.4.1\r\nfsspec                    2023.10.0\r\ngast                      0.5.4\r\ngradio                    3.50.2\r\ngradio_client             0.6.1\r\nh11                       0.14.0\r\nhttpcore                  1.0.2\r\nhttpx                     0.25.2\r\nhuggingface-hub           0.19.4\r\nidna                      3.6\r\nimportlib-metadata        7.0.0\r\nimportlib-resources       6.1.1\r\njieba                     0.42.1\r\nJinja2                    3.1.2\r\njmespath                  0.10.0\r\njoblib                    1.3.2\r\njsonschema                4.20.0\r\njsonschema-specifications 2023.11.2\r\nkiwisolver                1.4.5\r\nmarkdown-it-py            3.0.0\r\nMarkupSafe                2.1.3\r\nmatplotlib                3.8.2\r\nmdurl                     0.1.2\r\nmodelscope                1.10.0\r\nmpmath                    1.3.0\r\nmultidict                 6.0.4\r\nmultiprocess              0.70.15\r\nnetworkx                  3.2.1\r\nnltk                      3.8.1\r\nnumpy                     1.26.2\r\nnvidia-cublas-cu12        12.1.3.1\r\nnvidia-cuda-cupti-cu12    12.1.105\r\nnvidia-cuda-nvrtc-cu12    12.1.105\r\nnvidia-cuda-runtime-cu12  12.1.105\r\nnvidia-cudnn-cu12         8.9.2.26\r\nnvidia-cufft-cu12         11.0.2.54\r\nnvidia-curand-cu12        10.3.2.106\r\nnvidia-cusolver-cu12      11.4.5.107\r\nnvidia-cusparse-cu12      12.1.0.106\r\nnvidia-nccl-cu12          2.18.1\r\nnvidia-nvjitlink-cu12     12.3.101\r\nnvidia-nvtx-cu12          12.1.105\r\norjson                    3.9.10\r\noss2                      2.18.3\r\npackaging                 23.2\r\npandas                    2.1.4\r\npeft                      0.7.1\r\nPillow                    10.1.0\r\npip                       23.3.1\r\nplatformdirs              4.1.0\r\nprotobuf                  4.25.1\r\npsutil                    5.9.6\r\npyarrow                   14.0.1\r\npyarrow-hotfix            0.6\r\npycparser                 2.21\r\npycryptodome              3.19.0\r\npydantic                  2.5.2\r\npydantic_core             2.14.5\r\npydub                     0.25.1\r\nPygments                  2.17.2\r\npyparsing                 3.1.1\r\npython-dateutil           2.8.2\r\npython-multipart          0.0.6\r\npytz                      2023.3.post1\r\nPyYAML                    6.0.1\r\nreferencing               0.32.0\r\nregex                     2023.10.3\r\nrequests                  2.31.0\r\nrich                      13.7.0\r\nrouge-chinese             1.0.3\r\nrpds-py                   0.13.2\r\nsafetensors               0.4.1\r\nscipy                     1.11.4\r\nsemantic-version          2.10.0\r\nsentencepiece             0.1.99\r\nsetuptools                68.2.2\r\nshtab                     1.6.5\r\nsimplejson                3.19.2\r\nsix                       1.16.0\r\nsniffio                   1.3.0\r\nsortedcontainers          2.4.0\r\nsse-starlette             1.8.2\r\nstarlette                 0.27.0\r\nsympy                     1.12\r\ntiktoken                  0.5.2\r\ntokenizers                0.15.0\r\ntomli                     2.0.1\r\ntoolz                     0.12.0\r\ntorch                     2.1.2\r\ntqdm                      4.66.1\r\ntransformers              4.36.1\r\ntriton                    2.1.0\r\ntrl                       0.7.4\r\ntyping_extensions         4.9.0\r\ntyro                      0.6.0\r\ntzdata                    2023.3\r\nurllib3                   2.1.0\r\nuvicorn                   0.24.0.post1\r\nwebsockets                11.0.3\r\nwheel                     0.41.2\r\nxxhash                    3.4.1\r\nyapf                      0.40.2\r\nyarl                      1.9.4\r\nzipp                      3.17.0\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6504","id":2044541154,"node_id":"I_kwDODunzps553Tji","number":6504,"title":"Error Pushing to Hub","user":{"login":"Jiayi-Pan","id":55055083,"node_id":"MDQ6VXNlcjU1MDU1MDgz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/55055083?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Jiayi-Pan","html_url":"https:\/\/github.com\/Jiayi-Pan","followers_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/followers","following_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/orgs","repos_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/repos","events_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-16T01:05:22Z","updated_at":"2023-12-16T06:20:53Z","closed_at":"2023-12-16T06:20:53Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nError when trying to push a dataset in a special format to hub\n\n### Steps to reproduce the bug\n\n```\r\nimport datasets\r\nfrom datasets import Dataset\r\ndataset_dict = {\r\n    \"filename\": [\"apple\", \"banana\"],\r\n    \"token\": [[[1,2],[3,4]],[[1,2],[3,4]]],\r\n    \"label\": [0, 1],\r\n}\r\ndataset = Dataset.from_dict(dataset_dict)\r\ndataset = dataset.cast_column(\"token\", datasets.features.features.Array2D(shape=(2, 2),dtype=\"int16\"))\r\n\r\ndataset.push_to_hub(\"SequenceModel\/imagenet_val_256\")\r\n```\r\nError:\r\n```\r\n...\r\n\r\nConstructorError: could not determine a constructor for the tag 'tag:yaml.org,2002:python\/tuple'\r\n  in \"<unicode string>\", line 8, column 16:\r\n            shape: !!python\/tuple\r\n                   ^\r\n```\n\n### Expected behavior\n\nDataset being pushed to hub\n\n### Environment info\n\n- `datasets` version: 2.15.0\r\n- Platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.35\r\n- Python version: 3.11.5\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6503","id":2043847591,"node_id":"PR_kwDODunzps5iHgZf","number":6503,"title":"Fix streaming xnli","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-15T14:40:57Z","updated_at":"2023-12-15T14:51:06Z","closed_at":"2023-12-15T14:44:47Z","author_association":"MEMBER","active_lock_reason":null,"body":"This code was failing\r\n\r\n```python\r\n\r\nIn [1]: from datasets import load_dataset\r\n\r\nIn [2]: \r\n   ...:         ds = load_dataset(\"xnli\", \"all_languages\", split=\"test\", streaming=True)\r\n   ...: \r\n   ...:         sample_data = next(iter(ds))[\"premise\"]  # pick up one data\r\n   ...:         input_text = list(sample_data.values())\r\n```\r\n\r\n```\r\nFile ~\/hf\/datasets\/src\/datasets\/features\/translation.py:104, in TranslationVariableLanguages.encode_example(self, translation_dict)\r\n    102     return translation_dict\r\n    103 elif self.languages and set(translation_dict) - lang_set:\r\n--> 104     raise ValueError(\r\n    105         f'Some languages in example ({\", \".join(sorted(set(translation_dict) - lang_set))}) are not in valid set ({\", \".join(lang_set)}).'\r\n    106     )\r\n    108 # Convert dictionary into tuples, splitting out cases where there are\r\n    109 # multiple translations for a single language.\r\n    110 translation_tuples = []\r\n\r\nValueError: Some languages in example (language, translation) are not in valid set (ur, fr, hi, sw, vi, el, de, th, en, tr, zh, ar, bg, ru, es).\r\n```\r\n\r\nbecause in streaming mode we expect features encode methods to be no-ops if the example is already encoded.\r\n\r\nI fixed `TranslationVariableLanguages` to account for that","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6503","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6503","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6503.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6503.patch","merged_at":"2023-12-15T14:44:46Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6502","id":2043771731,"node_id":"PR_kwDODunzps5iHPt-","number":6502,"title":"Pickle support for `torch.Generator` objects","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-15T13:55:12Z","updated_at":"2023-12-15T15:04:33Z","closed_at":"2023-12-15T14:58:22Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix for https:\/\/discuss.huggingface.co\/t\/caching-a-dataset-processed-with-randomness\/65616","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6502","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6502","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6502.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6502.patch","merged_at":"2023-12-15T14:58:22Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6501","id":2043377240,"node_id":"I_kwDODunzps55y3ZY","number":6501,"title":" OverflowError: value too large to convert to int32_t ","user":{"login":"zhangfan-algo","id":47747764,"node_id":"MDQ6VXNlcjQ3NzQ3NzY0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47747764?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/zhangfan-algo","html_url":"https:\/\/github.com\/zhangfan-algo","followers_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/followers","following_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/orgs","repos_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/repos","events_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-15T10:10:21Z","updated_at":"2023-12-15T10:10:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/47747764\/f58044fb-ddda-48b6-ba68-7bbfef781630)\r\n\n\n### Steps to reproduce the bug\n\njust loading datasets \n\n### Expected behavior\n\nhow can I fix it\n\n### Environment info\n\npip install \/mnt\/cluster\/zhangfan\/study_info\/LLaMA-Factory\/peft-0.6.0-py3-none-any.whl\r\npip install huggingface_hub-0.19.4-py3-none-any.whl tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl transformers-4.36.1-py3-none-any.whl pyarrow_hotfix-0.6-py3-none-any.whl datasets-2.15.0-py3-none-any.whl tyro-0.5.18-py3-none-any.whl trl-0.7.4-py3-none-any.whl\r\n\r\ndone","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6500","id":2043258633,"node_id":"PR_kwDODunzps5iFc6e","number":6500,"title":"Enable setting config as default when push_to_hub","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-12-15T09:17:41Z","updated_at":"2023-12-18T11:56:11Z","closed_at":"2023-12-18T11:50:03Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix #6497.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6500","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6500","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6500.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6500.patch","merged_at":"2023-12-18T11:50:03Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6499","id":2043166976,"node_id":"PR_kwDODunzps5iFIUF","number":6499,"title":"docs: add reference Git over SSH","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-15T08:38:31Z","updated_at":"2023-12-15T11:48:47Z","closed_at":"2023-12-15T11:42:38Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"see https:\/\/discuss.huggingface.co\/t\/update-datasets-getting-started-to-new-git-security\/65893","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6499","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6499","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6499.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6499.patch","merged_at":"2023-12-15T11:42:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6498","id":2042075969,"node_id":"PR_kwDODunzps5iBcFj","number":6498,"title":"Fallback on dataset script if user wants to load default config","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-12-14T16:46:01Z","updated_at":"2023-12-15T13:16:56Z","closed_at":"2023-12-15T13:10:48Z","author_association":"MEMBER","active_lock_reason":null,"body":"Right now this code is failing on `main`:\r\n\r\n```python\r\nload_dataset(\"openbookqa\")\r\n```\r\n\r\nThis is because it tries to load the dataset from the Parquet export but the dataset has multiple configurations and the Parquet export doesn't know which one is the default one.\r\n\r\nI fixed this by simply falling back on using the dataset script (which tells the user to pass `trust_remote_code=True`):\r\n\r\n```python\r\nload_dataset(\"openbookqa\", trust_remote_code=True)\r\n```\r\n\r\nNote that if the user happened to specify a config name I don't fall back on the script since we can use the Parquet export in this case (no need to know which config is the default)\r\n\r\n\r\n```python\r\nload_dataset(\"openbookqa\", \"main\")\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6498","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6498","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6498.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6498.patch","merged_at":"2023-12-15T13:10:48Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6497","id":2041994274,"node_id":"I_kwDODunzps55tlwi","number":6497,"title":"Support setting a default config name in push_to_hub","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-12-14T15:59:03Z","updated_at":"2023-12-18T11:50:04Z","closed_at":"2023-12-18T11:50:04Z","author_association":"MEMBER","active_lock_reason":null,"body":"In order to convert script-datasets to no-script datasets, we need to support setting a default config name for those scripts that set one.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6496","id":2041589386,"node_id":"I_kwDODunzps55sC6K","number":6496,"title":"Error when writing a dataset to HF Hub: A commit has happened since. Please refresh and try again.","user":{"login":"GeorgesLorre","id":35808396,"node_id":"MDQ6VXNlcjM1ODA4Mzk2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35808396?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/GeorgesLorre","html_url":"https:\/\/github.com\/GeorgesLorre","followers_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/followers","following_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/orgs","repos_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/repos","events_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-14T11:24:54Z","updated_at":"2023-12-14T12:22:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the bug**\r\n\r\nGetting a `412 Client Error: Precondition Failed` when trying to write a dataset to the HF hub.\r\n\r\n```\r\nhuggingface_hub.utils._errors.HfHubHTTPError: 412 Client Error: Precondition Failed for url: https:\/\/huggingface.co\/api\/datasets\/GLorr\/test-dask\/commit\/main (Request ID: Root=1-657ae26f-3bd92bf861bb254b2cc0826c;50a09ab7-9347-406a-ba49-69f98abee9cc)\r\n\r\nA commit has happened since. Please refresh and try again.\r\n```\r\n\r\n**Steps to reproduce the bug**\r\nThis is a minimal reproducer:\r\n```\r\nimport dask.dataframe as dd\r\nimport pandas as pd\r\nimport random\r\nimport os\r\n\r\nimport huggingface_hub\r\n\r\nimport datasets\r\n\r\nhuggingface_hub.login(token=os.getenv(\"HF_TOKEN\"))\r\n\r\ndata = {\"number\": [random.randint(0,10) for _ in range(1000)]}\r\ndf = pd.DataFrame.from_dict(data)\r\ndataframe = dd.from_pandas(df, npartitions=1)\r\ndataframe = dataframe.repartition(npartitions=3)\r\n\r\nschema = datasets.Features({\"number\": datasets.Value(\"int64\")}).arrow_schema\r\n\r\nrepo_id = \"GLorr\/test-dask\"\r\nrepo_path = f\"hf:\/\/datasets\/{repo_id}\"\r\nhuggingface_hub.create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=True)\r\ndd.to_parquet(dataframe, path=f\"{repo_path}\/data\", schema=schema)\r\n```\r\n\r\n**Expected behavior**\r\nWould expect to write to the hub without any problem.\r\n\r\n**Environment info**\r\n```\r\ndatasets==2.15.0\r\nhuggingface-hub==0.19.4\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6494","id":2039684839,"node_id":"I_kwDODunzps55kx7n","number":6494,"title":"Image Data loaded Twice","user":{"login":"baowuzhida","id":28867010,"node_id":"MDQ6VXNlcjI4ODY3MDEw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/28867010?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/baowuzhida","html_url":"https:\/\/github.com\/baowuzhida","followers_url":"https:\/\/api.github.com\/users\/baowuzhida\/followers","following_url":"https:\/\/api.github.com\/users\/baowuzhida\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/baowuzhida\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/baowuzhida\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/baowuzhida\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/baowuzhida\/orgs","repos_url":"https:\/\/api.github.com\/users\/baowuzhida\/repos","events_url":"https:\/\/api.github.com\/users\/baowuzhida\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/baowuzhida\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-13T13:11:42Z","updated_at":"2023-12-13T13:11:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n![1702472610561](https:\/\/github.com\/huggingface\/datasets\/assets\/28867010\/4b7ef5e7-32c3-4b73-84cb-5de059caa0b6)\r\nWhen I learn from https:\/\/huggingface.co\/docs\/datasets\/image_load and try to load image data from a folder. I noticed that the image was read twice in the returned data. As you can see in the attached image, there are only four images in the train folder, but reading brings up eight images\n\n### Steps to reproduce the bug\n\nfrom datasets import Dataset, load_dataset\r\n\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"data\/\", drop_labels=False)\r\n# print(dataset[\"train\"][0][\"image\"] == dataset[\"train\"][1][\"image\"])\r\n\r\nprint(dataset)\r\nprint(dataset[\"train\"][\"image\"])\r\nprint(len(dataset[\"train\"][\"image\"]))\n\n### Expected behavior\n\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 8\r\n    })\r\n})\r\n[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D1CA8B0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D2452E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D245310>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D2453A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D245460>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D245430>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D2454F0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D245550>]\r\n8\n\n### Environment info\n\n- `datasets` version: 2.14.5\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.9.17\r\n- Huggingface_hub version: 0.19.4\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6495","id":2039708529,"node_id":"I_kwDODunzps55k3tx","number":6495,"title":"Newline characters don't behave as expected when calling dataset.info","user":{"login":"gerald-wrona","id":32300890,"node_id":"MDQ6VXNlcjMyMzAwODkw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32300890?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/gerald-wrona","html_url":"https:\/\/github.com\/gerald-wrona","followers_url":"https:\/\/api.github.com\/users\/gerald-wrona\/followers","following_url":"https:\/\/api.github.com\/users\/gerald-wrona\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/gerald-wrona\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/gerald-wrona\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/gerald-wrona\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/gerald-wrona\/orgs","repos_url":"https:\/\/api.github.com\/users\/gerald-wrona\/repos","events_url":"https:\/\/api.github.com\/users\/gerald-wrona\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/gerald-wrona\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-12T23:07:51Z","updated_at":"2023-12-13T13:24:22Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### System Info\n\n- `transformers` version: 4.32.1\r\n- Platform: Windows-10-10.0.19045-SP0\r\n- Python version: 3.11.5\r\n- Huggingface_hub version: 0.15.1\r\n- Safetensors version: 0.3.2\r\n- Accelerate version: not installed\r\n- Accelerate config: not found\r\n- PyTorch version (GPU?): 2.1.1+cpu (False)\r\n- Tensorflow version (GPU?): 2.15.0 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n\n### Who can help?\n\n@marios\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n[Source](https:\/\/huggingface.co\/docs\/datasets\/v2.2.1\/en\/access)\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\ndataset.info\r\n```\r\n\r\nDatasetInfo(description='GLUE, the General Language Understanding Evaluation benchmark\\n(https:\/\/gluebenchmark.com\/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems.\\n\\n', citation='@inproceedings{dolan2005automatically,\\n  title={Automatically constructing a corpus of sentential paraphrases},\\n  author={Dolan, William B and Brockett, Chris},\\n  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\\n  year={2005}\\n}\\n@inproceedings{wang2019glue,\\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\\n  note={In the Proceedings of ICLR.},\\n  year={2019}\\n}\\n', homepage='https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=52398', license='', features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='glue', dataset_name=None, config_name='mrpc', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=943843, num_examples=3668, shard_lengths=None, dataset_name='glue'), 'validation': SplitInfo(name='validation', num_bytes=105879, num_examples=408, shard_lengths=None, dataset_name='glue'), 'test': SplitInfo(name='test', num_bytes=442410, num_examples=1725, shard_lengths=None, dataset_name='glue')}, download_checksums={'https:\/\/dl.fbaipublicfiles.com\/glue\/data\/mrpc_dev_ids.tsv': {'num_bytes': 6222, 'checksum': None}, 'https:\/\/dl.fbaipublicfiles.com\/senteval\/senteval_data\/msr_paraphrase_train.txt': {'num_bytes': 1047044, 'checksum': None}, 'https:\/\/dl.fbaipublicfiles.com\/senteval\/senteval_data\/msr_paraphrase_test.txt': {'num_bytes': 441275, 'checksum': None}}, download_size=1494541, post_processing_size=None, dataset_size=1492132, size_in_bytes=2986673)\n\n### Expected behavior\n\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\ndataset.info\r\n```\r\nDatasetInfo(\r\n    description='GLUE, the General Language Understanding Evaluation benchmark\\n(https:\/\/gluebenchmark.com\/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems.\\n\\n', \r\n    citation='@inproceedings{dolan2005automatically,\\n  title={Automatically constructing a corpus of sentential paraphrases},\\n  author={Dolan, William B and Brockett, Chris},\\n  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\\n  year={2005}\\n}\\n@inproceedings{wang2019glue,\\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\\n  note={In the Proceedings of ICLR.},\\n  year={2019}\\n}\\n', homepage='https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=52398', \r\n    license='', \r\n    features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, builder_name='glue', config_name='mrpc', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=943851, num_examples=3668, dataset_name='glue'), 'validation': SplitInfo(name='validation', num_bytes=105887, num_examples=408, dataset_name='glue'), 'test': SplitInfo(name='test', num_bytes=442418, num_examples=1725, dataset_name='glue')}, \r\n    download_checksums={'https:\/\/dl.fbaipublicfiles.com\/glue\/data\/mrpc_dev_ids.tsv': {'num_bytes': 6222, 'checksum': '971d7767d81b997fd9060ade0ec23c4fc31cbb226a55d1bd4a1bac474eb81dc7'}, 'https:\/\/dl.fbaipublicfiles.com\/senteval\/senteval_data\/msr_paraphrase_train.txt': {'num_bytes': 1047044, 'checksum': '60a9b09084528f0673eedee2b69cb941920f0b8cd0eeccefc464a98768457f89'}, 'https:\/\/dl.fbaipublicfiles.com\/senteval\/senteval_data\/msr_paraphrase_test.txt': {'num_bytes': 441275, 'checksum': 'a04e271090879aaba6423d65b94950c089298587d9c084bf9cd7439bd785f784'}}, \r\n    download_size=1494541, \r\n    post_processing_size=None, \r\n    dataset_size=1492156, \r\n    size_in_bytes=2986697\r\n)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6493","id":2038221490,"node_id":"PR_kwDODunzps5h0XJK","number":6493,"title":"Lazy data files resolution and offline cache reload","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-12-12T17:15:17Z","updated_at":"2023-12-21T15:19:20Z","closed_at":"2023-12-21T15:13:11Z","author_association":"MEMBER","active_lock_reason":null,"body":"Includes both https:\/\/github.com\/huggingface\/datasets\/pull\/6458 and https:\/\/github.com\/huggingface\/datasets\/pull\/6459\r\n\r\nThis PR should be merged instead of the two individually, since they are conflicting\r\n\r\n## Offline cache reload\r\n\r\n it can reload datasets that were pushed to hub if they exist in the cache.\r\n\r\nexample:\r\n\r\n```python\r\n>>> Dataset.from_dict({\"a\": [1, 2]}).push_to_hub(\"lhoestq\/tmp\")\r\n>>> load_dataset(\"lhoestq\/tmp\")\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['a'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\nand later, without connection:\r\n\r\n```python\r\n>>> load_dataset(\"lhoestq\/tmp\")\r\nUsing the latest cached version of the dataset since lhoestq\/tmp couldn't be found on the Hugging Face Hub\r\nFound the latest cached dataset configuration 'default' at \/Users\/quentinlhoest\/.cache\/huggingface\/datasets\/lhoestq___tmp\/default\/0.0.0\/da0e902a945afeb9 (last modified on Wed Dec 13 14:55:52 2023).\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['a'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\n- Updated `CachedDatasetModuleFactory` to look for datasets in the cache at `<namespace>___<dataset_name>\/<config_id>`\r\n- Since the metadata configs parameters are not available in offline mode, we don't know which folder to load (config_id and hash change), so I simply load the latest one\r\n  - I instantiate a BuilderConfig even if there is no metadata config with the right config_name\r\n  - Its config_id is equal to the config_name to be able to retrieve it in the cache (no more suffix for configs from metadata configs)\r\n  - We can reload this config if offline mode by specifying the right config_name (same as online !)\r\n- Consequences of this change:\r\n  - Only when there are user's parameters it creates a custom builder config with config_id = config_name + user parameters hash\r\n  - the hash used to name the cache folder takes into account the metadata config and the dataset info, so that the right cache can be reloaded when there is internet connection without redownloading the data or resolving the data files. For local directories I hash the builder configs and dataset info, and for datasets on the hub I use the commit sha as hash.\r\n  - cache directories now look like `config\/version\/commit_sha` for hub datasets which is clean :)\r\n\r\nFix https:\/\/github.com\/huggingface\/datasets\/issues\/3547\r\n\r\n## Lazy data files resolution\r\n\r\nthis makes this code run in 2sec instead of >10sec\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"glue\", \"sst2\", streaming=True, trust_remote_code=False)\r\n```\r\n\r\nFor some datasets with many configs and files it can be up to 100x faster.\r\nThis is particularly important now that some datasets will be loaded from the Parquet export instead of the scripts.\r\n\r\nThe data files are only resolved in the builder `__init__`. To do so I added DataFilesPatternsList and DataFilesPatternsDict that have `.resolve()` to return resolved DataFilesList and DataFilesDict\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6493","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6493","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6493.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6493.patch","merged_at":"2023-12-21T15:13:11Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6492","id":2037987267,"node_id":"PR_kwDODunzps5hzjhQ","number":6492,"title":"Make push_to_hub return CommitInfo","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-12T15:18:16Z","updated_at":"2023-12-13T14:29:01Z","closed_at":"2023-12-13T14:22:41Z","author_association":"MEMBER","active_lock_reason":null,"body":"Make `push_to_hub` return `CommitInfo`.\r\n\r\nThis is useful, for example, if we pass `create_pr=True` and we want to know the created PR ID.\r\n\r\nCC: @severo for the use case in https:\/\/huggingface.co\/datasets\/jmhessel\/newyorker_caption_contest\/discussions\/4","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6492","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6492","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6492.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6492.patch","merged_at":"2023-12-13T14:22:41Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6491","id":2037690643,"node_id":"PR_kwDODunzps5hyiTY","number":6491,"title":"Fix metrics dead link","user":{"login":"qgallouedec","id":45557362,"node_id":"MDQ6VXNlcjQ1NTU3MzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45557362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/qgallouedec","html_url":"https:\/\/github.com\/qgallouedec","followers_url":"https:\/\/api.github.com\/users\/qgallouedec\/followers","following_url":"https:\/\/api.github.com\/users\/qgallouedec\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/qgallouedec\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/qgallouedec\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/qgallouedec\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/qgallouedec\/orgs","repos_url":"https:\/\/api.github.com\/users\/qgallouedec\/repos","events_url":"https:\/\/api.github.com\/users\/qgallouedec\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/qgallouedec\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-12T12:51:49Z","updated_at":"2023-12-21T15:15:08Z","closed_at":"2023-12-21T15:08:53Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6491","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6491","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6491.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6491.patch","merged_at":"2023-12-21T15:08:53Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6490","id":2037204892,"node_id":"I_kwDODunzps55bUec","number":6490,"title":"`load_dataset(...,save_infos=True)` not working without loading script","user":{"login":"morganveyret","id":114978051,"node_id":"U_kgDOBtptAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/114978051?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/morganveyret","html_url":"https:\/\/github.com\/morganveyret","followers_url":"https:\/\/api.github.com\/users\/morganveyret\/followers","following_url":"https:\/\/api.github.com\/users\/morganveyret\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/morganveyret\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/morganveyret\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/morganveyret\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/morganveyret\/orgs","repos_url":"https:\/\/api.github.com\/users\/morganveyret\/repos","events_url":"https:\/\/api.github.com\/users\/morganveyret\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/morganveyret\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-12T08:09:18Z","updated_at":"2023-12-12T08:36:22Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nIt seems that saving a dataset infos back into the card file is not working for datasets without a loading script.\r\n\r\nAfter tracking the problem a bit it looks like saving the infos uses `Builder.get_imported_module_dir()` as its destination directory.\r\nInternally this is a call to `inspect.getfile()` but since the actual builder class used is dynamically created (cf. `datasets.load.configure_builder_class`) this method actually return te path to the parent builder class (e.g. `datasets.packaged_modules.json.JSON`). \r\n\r\n\n\n### Steps to reproduce the bug\n\n1. Have a local dataset without any loading script\r\n2. Make sure there are no dataset infos in the README.md\r\n3. Load with `save_infos=True`\r\n4. No change in the dataset README.md\r\n5. A new README.md file is created in the directory of the parent builder class (e.g. for json in `...\/site-packages\/datasets\/packaged_modules\/json\/README.md`)\n\n### Expected behavior\n\nThe dataset README.md should be updated and no file should be created in the python environment.\n\n### Environment info\n\n- `datasets` version: 2.15.0\r\n- Platform: Linux-6.2.0-37-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.3\r\n- `fsspec` version: 2023.6.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6489","id":2036743777,"node_id":"I_kwDODunzps55Zj5h","number":6489,"title":"load_dataset imageflder for aws s3 path ","user":{"login":"segalinc","id":9353106,"node_id":"MDQ6VXNlcjkzNTMxMDY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9353106?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/segalinc","html_url":"https:\/\/github.com\/segalinc","followers_url":"https:\/\/api.github.com\/users\/segalinc\/followers","following_url":"https:\/\/api.github.com\/users\/segalinc\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/segalinc\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/segalinc\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/segalinc\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/segalinc\/orgs","repos_url":"https:\/\/api.github.com\/users\/segalinc\/repos","events_url":"https:\/\/api.github.com\/users\/segalinc\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/segalinc\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-12T00:08:43Z","updated_at":"2023-12-12T00:09:27Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nI would like to load a dataset from S3 using the imagefolder option \r\nsomething like \r\n`dataset = datasets.load_dataset('imagefolder', data_dir='s3:\/\/...\/lsun\/train\/bedroom', fs=S3FileSystem(), streaming=True)  `\r\n\r\n### Motivation\r\n\r\nno need of data_files\r\n\r\n### Your contribution\r\n\r\nno experience with this","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6488","id":2035899898,"node_id":"I_kwDODunzps55WV36","number":6488,"title":"429 Client Error","user":{"login":"sasaadi","id":7882383,"node_id":"MDQ6VXNlcjc4ODIzODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7882383?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sasaadi","html_url":"https:\/\/github.com\/sasaadi","followers_url":"https:\/\/api.github.com\/users\/sasaadi\/followers","following_url":"https:\/\/api.github.com\/users\/sasaadi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sasaadi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sasaadi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sasaadi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sasaadi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sasaadi\/repos","events_url":"https:\/\/api.github.com\/users\/sasaadi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sasaadi\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-11T15:06:01Z","updated_at":"2023-12-11T15:34:23Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello, I was downloading the following dataset and after 20% of data was downloaded, I started getting error 429. It is not resolved since a few days. How should I resolve it?\r\nThanks\r\n\r\nDataset:\r\nhttps:\/\/huggingface.co\/datasets\/cerebras\/SlimPajama-627B\r\n\r\nError:\r\n`requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https:\/\/huggingface.co\/datasets\/cerebras\/SlimPajama-627B\/resolve\/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543\/train\/chunk1\/example_train_3300.jsonl.zst`\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6487","id":2035424254,"node_id":"PR_kwDODunzps5hqyfV","number":6487,"title":"Update builder hash with info","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-11T11:09:16Z","updated_at":"2023-12-11T11:41:34Z","closed_at":"2023-12-11T11:41:34Z","author_association":"MEMBER","active_lock_reason":null,"body":"Currently if you change the `dataset_info` of a dataset (e.g. in the YAML part of the README.md), the cache ignores this change.\r\n\r\nThis is problematic because you want to regenerate a dataset if you change the features or the split sizes for example (e.g. after push_to_hub)\r\n\r\nIdeally we should take the resolved files into account as well but this will be for another PR","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6487","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6487","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6487.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6487.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6486","id":2035206206,"node_id":"PR_kwDODunzps5hqCSc","number":6486,"title":"Fix docs phrasing about supported formats when sharing a dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-11T09:21:22Z","updated_at":"2023-12-13T14:21:29Z","closed_at":"2023-12-13T14:15:21Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix docs phrasing.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6486","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6486","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6486.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6486.patch","merged_at":"2023-12-13T14:15:21Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6485","id":2035141884,"node_id":"I_kwDODunzps55Tcz8","number":6485,"title":"FileNotFoundError: [Errno 2] No such file or directory: 'nul'","user":{"login":"amanyara","id":73683903,"node_id":"MDQ6VXNlcjczNjgzOTAz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/73683903?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/amanyara","html_url":"https:\/\/github.com\/amanyara","followers_url":"https:\/\/api.github.com\/users\/amanyara\/followers","following_url":"https:\/\/api.github.com\/users\/amanyara\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/amanyara\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/amanyara\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/amanyara\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/amanyara\/orgs","repos_url":"https:\/\/api.github.com\/users\/amanyara\/repos","events_url":"https:\/\/api.github.com\/users\/amanyara\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/amanyara\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-11T08:52:13Z","updated_at":"2023-12-14T08:09:08Z","closed_at":"2023-12-14T08:09:08Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nit seems that sth wrong with my terrible \"bug body\" life, When i run this code, \"import datasets\"\r\ni meet this error FileNotFoundError: [Errno 2] No such file or directory: 'nul'\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/73683903\/3973c120-ebb1-42b7-bede-b9de053e861d)\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/73683903\/0496adff-a7a7-4dcb-929e-ec11ede71f04)\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n1.import datasets\r\n\r\n### Expected behavior\r\n\r\ni just run a single line code and stuct in this bug \r\n\r\n### Environment info\r\n\r\nOS: Windows10\r\nDatasets==2.15.0\r\npython=3.10","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6483","id":2032946981,"node_id":"I_kwDODunzps55LE8l","number":6483,"title":"Iterable Dataset: rename column clashes with remove column","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[{"id":3287858981,"node_id":"MDU6TGFiZWwzMjg3ODU4OTgx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/streaming","name":"streaming","color":"fef2c0","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-12-08T16:11:30Z","updated_at":"2023-12-08T16:27:16Z","closed_at":"2023-12-08T16:27:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nSuppose I have a two iterable datasets, one with the features: \r\n* `{\"audio\", \"text\", \"column_a\"}`\r\n\r\nAnd the other with the features:\r\n* `{\"audio\", \"sentence\", \"column_b\"}`\r\n\r\nI want to combine both datasets using `interleave_datasets`, which requires me to unify the column names. I would typically do this by:\r\n1. Renaming the common columns to the same name (e.g. `\"text\"` -> `\"sentence\"`)\r\n2. Removing the unwanted columns (e.g. `\"column_a\"`, `\"column_b\"`)\r\n\r\nHowever, the process of renaming and removing columns in an iterable dataset doesn't work, since we need to preserve the original text column, meaning we can't combine the datasets.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# load LS in streaming mode\r\ndataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", streaming=True)\r\n\r\n# check original features\r\ndataset_features = dataset.features.keys()\r\nprint(\"Original features: \", dataset_features)\r\n\r\n#\u00a0rename \"text\" -> \"sentence\"\r\ndataset = dataset.rename_column(\"text\", \"sentence\")\r\n\r\n# remove unwanted columns\r\nCOLUMNS_TO_KEEP = {\"audio\", \"sentence\"}\r\ndataset = dataset.remove_columns(set(dataset_features - COLUMNS_TO_KEEP))\r\n\r\n# stream first sample, should return \"audio\" and \"sentence\" columns\r\nprint(next(iter(dataset)))\r\n```\r\nTraceback:\r\n```python\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\nCell In[5], line 17\r\n     14 COLUMNS_TO_KEEP = {\"audio\", \"sentence\"}\r\n     15 dataset = dataset.remove_columns(set(dataset_features - COLUMNS_TO_KEEP))\r\n---> 17 print(next(iter(dataset)))\r\n\r\nFile ~\/datasets\/src\/datasets\/iterable_dataset.py:1353, in IterableDataset.__iter__(self)\r\n   1350         yield formatter.format_row(pa_table)\r\n   1351     return\r\n-> 1353 for key, example in ex_iterable:\r\n   1354     if self.features:\r\n   1355         # `IterableDataset` automatically fills missing columns with None.\r\n   1356         # This is done with `_apply_feature_types_on_example`.\r\n   1357         example = _apply_feature_types_on_example(\r\n   1358             example, self.features, token_per_repo_id=self._token_per_repo_id\r\n   1359         )\r\n\r\nFile ~\/datasets\/src\/datasets\/iterable_dataset.py:652, in MappedExamplesIterable.__iter__(self)\r\n    650     yield from ArrowExamplesIterable(self._iter_arrow, {})\r\n    651 else:\r\n--> 652     yield from self._iter()\r\n\r\nFile ~\/datasets\/src\/datasets\/iterable_dataset.py:729, in MappedExamplesIterable._iter(self)\r\n    727 if self.remove_columns:\r\n    728     for c in self.remove_columns:\r\n--> 729         del transformed_example[c]\r\n    730 yield key, transformed_example\r\n    731 current_idx += 1\r\n\r\nKeyError: 'text'\r\n```\r\n=> we see that `datasets` is looking for the column \"text\", even though we've renamed this to \"sentence\" and then removed the un-wanted \"text\" column from our dataset.\r\n\r\n### Expected behavior\r\n\r\nShould be able to rename and remove columns from iterable dataset.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.1.dev0\r\n- Platform: macOS-13.5.1-arm64-arm-64bit\r\n- Python version: 3.11.6\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.2\r\n- `fsspec` version: 2023.9.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6484","id":2033333294,"node_id":"I_kwDODunzps55MjQu","number":6484,"title":"[Feature Request] Dataset versioning","user":{"login":"kenfus","id":47979198,"node_id":"MDQ6VXNlcjQ3OTc5MTk4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47979198?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kenfus","html_url":"https:\/\/github.com\/kenfus","followers_url":"https:\/\/api.github.com\/users\/kenfus\/followers","following_url":"https:\/\/api.github.com\/users\/kenfus\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kenfus\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kenfus\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kenfus\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kenfus\/orgs","repos_url":"https:\/\/api.github.com\/users\/kenfus\/repos","events_url":"https:\/\/api.github.com\/users\/kenfus\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kenfus\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-08T16:01:35Z","updated_at":"2023-12-11T19:13:46Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nI am working on a project, where I would like to test different preprocessing methods for my ML-data. Thus, I would like to work a lot with revisions and compare them. Currently, I was not able to make it work with the revision keyword because it was not redownloading the data, it was reading in some cached data, until I put  `download_mode=\"force_redownload\"`, even though the reversion was different. \r\nOf course, I may have done something wrong or missed a setting somewhere! \r\n\r\n**Describe the solution you'd like**\r\nThe solution would allow me to easily work with revisions: \r\n- create a new dataset (by combining things, different preprocessing, ..) and give it a new revision (v.1.2.3), maybe like this:\r\n`dataset_audio.push_to_hub('kenfus\/xy', revision='v1.0.2')`\r\n\r\n- then, get the current revision as follows:         \r\n```\r\ndataset = load_dataset(\r\n            'kenfus\/xy', revision='v1.0.2',\r\n        )\r\n```\r\nthis downloads the new version and does not load in a different revision and all future map, filter, .. operations are done on this dataset and not loaded from cache produced from a different revision. \r\n- if I rerun the run, the caching should be smart enough in every step to not reuse a mapping operation on a different revision. \r\n\r\n**Describe alternatives you've considered**\r\nI created my own caching, putting `download_mode=\"force_redownload\"` and `load_from_cache_file=False,` everywhere.\r\n\r\n**Additional context**\r\nThanks a lot for your great work! Creating NLP datasets and training a model with them is really easy and straightforward with huggingface.\r\n\r\nThis is the data loading in my script:\r\n\r\n```\r\n    ## CREATE PATHS\r\n    prepared_dataset_path = os.path.join(\r\n        DATA_FOLDER, str(DATA_VERSION), \"prepared_dataset\"\r\n    )\r\n    os.makedirs(os.path.join(DATA_FOLDER, str(DATA_VERSION)), exist_ok=True)\r\n\r\n    ## LOAD DATASET\r\n    if os.path.exists(prepared_dataset_path):\r\n        print(\"Loading prepared dataset from disk...\")\r\n        dataset_prepared = load_from_disk(prepared_dataset_path)\r\n    else:\r\n        print(\"Loading dataset from HuggingFace Datasets...\")\r\n        dataset = load_dataset(\r\n            PATH_TO_DATASET, revision=DATA_VERSION, download_mode=\"force_redownload\"\r\n        )\r\n\r\n        print(\"Preparing dataset...\")\r\n        dataset_prepared = dataset.map(\r\n            prepare_dataset,\r\n            remove_columns=[\"audio\", \"transcription\"],\r\n            num_proc=os.cpu_count(),\r\n            load_from_cache_file=False,\r\n        )\r\n        dataset_prepared.save_to_disk(prepared_dataset_path)\r\n        del dataset\r\n\r\n    if CHECK_DATASET:\r\n        ## CHECK DATASET\r\n        dataset_prepared = dataset_prepared.map(\r\n            check_dimensions, num_proc=os.cpu_count(), load_from_cache_file=False\r\n        )\r\n        dataset_filtered = dataset_prepared.filter(\r\n            lambda example: not example[\"incorrect_dimension\"],\r\n            load_from_cache_file=False,\r\n        )\r\n\r\n        for example in dataset_prepared.filter(\r\n            lambda example: example[\"incorrect_dimension\"], load_from_cache_file=False\r\n        ):\r\n            print(example[\"path\"])\r\n\r\n        print(\r\n            f\"Number of examples with incorrect dimension:  {len(dataset_prepared) - len(dataset_filtered)}\"\r\n        )\r\n\r\n        print(\"Number of examples train: \", len(dataset_filtered[\"train\"]))\r\n        print(\"Number of examples test: \", len(dataset_filtered[\"test\"]))\r\n\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6482","id":2032675918,"node_id":"PR_kwDODunzps5hhl23","number":6482,"title":"Fix max lock length on unix","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-08T13:39:30Z","updated_at":"2023-12-12T11:53:32Z","closed_at":"2023-12-12T11:47:27Z","author_association":"MEMBER","active_lock_reason":null,"body":"reported in https:\/\/github.com\/huggingface\/datasets\/pull\/6482","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6482","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6482","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6482.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6482.patch","merged_at":"2023-12-12T11:47:27Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6481","id":2032650003,"node_id":"I_kwDODunzps55J8cT","number":6481,"title":"using torchrun, save_to_disk suddenly shows SIGTERM","user":{"login":"Ariya12138","id":85916625,"node_id":"MDQ6VXNlcjg1OTE2NjI1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/85916625?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Ariya12138","html_url":"https:\/\/github.com\/Ariya12138","followers_url":"https:\/\/api.github.com\/users\/Ariya12138\/followers","following_url":"https:\/\/api.github.com\/users\/Ariya12138\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Ariya12138\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Ariya12138\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Ariya12138\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Ariya12138\/orgs","repos_url":"https:\/\/api.github.com\/users\/Ariya12138\/repos","events_url":"https:\/\/api.github.com\/users\/Ariya12138\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Ariya12138\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-08T13:22:03Z","updated_at":"2023-12-08T13:22:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I run my code using the \"torchrun\" command, when the code reaches the \"save_to_disk\" part, suddenly I get the following warning and error messages:\r\nBecause the dataset is too large, the \"save_to_disk\" function splits it into 70 parts for saving. However, an error occurs suddenly when it reaches the 14th shard.\r\nWARNING: torch.distributed.elastic.multiprocessing.api: Sending process 2224968 closing signal SIGTERM\r\nERROR: torch.distributed.elastic.multiprocessing.api: failed (exitcode: -7). traceback: Signal 7 (SIGBUS) received by PID 2224967.\n\n### Steps to reproduce the bug\n\nds_shard = ds_shard.map(map_fn, *args, **kwargs)\r\nds_shard.save_to_disk(ds_shard_filepaths[rank])\r\n\r\nSaving the dataset (14\/70 shards):  20%|\u2588\u2588        | 875350\/4376702 [00:19<01:53, 30863.15 examples\/s]\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2224968 closing signal SIGTERM\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -7) local_rank: 0 (pid: 2224967) of binary: \/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/bin\/python\r\nTraceback (most recent call last):\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/bin\/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/lib\/python3.10\/site-packages\/torch\/distributed\/elastic\/multiprocessing\/errors\/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/lib\/python3.10\/site-packages\/torch\/distributed\/run.py\", line 794, in main\r\n    run(args)\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/lib\/python3.10\/site-packages\/torch\/distributed\/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/lib\/python3.10\/site-packages\/torch\/distributed\/launcher\/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/lib\/python3.10\/site-packages\/torch\/distributed\/launcher\/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n==========================================================\r\nrun.py FAILED\r\n----------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n----------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-12-08_20:09:04\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : -7 (pid: 2224967)\r\n  error_file: <N\/A>\r\n  traceback : Signal 7 (SIGBUS) received by PID 2224967\r\n\r\n\n\n### Expected behavior\n\nI hope it can save successfully without any issues, but it seems there is a problem.\n\n### Environment info\n\n`datasets` version: 2.14.6\r\n- Platform: Linux-4.19.90-24.4.v2101.ky10.aarch64-aarch64-with-glibc2.28\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 14.0.0\r\n- Pandas version: 2.1.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6480","id":2031116653,"node_id":"PR_kwDODunzps5hcS7P","number":6480,"title":"Add IterableDataset `__repr__`","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-07T16:31:50Z","updated_at":"2023-12-08T13:33:06Z","closed_at":"2023-12-08T13:26:54Z","author_association":"MEMBER","active_lock_reason":null,"body":"Example for glue sst2:\r\n\r\nDataset\r\n\r\n```\r\nDatasetDict({\r\n    test: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 1821\r\n    })\r\n    train: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 67349\r\n    })\r\n    validation: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 872\r\n    })\r\n})\r\n```\r\n\r\nIterableDataset (new)\r\n\r\n```\r\nIterableDatasetDict({\r\n    test: IterableDataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        n_shards: 1\r\n    })\r\n    train: IterableDataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        n_shards: 1\r\n    })\r\n    validation: IterableDataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        n_shards: 1\r\n    })\r\n})\r\n```\r\n\r\nIterableDataset (before)\r\n\r\n```\r\n{'test': <datasets.iterable_dataset.IterableDataset object at 0x130d421f0>, 'train': <datasets.iterable_dataset.IterableDataset object at 0x136f3aaf0>, 'validation': <datasets.iterable_dataset.IterableDataset object at 0x136f4b100>}\r\n{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6480","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6480","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6480.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6480.patch","merged_at":"2023-12-08T13:26:54Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6479","id":2029040121,"node_id":"PR_kwDODunzps5hVLom","number":6479,"title":"More robust preupload retry mechanism","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-06T17:19:38Z","updated_at":"2023-12-06T19:47:29Z","closed_at":"2023-12-06T19:41:06Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6479","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6479","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6479.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6479.patch","merged_at":"2023-12-06T19:41:06Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6478","id":2028071596,"node_id":"I_kwDODunzps544eqs","number":6478,"title":"How to load data from lakefs","user":{"login":"d710055071","id":12895488,"node_id":"MDQ6VXNlcjEyODk1NDg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12895488?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/d710055071","html_url":"https:\/\/github.com\/d710055071","followers_url":"https:\/\/api.github.com\/users\/d710055071\/followers","following_url":"https:\/\/api.github.com\/users\/d710055071\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/d710055071\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/d710055071\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/d710055071\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/d710055071\/orgs","repos_url":"https:\/\/api.github.com\/users\/d710055071\/repos","events_url":"https:\/\/api.github.com\/users\/d710055071\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/d710055071\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-06T09:04:11Z","updated_at":"2023-12-07T02:19:44Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"My dataset is stored on the company's lakefs server. How can I write code to load the dataset? It would be great if I could provide code examples or provide some references\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6477","id":2028022374,"node_id":"PR_kwDODunzps5hRq_N","number":6477,"title":"Fix PermissionError on Windows CI","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-06T08:34:53Z","updated_at":"2023-12-06T09:24:11Z","closed_at":"2023-12-06T09:17:52Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix #6476.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6477","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6477","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6477.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6477.patch","merged_at":"2023-12-06T09:17:52Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6476","id":2028018596,"node_id":"I_kwDODunzps544Ruk","number":6476,"title":"CI on windows is broken: PermissionError","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-12-06T08:32:53Z","updated_at":"2023-12-06T09:17:53Z","closed_at":"2023-12-06T09:17:53Z","author_association":"MEMBER","active_lock_reason":null,"body":"See: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/7104781624\/job\/19340572394\r\n```\r\nFAILED tests\/test_load.py::test_loading_from_the_datasets_hub - NotADirectoryError: [WinError 267] The directory name is invalid: 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfcnps56i\\\\hf-internal-testing___dataset_with_script\\\\default\\\\0.0.0\\\\c240e2be3370bdbd\\\\dataset_with_script-train.arrow'\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6475","id":2027373734,"node_id":"I_kwDODunzps5410Sm","number":6475,"title":"laion2B-en failed to load on Windows with PrefetchVirtualMemory failed","user":{"login":"doctorpangloss","id":2229300,"node_id":"MDQ6VXNlcjIyMjkzMDA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2229300?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/doctorpangloss","html_url":"https:\/\/github.com\/doctorpangloss","followers_url":"https:\/\/api.github.com\/users\/doctorpangloss\/followers","following_url":"https:\/\/api.github.com\/users\/doctorpangloss\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/doctorpangloss\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/doctorpangloss\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/doctorpangloss\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/doctorpangloss\/orgs","repos_url":"https:\/\/api.github.com\/users\/doctorpangloss\/repos","events_url":"https:\/\/api.github.com\/users\/doctorpangloss\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/doctorpangloss\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-12-06T00:07:34Z","updated_at":"2023-12-06T23:26:23Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI have downloaded laion2B-en, and I'm receiving the following error trying to load it:\r\n```\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128\/128 [00:00<00:00, 1173.79it\/s]\r\nTraceback (most recent call last):\r\n  File \"D:\\Art-Workspace\\src\\artworkspace\\tokeneval\\compute_frequencies.py\", line 31, in <module>\r\n    count = compute_frequencies()\r\n            ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Art-Workspace\\src\\artworkspace\\tokeneval\\compute_frequencies.py\", line 17, in compute_frequencies\r\n    laion2b_dataset = load_dataset(\"laion\/laion2B-en\", split=\"train\", cache_dir=_CACHE_DIR, keep_in_memory=False)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\load.py\", line 2165, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\builder.py\", line 1187, in as_dataset\r\n    datasets = map_nested(\r\n               ^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\utils\\py_utils.py\", line 456, in map_nested\r\n    return function(data_struct)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\builder.py\", line 1217, in _build_single_dataset\r\n    ds = self._as_dataset(\r\n         ^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\builder.py\", line 1291, in _as_dataset\r\n    dataset_kwargs = ArrowReader(cache_dir, self.info).read(\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 244, in read\r\n    return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 265, in read_files\r\n    pa_table = self._read_files(files, in_memory=in_memory)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 200, in _read_files\r\n    pa_table: Table = self._get_table_from_filename(f_dict, in_memory=in_memory)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 336, in _get_table_from_filename\r\n    table = ArrowReader.read_table(filename, in_memory=in_memory)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 357, in read_table\r\n    return table_cls.from_file(filename)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\table.py\", line 1059, in from_file\r\n    table = _memory_mapped_arrow_table_from_file(filename)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\table.py\", line 66, in _memory_mapped_arrow_table_from_file\r\n    pa_table = opened_stream.read_all()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow\\ipc.pxi\", line 757, in pyarrow.lib.RecordBatchReader.read_all\r\n  File \"pyarrow\\error.pxi\", line 91, in pyarrow.lib.check_status\r\nOSError: [WinError 8] PrefetchVirtualMemory failed. Detail: [Windows error 8] Not enough memory resources are available to process this command.\r\n```\r\n\r\nThis error is probably a red herring: https:\/\/stackoverflow.com\/questions\/50263929\/numpy-memmap-returns-not-enough-memory-while-there-are-plenty-available In other words, the issue is related to asking for a memory mapping of length N > M the length of the file on Windows. This gracefully succeeds on Linux.\r\n\r\nI have 1024 arrow files in my cache instead of 128 like in the repository for it. Probably related. I don't know why `datasets` reorganized\/rewrote the dataset in my cache to be 1024 slices instead of the original 128.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\n# as a huggingface developer, you may already have laion2B-en somewhere\r\n_CACHE_DIR = \".\"\r\n\r\nfrom datasets import load_dataset\r\nload_dataset(\"laion\/laion2B-en\", split=\"train\", cache_dir=_CACHE_DIR, keep_in_memory=False)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThis should correctly load as a memory mapped Arrow dataset.\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: Windows-10-10.0.20348-SP0 (this is windows 2022)\r\n- Python version: 3.11.4\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.2\r\n- `fsspec` version: 2023.10.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475\/timeline","performed_via_github_app":null,"state_reason":"reopened","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6474","id":2027006715,"node_id":"PR_kwDODunzps5hONZc","number":6474,"title":"Deprecate Beam API and download from HF GCS bucket","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-05T19:51:33Z","updated_at":"2023-12-10T17:55:50Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Deprecate the Beam API and download from the HF GCS bucked. \r\n\r\nTODO:\r\n- [ ] Deprecate the Beam-based [`wikipedia`](https:\/\/huggingface.co\/datasets\/wikipedia) in favor of [`wikimedia\/wikipedia`](https:\/\/huggingface.co\/datasets\/wikimedia\/wikipedia) ([Hub PR](https:\/\/huggingface.co\/datasets\/wikipedia\/discussions\/19))\r\n- [ ] Make [`natural_questions`](https:\/\/huggingface.co\/datasets\/natural_questions) a no-code dataset\r\n- [ ] Make [`wiki40b`](https:\/\/huggingface.co\/datasets\/wiki40b) a no-code dataset\r\n- [ ] Make [`wiki_dpr`](https:\/\/huggingface.co\/datasets\/wiki_dpr) an Arrow-based dataset","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6474","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6474","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6474.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6474.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6473","id":2026495084,"node_id":"PR_kwDODunzps5hMbvz","number":6473,"title":"Fix CI quality","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-05T15:36:23Z","updated_at":"2023-12-05T18:14:50Z","closed_at":"2023-12-05T18:08:41Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix #6472.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6473","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6473","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6473.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6473.patch","merged_at":"2023-12-05T18:08:41Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6472","id":2026493439,"node_id":"I_kwDODunzps54ydX_","number":6472,"title":"CI quality is broken","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":4296013012,"node_id":"LA_kwDODunzps8AAAABAA_01A","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/maintenance","name":"maintenance","color":"d4c5f9","default":false,"description":"Maintenance tasks"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-12-05T15:35:34Z","updated_at":"2023-12-06T08:17:34Z","closed_at":"2023-12-05T18:08:43Z","author_association":"MEMBER","active_lock_reason":null,"body":"See: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/7100835633\/job\/19327734359\r\n```\r\nWould reformat: src\/datasets\/features\/image.py\r\n1 file would be reformatted, 253 files left unchanged\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6471","id":2026100761,"node_id":"PR_kwDODunzps5hLEni","number":6471,"title":"Remove delete doc CI","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-05T12:37:50Z","updated_at":"2023-12-05T12:44:59Z","closed_at":"2023-12-05T12:38:50Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6471","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6471","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6471.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6471.patch","merged_at":"2023-12-05T12:38:50Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6470","id":2024724319,"node_id":"I_kwDODunzps54rtdf","number":6470,"title":"If an image in a dataset is corrupted, we get unescapable error","user":{"login":"chigozienri","id":14337872,"node_id":"MDQ6VXNlcjE0MzM3ODcy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14337872?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/chigozienri","html_url":"https:\/\/github.com\/chigozienri","followers_url":"https:\/\/api.github.com\/users\/chigozienri\/followers","following_url":"https:\/\/api.github.com\/users\/chigozienri\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/chigozienri\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/chigozienri\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/chigozienri\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/chigozienri\/orgs","repos_url":"https:\/\/api.github.com\/users\/chigozienri\/repos","events_url":"https:\/\/api.github.com\/users\/chigozienri\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/chigozienri\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-04T20:58:49Z","updated_at":"2023-12-04T20:58:49Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nExample discussed in detail here: https:\/\/huggingface.co\/datasets\/sasha\/birdsnap\/discussions\/1\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset, VerificationMode\r\ndataset = load_dataset(\r\n    'sasha\/birdsnap',\r\n    split=\"train\",\r\n    verification_mode=VerificationMode.ALL_CHECKS,\r\n    streaming=True # I recommend using streaming=True when reproducing, as this dataset is large\r\n)\r\nfor idx, row in enumerate(dataset):\r\n    # Iterating to 9287 took 7 minutes for me\r\n    # If you already have the data locally cached and set streaming=False, you see the same error just by with dataset[9287]\r\n    pass\r\n    # error at 9287 OSError: image file is truncated (45 bytes not processed)\r\n    # note that we can't avoid the error using a try\/except + continue inside the loop\r\n```\n\n### Expected behavior\n\nAble to escape errors in casting to Image() without killing the whole loop\n\n### Environment info\n\n- `datasets` version: 2.15.0\r\n- Platform: Linux-5.15.0-84-generic-x86_64-with-glibc2.31\r\n- Python version: 3.11.5\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.3\r\n- `fsspec` version: 2023.10.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6469","id":2023695839,"node_id":"PR_kwDODunzps5hC6xf","number":6469,"title":"Don't expand_info in HF glob","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-04T12:00:37Z","updated_at":"2023-12-15T13:18:37Z","closed_at":"2023-12-15T13:12:30Z","author_association":"MEMBER","active_lock_reason":null,"body":"Finally fix https:\/\/github.com\/huggingface\/datasets\/issues\/5537","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6469","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6469","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6469.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6469.patch","merged_at":"2023-12-15T13:12:30Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6468","id":2023617877,"node_id":"PR_kwDODunzps5hCpbN","number":6468,"title":"Use auth to get parquet export","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-04T11:18:27Z","updated_at":"2023-12-04T17:21:22Z","closed_at":"2023-12-04T17:15:11Z","author_association":"MEMBER","active_lock_reason":null,"body":"added `token` to the `_datasets_server` functions","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6468","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6468","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6468.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6468.patch","merged_at":"2023-12-04T17:15:11Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6467","id":2023174233,"node_id":"I_kwDODunzps54lzBZ","number":6467,"title":"New version release request","user":{"login":"LZHgrla","id":36994684,"node_id":"MDQ6VXNlcjM2OTk0Njg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36994684?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/LZHgrla","html_url":"https:\/\/github.com\/LZHgrla","followers_url":"https:\/\/api.github.com\/users\/LZHgrla\/followers","following_url":"https:\/\/api.github.com\/users\/LZHgrla\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/LZHgrla\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/LZHgrla\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/LZHgrla\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/LZHgrla\/orgs","repos_url":"https:\/\/api.github.com\/users\/LZHgrla\/repos","events_url":"https:\/\/api.github.com\/users\/LZHgrla\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/LZHgrla\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-04T07:08:26Z","updated_at":"2023-12-04T15:42:22Z","closed_at":"2023-12-04T15:42:22Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\r\n\r\nHi!\r\nI am using `datasets` in library `xtuner` and am highly interested in the features introduced since v2.15.0.\r\n\r\nTo avoid installation from source in our pypi wheels, we are eagerly waiting for the new release. So, Does your team have a new release plan for v2.15.1 and could you please share it with us?\r\n\r\nThanks very much!\r\n\r\n\r\n### Motivation\r\n\r\n.\r\n\r\n### Your contribution\r\n\r\n.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6566","id":2069495429,"node_id":"I_kwDODunzps57Wf6F","number":6566,"title":"I train controlnet_sdxl in bf16 datatype, got unsupported ERROR in datasets","user":{"login":"HelloWorldBeginner","id":25008090,"node_id":"MDQ6VXNlcjI1MDA4MDkw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/25008090?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/HelloWorldBeginner","html_url":"https:\/\/github.com\/HelloWorldBeginner","followers_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/followers","following_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/orgs","repos_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/repos","events_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/HelloWorldBeginner\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2024-01-08T02:37:03Z","updated_at":"2024-01-08T02:37:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n```\r\nTraceback (most recent call last):\r\n  File \"train_controlnet_sdxl.py\", line 1252, in <module>\r\n    main(args)\r\n  File \"train_controlnet_sdxl.py\", line 1013, in main\r\n    train_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 592, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 3093, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 3489, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_writer.py\", line 557, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow\/array.pxi\", line 248, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 113, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/arrow_writer.py\", line 191, in __arrow_array__\r\n    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/features\/features.py\", line 447, in cast_to_python_objects\r\n    return _cast_to_python_objects(\r\n  File \"\/home\/miniconda3\/envs\/mhh_df\/lib\/python3.8\/site-packages\/datasets\/features\/features.py\", line 324, in _cast_to_python_objects\r\n    for x in obj.detach().cpu().numpy()\r\nTypeError: Got unsupported ScalarType BFloat16\r\n```\n\n### Steps to reproduce the bug\n\nHere is my train script I use BF16 type\uff0cI use diffusers train my model\r\n```\r\nexport MODEL_DIR=\"\/home\/mhh\/sd_models\/stable-diffusion-xl-base-1.0\"\r\nexport OUTPUT_DIR=\".\/control_net\"\r\nexport VAE_NAME=\"\/home\/mhh\/sd_models\/sdxl-vae-fp16-fix\"\r\n\r\naccelerate launch train_controlnet_sdxl.py \\\r\n --pretrained_model_name_or_path=$MODEL_DIR \\\r\n --output_dir=$OUTPUT_DIR \\\r\n --pretrained_vae_model_name_or_path=$VAE_NAME \\\r\n --dataset_name=\/home\/mhh\/sd_datasets\/fusing\/fill50k \\\r\n --mixed_precision=\"bf16\" \\\r\n --resolution=1024 \\\r\n --learning_rate=1e-5 \\\r\n --max_train_steps=200 \\\r\n --validation_image \"\/home\/mhh\/sd_datasets\/controlnet_image\/conditioning_image_1.png\" \"\/home\/mhh\/sd_datasets\/controlnet_image\/conditioning_image_2.png\" \\\r\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\r\n --validation_steps=50 \\\r\n --train_batch_size=1 \\\r\n --gradient_accumulation_steps=4 \\\r\n --report_to=\"wandb\" \\\r\n --seed=42 \\\r\n```\n\n### Expected behavior\n\nWhen I changed the data type to fp16, it worked.\n\n### Environment info\n\ndatasets                2.16.1\r\nnumpy                   1.24.4","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6566\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6565","id":2068939670,"node_id":"I_kwDODunzps57UYOW","number":6565,"title":" `drop_last_batch=True` for IterableDataset map function is ignored with multiprocessing DataLoader ","user":{"login":"naba89","id":12119806,"node_id":"MDQ6VXNlcjEyMTE5ODA2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12119806?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/naba89","html_url":"https:\/\/github.com\/naba89","followers_url":"https:\/\/api.github.com\/users\/naba89\/followers","following_url":"https:\/\/api.github.com\/users\/naba89\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/naba89\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/naba89\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/naba89\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/naba89\/orgs","repos_url":"https:\/\/api.github.com\/users\/naba89\/repos","events_url":"https:\/\/api.github.com\/users\/naba89\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/naba89\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-07T02:46:50Z","updated_at":"2024-01-07T03:03:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nScenario:\r\n- Interleaving two iterable datasets of unequal lengths (`all_exhausted`), followed by a batch mapping with batch size 2 to effectively merge the two datasets and get a sample from each dataset in a single batch, with `drop_last_batch=True` to skip the last batch in case it doesn't have two samples.\r\n\r\nWhat works:\r\n- Using DataLoader with `num_workers=0`\r\n\r\nWhat does not work:\r\n- Using DataLoader with `num_workers=1`, errors in the last batch.\r\n\r\nBasically, `drop_last_batch=True` is ignored when using multiple dataloading workers.\r\n\r\nPlease take a look at the minimal repro script below.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import Dataset, interleave_datasets\r\nfrom torch.utils.data import DataLoader\r\n\r\n\r\ndef merge_samples(batch):\r\n    assert len(batch['a']) == 2, \"Batch size must be 2\"\r\n    batch['c'] = [batch['a'][0]]\r\n    batch['d'] = [batch['a'][1]]\r\n    return batch\r\n\r\n\r\ndef gen1():\r\n    for ii in range(1, 8385):\r\n        yield {\"a\": ii}\r\n\r\n\r\ndef gen2():\r\n    for ii in range(1, 5302):\r\n        yield {\"a\": ii}\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    dataset1 = Dataset.from_generator(gen1).to_iterable_dataset(num_shards=1024)\r\n    dataset2 = Dataset.from_generator(gen2).to_iterable_dataset(num_shards=1024)\r\n\r\n    interleaved = interleave_datasets([dataset1, dataset2], stopping_strategy=\"all_exhausted\")\r\n    mapped = interleaved.map(merge_samples, batched=True, batch_size=2, remove_columns=interleaved.column_names,\r\n                             drop_last_batch=True)\r\n\r\n    # Works\r\n    loader = DataLoader(mapped, batch_size=32, num_workers=0)\r\n    i = 0\r\n    for b in loader:\r\n        print(i, b['c'].shape, b['d'].shape)\r\n        i += 1\r\n\r\n    print(\"DataLoader with num_workers=0 works\")\r\n\r\n    # Doesn't work\r\n    loader = DataLoader(mapped, batch_size=32, num_workers=1)\r\n    i = 0\r\n    for b in loader:\r\n        print(i, b['c'].shape, b['d'].shape)\r\n        i += 1\r\n\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\n `drop_last_batch=True` should have same behaviour for `num_workers=0` and `num_workers>=1`\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.16.1\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.6.0\r\n\r\nI have also tested on Linux and got the same behavior.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6565\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6564","id":2068893194,"node_id":"I_kwDODunzps57UM4K","number":6564,"title":"Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method","user":{"login":"kopyl","id":17604849,"node_id":"MDQ6VXNlcjE3NjA0ODQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17604849?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kopyl","html_url":"https:\/\/github.com\/kopyl","followers_url":"https:\/\/api.github.com\/users\/kopyl\/followers","following_url":"https:\/\/api.github.com\/users\/kopyl\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kopyl\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kopyl\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kopyl\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kopyl\/orgs","repos_url":"https:\/\/api.github.com\/users\/kopyl\/repos","events_url":"https:\/\/api.github.com\/users\/kopyl\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kopyl\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2024-01-06T23:48:13Z","updated_at":"2024-01-06T23:48:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThe issue shall be open: https:\/\/github.com\/huggingface\/datasets\/issues\/6435\r\n\r\nWhen i try to pass `with_rank` to `Dataset.filter()`, i get this:\r\n\r\n`Dataset.filter() got an unexpected keyword argument 'with_rank'`\n\n### Steps to reproduce the bug\n\nRun notebook:\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/1WUNKph8BdP0on5ve3gQnh_PE0cFLQqTn?usp=sharing\n\n### Expected behavior\n\nShould work?\n\n### Environment info\n\nNVIDIA RTX 4090","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6564\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6563","id":2068302402,"node_id":"I_kwDODunzps57R8pC","number":6563,"title":"`ImportError`: cannot import name 'insecure_hashlib' from 'huggingface_hub.utils' (...\/huggingface_hub\/utils\/__init__.py)","user":{"login":"wasertech","id":79070834,"node_id":"MDQ6VXNlcjc5MDcwODM0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/79070834?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wasertech","html_url":"https:\/\/github.com\/wasertech","followers_url":"https:\/\/api.github.com\/users\/wasertech\/followers","following_url":"https:\/\/api.github.com\/users\/wasertech\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wasertech\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wasertech\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wasertech\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wasertech\/orgs","repos_url":"https:\/\/api.github.com\/users\/wasertech\/repos","events_url":"https:\/\/api.github.com\/users\/wasertech\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wasertech\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2024-01-06T02:28:54Z","updated_at":"2024-01-06T21:05:37Z","closed_at":"2024-01-06T16:13:27Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nYep its not [there](https:\/\/github.com\/huggingface\/huggingface_hub\/blob\/main\/src\/huggingface_hub\/utils\/__init__.py) anymore.\r\n\r\n```text\r\n+ python \/home\/trainer\/sft_train.py --model_name cognitivecomputations\/dolphin-2.2.1-mistral-7b --dataset_name wasertech\/OneOS --load_in_4bit --use_peft --batch_size 4 --num_train_epochs 1 --learning_rate 1.41e-5 --gradient_accumulation_steps 8 --seq_length 4096 --output_dir output --log_with wandb\r\nTraceback (most recent call last):\r\n  File \"\/home\/trainer\/sft_train.py\", line 22, in <module>\r\n    from datasets import load_dataset\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/__init__.py\", line 22, in <module>\r\n    from .arrow_dataset import Dataset\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 66, in <module>\r\n    from .arrow_reader import ArrowReader\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/arrow_reader.py\", line 30, in <module>\r\n    from .download.download_config import DownloadConfig\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/download\/__init__.py\", line 9, in <module>\r\n    from .download_manager import DownloadManager, DownloadMode\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/download\/download_manager.py\", line 31, in <module>\r\n    from ..utils import tqdm as hf_tqdm\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/utils\/__init__.py\", line 19, in <module>\r\n    from .info_utils import VerificationMode\r\n  File \"\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/datasets\/utils\/info_utils.py\", line 5, in <module>\r\n    from huggingface_hub.utils import insecure_hashlib\r\nImportError: cannot import name 'insecure_hashlib' from 'huggingface_hub.utils' (\/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\/huggingface_hub\/utils\/__init__.py)\r\n```\n\n### Steps to reproduce the bug\n\nUsing `datasets==2.16.1` and `huggingface_hub== 0.17.3`, load a dataset with `load_dataset`.\n\n### Expected behavior\n\nThe dataset should be (downloaded - if needed - and) returned.\n\n### Environment info\n\n```text\r\ntrainer@a311ae86939e:\/mnt$ pip show datasets\r\nName: datasets\r\nVersion: 2.16.1\r\nSummary: HuggingFace community-driven open-source library of datasets\r\nHome-page: https:\/\/github.com\/huggingface\/datasets\r\nAuthor: HuggingFace Inc.\r\nAuthor-email: thomas@huggingface.co\r\nLicense: Apache 2.0\r\nLocation: \/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\r\nRequires: packaging, pyyaml, multiprocess, pyarrow-hotfix, pandas, pyarrow, xxhash, dill, numpy, aiohttp, tqdm, fsspec, requests, filelock, huggingface-hub\r\nRequired-by: trl, lm-eval, evaluate\r\n\r\ntrainer@a311ae86939e:\/mnt$ pip show huggingface_hub\r\nName: huggingface-hub\r\nVersion: 0.17.3\r\nSummary: Client library to download and publish models, datasets and other repos on the huggingface.co hub\r\nHome-page: https:\/\/github.com\/huggingface\/huggingface_hub\r\nAuthor: Hugging Face, Inc.\r\nAuthor-email: julien@huggingface.co\r\nLicense: Apache\r\nLocation: \/home\/trainer\/llm-train\/lib\/python3.8\/site-packages\r\nRequires: requests, pyyaml, packaging, typing-extensions, tqdm, filelock, fsspec\r\nRequired-by: transformers, tokenizers, peft, evaluate, datasets, accelerate\r\n\r\ntrainer@a311ae86939e:\/mnt$ huggingface-cli env\r\n\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- huggingface_hub version: 0.17.3\r\n- Platform: Linux-6.5.13-7-MANJARO-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Running in iPython ?: No\r\n- Running in notebook ?: No\r\n- Running in Google Colab ?: No\r\n- Token path ?: \/home\/trainer\/.cache\/huggingface\/token\r\n- Has saved token ?: True\r\n- Who am I ?: wasertech\r\n- Configured git credential helpers: \r\n- FastAI: N\/A\r\n- Tensorflow: N\/A\r\n- Torch: 2.1.2\r\n- Jinja2: 3.1.2\r\n- Graphviz: N\/A\r\n- Pydot: N\/A\r\n- Pillow: 10.2.0\r\n- hf_transfer: N\/A\r\n- gradio: N\/A\r\n- tensorboard: N\/A\r\n- numpy: 1.24.4\r\n- pydantic: N\/A\r\n- aiohttp: 3.9.1\r\n- ENDPOINT: https:\/\/huggingface.co\r\n- HUGGINGFACE_HUB_CACHE: \/home\/trainer\/.cache\/huggingface\/hub\r\n- HUGGINGFACE_ASSETS_CACHE: \/home\/trainer\/.cache\/huggingface\/assets\r\n- HF_TOKEN_PATH: \/home\/trainer\/.cache\/huggingface\/token\r\n- HF_HUB_OFFLINE: False\r\n- HF_HUB_DISABLE_TELEMETRY: False\r\n- HF_HUB_DISABLE_PROGRESS_BARS: None\r\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\r\n- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\r\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\r\n- HF_HUB_ENABLE_HF_TRANSFER: False\r\n\r\n\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6563\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6562","id":2067904504,"node_id":"I_kwDODunzps57Qbf4","number":6562,"title":"datasets.DownloadMode.FORCE_REDOWNLOAD use cache to download dataset features with load_dataset function","user":{"login":"LsTam91","id":73234162,"node_id":"MDQ6VXNlcjczMjM0MTYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/73234162?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/LsTam91","html_url":"https:\/\/github.com\/LsTam91","followers_url":"https:\/\/api.github.com\/users\/LsTam91\/followers","following_url":"https:\/\/api.github.com\/users\/LsTam91\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/LsTam91\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/LsTam91\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/LsTam91\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/LsTam91\/orgs","repos_url":"https:\/\/api.github.com\/users\/LsTam91\/repos","events_url":"https:\/\/api.github.com\/users\/LsTam91\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/LsTam91\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2024-01-05T19:10:25Z","updated_at":"2024-01-05T19:10:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI have updated my dataset by adding a new feature, and push it to the hub. When I want to download it on my machine which contain the old version by using `datasets.load_dataset(\"your_dataset_name\", download_mode=datasets.DownloadMode.FORCE_REDOWNLOAD)` I get an error (paste bellow).\r\n\r\nSeems that the load_dataset function still use the old features schema instead of downloading everything new from the HUB.\r\n\r\nI find a way to go around this issue by manually deleting the old dataset cache. But from my understanding of `datasets.DownloadMode.FORCE_REDOWNLOAD` option, the dataset cache should be ignored.\n\n### Steps to reproduce the bug\n\n1. Download your dataset in your machine using `datasets.load_dataset`\r\n2. Create a new feature in your dataset and push it to the hub\r\n3. On the same machine redownload your dataset using `datasets.load_dataset(\"your_dataset_name\", download_mode=datasets.DownloadMode.FORCE_REDOWNLOAD)`\n\n### Expected behavior\n\n`\r\nValueError: Couldn't cast\r\nid: string\r\nlevel: string\r\ncontext: list<element: string>\r\n  child 0, element: string\r\ntype: string\r\nanswer: string\r\nquestion: string\r\nsupporting_facts: list<element: string>\r\n  child 0, element: string\r\nfra_answer: string\r\nfra_question: string\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"id\": {\"dtype\": \"string\", \"_type\": \"' + 490\r\nto\r\n{'id': Value(dtype='string', id=None), 'level': Value(dtype='string', id=None), 'context': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'type': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'supporting_facts': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError\r\n...\r\nDatasetGenerationError: An error occurred while generating the dataset`\n\n### Environment info\n\ndatasets-2.16.1 huggingface-hub-0.20.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6562\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6561","id":2067404951,"node_id":"I_kwDODunzps57OhiX","number":6561,"title":"Document YAML configuration with \"data_dir\"","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-05T14:03:33Z","updated_at":"2024-01-05T14:06:18Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"See https:\/\/huggingface.co\/datasets\/uonlp\/CulturaX\/discussions\/15#6597e83f185db94370d6bf50 for reference","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6561\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6560","id":2065637625,"node_id":"I_kwDODunzps57HyD5","number":6560,"title":"Support Video ","user":{"login":"yuvalkirstain","id":57996478,"node_id":"MDQ6VXNlcjU3OTk2NDc4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57996478?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yuvalkirstain","html_url":"https:\/\/github.com\/yuvalkirstain","followers_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/followers","following_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/orgs","repos_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/repos","events_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2024-01-04T13:10:58Z","updated_at":"2024-01-04T13:10:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nHF datasets are awesome in supporting text and images. Will be great to see such a support in videos :) \n\n### Motivation\n\nVideo generation :) \n\n### Your contribution\n\nWill probably be limited to raising this feature request ;)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6560\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6559","id":2065118332,"node_id":"I_kwDODunzps57FzR8","number":6559,"title":"Latest version 2.16.1, when load dataset error occurs. ValueError: BuilderConfig 'allenai--c4' not found. Available: ['default']","user":{"login":"zhulinJulia24","id":145004780,"node_id":"U_kgDOCKSY7A","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/145004780?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/zhulinJulia24","html_url":"https:\/\/github.com\/zhulinJulia24","followers_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/followers","following_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/orgs","repos_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/repos","events_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/zhulinJulia24\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2024-01-04T07:04:48Z","updated_at":"2024-01-05T01:26:26Z","closed_at":"2024-01-05T01:26:25Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\npython script is:\r\n```\r\n\r\nfrom datasets import load_dataset\r\ncache_dir = 'path\/to\/your\/cache\/directory'\r\ndataset = load_dataset('allenai\/c4','allenai--c4', data_files={'train': 'en\/c4-train.00000-of-01024.json.gz'}, split='train',  use_auth_token=False, cache_dir=cache_dir)\r\n```\r\n\r\n\r\nthe script success when datasets version is 2.14.7. \r\nwhen using 2.16.1, error occurs\r\n`\r\nValueError: BuilderConfig 'allenai--c4' not found. Available: ['default']`\n\n### Steps to reproduce the bug\n\n1. pip install datasets==2.16.1\r\n2. run python script:\r\n```\r\n\r\nfrom datasets import load_dataset\r\ncache_dir = 'path\/to\/your\/cache\/directory'\r\ndataset = load_dataset('allenai\/c4','allenai--c4', data_files={'train': 'en\/c4-train.00000-of-01024.json.gz'}, split='train',  use_auth_token=False, cache_dir=cache_dir)\r\n```\n\n### Expected behavior\n\nthe dataset should be loaded successful in the latest version. \n\n### Environment info\n\ndatasets 2.16.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6559\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6558","id":2064885984,"node_id":"I_kwDODunzps57E6jg","number":6558,"title":"OSError: image file is truncated (1 bytes not processed) #28323","user":{"login":"andysingal","id":20493493,"node_id":"MDQ6VXNlcjIwNDkzNDkz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20493493?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/andysingal","html_url":"https:\/\/github.com\/andysingal","followers_url":"https:\/\/api.github.com\/users\/andysingal\/followers","following_url":"https:\/\/api.github.com\/users\/andysingal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/andysingal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/andysingal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/andysingal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/andysingal\/orgs","repos_url":"https:\/\/api.github.com\/users\/andysingal\/repos","events_url":"https:\/\/api.github.com\/users\/andysingal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/andysingal\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2024-01-04T02:15:13Z","updated_at":"2024-01-04T02:15:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\nCell In[24], line 28\r\n     23     return example\r\n     25 # Filter the dataset\r\n     26 # filtered_dataset = dataset.filter(contains_number)\r\n     27 # Add the 'label' field in the dataset\r\n---> 28 labeled_dataset = dataset.filter(contains_number).map(add_label)\r\n     29 # View the structure of the updated dataset\r\n     30 print(labeled_dataset)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/dataset_dict.py:975, in DatasetDict.filter(self, function, with_indices, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, fn_kwargs, num_proc, desc)\r\n    972 if cache_file_names is None:\r\n    973     cache_file_names = {k: None for k in self}\r\n    974 return DatasetDict(\r\n--> 975     {\r\n    976         k: dataset.filter(\r\n    977             function=function,\r\n    978             with_indices=with_indices,\r\n    979             input_columns=input_columns,\r\n    980             batched=batched,\r\n    981             batch_size=batch_size,\r\n    982             keep_in_memory=keep_in_memory,\r\n    983             load_from_cache_file=load_from_cache_file,\r\n    984             cache_file_name=cache_file_names[k],\r\n    985             writer_batch_size=writer_batch_size,\r\n    986             fn_kwargs=fn_kwargs,\r\n    987             num_proc=num_proc,\r\n    988             desc=desc,\r\n    989         )\r\n    990         for k, dataset in self.items()\r\n    991     }\r\n    992 )\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/dataset_dict.py:976, in <dictcomp>(.0)\r\n    972 if cache_file_names is None:\r\n    973     cache_file_names = {k: None for k in self}\r\n    974 return DatasetDict(\r\n    975     {\r\n--> 976         k: dataset.filter(\r\n    977             function=function,\r\n    978             with_indices=with_indices,\r\n    979             input_columns=input_columns,\r\n    980             batched=batched,\r\n    981             batch_size=batch_size,\r\n    982             keep_in_memory=keep_in_memory,\r\n    983             load_from_cache_file=load_from_cache_file,\r\n    984             cache_file_name=cache_file_names[k],\r\n    985             writer_batch_size=writer_batch_size,\r\n    986             fn_kwargs=fn_kwargs,\r\n    987             num_proc=num_proc,\r\n    988             desc=desc,\r\n    989         )\r\n    990         for k, dataset in self.items()\r\n    991     }\r\n    992 )\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:557, in transmit_format.<locals>.wrapper(*args, **kwargs)\r\n    550 self_format = {\r\n    551     \"type\": self._format_type,\r\n    552     \"format_kwargs\": self._format_kwargs,\r\n    553     \"columns\": self._format_columns,\r\n    554     \"output_all_columns\": self._output_all_columns,\r\n    555 }\r\n    556 # apply actual function\r\n--> 557 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    558 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    559 # re-apply format to the output\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/fingerprint.py:481, in fingerprint_transform.<locals>._fingerprint.<locals>.wrapper(*args, **kwargs)\r\n    477             validate_fingerprint(kwargs[fingerprint_name])\r\n    479 # Call actual function\r\n--> 481 out = func(dataset, *args, **kwargs)\r\n    483 # Update fingerprint of in-place transforms + update in-place history of transforms\r\n    485 if inplace:  # update after calling func so that the fingerprint doesn't change if the function fails\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:3623, in Dataset.filter(self, function, with_indices, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   3620 if len(self) == 0:\r\n   3621     return self\r\n-> 3623 indices = self.map(\r\n   3624     function=partial(\r\n   3625         get_indices_from_mask_function, function, batched, with_indices, input_columns, self._indices\r\n   3626     ),\r\n   3627     with_indices=True,\r\n   3628     features=Features({\"indices\": Value(\"uint64\")}),\r\n   3629     batched=True,\r\n   3630     batch_size=batch_size,\r\n   3631     remove_columns=self.column_names,\r\n   3632     keep_in_memory=keep_in_memory,\r\n   3633     load_from_cache_file=load_from_cache_file,\r\n   3634     cache_file_name=cache_file_name,\r\n   3635     writer_batch_size=writer_batch_size,\r\n   3636     fn_kwargs=fn_kwargs,\r\n   3637     num_proc=num_proc,\r\n   3638     suffix_template=suffix_template,\r\n   3639     new_fingerprint=new_fingerprint,\r\n   3640     input_columns=input_columns,\r\n   3641     desc=desc or \"Filter\",\r\n   3642 )\r\n   3643 new_dataset = copy.deepcopy(self)\r\n   3644 new_dataset._indices = indices.data\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:592, in transmit_tasks.<locals>.wrapper(*args, **kwargs)\r\n    590     self: \"Dataset\" = kwargs.pop(\"self\")\r\n    591 # apply actual function\r\n--> 592 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    593 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    594 for dataset in datasets:\r\n    595     # Remove task templates if a column mapping of the template is no longer valid\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:557, in transmit_format.<locals>.wrapper(*args, **kwargs)\r\n    550 self_format = {\r\n    551     \"type\": self._format_type,\r\n    552     \"format_kwargs\": self._format_kwargs,\r\n    553     \"columns\": self._format_columns,\r\n    554     \"output_all_columns\": self._output_all_columns,\r\n    555 }\r\n    556 # apply actual function\r\n--> 557 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    558 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    559 # re-apply format to the output\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:3093, in Dataset.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   3087 if transformed_dataset is None:\r\n   3088     with hf_tqdm(\r\n   3089         unit=\" examples\",\r\n   3090         total=pbar_total,\r\n   3091         desc=desc or \"Map\",\r\n   3092     ) as pbar:\r\n-> 3093         for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n   3094             if done:\r\n   3095                 shards_done += 1\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:3470, in Dataset._map_single(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\r\n   3466 indices = list(\r\n   3467     range(*(slice(i, i + batch_size).indices(shard.num_rows)))\r\n   3468 )  # Something simpler?\r\n   3469 try:\r\n-> 3470     batch = apply_function_on_filtered_inputs(\r\n   3471         batch,\r\n   3472         indices,\r\n   3473         check_same_num_examples=len(shard.list_indexes()) > 0,\r\n   3474         offset=offset,\r\n   3475     )\r\n   3476 except NumExamplesMismatchError:\r\n   3477     raise DatasetTransformationNotAllowedError(\r\n   3478         \"Using `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn't create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\"\r\n   3479     ) from None\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:3349, in Dataset._map_single.<locals>.apply_function_on_filtered_inputs(pa_inputs, indices, check_same_num_examples, offset)\r\n   3347 if with_rank:\r\n   3348     additional_args += (rank,)\r\n-> 3349 processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n   3350 if isinstance(processed_inputs, LazyDict):\r\n   3351     processed_inputs = {\r\n   3352         k: v for k, v in processed_inputs.data.items() if k not in processed_inputs.keys_to_format\r\n   3353     }\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:6212, in get_indices_from_mask_function(function, batched, with_indices, input_columns, indices_mapping, *args, **fn_kwargs)\r\n   6209 if input_columns is None:\r\n   6210     # inputs only contains a batch of examples\r\n   6211     batch: dict = inputs[0]\r\n-> 6212     num_examples = len(batch[next(iter(batch.keys()))])\r\n   6213     for i in range(num_examples):\r\n   6214         example = {key: batch[key][i] for key in batch}\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py:272, in LazyDict.__getitem__(self, key)\r\n    270 value = self.data[key]\r\n    271 if key in self.keys_to_format:\r\n--> 272     value = self.format(key)\r\n    273     self.data[key] = value\r\n    274     self.keys_to_format.remove(key)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py:375, in LazyBatch.format(self, key)\r\n    374 def format(self, key):\r\n--> 375     return self.formatter.format_column(self.pa_table.select([key]))\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py:442, in PythonFormatter.format_column(self, pa_table)\r\n    440 def format_column(self, pa_table: pa.Table) -> list:\r\n    441     column = self.python_arrow_extractor().extract_column(pa_table)\r\n--> 442     column = self.python_features_decoder.decode_column(column, pa_table.column_names[0])\r\n    443     return column\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py:218, in PythonFeaturesDecoder.decode_column(self, column, column_name)\r\n    217 def decode_column(self, column: list, column_name: str) -> list:\r\n--> 218     return self.features.decode_column(column, column_name) if self.features else column\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/features\/features.py:1951, in Features.decode_column(self, column, column_name)\r\n   1938 def decode_column(self, column: list, column_name: str):\r\n   1939     \"\"\"Decode column with custom feature decoding.\r\n   1940 \r\n   1941     Args:\r\n   (...)\r\n   1948         `list[Any]`\r\n   1949     \"\"\"\r\n   1950     return (\r\n-> 1951         [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n   1952         if self._column_requires_decoding[column_name]\r\n   1953         else column\r\n   1954     )\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/features\/features.py:1951, in <listcomp>(.0)\r\n   1938 def decode_column(self, column: list, column_name: str):\r\n   1939     \"\"\"Decode column with custom feature decoding.\r\n   1940 \r\n   1941     Args:\r\n   (...)\r\n   1948         `list[Any]`\r\n   1949     \"\"\"\r\n   1950     return (\r\n-> 1951         [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n   1952         if self._column_requires_decoding[column_name]\r\n   1953         else column\r\n   1954     )\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/features\/features.py:1339, in decode_nested_example(schema, obj, token_per_repo_id)\r\n   1336 elif isinstance(schema, (Audio, Image)):\r\n   1337     # we pass the token to read and decode files from private repositories in streaming mode\r\n   1338     if obj is not None and schema.decode:\r\n-> 1339         return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n   1340 return obj\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/features\/image.py:185, in Image.decode_example(self, value, token_per_repo_id)\r\n    183 else:\r\n    184     image = PIL.Image.open(BytesIO(bytes_))\r\n--> 185 image.load()  # to avoid \"Too many open files\" errors\r\n    186 return image\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/PIL\/ImageFile.py:254, in ImageFile.load(self)\r\n    252         break\r\n    253     else:\r\n--> 254         raise OSError(\r\n    255             \"image file is truncated \"\r\n    256             f\"({len(b)} bytes not processed)\"\r\n    257         )\r\n    259 b = b + s\r\n    260 n, err_code = decoder.decode(b)\r\n\r\nOSError: image file is truncated (1 bytes not processed)\r\n```\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"mehul7\/captioned_military_aircraft\")\r\n\r\nfrom transformers import AutoImageProcessor\r\n\r\ncheckpoint = \"microsoft\/resnet-50\"\r\nimage_processor = AutoImageProcessor.from_pretrained(checkpoint)\r\n\r\nimport re\r\nfrom PIL import Image\r\nimport io\r\n\r\ndef contains_number(example):\r\n    try:\r\n        image = Image.open(io.BytesIO(example[\"image\"]['bytes']))\r\n        t = image_processor(images=image, return_tensors=\"pt\")['pixel_values']\r\n    except Exception as e:\r\n        print(f\"Error processing image\uff1a{example['text']}\")\r\n        return False\r\n    return bool(re.search(r'\\d', example['text']))\r\n\r\n# Define a function to add the 'label' field\r\ndef add_label(example):\r\n    lab = example['text'].split()\r\n    temp = 'NOT'\r\n    for item in lab:\r\n        if str(item[-1]).isdigit():\r\n            temp = item\r\n            break\r\n    example['label'] = temp\r\n    return example\r\n\r\n# Filter the dataset\r\n# filtered_dataset = dataset.filter(contains_number)\r\n# Add the 'label' field in the dataset\r\nlabeled_dataset = dataset.filter(contains_number).map(add_label)\r\n# View the structure of the updated dataset\r\nprint(labeled_dataset)\r\n```\r\n\r\n\n\n### Expected behavior\n\nneeds to form labels\r\nsame as : https:\/\/www.kaggle.com\/code\/jiabaowangts\/dataset-air\/notebook\n\n### Environment info\n\nKaggle notebook P100 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6558\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6557","id":2064341965,"node_id":"PR_kwDODunzps5jJ63z","number":6557,"title":"Support standalone yaml","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-03T16:47:35Z","updated_at":"2024-01-03T17:45:47Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"see (internal) https:\/\/huggingface.slack.com\/archives\/C02V51Q3800\/p1703885853581679","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6557\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6557","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6557","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6557.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6557.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6556","id":2064018208,"node_id":"PR_kwDODunzps5jI0nN","number":6556,"title":"Fix imagefolder with one image","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-03T13:13:02Z","updated_at":"2024-01-03T18:55:41Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"A dataset repository with one image and one metadata file was considered a JSON dataset instead of an ImageFolder dataset. This is because we pick the dataset type with the most compatible data file extensions present in the repository and it results in a tie in this case.\r\n\r\ne.g. for https:\/\/huggingface.co\/datasets\/multimodalart\/repro_1_image\r\n\r\nI fixed this by deprioritizing metadata files in the count.\r\n\r\nfix https:\/\/github.com\/huggingface\/datasets\/issues\/6545","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6556\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6556","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6556","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6556.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6556.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6555","id":2063841286,"node_id":"PR_kwDODunzps5jIM79","number":6555,"title":"Do not use Parquet exports if revision is passed","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2024-01-03T11:33:10Z","updated_at":"2024-01-03T12:44:05Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Fix #6554.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6555\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6555","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6555","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6555.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6555.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6554","id":2063839916,"node_id":"I_kwDODunzps57A7Ks","number":6554,"title":"Parquet exports are used even if revision is passed","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2024-01-03T11:32:26Z","updated_at":"2024-01-03T13:23:55Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"We should not used Parquet exports if `revision` is passed.\r\n\r\nI think this is a regression.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6554\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6553","id":2063474183,"node_id":"I_kwDODunzps56_h4H","number":6553,"title":"Cannot import name 'load_dataset' from .... module \u2018datasets\u2019","user":{"login":"ciaoyizhen","id":83450192,"node_id":"MDQ6VXNlcjgzNDUwMTky","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/83450192?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ciaoyizhen","html_url":"https:\/\/github.com\/ciaoyizhen","followers_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/followers","following_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/orgs","repos_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/repos","events_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ciaoyizhen\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-03T08:18:21Z","updated_at":"2024-01-03T08:25:19Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n\r\nuse python -m pip install datasets to install\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\n\n### Expected behavior\n\nit doesn't work\n\n### Environment info\n\ndatasets version==2.15.0\r\npython == 3.10.12\r\nlinux  version I don't know??","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6553\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6552","id":2063157187,"node_id":"I_kwDODunzps56-UfD","number":6552,"title":"Loading a dataset from Google Colab hangs at \"Resolving data files\".","user":{"login":"KelSolaar","id":99779,"node_id":"MDQ6VXNlcjk5Nzc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/99779?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/KelSolaar","html_url":"https:\/\/github.com\/KelSolaar","followers_url":"https:\/\/api.github.com\/users\/KelSolaar\/followers","following_url":"https:\/\/api.github.com\/users\/KelSolaar\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/KelSolaar\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/KelSolaar\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/KelSolaar\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/KelSolaar\/orgs","repos_url":"https:\/\/api.github.com\/users\/KelSolaar\/repos","events_url":"https:\/\/api.github.com\/users\/KelSolaar\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/KelSolaar\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2024-01-03T02:18:17Z","updated_at":"2024-01-06T05:04:36Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHello,\r\n\r\nI'm trying to load a dataset from Google Colab but the process hangs at `Resolving data files`:\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/99779\/7175ad85-e571-46ed-9f87-92653985777d)\r\n\r\nIt is happening when the `_get_origin_metadata` definition is invoked:\r\n\r\n```python\r\ndef _get_origin_metadata(\r\n    data_files: List[str],\r\n    max_workers=64,\r\n    download_config: Optional[DownloadConfig] = None,\r\n) -> Tuple[str]:\r\n    return thread_map(\r\n        partial(_get_single_origin_metadata, download_config=download_config),\r\n        data_files,\r\n        max_workers=max_workers,\r\n        tqdm_class=hf_tqdm,\r\n        desc=\"Resolving data files\",\r\n        disable=len(data_files) <= 16,\r\n```\r\n\r\nThe thread is then stuck at `waiter.acquire()` in the builtin `threading.py` file.\r\n\r\nI can load the dataset just fine on my machine.\r\n\r\nCheers,\r\n\r\nThomas\r\n\n\n### Steps to reproduce the bug\n\nIn Google Colab:\r\n\r\n```python\r\n!pip install datasets\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"colour-science\/color-checker-detection-dataset\")\r\n```\n\n### Expected behavior\n\nThe dataset should be loaded.\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.1\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6552\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6551","id":2062768400,"node_id":"PR_kwDODunzps5jEi1C","number":6551,"title":"Fix parallel downloads for datasets without scripts","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2024-01-02T18:06:18Z","updated_at":"2024-01-06T20:14:57Z","closed_at":"2024-01-03T13:19:48Z","author_association":"MEMBER","active_lock_reason":null,"body":"Enable parallel downloads using multiprocessing when `num_proc` is passed to `load_dataset`.\r\n\r\nIt was enabled for datasets with scripts already (if they passed lists to `dl_manager.download`) but not for no-script datasets (we pass dicts {split: [list of files]} to `dl_manager.download` for those ones).\r\n\r\nI fixed this by parallelising on the lists contained in the data files dicts when possible.\r\n\r\nI also added a context manager `stack_multiprocessing_download_progress_bars` in `DownloadManager` to stack the progress bard of the downloads (from `cached_path(...)` calls). Otherwise the progress bars overlap each other with an annoying flickering effect.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551\/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":2,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6551\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6551","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6551","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6551.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6551.patch","merged_at":"2024-01-03T13:19:47Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6550","id":2062556493,"node_id":"PR_kwDODunzps5jD1OL","number":6550,"title":"Multi gpu docs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-02T15:11:58Z","updated_at":"2024-01-02T15:16:56Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"after discussions in https:\/\/github.com\/huggingface\/datasets\/pull\/6415","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6550\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6550","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6550","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6550.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6550.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6549","id":2062420259,"node_id":"I_kwDODunzps567gkj","number":6549,"title":"Loading from hf hub with clearer error message","user":{"login":"thomwolf","id":7353373,"node_id":"MDQ6VXNlcjczNTMzNzM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7353373?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomwolf","html_url":"https:\/\/github.com\/thomwolf","followers_url":"https:\/\/api.github.com\/users\/thomwolf\/followers","following_url":"https:\/\/api.github.com\/users\/thomwolf\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomwolf\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomwolf\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomwolf\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomwolf\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomwolf\/repos","events_url":"https:\/\/api.github.com\/users\/thomwolf\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomwolf\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2024-01-02T13:26:34Z","updated_at":"2024-01-02T14:06:49Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"### Feature request\n\nShouldn't this kinda work ?\r\n```\r\nDataset.from_json(\"hf:\/\/datasets\/HuggingFaceTB\/eval_data\/resolve\/main\/eval_data_context_and_answers.json\")\r\n```\r\nI got an error\r\n```\r\nFile ~\/miniconda3\/envs\/datatrove\/lib\/python3.10\/site-packages\/datasets\/data_files.py:380, in resolve_pattern(pattern, base_path, allowed_extensions, download_config)\r\n    378     if allowed_extensions is not None:\r\n    379         error_msg += f\" with any supported extension {list(allowed_extensions)}\"\r\n--> 380     raise FileNotFoundError(error_msg)\r\n    381 return out\r\n\r\nFileNotFoundError: Unable to find 'hf:\/\/datasets\/HuggingFaceTB\/eval_data\/resolve\/main\/eval_data_context_and_answers.json'\r\n(I'm logged in)\r\n```\r\n\r\nFix: the correct path is\r\n```\r\nhf:\/\/datasets\/HuggingFaceTB\/eval_data\/eval_data_context_and_answers.json\r\n```\r\n\r\nProposal: raise a clearer error\n\n### Motivation\n\nClearer error message\n\n### Your contribution\n\nCan open a PR","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6549\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6548","id":2061047984,"node_id":"I_kwDODunzps562Riw","number":6548,"title":"Skip if a dataset has issues","user":{"login":"hadianasliwa","id":143214684,"node_id":"U_kgDOCIlIXA","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/143214684?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/hadianasliwa","html_url":"https:\/\/github.com\/hadianasliwa","followers_url":"https:\/\/api.github.com\/users\/hadianasliwa\/followers","following_url":"https:\/\/api.github.com\/users\/hadianasliwa\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/hadianasliwa\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/hadianasliwa\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/hadianasliwa\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/hadianasliwa\/orgs","repos_url":"https:\/\/api.github.com\/users\/hadianasliwa\/repos","events_url":"https:\/\/api.github.com\/users\/hadianasliwa\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/hadianasliwa\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-31T12:41:26Z","updated_at":"2024-01-02T10:33:17Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHello everyone,\r\nI'm using **load_datasets** from **huggingface** to download the datasets and I'm facing an issue, the download starts but it reaches some state and then  fails with the following error:\r\nCouldn't reach https:\/\/huggingface.co\/datasets\/wikimedia\/wikipedia\/resolve\/4cb9b0d719291f1a10f96f67d609c5d442980dc9\/20231101.ext\/train-00000-of-00001.parquet\r\n\r\nFailed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))')))\r\n\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/143214684\/8847d9cb-529e-4eda-9c76-282713dfa3af)\r\n\r\nso I was wondering is there a parameter to be passed to load_dataset() to skip files that can't be downloaded??\n\n### Steps to reproduce the bug\n\nParameter to be passed to load_dataset() of huggingface to skip files that can't be downloaded??\n\n### Expected behavior\n\nload_dataset() finishes without error\n\n### Environment info\n\nNone","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6548\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6547","id":2060796927,"node_id":"PR_kwDODunzps5i-Jni","number":6547,"title":"set dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-30T16:47:17Z","updated_at":"2023-12-30T16:53:38Z","closed_at":"2023-12-30T16:47:27Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6547\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6547","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6547","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6547.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6547.patch","merged_at":"2023-12-30T16:47:27Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6546","id":2060796369,"node_id":"PR_kwDODunzps5i-Jgv","number":6546,"title":"Release: 2.16.1","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-30T16:44:51Z","updated_at":"2023-12-30T16:52:07Z","closed_at":"2023-12-30T16:45:52Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6546\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6546","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6546","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6546.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6546.patch","merged_at":"2023-12-30T16:45:52Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6545","id":2060789507,"node_id":"I_kwDODunzps561ScD","number":6545,"title":"`image` column not automatically inferred if image dataset only contains 1 image","user":{"login":"apolinario","id":788417,"node_id":"MDQ6VXNlcjc4ODQxNw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/788417?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/apolinario","html_url":"https:\/\/github.com\/apolinario","followers_url":"https:\/\/api.github.com\/users\/apolinario\/followers","following_url":"https:\/\/api.github.com\/users\/apolinario\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/apolinario\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/apolinario\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/apolinario\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/apolinario\/orgs","repos_url":"https:\/\/api.github.com\/users\/apolinario\/repos","events_url":"https:\/\/api.github.com\/users\/apolinario\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/apolinario\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-30T16:17:29Z","updated_at":"2024-01-01T11:49:48Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nBy default, the standard Image Dataset maps out `file_name` to `image` when loading an Image Dataset. \r\n\r\nHowever, if the dataset contains only 1 image, this does not take place \r\n\r\n### Steps to reproduce the bug\r\n\r\nInput\r\n(dataset with one image `multimodalart\/repro_1_image`) \r\n```py\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"multimodalart\/repro_1_image\")\r\ndataset\r\n```\r\n\r\nOutput: \r\n```py\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['file_name', 'prompt'],\r\n        num_rows: 1\r\n    })\r\n})\r\n```\r\n\r\nInput \r\n(dataset with 2+ images `multimodalart\/repro_2_image`) \r\n```py\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"multimodalart\/repro_2_image\")\r\ndataset\r\n```\r\n\r\nOutput:\r\n```py\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'prompt'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\n### Expected behavior\r\n\r\nExpected to map `file_name`  \u2192 `image` for all dataset sizes, including 1. \r\n\r\n### Environment info\r\n\r\nBoth latest main and 2.16.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6545\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6544","id":2060782594,"node_id":"PR_kwDODunzps5i-G4_","number":6544,"title":"Fix custom configs from script","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-30T15:51:25Z","updated_at":"2024-01-02T11:02:39Z","closed_at":"2023-12-30T16:09:49Z","author_association":"MEMBER","active_lock_reason":null,"body":"We should not use the parquet export when the user is passing config_kwargs\r\n\r\nI also fixed a regression that would disallow creating a custom config when a dataset has multiple predefined configs\r\n\r\nfix https:\/\/github.com\/huggingface\/datasets\/issues\/6533","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6544\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6544","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6544","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6544.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6544.patch","merged_at":"2023-12-30T16:09:49Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6543","id":2060776174,"node_id":"PR_kwDODunzps5i-Frx","number":6543,"title":"Fix dl_manager.extract returning FileNotFoundError","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-30T15:24:50Z","updated_at":"2023-12-30T16:00:06Z","closed_at":"2023-12-30T15:53:59Z","author_association":"MEMBER","active_lock_reason":null,"body":"The dl_manager base path is remote (e.g. a hf:\/\/ path), so local cached paths should be passed as absolute paths.\r\nThis could happen if users provide a relative path as `cache_dir`\r\n\r\nfix https:\/\/github.com\/huggingface\/datasets\/issues\/6536","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6543\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6543","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6543","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6543.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6543.patch","merged_at":"2023-12-30T15:53:59Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6542","id":2059198575,"node_id":"I_kwDODunzps56vOBv","number":6542,"title":"Datasets : wikipedia 20220301.en error ","user":{"login":"ppx666","id":53203620,"node_id":"MDQ6VXNlcjUzMjAzNjIw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/53203620?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ppx666","html_url":"https:\/\/github.com\/ppx666","followers_url":"https:\/\/api.github.com\/users\/ppx666\/followers","following_url":"https:\/\/api.github.com\/users\/ppx666\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ppx666\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ppx666\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ppx666\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ppx666\/orgs","repos_url":"https:\/\/api.github.com\/users\/ppx666\/repos","events_url":"https:\/\/api.github.com\/users\/ppx666\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ppx666\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-29T08:34:51Z","updated_at":"2024-01-02T13:21:06Z","closed_at":"2024-01-02T13:20:30Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I used load_dataset to download this data set, the following error occurred. The main problem was that the target data did not exist.\n\n### Steps to reproduce the bug\n\n1.I tried downloading directly.\r\n```python\r\nwiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\")\r\n```\r\nAn exception occurred\r\n```\r\nMissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https:\/\/beam.apache.org\/documentation\/runners\/capability-matrix\/\r\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \r\nExample of usage: \r\n\t`load_dataset('wikipedia', '20220301.en', beam_runner='DirectRunner')`\r\n```\r\n2.I modified the code as prompted.\r\n```python\r\nwiki_dataset = load_dataset('wikipedia', '20220301.en', beam_runner='DirectRunner')\r\n```\r\nAn exception occurred:\r\n```\r\nFileNotFoundError: Couldn't find file at https:\/\/dumps.wikimedia.org\/enwiki\/20220301\/dumpstatus.json\r\n```\r\n\n\n### Expected behavior\n\nI searched in the parent directory of the corresponding URL, but there was no corresponding \"20220301\" directory.\r\nI really need this data set and hope to provide a download method.\n\n### Environment info\n\npython 3.8\r\ndatasets 2.16.0\r\napache-beam 2.52.0\r\ndill 0.3.7\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6542\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6541","id":2058983826,"node_id":"I_kwDODunzps56uZmS","number":6541,"title":"Dataset not loading successfully.","user":{"login":"hi-sushanta","id":93595990,"node_id":"U_kgDOBZQpVg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93595990?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/hi-sushanta","html_url":"https:\/\/github.com\/hi-sushanta","followers_url":"https:\/\/api.github.com\/users\/hi-sushanta\/followers","following_url":"https:\/\/api.github.com\/users\/hi-sushanta\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/hi-sushanta\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/hi-sushanta\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/hi-sushanta\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/hi-sushanta\/orgs","repos_url":"https:\/\/api.github.com\/users\/hi-sushanta\/repos","events_url":"https:\/\/api.github.com\/users\/hi-sushanta\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/hi-sushanta\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-29T01:35:47Z","updated_at":"2023-12-29T01:35:47Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I run down the below code shows this error: AttributeError: module 'numpy' has no attribute '_no_nep50_warning'\r\nI also added this issue in transformers library please check out: [link](https:\/\/github.com\/huggingface\/transformers\/issues\/28099)\n\n### Steps to reproduce the bug\n\n## Reproduction\r\nHi, please check this line of code, when I run Show attribute error.\r\n\r\n```\r\nfrom datasets import load_dataset\r\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\r\n\r\n# Select an audio file and read it:\r\nds = load_dataset(\"hf-internal-testing\/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\naudio_sample = ds[0][\"audio\"]\r\nwaveform = audio_sample[\"array\"]\r\nsampling_rate = audio_sample[\"sampling_rate\"]\r\n\r\n# Load the Whisper model in Hugging Face format:\r\nprocessor = WhisperProcessor.from_pretrained(\"openai\/whisper-tiny.en\")\r\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai\/whisper-tiny.en\")\r\n\r\n# Use the model and processor to transcribe the audio:\r\ninput_features = processor(\r\n    waveform, sampling_rate=sampling_rate, return_tensors=\"pt\"\r\n).input_features\r\n\r\n# Generate token ids\r\npredicted_ids = model.generate(input_features)\r\n\r\n# Decode token ids to text\r\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\r\n\r\ntranscription[0]\r\n\r\n```\r\n\r\n**Attribute Error**\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[9], line 6\r\n      4 # Select an audio file and read it:\r\n      5 ds = load_dataset(\"hf-internal-testing\/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\n----> 6 audio_sample = ds[0][\"audio\"]\r\n      7 waveform = audio_sample[\"array\"]\r\n      8 sampling_rate = audio_sample[\"sampling_rate\"]\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py:2795, in Dataset.__getitem__(self, key)\r\n   2793 def __getitem__(self, key):  # noqa: F811\r\n   2794     \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 2795     return self._getitem(key)\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py:2780, in Dataset._getitem(self, key, **kwargs)\r\n   2778 formatter = get_formatter(format_type, features=self._info.features, **format_kwargs)\r\n   2779 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n-> 2780 formatted_output = format_table(\r\n   2781     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n   2782 )\r\n   2783 return formatted_output\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py:629, in format_table(table, key, formatter, format_columns, output_all_columns)\r\n    627 python_formatter = PythonFormatter(features=formatter.features)\r\n    628 if format_columns is None:\r\n--> 629     return formatter(pa_table, query_type=query_type)\r\n    630 elif query_type == \"column\":\r\n    631     if key in format_columns:\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py:396, in Formatter.__call__(self, pa_table, query_type)\r\n    394 def __call__(self, pa_table: pa.Table, query_type: str) -> Union[RowFormat, ColumnFormat, BatchFormat]:\r\n    395     if query_type == \"row\":\r\n--> 396         return self.format_row(pa_table)\r\n    397     elif query_type == \"column\":\r\n    398         return self.format_column(pa_table)\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py:437, in PythonFormatter.format_row(self, pa_table)\r\n    435     return LazyRow(pa_table, self)\r\n    436 row = self.python_arrow_extractor().extract_row(pa_table)\r\n--> 437 row = self.python_features_decoder.decode_row(row)\r\n    438 return row\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py:215, in PythonFeaturesDecoder.decode_row(self, row)\r\n    214 def decode_row(self, row: dict) -> dict:\r\n--> 215     return self.features.decode_example(row) if self.features else row\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/features\/features.py:1917, in Features.decode_example(self, example, token_per_repo_id)\r\n   1903 def decode_example(self, example: dict, token_per_repo_id: Optional[Dict[str, Union[str, bool, None]]] = None):\r\n   1904     \"\"\"Decode example with custom feature decoding.\r\n   1905 \r\n   1906     Args:\r\n   (...)\r\n   1914         `dict[str, Any]`\r\n   1915     \"\"\"\r\n-> 1917     return {\r\n   1918         column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\r\n   1919         if self._column_requires_decoding[column_name]\r\n   1920         else value\r\n   1921         for column_name, (feature, value) in zip_dict(\r\n   1922             {key: value for key, value in self.items() if key in example}, example\r\n   1923         )\r\n   1924     }\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/features\/features.py:1918, in <dictcomp>(.0)\r\n   1903 def decode_example(self, example: dict, token_per_repo_id: Optional[Dict[str, Union[str, bool, None]]] = None):\r\n   1904     \"\"\"Decode example with custom feature decoding.\r\n   1905 \r\n   1906     Args:\r\n   (...)\r\n   1914         `dict[str, Any]`\r\n   1915     \"\"\"\r\n   1917     return {\r\n-> 1918         column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\r\n   1919         if self._column_requires_decoding[column_name]\r\n   1920         else value\r\n   1921         for column_name, (feature, value) in zip_dict(\r\n   1922             {key: value for key, value in self.items() if key in example}, example\r\n   1923         )\r\n   1924     }\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/features\/features.py:1339, in decode_nested_example(schema, obj, token_per_repo_id)\r\n   1336 elif isinstance(schema, (Audio, Image)):\r\n   1337     # we pass the token to read and decode files from private repositories in streaming mode\r\n   1338     if obj is not None and schema.decode:\r\n-> 1339         return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n   1340 return obj\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/datasets\/features\/audio.py:191, in Audio.decode_example(self, value, token_per_repo_id)\r\n    189 array = array.T\r\n    190 if self.mono:\r\n--> 191     array = librosa.to_mono(array)\r\n    192 if self.sampling_rate and self.sampling_rate != sampling_rate:\r\n    193     array = librosa.resample(array, orig_sr=sampling_rate, target_sr=self.sampling_rate)\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/lazy_loader\/__init__.py:78, in attach.<locals>.__getattr__(name)\r\n     76 submod_path = f\"{package_name}.{attr_to_modules[name]}\"\r\n     77 submod = importlib.import_module(submod_path)\r\n---> 78 attr = getattr(submod, name)\r\n     80 # If the attribute lives in a file (module) with the same\r\n     81 # name as the attribute, ensure that the attribute and *not*\r\n     82 # the module is accessible on the package.\r\n     83 if name == attr_to_modules[name]:\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/lazy_loader\/__init__.py:77, in attach.<locals>.__getattr__(name)\r\n     75 elif name in attr_to_modules:\r\n     76     submod_path = f\"{package_name}.{attr_to_modules[name]}\"\r\n---> 77     submod = importlib.import_module(submod_path)\r\n     78     attr = getattr(submod, name)\r\n     80     # If the attribute lives in a file (module) with the same\r\n     81     # name as the attribute, ensure that the attribute and *not*\r\n     82     # the module is accessible on the package.\r\n\r\nFile \/usr\/lib\/python3.8\/importlib\/__init__.py:127, in import_module(name, package)\r\n    125             break\r\n    126         level += 1\r\n--> 127 return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\nFile <frozen importlib._bootstrap>:1014, in _gcd_import(name, package, level)\r\n\r\nFile <frozen importlib._bootstrap>:991, in _find_and_load(name, import_)\r\n\r\nFile <frozen importlib._bootstrap>:975, in _find_and_load_unlocked(name, import_)\r\n\r\nFile <frozen importlib._bootstrap>:671, in _load_unlocked(spec)\r\n\r\nFile <frozen importlib._bootstrap_external>:848, in exec_module(self, module)\r\n\r\nFile <frozen importlib._bootstrap>:219, in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/librosa\/core\/audio.py:13\r\n     11 import audioread\r\n     12 import numpy as np\r\n---> 13 import scipy.signal\r\n     14 import soxr\r\n     15 import lazy_loader as lazy\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/signal\/__init__.py:323\r\n    314 from ._spline import (  # noqa: F401\r\n    315     cspline2d,\r\n    316     qspline2d,\r\n   (...)\r\n    319     symiirorder2,\r\n    320 )\r\n    322 from ._bsplines import *\r\n--> 323 from ._filter_design import *\r\n    324 from ._fir_filter_design import *\r\n    325 from ._ltisys import *\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/signal\/_filter_design.py:16\r\n     13 from numpy.polynomial.polynomial import polyval as npp_polyval\r\n     14 from numpy.polynomial.polynomial import polyvalfromroots\r\n---> 16 from scipy import special, optimize, fft as sp_fft\r\n     17 from scipy.special import comb\r\n     18 from scipy._lib._util import float_factorial\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/optimize\/__init__.py:405\r\n      1 \"\"\"\r\n      2 =====================================================\r\n      3 Optimization and root finding (:mod:`scipy.optimize`)\r\n   (...)\r\n    401 \r\n    402 \"\"\"\r\n    404 from ._optimize import *\r\n--> 405 from ._minimize import *\r\n    406 from ._root import *\r\n    407 from ._root_scalar import *\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/optimize\/_minimize.py:26\r\n     24 from ._trustregion_krylov import _minimize_trust_krylov\r\n     25 from ._trustregion_exact import _minimize_trustregion_exact\r\n---> 26 from ._trustregion_constr import _minimize_trustregion_constr\r\n     28 # constrained minimization\r\n     29 from ._lbfgsb_py import _minimize_lbfgsb\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/optimize\/_trustregion_constr\/__init__.py:4\r\n      1 \"\"\"This module contains the equality constrained SQP solver.\"\"\"\r\n----> 4 from .minimize_trustregion_constr import _minimize_trustregion_constr\r\n      6 __all__ = ['_minimize_trustregion_constr']\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/optimize\/_trustregion_constr\/minimize_trustregion_constr.py:5\r\n      3 from scipy.sparse.linalg import LinearOperator\r\n      4 from .._differentiable_functions import VectorFunction\r\n----> 5 from .._constraints import (\r\n      6     NonlinearConstraint, LinearConstraint, PreparedConstraint, strict_bounds)\r\n      7 from .._hessian_update_strategy import BFGS\r\n      8 from .._optimize import OptimizeResult\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/scipy\/optimize\/_constraints.py:8\r\n      6 from ._optimize import OptimizeWarning\r\n      7 from warnings import warn, catch_warnings, simplefilter\r\n----> 8 from numpy.testing import suppress_warnings\r\n      9 from scipy.sparse import issparse\r\n     12 def _arr_to_scalar(x):\r\n     13     # If x is a numpy array, return x.item().  This will\r\n     14     # fail if the array has more than one element.\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/numpy\/testing\/__init__.py:11\r\n      8 from unittest import TestCase\r\n     10 from . import _private\r\n---> 11 from ._private.utils import *\r\n     12 from ._private.utils import (_assert_valid_refcount, _gen_alignment_data)\r\n     13 from ._private import extbuild, decorators as dec\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/numpy\/testing\/_private\/utils.py:480\r\n    476         pprint.pprint(desired, msg)\r\n    477         raise AssertionError(msg.getvalue())\r\n--> 480 @np._no_nep50_warning()\r\n    481 def assert_almost_equal(actual,desired,decimal=7,err_msg='',verbose=True):\r\n    482     \"\"\"\r\n    483     Raises an AssertionError if two items are not equal up to desired\r\n    484     precision.\r\n   (...)\r\n    548 \r\n    549     \"\"\"\r\n    550     __tracebackhide__ = True  # Hide traceback for py.test\r\n\r\nFile \/opt\/pytorch\/lib\/python3.8\/site-packages\/numpy\/__init__.py:313, in __getattr__(attr)\r\n    305     raise AttributeError(__former_attrs__[attr])\r\n    307 # Importing Tester requires importing all of UnitTest which is not a\r\n    308 # cheap import Since it is mainly used in test suits, we lazy import it\r\n    309 # here to save on the order of 10 ms of import time for most users\r\n    310 #\r\n    311 # The previous way Tester was imported also had a side effect of adding\r\n    312 # the full `numpy.testing` namespace\r\n--> 313 if attr == 'testing':\r\n    314     import numpy.testing as testing\r\n    315     return testing\r\n\r\nAttributeError: module 'numpy' has no attribute '_no_nep50_warning'\r\n\r\n```\n\n### Expected behavior\n\n``` ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.' ```\r\n\r\nAlso, make sure this script is provided for your official website so please update:\r\n[script](https:\/\/huggingface.co\/docs\/transformers\/model_doc\/whisper)\n\n### Environment info\n\n**System Info**\r\n* transformers -> 4.36.1\r\n* datasets -> 2.15.0\r\n* huggingface_hub -> 0.19.4\r\n* python -> 3.8.10\r\n* accelerate -> 0.25.0\r\n* pytorch -> 2.0.1+cpu\r\n* Using GPU in Script -> No\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6541\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6540","id":2058965157,"node_id":"I_kwDODunzps56uVCl","number":6540,"title":"Extreme inefficiency for `save_to_disk` when merging datasets","user":{"login":"KatarinaYuan","id":43512683,"node_id":"MDQ6VXNlcjQzNTEyNjgz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/43512683?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/KatarinaYuan","html_url":"https:\/\/github.com\/KatarinaYuan","followers_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/followers","following_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/orgs","repos_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/repos","events_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/KatarinaYuan\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-29T00:44:35Z","updated_at":"2023-12-30T15:05:48Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi, I tried to merge in total 22M sequences of data, where each sequence is of maximum length 2000. I found that merging these datasets and then `save_to_disk` is extremely slow because of flattening the indices. Wondering if you have any suggestions or guidance on this. Thank you very much! \n\n### Steps to reproduce the bug\n\nThe source data is too big to demonstrate\n\n### Expected behavior\n\nThe source data is too big to demonstrate\n\n### Environment info\n\npython                     3.9.0\r\ndatasets                  2.7.0\r\npytorch                   2.0.0 \r\ntokenizers                0.13.1\r\ntransformers              4.31.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6540\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6539","id":2058493960,"node_id":"I_kwDODunzps56siAI","number":6539,"title":"'Repo card metadata block was not found' when loading a pragmeval dataset","user":{"login":"lambdaofgod","id":3647577,"node_id":"MDQ6VXNlcjM2NDc1Nzc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3647577?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lambdaofgod","html_url":"https:\/\/github.com\/lambdaofgod","followers_url":"https:\/\/api.github.com\/users\/lambdaofgod\/followers","following_url":"https:\/\/api.github.com\/users\/lambdaofgod\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lambdaofgod\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lambdaofgod\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lambdaofgod\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lambdaofgod\/orgs","repos_url":"https:\/\/api.github.com\/users\/lambdaofgod\/repos","events_url":"https:\/\/api.github.com\/users\/lambdaofgod\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lambdaofgod\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-28T14:18:25Z","updated_at":"2023-12-28T14:18:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI can't load dataset subsets of 'pragmeval'.\r\n\r\nThe funny thing is I ran the dataset author's [colab notebook](https:\/\/colab.research.google.com\/drive\/1sg--LF4z7XR1wxAOfp0-3d4J6kQ9nj_A?usp=sharing) and it works just fine. I tried to install exactly the same packages that are installed on colab using poetry, so my environment info only differs from the one from colab in linux version - I still get the same bug outside colab.\n\n### Steps to reproduce the bug\n\nInstall dependencies with poetry\r\n\r\npyproject.toml\r\n```\r\n[tool.poetry]\r\nname = \"project\"\r\nversion = \"0.1.0\"\r\ndescription = \"\"\r\nauthors = []\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.10\"\r\ndatasets = \"2.16.0\"\r\npandas = \"1.5.3\"\r\npyarrow = \"10.0.1\"\r\nhuggingface-hub = \"0.19.4\"\r\nfsspec = \"2023.6.0\"\r\n\r\n\r\n[build-system]\r\nrequires = [\"poetry-core\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n```\r\n\r\n`poetry run python -c \"import datasets; print(datasets.get_dataset_config_names('pragmeval'))`\r\n\r\nprints ['default']\n\n### Expected behavior\n\nThe command should print\r\n\r\n```\r\n['emergent',\r\n 'emobank-arousal',\r\n 'emobank-dominance',\r\n 'emobank-valence',\r\n 'gum',\r\n 'mrda',\r\n 'pdtb',\r\n 'persuasiveness-claimtype',\r\n 'persuasiveness-eloquence',\r\n 'persuasiveness-premisetype',\r\n 'persuasiveness-relevance',\r\n 'persuasiveness-specificity',\r\n 'persuasiveness-strength',\r\n 'sarcasm',\r\n 'squinky-formality',\r\n 'squinky-implicature',\r\n 'squinky-informativeness',\r\n 'stac',\r\n 'switchboard',\r\n 'verifiability']\r\n```\n\n### Environment info\n\n- `datasets` version: 2.16.0\r\n- Platform: Linux-6.2.0-37-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6539\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6538","id":2057377630,"node_id":"I_kwDODunzps56oRde","number":6538,"title":"ImportError: cannot import name 'SchemaInferenceError' from 'datasets.arrow_writer' (\/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py)","user":{"login":"Sonali-Behera-TRT","id":131662185,"node_id":"U_kgDOB9kBaQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/131662185?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT","html_url":"https:\/\/github.com\/Sonali-Behera-TRT","followers_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/followers","following_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/orgs","repos_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/repos","events_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Sonali-Behera-TRT\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":15,"created_at":"2023-12-27T13:31:16Z","updated_at":"2024-01-03T10:06:47Z","closed_at":"2024-01-03T10:04:58Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhile importing from packages getting the error\r\nCode:\r\n\r\n```\r\nimport os\r\nimport torch\r\nfrom datasets import load_dataset, Dataset\r\nfrom transformers import (\r\n    AutoModelForCausalLM,\r\n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n    HfArgumentParser,\r\n    TrainingArguments,\r\n    pipeline,\r\n    logging\r\n)\r\nfrom peft import LoraConfig, PeftModel\r\nfrom trl import SFTTrainer\r\nfrom huggingface_hub import login\r\nimport pandas as pd\r\n```\r\n\r\nError:\r\n````\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nCell In[5], line 14\r\n      4 from transformers import (\r\n      5     AutoModelForCausalLM,\r\n      6     AutoTokenizer,\r\n   (...)\r\n     11     logging\r\n     12 )\r\n     13 from peft import LoraConfig, PeftModel\r\n---> 14 from trl import SFTTrainer\r\n     15 from huggingface_hub import login\r\n     16 import pandas as pd\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/trl\/__init__.py:21\r\n      8 from .import_utils import (\r\n      9     is_diffusers_available,\r\n     10     is_npu_available,\r\n   (...)\r\n     13     is_xpu_available,\r\n     14 )\r\n     15 from .models import (\r\n     16     AutoModelForCausalLMWithValueHead,\r\n     17     AutoModelForSeq2SeqLMWithValueHead,\r\n     18     PreTrainedModelWrapper,\r\n     19     create_reference_model,\r\n     20 )\r\n---> 21 from .trainer import (\r\n     22     DataCollatorForCompletionOnlyLM,\r\n     23     DPOTrainer,\r\n     24     IterativeSFTTrainer,\r\n     25     PPOConfig,\r\n     26     PPOTrainer,\r\n     27     RewardConfig,\r\n     28     RewardTrainer,\r\n     29     SFTTrainer,\r\n     30 )\r\n     33 if is_diffusers_available():\r\n     34     from .models import (\r\n     35         DDPOPipelineOutput,\r\n     36         DDPOSchedulerOutput,\r\n     37         DDPOStableDiffusionPipeline,\r\n     38         DefaultDDPOStableDiffusionPipeline,\r\n     39     )\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/trl\/trainer\/__init__.py:44\r\n     42 from .ppo_trainer import PPOTrainer\r\n     43 from .reward_trainer import RewardTrainer, compute_accuracy\r\n---> 44 from .sft_trainer import SFTTrainer\r\n     45 from .training_configs import RewardConfig\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/trl\/trainer\/sft_trainer.py:23\r\n     21 import torch.nn as nn\r\n     22 from datasets import Dataset\r\n---> 23 from datasets.arrow_writer import SchemaInferenceError\r\n     24 from datasets.builder import DatasetGenerationError\r\n     25 from transformers import (\r\n     26     AutoModelForCausalLM,\r\n     27     AutoTokenizer,\r\n   (...)\r\n     33     TrainingArguments,\r\n     34 )\r\n\r\nImportError: cannot import name 'SchemaInferenceError' from 'datasets.arrow_writer' (\/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\r\n\r\n````\r\n\r\ntransformers version: 4.36.2\r\npython version: 3.10.12\r\ndatasets version: 2.16.1\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Install packages\r\n```\r\n!pip install -U datasets trl accelerate peft bitsandbytes transformers trl huggingface_hub\r\n```\r\n\r\n2. import packages\r\n```\r\nimport os\r\nimport torch\r\nfrom datasets import load_dataset, Dataset\r\nfrom transformers import (\r\n    AutoModelForCausalLM,\r\n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n    HfArgumentParser,\r\n    TrainingArguments,\r\n    pipeline,\r\n    logging\r\n)\r\nfrom peft import LoraConfig, PeftModel\r\nfrom trl import SFTTrainer\r\nfrom huggingface_hub import login\r\nimport pandas as pd\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo error while importing\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.16.0\r\n- Platform: Linux-5.15.133+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6538\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6537","id":2057132173,"node_id":"I_kwDODunzps56nViN","number":6537,"title":"Adding support for netCDF (*.nc) files","user":{"login":"shermansiu","id":12627125,"node_id":"MDQ6VXNlcjEyNjI3MTI1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12627125?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shermansiu","html_url":"https:\/\/github.com\/shermansiu","followers_url":"https:\/\/api.github.com\/users\/shermansiu\/followers","following_url":"https:\/\/api.github.com\/users\/shermansiu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shermansiu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shermansiu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shermansiu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shermansiu\/orgs","repos_url":"https:\/\/api.github.com\/users\/shermansiu\/repos","events_url":"https:\/\/api.github.com\/users\/shermansiu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shermansiu\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-27T09:27:29Z","updated_at":"2023-12-27T20:46:53Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nnetCDF (*.nc) is a file format for storing multidimensional scientific data, which is used by packages like `xarray` (labelled multi-dimensional arrays in Python). It would be nice to have native support for netCDF in `datasets`.\n\n### Motivation\n\nWhen uploading *.nc files onto Huggingface Hub through the `datasets` API, I would like to be able to preview the dataset without converting it to another format.\n\n### Your contribution\n\nI can submit a PR, provided I have the time.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6537\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6536","id":2056863239,"node_id":"I_kwDODunzps56mT4H","number":6536,"title":"datasets.load_dataset raises FileNotFoundError for datasets==2.16.0","user":{"login":"ArvinZhuang","id":46237844,"node_id":"MDQ6VXNlcjQ2MjM3ODQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/46237844?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ArvinZhuang","html_url":"https:\/\/github.com\/ArvinZhuang","followers_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/followers","following_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/orgs","repos_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/repos","events_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ArvinZhuang\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-12-27T03:15:48Z","updated_at":"2023-12-30T18:58:04Z","closed_at":"2023-12-30T15:54:00Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nSeems `datasets.load_dataset` raises FileNotFoundError for some hub datasets with the latest `datasets==2.16.0`\n\n### Steps to reproduce the bug\n\nFor example `pip install datasets==2.16.0`\r\nthen\r\n\r\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset(\"wentingzhao\/anthropic-hh-first-prompt\", cache_dir='cache1')[\"train\"]\r\n\r\n```\r\n\r\nThis will raise:\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 2545, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 1003, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 1076, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/packaged_modules\/parquet\/parquet.py\", line 43, in _split_generators\r\n    data_files = dl_manager.download_and_extract(self.config.data_files)\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/download\/download_manager.py\", line 566, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/download\/download_manager.py\", line 539, in extract\r\n    extracted_paths = map_nested(\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 466, in map_nested\r\n    mapped = [\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 467, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True, None))\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 387, in _single_map_nested\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 387, in <listcomp>\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 370, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/download\/download_manager.py\", line 451, in _download\r\n    out = cached_path(url_or_filename, download_config=download_config)\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/file_utils.py\", line 188, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"\/Users\/xxx\/miniconda3\/envs\/env\/lib\/python3.9\/site-packages\/datasets\/utils\/file_utils.py\", line 570, in get_from_cache\r\n    raise FileNotFoundError(f\"Couldn't find file at {url}\")\r\nFileNotFoundError: Couldn't find file at https:\/\/huggingface.co\/datasets\/wentingzhao\/anthropic-hh-first-prompt\/resolve\/11b393a5545f706a357ebcd4a5285d93db176715\/cache1\/downloads\/87d66c365626feca116cba323c4856c9aae056e4503f09f23e34aa085eb9de15\r\n```\r\n\r\nHowever, seems it works fine for some datasets, for example, if works fine for `datasets.load_dataset(\"ag_news\", cache_dir='cache2')[\"test\"]`\r\n\r\nBut the dataset works fine for datasets==2.15.0, for example `pip install datasets==2.15.0`,\r\nthen\r\n\r\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset(\"wentingzhao\/anthropic-hh-first-prompt\", cache_dir='cache3')[\"train\"]\r\n\r\nDataset({\r\n    features: ['user', 'system', 'source'],\r\n    num_rows: 8552\r\n})\r\n```\r\n\r\n\r\n\n\n### Expected behavior\n\n2.16.0 should work as same as 2.15.0 for all datasets\n\n### Environment info\n\npython3.9\r\nconda env\r\ntested on MacOS and Linux","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6536\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6535","id":2056264339,"node_id":"I_kwDODunzps56kBqT","number":6535,"title":"IndexError: Invalid key: 47682 is out of bounds for size 0 while using PEFT","user":{"login":"MahavirDabas18","id":57484266,"node_id":"MDQ6VXNlcjU3NDg0MjY2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57484266?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/MahavirDabas18","html_url":"https:\/\/github.com\/MahavirDabas18","followers_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/followers","following_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/orgs","repos_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/repos","events_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/MahavirDabas18\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-26T10:14:33Z","updated_at":"2024-01-02T13:26:45Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am trying to fine-tune the t5 model on the paraphrasing task. While running the same code without-\r\n\r\nmodel = get_peft_model(model, config)\r\n\r\nthe model trains without any issues. However, using the model returned from get_peft_model raises the following error due to datasets-\r\n\r\nIndexError: Invalid key: 47682 is out of bounds for size 0.\r\n\r\nI had raised this in https:\/\/github.com\/huggingface\/peft\/issues\/1299#issue-2056173386 and they suggested that I raise it here.\r\n\r\nHere is the complete error-\r\n\r\nIndexError Traceback (most recent call last)\r\nin <cell line: 1>()\r\n----> 1 trainer.train()\r\n\r\n11 frames\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/trainer.py](https:\/\/localhost:8080\/#) in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\r\n1553 hf_hub_utils.enable_progress_bars()\r\n1554 else:\r\n-> 1555 return inner_training_loop(\r\n1556 args=args,\r\n1557 resume_from_checkpoint=resume_from_checkpoint,\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/trainer.py](https:\/\/localhost:8080\/#) in _inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\r\n1836\r\n1837 step = -1\r\n-> 1838 for step, inputs in enumerate(epoch_iterator):\r\n1839 total_batched_samples += 1\r\n1840 if rng_to_sync:\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/accelerate\/data_loader.py](https:\/\/localhost:8080\/#) in iter(self)\r\n446 # We iterate one batch ahead to check when we are at the end\r\n447 try:\r\n--> 448 current_batch = next(dataloader_iter)\r\n449 except StopIteration:\r\n450 yield\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/utils\/data\/dataloader.py](https:\/\/localhost:8080\/#) in next(self)\r\n628 # TODO(https:\/\/github.com\/pytorch\/pytorch\/issues\/76750)\r\n629 self._reset() # type: ignore[call-arg]\r\n--> 630 data = self._next_data()\r\n631 self._num_yielded += 1\r\n632 if self._dataset_kind == _DatasetKind.Iterable and \\\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/utils\/data\/dataloader.py](https:\/\/localhost:8080\/#) in _next_data(self)\r\n672 def _next_data(self):\r\n673 index = self._next_index() # may raise StopIteration\r\n--> 674 data = self._dataset_fetcher.fetch(index) # may raise StopIteration\r\n675 if self._pin_memory:\r\n676 data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/utils\/data\/_utils\/fetch.py](https:\/\/localhost:8080\/#) in fetch(self, possibly_batched_index)\r\n47 if self.auto_collation:\r\n48 if hasattr(self.dataset, \"getitems\") and self.dataset.getitems:\r\n---> 49 data = self.dataset.getitems(possibly_batched_index)\r\n50 else:\r\n51 data = [self.dataset[idx] for idx in possibly_batched_index]\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in getitems(self, keys)\r\n2802 def getitems(self, keys: List) -> List:\r\n2803 \"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\r\n-> 2804 batch = self.getitem(keys)\r\n2805 n_examples = len(batch[next(iter(batch))])\r\n2806 return [{col: array[i] for col, array in batch.items()} for i in range(n_examples)]\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in getitem(self, key)\r\n2798 def getitem(self, key): # noqa: F811\r\n2799 \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 2800 return self._getitem(key)\r\n2801\r\n2802 def getitems(self, keys: List) -> List:\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in _getitem(self, key, **kwargs)\r\n2782 format_kwargs = format_kwargs if format_kwargs is not None else {}\r\n2783 formatter = get_formatter(format_type, features=self._info.features, **format_kwargs)\r\n-> 2784 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n2785 formatted_output = format_table(\r\n2786 pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py](https:\/\/localhost:8080\/#) in query_table(table, key, indices)\r\n581 else:\r\n582 size = indices.num_rows if indices is not None else table.num_rows\r\n--> 583 _check_valid_index_key(key, size)\r\n584 # Query the main table\r\n585 if indices is None:\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py](https:\/\/localhost:8080\/#) in _check_valid_index_key(key, size)\r\n534 elif isinstance(key, Iterable):\r\n535 if len(key) > 0:\r\n--> 536 _check_valid_index_key(int(max(key)), size=size)\r\n537 _check_valid_index_key(int(min(key)), size=size)\r\n538 else:\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py](https:\/\/localhost:8080\/#) in _check_valid_index_key(key, size)\r\n524 if isinstance(key, int):\r\n525 if (key < 0 and key + size < 0) or (key >= size):\r\n--> 526 raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\r\n527 return\r\n528 elif isinstance(key, slice):\r\n\r\nIndexError: Invalid key: 47682 is out of bounds for size 0\n\n### Steps to reproduce the bug\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n\r\n#defining model name for tokenizer and model loading\r\nmodel_name= \"t5-small\"\r\n#loading the tokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\ndef preprocess_function(data, tokenizer):\r\ninputs = [f\"Paraphrase this sentence: {doc}\" for doc in data[\"text\"]]\r\nmodel_inputs = tokenizer(inputs, max_length=150, truncation=True)\r\nlabels = [ast.literal_eval(i)[0] for i in data['paraphrases']]\r\nlabels = tokenizer(labels, max_length=150, truncation=True)\r\nmodel_inputs[\"labels\"] = labels[\"input_ids\"]\r\nreturn model_inputs\r\n\r\ntrain_dataset = load_dataset(\"humarin\/chatgpt-paraphrases\", split=\"train\").shuffle(seed=42).select(range(50000))\r\nval_dataset = load_dataset(\"humarin\/chatgpt-paraphrases\", split=\"train\").shuffle(seed=42).select(range(50000,55000))\r\n\r\ntokenized_train = train_dataset.map(lambda batch: preprocess_function(batch, tokenizer), batched=True)\r\ntokenized_val = val_dataset.map(lambda batch: preprocess_function(batch, tokenizer), batched=True)\r\n\r\ndef print_trainable_parameters(model):\r\n\"\"\"\r\nPrints the number of trainable parameters in the model.\r\n\"\"\"\r\ntrainable_params = 0\r\nall_param = 0\r\nfor _, param in model.named_parameters():\r\nall_param += param.numel()\r\nif param.requires_grad:\r\ntrainable_params += param.numel()\r\nprint(\r\nf\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params \/ all_param}\"\r\n)\r\n\r\nconfig = LoraConfig(\r\nr=16, #attention heads\r\nlora_alpha=32, #alpha scaling\r\nlora_dropout=0.05,\r\nbias=\"none\",\r\ntask_type=\"Seq2Seq\"\r\n)\r\n\r\n#loading the model\r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\r\n\r\nmodel = get_peft_model(model, config)\r\nprint_trainable_parameters(model)\r\n\r\n#loading the data collator\r\ndata_collator = DataCollatorForSeq2Seq(\r\ntokenizer=tokenizer,\r\nmodel=model,\r\nlabel_pad_token_id=-100,\r\npadding=\"longest\"\r\n)\r\n\r\n#defining the training arguments\r\ntraining_args = Seq2SeqTrainingArguments(\r\noutput_dir=os.getcwd(),\r\nevaluation_strategy=\"epoch\",\r\nsave_strategy=\"epoch\",\r\nlearning_rate=2e-5,\r\nper_device_train_batch_size=16,\r\nper_device_eval_batch_size=16,\r\nweight_decay=1e-3,\r\nsave_total_limit=3,\r\nload_best_model_at_end=True,\r\nnum_train_epochs=1,\r\npredict_with_generate=True\r\n)\r\n\r\ndef compute_metric_with_extra(tokenizer):\r\ndef compute_metrics(eval_preds):\r\nmetric = evaluate.load('rouge')\r\npreds, labels = eval_preds\r\n\r\n    # decode preds and labels\r\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\r\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\r\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\r\n\r\n    # rougeLSum expects newline after each sentence\r\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\r\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\r\n\r\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\r\n    return result\r\n\r\nreturn compute_metrics\r\ntrainer = Seq2SeqTrainer(\r\nmodel=model,\r\nargs=training_args,\r\ntrain_dataset=tokenized_train,\r\neval_dataset=tokenized_val,\r\ntokenizer=tokenizer,\r\ndata_collator=data_collator,\r\ncompute_metrics= compute_metric_with_extra(tokenizer)\r\n)\r\n\r\ntrainer.train()\n\n### Expected behavior\n\nI would want the trainer to train normally as it was before I used-\r\n\r\nmodel = get_peft_model(model, config)\r\n\r\n\n\n### Environment info\n\ndatasets version- 2.16.0\r\npeft version- 0.7.1\r\ntransformers version- 4.35.2\r\naccelerate version- 0.25.0\r\npython- 3.10.12\r\nenviroment- google colab","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6535\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6534","id":2056002548,"node_id":"I_kwDODunzps56jBv0","number":6534,"title":"How to configure multiple folders in the same zip package","user":{"login":"d710055071","id":12895488,"node_id":"MDQ6VXNlcjEyODk1NDg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12895488?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/d710055071","html_url":"https:\/\/github.com\/d710055071","followers_url":"https:\/\/api.github.com\/users\/d710055071\/followers","following_url":"https:\/\/api.github.com\/users\/d710055071\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/d710055071\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/d710055071\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/d710055071\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/d710055071\/orgs","repos_url":"https:\/\/api.github.com\/users\/d710055071\/repos","events_url":"https:\/\/api.github.com\/users\/d710055071\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/d710055071\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-26T03:56:20Z","updated_at":"2023-12-26T06:31:16Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"How should I write \"config\" in readme when all the data, such as train test, is in a zip file\r\n\r\ntrain floder and test floder in data.zip","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6534\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6533","id":2055929101,"node_id":"I_kwDODunzps56iv0N","number":6533,"title":"ted_talks_iwslt | Error: Config name is missing","user":{"login":"rayliuca","id":35850903,"node_id":"MDQ6VXNlcjM1ODUwOTAz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35850903?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rayliuca","html_url":"https:\/\/github.com\/rayliuca","followers_url":"https:\/\/api.github.com\/users\/rayliuca\/followers","following_url":"https:\/\/api.github.com\/users\/rayliuca\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rayliuca\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rayliuca\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rayliuca\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rayliuca\/orgs","repos_url":"https:\/\/api.github.com\/users\/rayliuca\/repos","events_url":"https:\/\/api.github.com\/users\/rayliuca\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rayliuca\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-12-26T00:38:18Z","updated_at":"2023-12-30T18:58:21Z","closed_at":"2023-12-30T16:09:50Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nRunning load_dataset using the newest `datasets` library like below on the ted_talks_iwslt using year pair data will throw an error \"Config name is missing\"\r\n\r\nsee also:\r\nhttps:\/\/huggingface.co\/datasets\/ted_talks_iwslt\/discussions\/3\r\n\r\nlikely caused by #6493, where the `and not config_kwargs` part in the if logic was removed \r\n https:\/\/github.com\/huggingface\/datasets\/blob\/ef3b5dd3633995c95d77f35fb17f89ff44990bc4\/src\/datasets\/builder.py#L512\n\n### Steps to reproduce the bug\n\nrun: \r\n```python\r\nload_dataset(\"ted_talks_iwslt\", language_pair=(\"ja\", \"en\"), year=\"2015\")\r\n```\n\n### Expected behavior\n\nLoad the data without error\n\n### Environment info\n\ndatasets 2.16.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6533\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6532","id":2055631201,"node_id":"I_kwDODunzps56hnFh","number":6532,"title":"[Feature request] Indexing datasets by a customly-defined id field to enable random access dataset items via the id","user":{"login":"Yu-Shi","id":3377221,"node_id":"MDQ6VXNlcjMzNzcyMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3377221?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Yu-Shi","html_url":"https:\/\/github.com\/Yu-Shi","followers_url":"https:\/\/api.github.com\/users\/Yu-Shi\/followers","following_url":"https:\/\/api.github.com\/users\/Yu-Shi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Yu-Shi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Yu-Shi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Yu-Shi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Yu-Shi\/orgs","repos_url":"https:\/\/api.github.com\/users\/Yu-Shi\/repos","events_url":"https:\/\/api.github.com\/users\/Yu-Shi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Yu-Shi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-25T11:37:10Z","updated_at":"2024-01-02T13:52:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nSome datasets may contain an id-like field, for example the `id` field in [wikimedia\/wikipedia](https:\/\/huggingface.co\/datasets\/wikimedia\/wikipedia) and the `_id` field in [BeIR\/dbpedia-entity](https:\/\/huggingface.co\/datasets\/BeIR\/dbpedia-entity). HF datasets support efficient random access via row, but not via this kinds of id fields. I wonder if it is possible to add support for indexing by a custom \"id-like\" field to enable random access via such ids. The ids may be numbers or strings.\r\n\r\n### Motivation\r\n\r\nIn some cases, especially during inference\/evaluation, I may want to find out the item that has a specified id, defined by the dataset itself.\r\n\r\nFor example, in a typical re-ranking setting in information retrieval, the user may want to re-rank the set of candidate documents of each query. The input is usually presented in a TREC-style run file, with the following format:\r\n\r\n```\r\n<qid> Q0 <docno> <rank> <score> <tag>\r\n```\r\n\r\nThe re-ranking program should be able to fetch the queries and documents according to the `<qid>` and `<docno>`, which are the original id defined in the query\/document datasets. To accomplish this, I have to iterate over the whole HF dataset to get the mapping from real ids to row ids every time I start the program, which is time-consuming. Thus I want HF dataset to provide options for users to index by a custom id column, not by row.\r\n\r\n### Your contribution\r\n\r\nI'm not an expert in this project and I'm afraid that I'm not able to make contributions on the code.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6532\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6531","id":2055201605,"node_id":"PR_kwDODunzps5it5Sm","number":6531,"title":"Add polars compatibility","user":{"login":"psmyth94","id":11325244,"node_id":"MDQ6VXNlcjExMzI1MjQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11325244?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/psmyth94","html_url":"https:\/\/github.com\/psmyth94","followers_url":"https:\/\/api.github.com\/users\/psmyth94\/followers","following_url":"https:\/\/api.github.com\/users\/psmyth94\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/psmyth94\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/psmyth94\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/psmyth94\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/psmyth94\/orgs","repos_url":"https:\/\/api.github.com\/users\/psmyth94\/repos","events_url":"https:\/\/api.github.com\/users\/psmyth94\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/psmyth94\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-24T20:03:23Z","updated_at":"2023-12-24T20:03:23Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hey there,\r\n\r\nI've just finished adding support to convert and format to `polars.DataFrame`. This was in response to the open issue about integrating Polars [#3334](https:\/\/github.com\/huggingface\/datasets\/issues\/3334). Datasets can be switched to Polars format via `Dataset.set_format(\"polars\")`. I've also included `to_polars` and `from_polars`. All polars functions are checked via config.POLARS_AVAILABLE. \r\n\r\nA few notes:\r\nThis only supports `DataFrames` and not `LazyFrames`. This probably could be integrated fairly easily via `is_lazy` args in `set_format`, and `to_polars`.\r\n\r\nLet me know your feedbacks.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6531\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6531","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6531","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6531.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6531.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6530","id":2054817609,"node_id":"I_kwDODunzps56egdJ","number":6530,"title":"Impossible to save a mapped dataset to disk","user":{"login":"kopyl","id":17604849,"node_id":"MDQ6VXNlcjE3NjA0ODQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17604849?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kopyl","html_url":"https:\/\/github.com\/kopyl","followers_url":"https:\/\/api.github.com\/users\/kopyl\/followers","following_url":"https:\/\/api.github.com\/users\/kopyl\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kopyl\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kopyl\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kopyl\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kopyl\/orgs","repos_url":"https:\/\/api.github.com\/users\/kopyl\/repos","events_url":"https:\/\/api.github.com\/users\/kopyl\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kopyl\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-23T15:18:27Z","updated_at":"2023-12-24T09:40:30Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI want to play around with different hyperparameters when training but don't want to re-map my dataset with 3 million samples each time for tens of hours when I [fully fine-tune SDXL](https:\/\/github.com\/huggingface\/diffusers\/blob\/main\/examples\/text_to_image\/train_text_to_image_sdxl.py).\r\n\r\nAfter I do the mapping like this:\r\n```\r\ntrain_dataset = train_dataset.map(compute_embeddings_fn, batched=True)\r\ntrain_dataset = train_dataset.map(\r\n    compute_vae_encodings_fn,\r\n    batched=True,\r\n    batch_size=16,\r\n)\r\n```\r\nand try to save it like this:\r\n`train_dataset.save_to_disk(\"test\")`\r\ni get this error ([full traceback](https:\/\/pastebin.com\/kq3vt739)):\r\n```\r\nTypeError: Object of type function is not JSON serializable\r\nThe format kwargs must be JSON serializable, but key 'transform' isn't.\r\n```\r\n\r\nBut what is interesting is that pushing to hub works like that:\r\n`train_dataset.push_to_hub(\"kopyl\/mapped-833-icons-sdxl-1024-dataset\", token=True)`\r\nHere is the link of the pushed dataset: https:\/\/huggingface.co\/datasets\/kopyl\/mapped-833-icons-sdxl-1024-dataset\r\n\r\n### Steps to reproduce the bug\r\n\r\nHere is the self-contained notebook:\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/1RtCsEMVcwWcMwlWURk_cj_9xUBHz065M?usp=sharing\r\n\r\n### Expected behavior\r\n\r\nIt should be easily saved to disk\r\n\r\n### Environment info\r\n\r\nNVIDIA A100, Linux (NC24ads A100 v4 from Azure), CUDA 12.2.\r\n\r\n[pip freeze](https:\/\/pastebin.com\/QTNb6iru)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6530\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6529","id":2054209449,"node_id":"I_kwDODunzps56cL-p","number":6529,"title":"Impossible to only download a test split","user":{"login":"ysig","id":28439529,"node_id":"MDQ6VXNlcjI4NDM5NTI5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/28439529?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ysig","html_url":"https:\/\/github.com\/ysig","followers_url":"https:\/\/api.github.com\/users\/ysig\/followers","following_url":"https:\/\/api.github.com\/users\/ysig\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ysig\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ysig\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ysig\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ysig\/orgs","repos_url":"https:\/\/api.github.com\/users\/ysig\/repos","events_url":"https:\/\/api.github.com\/users\/ysig\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ysig\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-22T16:56:32Z","updated_at":"2023-12-22T20:46:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I've spent a significant amount of time trying to locate the split object inside my _split_generators() custom function.\r\nThen after diving [in the code](https:\/\/github.com\/huggingface\/datasets\/blob\/5ff3670c18ed34fa8ddfa70a9aa403ae6cc9ad54\/src\/datasets\/load.py#L2558) I realized that `download_and_prepare` is executed before! split is passed to the dataset builder in `as_dataset`.\r\n\r\nIf I'm not missing something, this seems like bad design, for the following use case:\r\n\r\n> Imagine there is a huge dataset that has an evaluation test set and you want to just download and run just to compare your method.\r\n\r\nIs there a current workaround that can help me achieve the same result?\r\n\r\nThank you,","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6529\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6528","id":2053996494,"node_id":"PR_kwDODunzps5ip9JH","number":6528,"title":"set dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-22T14:23:18Z","updated_at":"2023-12-22T14:31:42Z","closed_at":"2023-12-22T14:25:34Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6528\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6528","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6528","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6528.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6528.patch","merged_at":"2023-12-22T14:25:34Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6527","id":2053966748,"node_id":"PR_kwDODunzps5ip2vd","number":6527,"title":"Release: 2.16.0","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-22T13:59:56Z","updated_at":"2023-12-22T14:24:12Z","closed_at":"2023-12-22T14:17:55Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6527\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6527","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6527","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6527.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6527.patch","merged_at":"2023-12-22T14:17:55Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6526","id":2053726451,"node_id":"PR_kwDODunzps5ipB5v","number":6526,"title":"Preserve order of configs and splits when using Parquet exports","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-22T10:35:56Z","updated_at":"2023-12-22T11:42:22Z","closed_at":"2023-12-22T11:36:14Z","author_association":"MEMBER","active_lock_reason":null,"body":"Preserve order of configs and splits, as defined in dataset infos.\r\n\r\nFix #6521.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6526\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6526","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6526","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6526.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6526.patch","merged_at":"2023-12-22T11:36:14Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6525","id":2053119357,"node_id":"PR_kwDODunzps5im-lL","number":6525,"title":"BBox type","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-21T22:13:27Z","updated_at":"2023-12-21T22:39:28Z","closed_at":"2023-12-21T22:39:27Z","author_association":"MEMBER","active_lock_reason":null,"body":"see [internal discussion](https:\/\/huggingface.slack.com\/archives\/C02EK7C3SHW\/p1703097195609209)\r\n\r\nDraft to get some feedback on a possible `BBox` feature type that can be used to get object detection bounding boxes data in one format or another.\r\n\r\n```python\r\n>>> from datasets import load_dataset, BBox\r\n>>> ds = load_dataset(\"svhn\", \"full_numbers\", split=\"train\")\r\n>>> ds[0]\r\n{\r\n  'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=107x46 at 0x126409BE0>,\r\n  'digits': {'bbox': [[38, 1, 21, 40], [57, 3, 16, 40]], 'label': [4, 6]}\r\n}\r\n>>> ds = ds.rename_column(\"digits\", \"annotations\").cast_column(\"annotations\", BBox(format=\"coco\"))\r\n>>> ds[0]\r\n{\r\n  'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=107x46 at 0x147730070>,\r\n  'annotations': [{'bbox': [38, 1, 21, 40], 'category_id': 4}, {'bbox': [57, 3, 16, 40], 'category_id': 6}]\r\n}\r\n```\r\n\r\nnote that it's a type for a list of bounding boxes, not just one - which would be needed to switch from a format to another using type casting.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6525\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6525","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6525","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6525.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6525.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6524","id":2053076311,"node_id":"I_kwDODunzps56X3VX","number":6524,"title":"Streaming the Pile: Missing Files","user":{"login":"FelixLabelle","id":23347756,"node_id":"MDQ6VXNlcjIzMzQ3NzU2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23347756?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/FelixLabelle","html_url":"https:\/\/github.com\/FelixLabelle","followers_url":"https:\/\/api.github.com\/users\/FelixLabelle\/followers","following_url":"https:\/\/api.github.com\/users\/FelixLabelle\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/FelixLabelle\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/FelixLabelle\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/FelixLabelle\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/FelixLabelle\/orgs","repos_url":"https:\/\/api.github.com\/users\/FelixLabelle\/repos","events_url":"https:\/\/api.github.com\/users\/FelixLabelle\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/FelixLabelle\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-12-21T21:25:09Z","updated_at":"2023-12-22T09:17:05Z","closed_at":"2023-12-22T09:17:05Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThe pile does not stream, a \"File not Found error\" is returned. It looks like the Pile's files have been moved.\n\n### Steps to reproduce the bug\n\nTo reproduce run the following code:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('EleutherAI\/pile', 'en', split='train', streaming=True)\r\nnext(iter(dataset))\r\n```\r\n\r\nI get the following error:\r\n`FileNotFoundError: https:\/\/the-eye.eu\/public\/AI\/pile\/train\/00.jsonl.zst`\n\n### Expected behavior\n\nReturn the data in a stream.\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.11.5\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6524\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6523","id":2052643484,"node_id":"PR_kwDODunzps5ilV6d","number":6523,"title":"fix tests","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-21T15:36:21Z","updated_at":"2023-12-21T15:56:54Z","closed_at":"2023-12-21T15:50:38Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6523\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6523","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6523","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6523.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6523.patch","merged_at":"2023-12-21T15:50:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6522","id":2052332528,"node_id":"I_kwDODunzps56VBvw","number":6522,"title":"Loading HF Hub Dataset (private org repo) fails to load all features","user":{"login":"versipellis","id":6579034,"node_id":"MDQ6VXNlcjY1NzkwMzQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6579034?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/versipellis","html_url":"https:\/\/github.com\/versipellis","followers_url":"https:\/\/api.github.com\/users\/versipellis\/followers","following_url":"https:\/\/api.github.com\/users\/versipellis\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/versipellis\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/versipellis\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/versipellis\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/versipellis\/orgs","repos_url":"https:\/\/api.github.com\/users\/versipellis\/repos","events_url":"https:\/\/api.github.com\/users\/versipellis\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/versipellis\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-21T12:26:35Z","updated_at":"2023-12-21T13:24:31Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen pushing a `Dataset` with multiple `Features` (`input`, `output`, `tags`) to Huggingface Hub (private org repo), and later downloading the `Dataset`, only `input` and `output` load - I believe the expected behavior is for all `Features` to be loaded by default?\r\n\r\n### Steps to reproduce the bug\r\n\r\nPushing the data. `data_concat` is a `list` of `dict`s.\r\n```python\r\nfor datum in data_concat:\r\n    datum_tags = {d[\"key\"]: d[\"value\"] for d in datum[\"tags\"]}\r\n    split_fraction = # some logic that generates a train\/test split number\r\n    if split_faction < test_fraction:\r\n        data_test.append(datum)\r\n    else:\r\n        data_train.append(datum)\r\n\r\ndataset = DatasetDict(\r\n    {\r\n        \"train\": Dataset.from_list(data_train),\r\n        \"test\": Dataset.from_list(data_test),\r\n        \"full\": Dataset.from_list(data_concat),\r\n    },\r\n)\r\n\r\ndataset_shuffled = dataset.shuffle(seed=shuffle_seed)\r\n\r\ndataset_shuffled.push_to_hub(\r\n    repo_id=hf_repo_id,\r\n    private=True,\r\n    config_name=m,\r\n    revision=revision,\r\n    token=hf_token,\r\n)\r\n```\r\n\r\nLoading it later:\r\n```python\r\n        dataset = datasets.load_dataset(\r\n            path=hf_repo_id,\r\n            name=name,\r\n            token=hf_token,\r\n        )\r\n```\r\n\r\nProduces:\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    test: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    full: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: <obfuscated>\r\n    })\r\n})\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe expected result is below:\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['input', 'output', 'tags'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    test: Dataset({\r\n        features: ['input', 'output', 'tags'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    full: Dataset({\r\n        features: ['input', 'output', 'tags'],\r\n        num_rows: <obfuscated>\r\n    })\r\n})\r\n```\r\n\r\nMy workaround is as follows:\r\n```python\r\ndsinfo = datasets.get_dataset_config_info(\r\n    path=data_files,\r\n    config_name=data_config,\r\n    token=hf_token,\r\n)\r\n\r\nallfeatures = dsinfo.features.copy()\r\nif \"tags\" not in allfeatures:\r\n    allfeatures[\"tags\"] = [{\"key\": Value(dtype=\"string\", id=None), \"value\": Value(dtype=\"string\", id=None)}]\r\n\r\ndataset = datasets.load_dataset(\r\n    path=data_files,\r\n    name=data_config,\r\n    features=allfeatures,\r\n    token=hf_token,\r\n)\r\n```\r\n\r\nInterestingly enough (and perhaps a related bug?), if I don't add the `tags` to `allfeatures` above (i.e. only loading `input` and `output`), it throws an error when executing `load_dataset`:\r\n\r\n```\r\nValueError: Couldn't cast\r\ntags: list<element: struct<key: string, value: string>>\r\n  child 0, element: struct<key: string, value: string>\r\n      child 0, key: string\r\n      child 1, value: string\r\ninput: <obfuscated>\r\noutput: <obfuscated>\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"tags\": [{\"key\": {\"dtype\": \"string\",' + 532\r\nto\r\n{'input': <obfuscated>, 'output': <obfuscated>\r\nbecause column names don't match\r\n```\r\n\r\nTraceback for this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/Users\/bt\/github\/core\/.venv\/lib\/python3.11\/site-packages\/datasets\/load.py\", line 2152, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/Users\/bt\/github\/core\/.venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 948, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/Users\/bt\/github\/core\/.venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1043, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/Users\/bt\/github\/core\/.venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1805, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"\/Users\/bt\/github\/core\/.venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1950, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: macOS-14.0-arm64-arm-64bit\r\n- Python version: 3.11.5\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6522\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6521","id":2052229538,"node_id":"I_kwDODunzps56Uomi","number":6521,"title":"The order of the splits is not preserved","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-12-21T11:17:27Z","updated_at":"2023-12-22T11:36:15Z","closed_at":"2023-12-22T11:36:15Z","author_association":"MEMBER","active_lock_reason":null,"body":"We had a regression and the order of the splits is not preserved. They are alphabetically sorted, instead of preserving original \"train\", \"validation\", \"test\" order.\r\n\r\nCheck: In branch \"main\"\r\n```python\r\nIn [9]: dataset = load_dataset(\"adversarial_qa\", '\"adversarialQA\")\r\n\r\nIn [10]: dataset\r\nOut[10]: \r\nDatasetDict({\r\n    test: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n    train: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 30000\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n})\r\n```\r\n\r\nBefore (2.15.0) it was:\r\n```python\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 30000\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n    test: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n})\r\n```\r\n\r\nSee issues: \r\n- https:\/\/huggingface.co\/datasets\/adversarial_qa\/discussions\/3\r\n- https:\/\/huggingface.co\/datasets\/beans\/discussions\/4\r\n\r\nThis is a regression because it was previously fixed. See:\r\n- #6196\r\n- #5728","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6521\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6520","id":2052059078,"node_id":"PR_kwDODunzps5ijUiw","number":6520,"title":"Support commit_description parameter in push_to_hub","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-21T09:36:11Z","updated_at":"2023-12-21T14:49:47Z","closed_at":"2023-12-21T14:43:35Z","author_association":"MEMBER","active_lock_reason":null,"body":"Support `commit_description` parameter in `push_to_hub`.\r\n\r\nCC: @Wauplin ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6520\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6520","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6520","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6520.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6520.patch","merged_at":"2023-12-21T14:43:35Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6519","id":2050759824,"node_id":"PR_kwDODunzps5ie4MA","number":6519,"title":"Support push_to_hub canonical datasets","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-12-20T15:16:45Z","updated_at":"2023-12-21T14:48:20Z","closed_at":"2023-12-21T14:40:57Z","author_association":"MEMBER","active_lock_reason":null,"body":"Support `push_to_hub` canonical datasets.\r\n\r\nThis is necessary in the Space to convert script-datasets to Parquet: https:\/\/huggingface.co\/spaces\/albertvillanova\/convert-dataset-to-parquet\r\n\r\nNote that before this PR, the `repo_id` \"dataset_name\" was transformed to \"user\/dataset_name\". This behavior was introduced by:\r\n- #6269","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6519\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6519","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6519","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6519.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6519.patch","merged_at":"2023-12-21T14:40:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6518","id":2050137038,"node_id":"PR_kwDODunzps5icu-W","number":6518,"title":"fix get_metadata_patterns function args error","user":{"login":"d710055071","id":12895488,"node_id":"MDQ6VXNlcjEyODk1NDg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12895488?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/d710055071","html_url":"https:\/\/github.com\/d710055071","followers_url":"https:\/\/api.github.com\/users\/d710055071\/followers","following_url":"https:\/\/api.github.com\/users\/d710055071\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/d710055071\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/d710055071\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/d710055071\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/d710055071\/orgs","repos_url":"https:\/\/api.github.com\/users\/d710055071\/repos","events_url":"https:\/\/api.github.com\/users\/d710055071\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/d710055071\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-20T09:06:22Z","updated_at":"2023-12-21T15:14:17Z","closed_at":"2023-12-21T15:07:57Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Bug get_metadata_patterns arg error https:\/\/github.com\/huggingface\/datasets\/issues\/6517","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6518\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6518","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6518","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6518.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6518.patch","merged_at":"2023-12-21T15:07:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6517","id":2050121588,"node_id":"I_kwDODunzps56Ml90","number":6517,"title":"Bug get_metadata_patterns arg error","user":{"login":"d710055071","id":12895488,"node_id":"MDQ6VXNlcjEyODk1NDg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12895488?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/d710055071","html_url":"https:\/\/github.com\/d710055071","followers_url":"https:\/\/api.github.com\/users\/d710055071\/followers","following_url":"https:\/\/api.github.com\/users\/d710055071\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/d710055071\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/d710055071\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/d710055071\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/d710055071\/orgs","repos_url":"https:\/\/api.github.com\/users\/d710055071\/repos","events_url":"https:\/\/api.github.com\/users\/d710055071\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/d710055071\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-20T08:56:44Z","updated_at":"2023-12-22T00:24:23Z","closed_at":"2023-12-22T00:24:23Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"https:\/\/github.com\/huggingface\/datasets\/blob\/3f149204a2a5948287adcade5e90707aa5207a92\/src\/datasets\/load.py#L1240C1-L1240C69\r\nmetadata_patterns = get_metadata_patterns(base_path, download_config=self.download_config)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6517\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6516","id":2050033322,"node_id":"PR_kwDODunzps5icYX0","number":6516,"title":"Support huggingface-hub pre-releases","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-20T07:52:29Z","updated_at":"2023-12-20T08:51:34Z","closed_at":"2023-12-20T08:44:44Z","author_association":"MEMBER","active_lock_reason":null,"body":"Support `huggingface-hub` pre-releases.\r\n\r\nThis way we will have our CI green when testing `huggingface-hub` release candidates. See: https:\/\/github.com\/huggingface\/datasets\/tree\/ci-test-huggingface-hub-v0.20.0.rc1\r\n\r\nClose #6513.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6516\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6516","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6516","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6516.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6516.patch","merged_at":"2023-12-20T08:44:44Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6515","id":2049724251,"node_id":"I_kwDODunzps56LE9b","number":6515,"title":"Why call http_head() when fsspec_head()  succeeds","user":{"login":"d710055071","id":12895488,"node_id":"MDQ6VXNlcjEyODk1NDg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12895488?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/d710055071","html_url":"https:\/\/github.com\/d710055071","followers_url":"https:\/\/api.github.com\/users\/d710055071\/followers","following_url":"https:\/\/api.github.com\/users\/d710055071\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/d710055071\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/d710055071\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/d710055071\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/d710055071\/orgs","repos_url":"https:\/\/api.github.com\/users\/d710055071\/repos","events_url":"https:\/\/api.github.com\/users\/d710055071\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/d710055071\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-20T02:25:51Z","updated_at":"2023-12-26T05:35:46Z","closed_at":"2023-12-26T05:35:46Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"https:\/\/github.com\/huggingface\/datasets\/blob\/a91582de288d98e94bcb5ab634ca1cfeeff544c5\/src\/datasets\/utils\/file_utils.py#L510C1-L523C14","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6515\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6514","id":2049600663,"node_id":"PR_kwDODunzps5ia6Os","number":6514,"title":"Cache backward compatibility with 2.15.0","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-12-19T23:52:25Z","updated_at":"2023-12-21T21:14:11Z","closed_at":"2023-12-21T21:07:55Z","author_association":"MEMBER","active_lock_reason":null,"body":"...for datasets without scripts\r\n\r\nIt takes into account the changes in cache from\r\n- https:\/\/github.com\/huggingface\/datasets\/pull\/6493: switch to `config\/version\/commit_sha` schema\r\n- https:\/\/github.com\/huggingface\/datasets\/pull\/6454: fix `DataFilesDict` keys ordering when hashing\r\n\r\nrequires https:\/\/github.com\/huggingface\/datasets\/pull\/6493 to be merged","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6514\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6514","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6514","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6514.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6514.patch","merged_at":"2023-12-21T21:07:55Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6513","id":2048869151,"node_id":"I_kwDODunzps56H0Mf","number":6513,"title":"Support huggingface-hub 0.20.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-19T15:15:46Z","updated_at":"2023-12-20T08:44:45Z","closed_at":"2023-12-20T08:44:45Z","author_association":"MEMBER","active_lock_reason":null,"body":"CI to test the support of `huggingface-hub` 0.20.0: https:\/\/github.com\/huggingface\/datasets\/compare\/main...ci-test-huggingface-hub-v0.20.0.rc1\r\n\r\nWe need to merge:\r\n- #6510 \r\n- #6512\r\n- #6516","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6513\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6512","id":2048795819,"node_id":"PR_kwDODunzps5iYI5z","number":6512,"title":"Remove deprecated HfFolder","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-19T14:40:49Z","updated_at":"2023-12-19T20:21:13Z","closed_at":"2023-12-19T20:14:30Z","author_association":"MEMBER","active_lock_reason":null,"body":"...and use `huggingface_hub.get_token()` instead","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6512\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6512","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6512","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6512.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6512.patch","merged_at":"2023-12-19T20:14:30Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6511","id":2048465958,"node_id":"PR_kwDODunzps5iXAXR","number":6511,"title":"Implement get dataset default config name","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-19T11:26:19Z","updated_at":"2023-12-21T14:48:57Z","closed_at":"2023-12-21T14:42:41Z","author_association":"MEMBER","active_lock_reason":null,"body":"Implement `get_dataset_default_config_name`.\r\n\r\nNow that we support setting a configuration as default in `push_to_hub` (see #6500), we need a programmatically way to know in advance which is the default configuration. This will be used in the Space to convert script-datasets to Parquet: https:\/\/huggingface.co\/spaces\/albertvillanova\/convert-dataset-to-parquet\r\n\r\nFollow-up of:\r\n- #6500\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6511\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6511","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6511","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6511.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6511.patch","merged_at":"2023-12-21T14:42:40Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6510","id":2046928742,"node_id":"PR_kwDODunzps5iRyiV","number":6510,"title":"Replace `list_files_info` with `list_repo_tree` in `push_to_hub`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-18T15:34:19Z","updated_at":"2023-12-19T18:05:47Z","closed_at":"2023-12-19T17:58:34Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Starting from `huggingface_hub` 0.20.0, `list_files_info` will be deprecated in favor of `list_repo_tree` (see https:\/\/github.com\/huggingface\/huggingface_hub\/pull\/1910)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6510\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6510","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6510","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6510.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6510.patch","merged_at":"2023-12-19T17:58:34Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6509","id":2046720869,"node_id":"PR_kwDODunzps5iREyE","number":6509,"title":"Better cast error when generating dataset","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-18T13:57:24Z","updated_at":"2023-12-19T09:37:12Z","closed_at":"2023-12-19T09:31:03Z","author_association":"MEMBER","active_lock_reason":null,"body":"I want to improve the error message for datasets like https:\/\/huggingface.co\/datasets\/m-a-p\/COIG-CQIA\r\n\r\nCc @albertvillanova @severo is this new error ok ? Or should I use a dedicated error class ?\r\n\r\nNew:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1920, in _prepare_split_single\r\n    writer.write_table(table)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/arrow_writer.py\", line 574, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/table.py\", line 2322, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/table.py\", line 2276, in cast_table_to_schema\r\n    raise CastError(\r\ndatasets.table.CastError: Couldn't cast\r\ninstruction: string\r\nother: string\r\nindex: string\r\ndomain: list<item: string>\r\n  child 0, item: string\r\noutput: string\r\ntask_type: struct<major: list<item: string>, minor: list<item: string>>\r\n  child 0, major: list<item: string>\r\n      child 0, item: string\r\n  child 1, minor: list<item: string>\r\n      child 0, item: string\r\ntask_name_in_eng: string\r\ninput: string\r\nto\r\n{'answer_from': Value(dtype='string', id=None), 'instruction': Value(dtype='string', id=None), 'human_verified': Value(dtype='bool', id=None), 'domain': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'output': Value(dtype='string', id=None), 'task_type': {'major': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'minor': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'copyright': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None)}\r\nbecause column names don't match\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/playground\/ttest.py\", line 74, in <module>\r\n    load_dataset(\"m-a-p\/COIG-CQIA\")\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/load.py\", line 2529, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 936, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1031, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1791, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1922, in _prepare_split_single\r\n    raise DatasetGenerationCastError.from_cast_error(\r\ndatasets.exceptions.DatasetGenerationCastError: An error occurred while generating the dataset\r\n\r\nAll the data files must have the same columns, but at some point there are 3 new columns (other, index, task_name_in_eng) and 3 missing columns (answer_from, copyright, human_verified).\r\n\r\nThis happened while the json dataset builder was generating data using\r\n\r\nhf:\/\/datasets\/m-a-p\/COIG-CQIA\/coig_pc\/coig_pc_core_sample.json (at revision b7b7ecf290f6515036c7c04bd8537228ac2eb474)\r\n\r\nPlease either edit the data files to have matching columns, or separate them into different configurations (see docs at https:\/\/hf.co\/docs\/hub\/datasets-manual-configuration#multiple-configurations)\r\n```\r\n\r\nPreviously:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1931, in _prepare_split_single\r\n    writer.write_table(table)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/arrow_writer.py\", line 574, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/table.py\", line 2295, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/table.py\", line 2253, in cast_table_to_schema\r\n    raise ValueError(f\"Couldn't cast\\n{table.schema}\\nto\\n{features}\\nbecause column names don't match\")\r\nValueError: Couldn't cast\r\ntask_type: struct<major: list<item: string>, minor: list<item: string>>\r\n  child 0, major: list<item: string>\r\n      child 0, item: string\r\n  child 1, minor: list<item: string>\r\n      child 0, item: string\r\nother: string\r\ninstruction: string\r\ntask_name_in_eng: string\r\ndomain: list<item: string>\r\n  child 0, item: string\r\nindex: string\r\noutput: string\r\ninput: string\r\nto\r\n{'human_verified': Value(dtype='bool', id=None), 'task_type': {'major': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'minor': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'answer_from': Value(dtype='string', id=None), 'copyright': Value(dtype='string', id=None), 'instruction': Value(dtype='string', id=None), 'domain': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'output': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/playground\/ttest.py\", line 74, in <module>\r\n    load_dataset(\"m-a-p\/COIG-CQIA\")\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/load.py\", line 2529, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 949, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1044, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1804, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"\/Users\/quentinlhoest\/hf\/datasets\/src\/datasets\/builder.py\", line 1949, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6509\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6509","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6509","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6509.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6509.patch","merged_at":"2023-12-19T09:31:03Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6508","id":2045733273,"node_id":"PR_kwDODunzps5iNvAu","number":6508,"title":"Read GeoParquet files using parquet reader","user":{"login":"weiji14","id":23487320,"node_id":"MDQ6VXNlcjIzNDg3MzIw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23487320?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/weiji14","html_url":"https:\/\/github.com\/weiji14","followers_url":"https:\/\/api.github.com\/users\/weiji14\/followers","following_url":"https:\/\/api.github.com\/users\/weiji14\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/weiji14\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/weiji14\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/weiji14\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/weiji14\/orgs","repos_url":"https:\/\/api.github.com\/users\/weiji14\/repos","events_url":"https:\/\/api.github.com\/users\/weiji14\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/weiji14\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-12-18T04:50:37Z","updated_at":"2024-01-02T15:40:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Let GeoParquet files with the file extension `*.geoparquet` or `*.gpq` be readable by the default parquet reader.\r\n\r\nThose two file extensions are the ones most commonly used for GeoParquet files, and is included in the `gpq` validator tool at https:\/\/github.com\/planetlabs\/gpq\/blob\/e5576b4ee7306b4d2259d56c879465a9364dab90\/cmd\/gpq\/command\/convert.go#L73-L75\r\n\r\nAddresses https:\/\/github.com\/huggingface\/datasets\/issues\/6438","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6508\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6508","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6508","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6508.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6508.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6507","id":2045152928,"node_id":"I_kwDODunzps555o6g","number":6507,"title":"where is glue_metric.py> @Frankie123421 what was the resolution to this?","user":{"login":"Mcccccc1024","id":119146162,"node_id":"U_kgDOBxoGsg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/119146162?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Mcccccc1024","html_url":"https:\/\/github.com\/Mcccccc1024","followers_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/followers","following_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/orgs","repos_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/repos","events_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Mcccccc1024\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-17T09:58:25Z","updated_at":"2023-12-18T11:42:49Z","closed_at":"2023-12-18T11:42:49Z","author_association":"NONE","active_lock_reason":null,"body":"              > @Frankie123421 what was the resolution to this?\r\n\r\nuse glue_metric.py instead of glue.py in load_metric\r\n\r\n_Originally posted by @Frankie123421 in https:\/\/github.com\/huggingface\/datasets\/issues\/2117#issuecomment-905093763_\r\n            ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6507\/timeline","performed_via_github_app":null,"state_reason":"not_planned","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6506","id":2044975038,"node_id":"I_kwDODunzps5549e-","number":6506,"title":"Incorrect test set labels for RTE and CoLA datasets via load_dataset","user":{"login":"emreonal11","id":73316684,"node_id":"MDQ6VXNlcjczMzE2Njg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/73316684?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/emreonal11","html_url":"https:\/\/github.com\/emreonal11","followers_url":"https:\/\/api.github.com\/users\/emreonal11\/followers","following_url":"https:\/\/api.github.com\/users\/emreonal11\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/emreonal11\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/emreonal11\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/emreonal11\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/emreonal11\/orgs","repos_url":"https:\/\/api.github.com\/users\/emreonal11\/repos","events_url":"https:\/\/api.github.com\/users\/emreonal11\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/emreonal11\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-16T22:06:08Z","updated_at":"2023-12-21T09:57:57Z","closed_at":"2023-12-21T09:57:57Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nThe test set labels for the RTE and CoLA datasets when loading via datasets load_dataset are all -1.\r\nEdit: It appears this is also the case for every other dataset except for MRPC (stsb, sst2, qqp, mnli (both matched and mismatched), qnli, wnli, ax). Is this intended behavior to safeguard the test set for evaluation purposes?\r\n\r\n### Steps to reproduce the bug\r\n\r\n!pip install datasets\r\nfrom datasets import load_dataset\r\n\r\nrte_data = load_dataset('glue', 'rte')\r\ncola_data = load_dataset('glue', 'cola')\r\nprint(rte_data['test'][0:30]['label'])\r\nprint(cola_data['test'][0:30]['label'])\r\n\r\nOutput:\r\n[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\r\n[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\r\n\r\nThe non-label test data seems to be fine:\r\ne.g. rte_data['test'][1] is:\r\n{'sentence1': \"Authorities in Brazil say that more than 200 people are being held hostage in a prison in the country's remote, Amazonian-jungle state of Rondonia.\",\r\n 'sentence2': 'Authorities in Brazil hold 200 people as hostage.',\r\n 'label': -1,\r\n 'idx': 1}\r\nTraining and validation data are also fine:\r\ne.g. rte_data['train][0] is:\r\n{'sentence1': 'No Weapons of Mass Destruction Found in Iraq Yet.',\r\n 'sentence2': 'Weapons of Mass Destruction Found in Iraq.',\r\n 'label': 1,\r\n 'idx': 0}\r\n\r\n\r\n### Expected behavior\r\n\r\nExpected the labels to be binary 0\/1 values; Got all -1s instead\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6506\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6505","id":2044721288,"node_id":"I_kwDODunzps553_iI","number":6505,"title":"Got stuck when I trying to load a dataset","user":{"login":"yirenpingsheng","id":18232551,"node_id":"MDQ6VXNlcjE4MjMyNTUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/18232551?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yirenpingsheng","html_url":"https:\/\/github.com\/yirenpingsheng","followers_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/followers","following_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/orgs","repos_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/repos","events_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yirenpingsheng\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-16T11:51:07Z","updated_at":"2024-01-06T15:28:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHello, everyone. I met a problem when I am trying to load a data file using load_dataset method on a Debian 10 system. The data file is not very large, only 1.63MB with 600 records. \r\nHere is my code:\r\n\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('json', data_files='mypath\/oaast_rm_zh.json')\r\n\r\nI waited it for 20 minutes. It still no response. I cannot using Ctrl+C to cancel the command. I have to use Ctrl+Z to kill it. I also try it with a txt file, it still no response in a long time. \r\n\r\nI can load the same file successfully using my laptop (windows 10, python 3.8.5, datasets==2.14.5). I can also make it on another computer (Ubuntu 20.04.5 LTS, python 3.10.13, datasets 2.14.7). It only takes me 1-2 miniutes.\r\n\r\nCould you give me some suggestions? Thank you.\r\n\r\n\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('json', data_files='mypath\/oaast_rm_zh.json')\n\n### Expected behavior\n\nI hope it can load the file successfully.\n\n### Environment info\n\nOS: Debian GNU\/Linux 10\r\nPython: Python 3.10.13\r\nPip list:\r\nPackage                   Version\r\n------------------------- ------------\r\naccelerate                0.25.0\r\naddict                    2.4.0\r\naiofiles                  23.2.1\r\naiohttp                   3.9.1\r\naiosignal                 1.3.1\r\naliyun-python-sdk-core    2.14.0\r\naliyun-python-sdk-kms     2.16.2\r\naltair                    5.2.0\r\nannotated-types           0.6.0\r\nanyio                     3.7.1\r\nasync-timeout             4.0.3\r\nattrs                     23.1.0\r\ncertifi                   2023.11.17\r\ncffi                      1.16.0\r\ncharset-normalizer        3.3.2\r\nclick                     8.1.7\r\ncontourpy                 1.2.0\r\ncrcmod                    1.7\r\ncryptography              41.0.7\r\ncycler                    0.12.1\r\ndatasets                  2.14.7\r\ndill                      0.3.7\r\ndocstring-parser          0.15\r\neinops                    0.7.0\r\nexceptiongroup            1.2.0\r\nfastapi                   0.105.0\r\nffmpy                     0.3.1\r\nfilelock                  3.13.1\r\nfonttools                 4.46.0\r\nfrozenlist                1.4.1\r\nfsspec                    2023.10.0\r\ngast                      0.5.4\r\ngradio                    3.50.2\r\ngradio_client             0.6.1\r\nh11                       0.14.0\r\nhttpcore                  1.0.2\r\nhttpx                     0.25.2\r\nhuggingface-hub           0.19.4\r\nidna                      3.6\r\nimportlib-metadata        7.0.0\r\nimportlib-resources       6.1.1\r\njieba                     0.42.1\r\nJinja2                    3.1.2\r\njmespath                  0.10.0\r\njoblib                    1.3.2\r\njsonschema                4.20.0\r\njsonschema-specifications 2023.11.2\r\nkiwisolver                1.4.5\r\nmarkdown-it-py            3.0.0\r\nMarkupSafe                2.1.3\r\nmatplotlib                3.8.2\r\nmdurl                     0.1.2\r\nmodelscope                1.10.0\r\nmpmath                    1.3.0\r\nmultidict                 6.0.4\r\nmultiprocess              0.70.15\r\nnetworkx                  3.2.1\r\nnltk                      3.8.1\r\nnumpy                     1.26.2\r\nnvidia-cublas-cu12        12.1.3.1\r\nnvidia-cuda-cupti-cu12    12.1.105\r\nnvidia-cuda-nvrtc-cu12    12.1.105\r\nnvidia-cuda-runtime-cu12  12.1.105\r\nnvidia-cudnn-cu12         8.9.2.26\r\nnvidia-cufft-cu12         11.0.2.54\r\nnvidia-curand-cu12        10.3.2.106\r\nnvidia-cusolver-cu12      11.4.5.107\r\nnvidia-cusparse-cu12      12.1.0.106\r\nnvidia-nccl-cu12          2.18.1\r\nnvidia-nvjitlink-cu12     12.3.101\r\nnvidia-nvtx-cu12          12.1.105\r\norjson                    3.9.10\r\noss2                      2.18.3\r\npackaging                 23.2\r\npandas                    2.1.4\r\npeft                      0.7.1\r\nPillow                    10.1.0\r\npip                       23.3.1\r\nplatformdirs              4.1.0\r\nprotobuf                  4.25.1\r\npsutil                    5.9.6\r\npyarrow                   14.0.1\r\npyarrow-hotfix            0.6\r\npycparser                 2.21\r\npycryptodome              3.19.0\r\npydantic                  2.5.2\r\npydantic_core             2.14.5\r\npydub                     0.25.1\r\nPygments                  2.17.2\r\npyparsing                 3.1.1\r\npython-dateutil           2.8.2\r\npython-multipart          0.0.6\r\npytz                      2023.3.post1\r\nPyYAML                    6.0.1\r\nreferencing               0.32.0\r\nregex                     2023.10.3\r\nrequests                  2.31.0\r\nrich                      13.7.0\r\nrouge-chinese             1.0.3\r\nrpds-py                   0.13.2\r\nsafetensors               0.4.1\r\nscipy                     1.11.4\r\nsemantic-version          2.10.0\r\nsentencepiece             0.1.99\r\nsetuptools                68.2.2\r\nshtab                     1.6.5\r\nsimplejson                3.19.2\r\nsix                       1.16.0\r\nsniffio                   1.3.0\r\nsortedcontainers          2.4.0\r\nsse-starlette             1.8.2\r\nstarlette                 0.27.0\r\nsympy                     1.12\r\ntiktoken                  0.5.2\r\ntokenizers                0.15.0\r\ntomli                     2.0.1\r\ntoolz                     0.12.0\r\ntorch                     2.1.2\r\ntqdm                      4.66.1\r\ntransformers              4.36.1\r\ntriton                    2.1.0\r\ntrl                       0.7.4\r\ntyping_extensions         4.9.0\r\ntyro                      0.6.0\r\ntzdata                    2023.3\r\nurllib3                   2.1.0\r\nuvicorn                   0.24.0.post1\r\nwebsockets                11.0.3\r\nwheel                     0.41.2\r\nxxhash                    3.4.1\r\nyapf                      0.40.2\r\nyarl                      1.9.4\r\nzipp                      3.17.0\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6505\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6504","id":2044541154,"node_id":"I_kwDODunzps553Tji","number":6504,"title":"Error Pushing to Hub","user":{"login":"Jiayi-Pan","id":55055083,"node_id":"MDQ6VXNlcjU1MDU1MDgz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/55055083?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Jiayi-Pan","html_url":"https:\/\/github.com\/Jiayi-Pan","followers_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/followers","following_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/orgs","repos_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/repos","events_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Jiayi-Pan\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-16T01:05:22Z","updated_at":"2023-12-16T06:20:53Z","closed_at":"2023-12-16T06:20:53Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nError when trying to push a dataset in a special format to hub\n\n### Steps to reproduce the bug\n\n```\r\nimport datasets\r\nfrom datasets import Dataset\r\ndataset_dict = {\r\n    \"filename\": [\"apple\", \"banana\"],\r\n    \"token\": [[[1,2],[3,4]],[[1,2],[3,4]]],\r\n    \"label\": [0, 1],\r\n}\r\ndataset = Dataset.from_dict(dataset_dict)\r\ndataset = dataset.cast_column(\"token\", datasets.features.features.Array2D(shape=(2, 2),dtype=\"int16\"))\r\n\r\ndataset.push_to_hub(\"SequenceModel\/imagenet_val_256\")\r\n```\r\nError:\r\n```\r\n...\r\n\r\nConstructorError: could not determine a constructor for the tag 'tag:yaml.org,2002:python\/tuple'\r\n  in \"<unicode string>\", line 8, column 16:\r\n            shape: !!python\/tuple\r\n                   ^\r\n```\n\n### Expected behavior\n\nDataset being pushed to hub\n\n### Environment info\n\n- `datasets` version: 2.15.0\r\n- Platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.35\r\n- Python version: 3.11.5\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6504\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6503","id":2043847591,"node_id":"PR_kwDODunzps5iHgZf","number":6503,"title":"Fix streaming xnli","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-15T14:40:57Z","updated_at":"2023-12-15T14:51:06Z","closed_at":"2023-12-15T14:44:47Z","author_association":"MEMBER","active_lock_reason":null,"body":"This code was failing\r\n\r\n```python\r\n\r\nIn [1]: from datasets import load_dataset\r\n\r\nIn [2]: \r\n   ...:         ds = load_dataset(\"xnli\", \"all_languages\", split=\"test\", streaming=True)\r\n   ...: \r\n   ...:         sample_data = next(iter(ds))[\"premise\"]  # pick up one data\r\n   ...:         input_text = list(sample_data.values())\r\n```\r\n\r\n```\r\nFile ~\/hf\/datasets\/src\/datasets\/features\/translation.py:104, in TranslationVariableLanguages.encode_example(self, translation_dict)\r\n    102     return translation_dict\r\n    103 elif self.languages and set(translation_dict) - lang_set:\r\n--> 104     raise ValueError(\r\n    105         f'Some languages in example ({\", \".join(sorted(set(translation_dict) - lang_set))}) are not in valid set ({\", \".join(lang_set)}).'\r\n    106     )\r\n    108 # Convert dictionary into tuples, splitting out cases where there are\r\n    109 # multiple translations for a single language.\r\n    110 translation_tuples = []\r\n\r\nValueError: Some languages in example (language, translation) are not in valid set (ur, fr, hi, sw, vi, el, de, th, en, tr, zh, ar, bg, ru, es).\r\n```\r\n\r\nbecause in streaming mode we expect features encode methods to be no-ops if the example is already encoded.\r\n\r\nI fixed `TranslationVariableLanguages` to account for that","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6503\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6503","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6503","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6503.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6503.patch","merged_at":"2023-12-15T14:44:46Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6502","id":2043771731,"node_id":"PR_kwDODunzps5iHPt-","number":6502,"title":"Pickle support for `torch.Generator` objects","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-15T13:55:12Z","updated_at":"2023-12-15T15:04:33Z","closed_at":"2023-12-15T14:58:22Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix for https:\/\/discuss.huggingface.co\/t\/caching-a-dataset-processed-with-randomness\/65616","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6502\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6502","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6502","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6502.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6502.patch","merged_at":"2023-12-15T14:58:22Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6501","id":2043377240,"node_id":"I_kwDODunzps55y3ZY","number":6501,"title":" OverflowError: value too large to convert to int32_t ","user":{"login":"zhangfan-algo","id":47747764,"node_id":"MDQ6VXNlcjQ3NzQ3NzY0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47747764?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/zhangfan-algo","html_url":"https:\/\/github.com\/zhangfan-algo","followers_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/followers","following_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/orgs","repos_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/repos","events_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/zhangfan-algo\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-15T10:10:21Z","updated_at":"2023-12-15T10:10:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/47747764\/f58044fb-ddda-48b6-ba68-7bbfef781630)\r\n\n\n### Steps to reproduce the bug\n\njust loading datasets \n\n### Expected behavior\n\nhow can I fix it\n\n### Environment info\n\npip install \/mnt\/cluster\/zhangfan\/study_info\/LLaMA-Factory\/peft-0.6.0-py3-none-any.whl\r\npip install huggingface_hub-0.19.4-py3-none-any.whl tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl transformers-4.36.1-py3-none-any.whl pyarrow_hotfix-0.6-py3-none-any.whl datasets-2.15.0-py3-none-any.whl tyro-0.5.18-py3-none-any.whl trl-0.7.4-py3-none-any.whl\r\n\r\ndone","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6501\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6500","id":2043258633,"node_id":"PR_kwDODunzps5iFc6e","number":6500,"title":"Enable setting config as default when push_to_hub","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-12-15T09:17:41Z","updated_at":"2023-12-18T11:56:11Z","closed_at":"2023-12-18T11:50:03Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix #6497.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6500\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6500","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6500","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6500.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6500.patch","merged_at":"2023-12-18T11:50:03Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6499","id":2043166976,"node_id":"PR_kwDODunzps5iFIUF","number":6499,"title":"docs: add reference Git over SSH","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-15T08:38:31Z","updated_at":"2023-12-15T11:48:47Z","closed_at":"2023-12-15T11:42:38Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"see https:\/\/discuss.huggingface.co\/t\/update-datasets-getting-started-to-new-git-security\/65893","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6499\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6499","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6499","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6499.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6499.patch","merged_at":"2023-12-15T11:42:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6498","id":2042075969,"node_id":"PR_kwDODunzps5iBcFj","number":6498,"title":"Fallback on dataset script if user wants to load default config","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-12-14T16:46:01Z","updated_at":"2023-12-15T13:16:56Z","closed_at":"2023-12-15T13:10:48Z","author_association":"MEMBER","active_lock_reason":null,"body":"Right now this code is failing on `main`:\r\n\r\n```python\r\nload_dataset(\"openbookqa\")\r\n```\r\n\r\nThis is because it tries to load the dataset from the Parquet export but the dataset has multiple configurations and the Parquet export doesn't know which one is the default one.\r\n\r\nI fixed this by simply falling back on using the dataset script (which tells the user to pass `trust_remote_code=True`):\r\n\r\n```python\r\nload_dataset(\"openbookqa\", trust_remote_code=True)\r\n```\r\n\r\nNote that if the user happened to specify a config name I don't fall back on the script since we can use the Parquet export in this case (no need to know which config is the default)\r\n\r\n\r\n```python\r\nload_dataset(\"openbookqa\", \"main\")\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6498\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6498","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6498","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6498.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6498.patch","merged_at":"2023-12-15T13:10:48Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6497","id":2041994274,"node_id":"I_kwDODunzps55tlwi","number":6497,"title":"Support setting a default config name in push_to_hub","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-12-14T15:59:03Z","updated_at":"2023-12-18T11:50:04Z","closed_at":"2023-12-18T11:50:04Z","author_association":"MEMBER","active_lock_reason":null,"body":"In order to convert script-datasets to no-script datasets, we need to support setting a default config name for those scripts that set one.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6497\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6496","id":2041589386,"node_id":"I_kwDODunzps55sC6K","number":6496,"title":"Error when writing a dataset to HF Hub: A commit has happened since. Please refresh and try again.","user":{"login":"GeorgesLorre","id":35808396,"node_id":"MDQ6VXNlcjM1ODA4Mzk2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35808396?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/GeorgesLorre","html_url":"https:\/\/github.com\/GeorgesLorre","followers_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/followers","following_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/orgs","repos_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/repos","events_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/GeorgesLorre\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-14T11:24:54Z","updated_at":"2023-12-14T12:22:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the bug**\r\n\r\nGetting a `412 Client Error: Precondition Failed` when trying to write a dataset to the HF hub.\r\n\r\n```\r\nhuggingface_hub.utils._errors.HfHubHTTPError: 412 Client Error: Precondition Failed for url: https:\/\/huggingface.co\/api\/datasets\/GLorr\/test-dask\/commit\/main (Request ID: Root=1-657ae26f-3bd92bf861bb254b2cc0826c;50a09ab7-9347-406a-ba49-69f98abee9cc)\r\n\r\nA commit has happened since. Please refresh and try again.\r\n```\r\n\r\n**Steps to reproduce the bug**\r\nThis is a minimal reproducer:\r\n```\r\nimport dask.dataframe as dd\r\nimport pandas as pd\r\nimport random\r\nimport os\r\n\r\nimport huggingface_hub\r\n\r\nimport datasets\r\n\r\nhuggingface_hub.login(token=os.getenv(\"HF_TOKEN\"))\r\n\r\ndata = {\"number\": [random.randint(0,10) for _ in range(1000)]}\r\ndf = pd.DataFrame.from_dict(data)\r\ndataframe = dd.from_pandas(df, npartitions=1)\r\ndataframe = dataframe.repartition(npartitions=3)\r\n\r\nschema = datasets.Features({\"number\": datasets.Value(\"int64\")}).arrow_schema\r\n\r\nrepo_id = \"GLorr\/test-dask\"\r\nrepo_path = f\"hf:\/\/datasets\/{repo_id}\"\r\nhuggingface_hub.create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=True)\r\ndd.to_parquet(dataframe, path=f\"{repo_path}\/data\", schema=schema)\r\n```\r\n\r\n**Expected behavior**\r\nWould expect to write to the hub without any problem.\r\n\r\n**Environment info**\r\n```\r\ndatasets==2.15.0\r\nhuggingface-hub==0.19.4\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6496\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6494","id":2039684839,"node_id":"I_kwDODunzps55kx7n","number":6494,"title":"Image Data loaded Twice","user":{"login":"baowuzhida","id":28867010,"node_id":"MDQ6VXNlcjI4ODY3MDEw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/28867010?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/baowuzhida","html_url":"https:\/\/github.com\/baowuzhida","followers_url":"https:\/\/api.github.com\/users\/baowuzhida\/followers","following_url":"https:\/\/api.github.com\/users\/baowuzhida\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/baowuzhida\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/baowuzhida\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/baowuzhida\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/baowuzhida\/orgs","repos_url":"https:\/\/api.github.com\/users\/baowuzhida\/repos","events_url":"https:\/\/api.github.com\/users\/baowuzhida\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/baowuzhida\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-13T13:11:42Z","updated_at":"2023-12-13T13:11:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n![1702472610561](https:\/\/github.com\/huggingface\/datasets\/assets\/28867010\/4b7ef5e7-32c3-4b73-84cb-5de059caa0b6)\r\nWhen I learn from https:\/\/huggingface.co\/docs\/datasets\/image_load and try to load image data from a folder. I noticed that the image was read twice in the returned data. As you can see in the attached image, there are only four images in the train folder, but reading brings up eight images\n\n### Steps to reproduce the bug\n\nfrom datasets import Dataset, load_dataset\r\n\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"data\/\", drop_labels=False)\r\n# print(dataset[\"train\"][0][\"image\"] == dataset[\"train\"][1][\"image\"])\r\n\r\nprint(dataset)\r\nprint(dataset[\"train\"][\"image\"])\r\nprint(len(dataset[\"train\"][\"image\"]))\n\n### Expected behavior\n\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 8\r\n    })\r\n})\r\n[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D1CA8B0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D2452E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D245310>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D2453A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D245460>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D245430>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D2454F0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D245550>]\r\n8\n\n### Environment info\n\n- `datasets` version: 2.14.5\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.9.17\r\n- Huggingface_hub version: 0.19.4\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6494\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6495","id":2039708529,"node_id":"I_kwDODunzps55k3tx","number":6495,"title":"Newline characters don't behave as expected when calling dataset.info","user":{"login":"gerald-wrona","id":32300890,"node_id":"MDQ6VXNlcjMyMzAwODkw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32300890?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/gerald-wrona","html_url":"https:\/\/github.com\/gerald-wrona","followers_url":"https:\/\/api.github.com\/users\/gerald-wrona\/followers","following_url":"https:\/\/api.github.com\/users\/gerald-wrona\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/gerald-wrona\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/gerald-wrona\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/gerald-wrona\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/gerald-wrona\/orgs","repos_url":"https:\/\/api.github.com\/users\/gerald-wrona\/repos","events_url":"https:\/\/api.github.com\/users\/gerald-wrona\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/gerald-wrona\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-12T23:07:51Z","updated_at":"2023-12-13T13:24:22Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### System Info\n\n- `transformers` version: 4.32.1\r\n- Platform: Windows-10-10.0.19045-SP0\r\n- Python version: 3.11.5\r\n- Huggingface_hub version: 0.15.1\r\n- Safetensors version: 0.3.2\r\n- Accelerate version: not installed\r\n- Accelerate config: not found\r\n- PyTorch version (GPU?): 2.1.1+cpu (False)\r\n- Tensorflow version (GPU?): 2.15.0 (False)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n\n### Who can help?\n\n@marios\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n[Source](https:\/\/huggingface.co\/docs\/datasets\/v2.2.1\/en\/access)\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\ndataset.info\r\n```\r\n\r\nDatasetInfo(description='GLUE, the General Language Understanding Evaluation benchmark\\n(https:\/\/gluebenchmark.com\/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems.\\n\\n', citation='@inproceedings{dolan2005automatically,\\n  title={Automatically constructing a corpus of sentential paraphrases},\\n  author={Dolan, William B and Brockett, Chris},\\n  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\\n  year={2005}\\n}\\n@inproceedings{wang2019glue,\\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\\n  note={In the Proceedings of ICLR.},\\n  year={2019}\\n}\\n', homepage='https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=52398', license='', features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='glue', dataset_name=None, config_name='mrpc', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=943843, num_examples=3668, shard_lengths=None, dataset_name='glue'), 'validation': SplitInfo(name='validation', num_bytes=105879, num_examples=408, shard_lengths=None, dataset_name='glue'), 'test': SplitInfo(name='test', num_bytes=442410, num_examples=1725, shard_lengths=None, dataset_name='glue')}, download_checksums={'https:\/\/dl.fbaipublicfiles.com\/glue\/data\/mrpc_dev_ids.tsv': {'num_bytes': 6222, 'checksum': None}, 'https:\/\/dl.fbaipublicfiles.com\/senteval\/senteval_data\/msr_paraphrase_train.txt': {'num_bytes': 1047044, 'checksum': None}, 'https:\/\/dl.fbaipublicfiles.com\/senteval\/senteval_data\/msr_paraphrase_test.txt': {'num_bytes': 441275, 'checksum': None}}, download_size=1494541, post_processing_size=None, dataset_size=1492132, size_in_bytes=2986673)\n\n### Expected behavior\n\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\ndataset.info\r\n```\r\nDatasetInfo(\r\n    description='GLUE, the General Language Understanding Evaluation benchmark\\n(https:\/\/gluebenchmark.com\/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems.\\n\\n', \r\n    citation='@inproceedings{dolan2005automatically,\\n  title={Automatically constructing a corpus of sentential paraphrases},\\n  author={Dolan, William B and Brockett, Chris},\\n  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\\n  year={2005}\\n}\\n@inproceedings{wang2019glue,\\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\\n  note={In the Proceedings of ICLR.},\\n  year={2019}\\n}\\n', homepage='https:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=52398', \r\n    license='', \r\n    features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, builder_name='glue', config_name='mrpc', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=943851, num_examples=3668, dataset_name='glue'), 'validation': SplitInfo(name='validation', num_bytes=105887, num_examples=408, dataset_name='glue'), 'test': SplitInfo(name='test', num_bytes=442418, num_examples=1725, dataset_name='glue')}, \r\n    download_checksums={'https:\/\/dl.fbaipublicfiles.com\/glue\/data\/mrpc_dev_ids.tsv': {'num_bytes': 6222, 'checksum': '971d7767d81b997fd9060ade0ec23c4fc31cbb226a55d1bd4a1bac474eb81dc7'}, 'https:\/\/dl.fbaipublicfiles.com\/senteval\/senteval_data\/msr_paraphrase_train.txt': {'num_bytes': 1047044, 'checksum': '60a9b09084528f0673eedee2b69cb941920f0b8cd0eeccefc464a98768457f89'}, 'https:\/\/dl.fbaipublicfiles.com\/senteval\/senteval_data\/msr_paraphrase_test.txt': {'num_bytes': 441275, 'checksum': 'a04e271090879aaba6423d65b94950c089298587d9c084bf9cd7439bd785f784'}}, \r\n    download_size=1494541, \r\n    post_processing_size=None, \r\n    dataset_size=1492156, \r\n    size_in_bytes=2986697\r\n)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6495\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6493","id":2038221490,"node_id":"PR_kwDODunzps5h0XJK","number":6493,"title":"Lazy data files resolution and offline cache reload","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-12-12T17:15:17Z","updated_at":"2023-12-21T15:19:20Z","closed_at":"2023-12-21T15:13:11Z","author_association":"MEMBER","active_lock_reason":null,"body":"Includes both https:\/\/github.com\/huggingface\/datasets\/pull\/6458 and https:\/\/github.com\/huggingface\/datasets\/pull\/6459\r\n\r\nThis PR should be merged instead of the two individually, since they are conflicting\r\n\r\n## Offline cache reload\r\n\r\n it can reload datasets that were pushed to hub if they exist in the cache.\r\n\r\nexample:\r\n\r\n```python\r\n>>> Dataset.from_dict({\"a\": [1, 2]}).push_to_hub(\"lhoestq\/tmp\")\r\n>>> load_dataset(\"lhoestq\/tmp\")\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['a'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\nand later, without connection:\r\n\r\n```python\r\n>>> load_dataset(\"lhoestq\/tmp\")\r\nUsing the latest cached version of the dataset since lhoestq\/tmp couldn't be found on the Hugging Face Hub\r\nFound the latest cached dataset configuration 'default' at \/Users\/quentinlhoest\/.cache\/huggingface\/datasets\/lhoestq___tmp\/default\/0.0.0\/da0e902a945afeb9 (last modified on Wed Dec 13 14:55:52 2023).\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['a'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\n- Updated `CachedDatasetModuleFactory` to look for datasets in the cache at `<namespace>___<dataset_name>\/<config_id>`\r\n- Since the metadata configs parameters are not available in offline mode, we don't know which folder to load (config_id and hash change), so I simply load the latest one\r\n  - I instantiate a BuilderConfig even if there is no metadata config with the right config_name\r\n  - Its config_id is equal to the config_name to be able to retrieve it in the cache (no more suffix for configs from metadata configs)\r\n  - We can reload this config if offline mode by specifying the right config_name (same as online !)\r\n- Consequences of this change:\r\n  - Only when there are user's parameters it creates a custom builder config with config_id = config_name + user parameters hash\r\n  - the hash used to name the cache folder takes into account the metadata config and the dataset info, so that the right cache can be reloaded when there is internet connection without redownloading the data or resolving the data files. For local directories I hash the builder configs and dataset info, and for datasets on the hub I use the commit sha as hash.\r\n  - cache directories now look like `config\/version\/commit_sha` for hub datasets which is clean :)\r\n\r\nFix https:\/\/github.com\/huggingface\/datasets\/issues\/3547\r\n\r\n## Lazy data files resolution\r\n\r\nthis makes this code run in 2sec instead of >10sec\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"glue\", \"sst2\", streaming=True, trust_remote_code=False)\r\n```\r\n\r\nFor some datasets with many configs and files it can be up to 100x faster.\r\nThis is particularly important now that some datasets will be loaded from the Parquet export instead of the scripts.\r\n\r\nThe data files are only resolved in the builder `__init__`. To do so I added DataFilesPatternsList and DataFilesPatternsDict that have `.resolve()` to return resolved DataFilesList and DataFilesDict\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6493\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6493","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6493","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6493.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6493.patch","merged_at":"2023-12-21T15:13:11Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6492","id":2037987267,"node_id":"PR_kwDODunzps5hzjhQ","number":6492,"title":"Make push_to_hub return CommitInfo","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-12T15:18:16Z","updated_at":"2023-12-13T14:29:01Z","closed_at":"2023-12-13T14:22:41Z","author_association":"MEMBER","active_lock_reason":null,"body":"Make `push_to_hub` return `CommitInfo`.\r\n\r\nThis is useful, for example, if we pass `create_pr=True` and we want to know the created PR ID.\r\n\r\nCC: @severo for the use case in https:\/\/huggingface.co\/datasets\/jmhessel\/newyorker_caption_contest\/discussions\/4","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6492\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6492","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6492","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6492.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6492.patch","merged_at":"2023-12-13T14:22:41Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6491","id":2037690643,"node_id":"PR_kwDODunzps5hyiTY","number":6491,"title":"Fix metrics dead link","user":{"login":"qgallouedec","id":45557362,"node_id":"MDQ6VXNlcjQ1NTU3MzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45557362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/qgallouedec","html_url":"https:\/\/github.com\/qgallouedec","followers_url":"https:\/\/api.github.com\/users\/qgallouedec\/followers","following_url":"https:\/\/api.github.com\/users\/qgallouedec\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/qgallouedec\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/qgallouedec\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/qgallouedec\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/qgallouedec\/orgs","repos_url":"https:\/\/api.github.com\/users\/qgallouedec\/repos","events_url":"https:\/\/api.github.com\/users\/qgallouedec\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/qgallouedec\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-12T12:51:49Z","updated_at":"2023-12-21T15:15:08Z","closed_at":"2023-12-21T15:08:53Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6491\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6491","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6491","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6491.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6491.patch","merged_at":"2023-12-21T15:08:53Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6490","id":2037204892,"node_id":"I_kwDODunzps55bUec","number":6490,"title":"`load_dataset(...,save_infos=True)` not working without loading script","user":{"login":"morganveyret","id":114978051,"node_id":"U_kgDOBtptAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/114978051?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/morganveyret","html_url":"https:\/\/github.com\/morganveyret","followers_url":"https:\/\/api.github.com\/users\/morganveyret\/followers","following_url":"https:\/\/api.github.com\/users\/morganveyret\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/morganveyret\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/morganveyret\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/morganveyret\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/morganveyret\/orgs","repos_url":"https:\/\/api.github.com\/users\/morganveyret\/repos","events_url":"https:\/\/api.github.com\/users\/morganveyret\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/morganveyret\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-12T08:09:18Z","updated_at":"2023-12-12T08:36:22Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nIt seems that saving a dataset infos back into the card file is not working for datasets without a loading script.\r\n\r\nAfter tracking the problem a bit it looks like saving the infos uses `Builder.get_imported_module_dir()` as its destination directory.\r\nInternally this is a call to `inspect.getfile()` but since the actual builder class used is dynamically created (cf. `datasets.load.configure_builder_class`) this method actually return te path to the parent builder class (e.g. `datasets.packaged_modules.json.JSON`). \r\n\r\n\n\n### Steps to reproduce the bug\n\n1. Have a local dataset without any loading script\r\n2. Make sure there are no dataset infos in the README.md\r\n3. Load with `save_infos=True`\r\n4. No change in the dataset README.md\r\n5. A new README.md file is created in the directory of the parent builder class (e.g. for json in `...\/site-packages\/datasets\/packaged_modules\/json\/README.md`)\n\n### Expected behavior\n\nThe dataset README.md should be updated and no file should be created in the python environment.\n\n### Environment info\n\n- `datasets` version: 2.15.0\r\n- Platform: Linux-6.2.0-37-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.3\r\n- `fsspec` version: 2023.6.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6490\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6489","id":2036743777,"node_id":"I_kwDODunzps55Zj5h","number":6489,"title":"load_dataset imageflder for aws s3 path ","user":{"login":"segalinc","id":9353106,"node_id":"MDQ6VXNlcjkzNTMxMDY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9353106?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/segalinc","html_url":"https:\/\/github.com\/segalinc","followers_url":"https:\/\/api.github.com\/users\/segalinc\/followers","following_url":"https:\/\/api.github.com\/users\/segalinc\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/segalinc\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/segalinc\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/segalinc\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/segalinc\/orgs","repos_url":"https:\/\/api.github.com\/users\/segalinc\/repos","events_url":"https:\/\/api.github.com\/users\/segalinc\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/segalinc\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-12T00:08:43Z","updated_at":"2023-12-12T00:09:27Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nI would like to load a dataset from S3 using the imagefolder option \r\nsomething like \r\n`dataset = datasets.load_dataset('imagefolder', data_dir='s3:\/\/...\/lsun\/train\/bedroom', fs=S3FileSystem(), streaming=True)  `\r\n\r\n### Motivation\r\n\r\nno need of data_files\r\n\r\n### Your contribution\r\n\r\nno experience with this","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6489\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6488","id":2035899898,"node_id":"I_kwDODunzps55WV36","number":6488,"title":"429 Client Error","user":{"login":"sasaadi","id":7882383,"node_id":"MDQ6VXNlcjc4ODIzODM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7882383?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sasaadi","html_url":"https:\/\/github.com\/sasaadi","followers_url":"https:\/\/api.github.com\/users\/sasaadi\/followers","following_url":"https:\/\/api.github.com\/users\/sasaadi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sasaadi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sasaadi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sasaadi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sasaadi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sasaadi\/repos","events_url":"https:\/\/api.github.com\/users\/sasaadi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sasaadi\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-11T15:06:01Z","updated_at":"2023-12-11T15:34:23Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello, I was downloading the following dataset and after 20% of data was downloaded, I started getting error 429. It is not resolved since a few days. How should I resolve it?\r\nThanks\r\n\r\nDataset:\r\nhttps:\/\/huggingface.co\/datasets\/cerebras\/SlimPajama-627B\r\n\r\nError:\r\n`requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https:\/\/huggingface.co\/datasets\/cerebras\/SlimPajama-627B\/resolve\/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543\/train\/chunk1\/example_train_3300.jsonl.zst`\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6488\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6487","id":2035424254,"node_id":"PR_kwDODunzps5hqyfV","number":6487,"title":"Update builder hash with info","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-11T11:09:16Z","updated_at":"2023-12-11T11:41:34Z","closed_at":"2023-12-11T11:41:34Z","author_association":"MEMBER","active_lock_reason":null,"body":"Currently if you change the `dataset_info` of a dataset (e.g. in the YAML part of the README.md), the cache ignores this change.\r\n\r\nThis is problematic because you want to regenerate a dataset if you change the features or the split sizes for example (e.g. after push_to_hub)\r\n\r\nIdeally we should take the resolved files into account as well but this will be for another PR","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6487\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6487","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6487","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6487.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6487.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6486","id":2035206206,"node_id":"PR_kwDODunzps5hqCSc","number":6486,"title":"Fix docs phrasing about supported formats when sharing a dataset","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-11T09:21:22Z","updated_at":"2023-12-13T14:21:29Z","closed_at":"2023-12-13T14:15:21Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix docs phrasing.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6486\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6486","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6486","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6486.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6486.patch","merged_at":"2023-12-13T14:15:21Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6485","id":2035141884,"node_id":"I_kwDODunzps55Tcz8","number":6485,"title":"FileNotFoundError: [Errno 2] No such file or directory: 'nul'","user":{"login":"amanyara","id":73683903,"node_id":"MDQ6VXNlcjczNjgzOTAz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/73683903?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/amanyara","html_url":"https:\/\/github.com\/amanyara","followers_url":"https:\/\/api.github.com\/users\/amanyara\/followers","following_url":"https:\/\/api.github.com\/users\/amanyara\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/amanyara\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/amanyara\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/amanyara\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/amanyara\/orgs","repos_url":"https:\/\/api.github.com\/users\/amanyara\/repos","events_url":"https:\/\/api.github.com\/users\/amanyara\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/amanyara\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-11T08:52:13Z","updated_at":"2023-12-14T08:09:08Z","closed_at":"2023-12-14T08:09:08Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nit seems that sth wrong with my terrible \"bug body\" life, When i run this code, \"import datasets\"\r\ni meet this error FileNotFoundError: [Errno 2] No such file or directory: 'nul'\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/73683903\/3973c120-ebb1-42b7-bede-b9de053e861d)\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/73683903\/0496adff-a7a7-4dcb-929e-ec11ede71f04)\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n1.import datasets\r\n\r\n### Expected behavior\r\n\r\ni just run a single line code and stuct in this bug \r\n\r\n### Environment info\r\n\r\nOS: Windows10\r\nDatasets==2.15.0\r\npython=3.10","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6485\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6483","id":2032946981,"node_id":"I_kwDODunzps55LE8l","number":6483,"title":"Iterable Dataset: rename column clashes with remove column","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[{"id":3287858981,"node_id":"MDU6TGFiZWwzMjg3ODU4OTgx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/streaming","name":"streaming","color":"fef2c0","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-12-08T16:11:30Z","updated_at":"2023-12-08T16:27:16Z","closed_at":"2023-12-08T16:27:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nSuppose I have a two iterable datasets, one with the features: \r\n* `{\"audio\", \"text\", \"column_a\"}`\r\n\r\nAnd the other with the features:\r\n* `{\"audio\", \"sentence\", \"column_b\"}`\r\n\r\nI want to combine both datasets using `interleave_datasets`, which requires me to unify the column names. I would typically do this by:\r\n1. Renaming the common columns to the same name (e.g. `\"text\"` -> `\"sentence\"`)\r\n2. Removing the unwanted columns (e.g. `\"column_a\"`, `\"column_b\"`)\r\n\r\nHowever, the process of renaming and removing columns in an iterable dataset doesn't work, since we need to preserve the original text column, meaning we can't combine the datasets.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# load LS in streaming mode\r\ndataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", streaming=True)\r\n\r\n# check original features\r\ndataset_features = dataset.features.keys()\r\nprint(\"Original features: \", dataset_features)\r\n\r\n#\u00a0rename \"text\" -> \"sentence\"\r\ndataset = dataset.rename_column(\"text\", \"sentence\")\r\n\r\n# remove unwanted columns\r\nCOLUMNS_TO_KEEP = {\"audio\", \"sentence\"}\r\ndataset = dataset.remove_columns(set(dataset_features - COLUMNS_TO_KEEP))\r\n\r\n# stream first sample, should return \"audio\" and \"sentence\" columns\r\nprint(next(iter(dataset)))\r\n```\r\nTraceback:\r\n```python\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\nCell In[5], line 17\r\n     14 COLUMNS_TO_KEEP = {\"audio\", \"sentence\"}\r\n     15 dataset = dataset.remove_columns(set(dataset_features - COLUMNS_TO_KEEP))\r\n---> 17 print(next(iter(dataset)))\r\n\r\nFile ~\/datasets\/src\/datasets\/iterable_dataset.py:1353, in IterableDataset.__iter__(self)\r\n   1350         yield formatter.format_row(pa_table)\r\n   1351     return\r\n-> 1353 for key, example in ex_iterable:\r\n   1354     if self.features:\r\n   1355         # `IterableDataset` automatically fills missing columns with None.\r\n   1356         # This is done with `_apply_feature_types_on_example`.\r\n   1357         example = _apply_feature_types_on_example(\r\n   1358             example, self.features, token_per_repo_id=self._token_per_repo_id\r\n   1359         )\r\n\r\nFile ~\/datasets\/src\/datasets\/iterable_dataset.py:652, in MappedExamplesIterable.__iter__(self)\r\n    650     yield from ArrowExamplesIterable(self._iter_arrow, {})\r\n    651 else:\r\n--> 652     yield from self._iter()\r\n\r\nFile ~\/datasets\/src\/datasets\/iterable_dataset.py:729, in MappedExamplesIterable._iter(self)\r\n    727 if self.remove_columns:\r\n    728     for c in self.remove_columns:\r\n--> 729         del transformed_example[c]\r\n    730 yield key, transformed_example\r\n    731 current_idx += 1\r\n\r\nKeyError: 'text'\r\n```\r\n=> we see that `datasets` is looking for the column \"text\", even though we've renamed this to \"sentence\" and then removed the un-wanted \"text\" column from our dataset.\r\n\r\n### Expected behavior\r\n\r\nShould be able to rename and remove columns from iterable dataset.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.1.dev0\r\n- Platform: macOS-13.5.1-arm64-arm-64bit\r\n- Python version: 3.11.6\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.2\r\n- `fsspec` version: 2023.9.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6483\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6484","id":2033333294,"node_id":"I_kwDODunzps55MjQu","number":6484,"title":"[Feature Request] Dataset versioning","user":{"login":"kenfus","id":47979198,"node_id":"MDQ6VXNlcjQ3OTc5MTk4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47979198?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kenfus","html_url":"https:\/\/github.com\/kenfus","followers_url":"https:\/\/api.github.com\/users\/kenfus\/followers","following_url":"https:\/\/api.github.com\/users\/kenfus\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kenfus\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kenfus\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kenfus\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kenfus\/orgs","repos_url":"https:\/\/api.github.com\/users\/kenfus\/repos","events_url":"https:\/\/api.github.com\/users\/kenfus\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kenfus\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-08T16:01:35Z","updated_at":"2023-12-11T19:13:46Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nI am working on a project, where I would like to test different preprocessing methods for my ML-data. Thus, I would like to work a lot with revisions and compare them. Currently, I was not able to make it work with the revision keyword because it was not redownloading the data, it was reading in some cached data, until I put  `download_mode=\"force_redownload\"`, even though the reversion was different. \r\nOf course, I may have done something wrong or missed a setting somewhere! \r\n\r\n**Describe the solution you'd like**\r\nThe solution would allow me to easily work with revisions: \r\n- create a new dataset (by combining things, different preprocessing, ..) and give it a new revision (v.1.2.3), maybe like this:\r\n`dataset_audio.push_to_hub('kenfus\/xy', revision='v1.0.2')`\r\n\r\n- then, get the current revision as follows:         \r\n```\r\ndataset = load_dataset(\r\n            'kenfus\/xy', revision='v1.0.2',\r\n        )\r\n```\r\nthis downloads the new version and does not load in a different revision and all future map, filter, .. operations are done on this dataset and not loaded from cache produced from a different revision. \r\n- if I rerun the run, the caching should be smart enough in every step to not reuse a mapping operation on a different revision. \r\n\r\n**Describe alternatives you've considered**\r\nI created my own caching, putting `download_mode=\"force_redownload\"` and `load_from_cache_file=False,` everywhere.\r\n\r\n**Additional context**\r\nThanks a lot for your great work! Creating NLP datasets and training a model with them is really easy and straightforward with huggingface.\r\n\r\nThis is the data loading in my script:\r\n\r\n```\r\n    ## CREATE PATHS\r\n    prepared_dataset_path = os.path.join(\r\n        DATA_FOLDER, str(DATA_VERSION), \"prepared_dataset\"\r\n    )\r\n    os.makedirs(os.path.join(DATA_FOLDER, str(DATA_VERSION)), exist_ok=True)\r\n\r\n    ## LOAD DATASET\r\n    if os.path.exists(prepared_dataset_path):\r\n        print(\"Loading prepared dataset from disk...\")\r\n        dataset_prepared = load_from_disk(prepared_dataset_path)\r\n    else:\r\n        print(\"Loading dataset from HuggingFace Datasets...\")\r\n        dataset = load_dataset(\r\n            PATH_TO_DATASET, revision=DATA_VERSION, download_mode=\"force_redownload\"\r\n        )\r\n\r\n        print(\"Preparing dataset...\")\r\n        dataset_prepared = dataset.map(\r\n            prepare_dataset,\r\n            remove_columns=[\"audio\", \"transcription\"],\r\n            num_proc=os.cpu_count(),\r\n            load_from_cache_file=False,\r\n        )\r\n        dataset_prepared.save_to_disk(prepared_dataset_path)\r\n        del dataset\r\n\r\n    if CHECK_DATASET:\r\n        ## CHECK DATASET\r\n        dataset_prepared = dataset_prepared.map(\r\n            check_dimensions, num_proc=os.cpu_count(), load_from_cache_file=False\r\n        )\r\n        dataset_filtered = dataset_prepared.filter(\r\n            lambda example: not example[\"incorrect_dimension\"],\r\n            load_from_cache_file=False,\r\n        )\r\n\r\n        for example in dataset_prepared.filter(\r\n            lambda example: example[\"incorrect_dimension\"], load_from_cache_file=False\r\n        ):\r\n            print(example[\"path\"])\r\n\r\n        print(\r\n            f\"Number of examples with incorrect dimension:  {len(dataset_prepared) - len(dataset_filtered)}\"\r\n        )\r\n\r\n        print(\"Number of examples train: \", len(dataset_filtered[\"train\"]))\r\n        print(\"Number of examples test: \", len(dataset_filtered[\"test\"]))\r\n\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6484\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6482","id":2032675918,"node_id":"PR_kwDODunzps5hhl23","number":6482,"title":"Fix max lock length on unix","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-08T13:39:30Z","updated_at":"2023-12-12T11:53:32Z","closed_at":"2023-12-12T11:47:27Z","author_association":"MEMBER","active_lock_reason":null,"body":"reported in https:\/\/github.com\/huggingface\/datasets\/pull\/6482","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6482\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6482","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6482","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6482.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6482.patch","merged_at":"2023-12-12T11:47:27Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6481","id":2032650003,"node_id":"I_kwDODunzps55J8cT","number":6481,"title":"using torchrun, save_to_disk suddenly shows SIGTERM","user":{"login":"Ariya12138","id":85916625,"node_id":"MDQ6VXNlcjg1OTE2NjI1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/85916625?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Ariya12138","html_url":"https:\/\/github.com\/Ariya12138","followers_url":"https:\/\/api.github.com\/users\/Ariya12138\/followers","following_url":"https:\/\/api.github.com\/users\/Ariya12138\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Ariya12138\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Ariya12138\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Ariya12138\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Ariya12138\/orgs","repos_url":"https:\/\/api.github.com\/users\/Ariya12138\/repos","events_url":"https:\/\/api.github.com\/users\/Ariya12138\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Ariya12138\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-08T13:22:03Z","updated_at":"2023-12-08T13:22:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I run my code using the \"torchrun\" command, when the code reaches the \"save_to_disk\" part, suddenly I get the following warning and error messages:\r\nBecause the dataset is too large, the \"save_to_disk\" function splits it into 70 parts for saving. However, an error occurs suddenly when it reaches the 14th shard.\r\nWARNING: torch.distributed.elastic.multiprocessing.api: Sending process 2224968 closing signal SIGTERM\r\nERROR: torch.distributed.elastic.multiprocessing.api: failed (exitcode: -7). traceback: Signal 7 (SIGBUS) received by PID 2224967.\n\n### Steps to reproduce the bug\n\nds_shard = ds_shard.map(map_fn, *args, **kwargs)\r\nds_shard.save_to_disk(ds_shard_filepaths[rank])\r\n\r\nSaving the dataset (14\/70 shards):  20%|\u2588\u2588        | 875350\/4376702 [00:19<01:53, 30863.15 examples\/s]\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2224968 closing signal SIGTERM\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -7) local_rank: 0 (pid: 2224967) of binary: \/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/bin\/python\r\nTraceback (most recent call last):\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/bin\/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/lib\/python3.10\/site-packages\/torch\/distributed\/elastic\/multiprocessing\/errors\/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/lib\/python3.10\/site-packages\/torch\/distributed\/run.py\", line 794, in main\r\n    run(args)\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/lib\/python3.10\/site-packages\/torch\/distributed\/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/lib\/python3.10\/site-packages\/torch\/distributed\/launcher\/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"\/home\/bingxing2\/home\/scx6964\/.conda\/envs\/ariya235\/lib\/python3.10\/site-packages\/torch\/distributed\/launcher\/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n==========================================================\r\nrun.py FAILED\r\n----------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n----------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-12-08_20:09:04\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : -7 (pid: 2224967)\r\n  error_file: <N\/A>\r\n  traceback : Signal 7 (SIGBUS) received by PID 2224967\r\n\r\n\n\n### Expected behavior\n\nI hope it can save successfully without any issues, but it seems there is a problem.\n\n### Environment info\n\n`datasets` version: 2.14.6\r\n- Platform: Linux-4.19.90-24.4.v2101.ky10.aarch64-aarch64-with-glibc2.28\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 14.0.0\r\n- Pandas version: 2.1.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6481\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6480","id":2031116653,"node_id":"PR_kwDODunzps5hcS7P","number":6480,"title":"Add IterableDataset `__repr__`","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-07T16:31:50Z","updated_at":"2023-12-08T13:33:06Z","closed_at":"2023-12-08T13:26:54Z","author_association":"MEMBER","active_lock_reason":null,"body":"Example for glue sst2:\r\n\r\nDataset\r\n\r\n```\r\nDatasetDict({\r\n    test: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 1821\r\n    })\r\n    train: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 67349\r\n    })\r\n    validation: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 872\r\n    })\r\n})\r\n```\r\n\r\nIterableDataset (new)\r\n\r\n```\r\nIterableDatasetDict({\r\n    test: IterableDataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        n_shards: 1\r\n    })\r\n    train: IterableDataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        n_shards: 1\r\n    })\r\n    validation: IterableDataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        n_shards: 1\r\n    })\r\n})\r\n```\r\n\r\nIterableDataset (before)\r\n\r\n```\r\n{'test': <datasets.iterable_dataset.IterableDataset object at 0x130d421f0>, 'train': <datasets.iterable_dataset.IterableDataset object at 0x136f3aaf0>, 'validation': <datasets.iterable_dataset.IterableDataset object at 0x136f4b100>}\r\n{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6480\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6480","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6480","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6480.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6480.patch","merged_at":"2023-12-08T13:26:54Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6479","id":2029040121,"node_id":"PR_kwDODunzps5hVLom","number":6479,"title":"More robust preupload retry mechanism","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-06T17:19:38Z","updated_at":"2023-12-06T19:47:29Z","closed_at":"2023-12-06T19:41:06Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6479\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6479","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6479","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6479.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6479.patch","merged_at":"2023-12-06T19:41:06Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6478","id":2028071596,"node_id":"I_kwDODunzps544eqs","number":6478,"title":"How to load data from lakefs","user":{"login":"d710055071","id":12895488,"node_id":"MDQ6VXNlcjEyODk1NDg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12895488?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/d710055071","html_url":"https:\/\/github.com\/d710055071","followers_url":"https:\/\/api.github.com\/users\/d710055071\/followers","following_url":"https:\/\/api.github.com\/users\/d710055071\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/d710055071\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/d710055071\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/d710055071\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/d710055071\/orgs","repos_url":"https:\/\/api.github.com\/users\/d710055071\/repos","events_url":"https:\/\/api.github.com\/users\/d710055071\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/d710055071\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-06T09:04:11Z","updated_at":"2023-12-07T02:19:44Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"My dataset is stored on the company's lakefs server. How can I write code to load the dataset? It would be great if I could provide code examples or provide some references\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6478\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6477","id":2028022374,"node_id":"PR_kwDODunzps5hRq_N","number":6477,"title":"Fix PermissionError on Windows CI","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-06T08:34:53Z","updated_at":"2023-12-06T09:24:11Z","closed_at":"2023-12-06T09:17:52Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix #6476.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6477\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6477","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6477","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6477.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6477.patch","merged_at":"2023-12-06T09:17:52Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6476","id":2028018596,"node_id":"I_kwDODunzps544Ruk","number":6476,"title":"CI on windows is broken: PermissionError","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-12-06T08:32:53Z","updated_at":"2023-12-06T09:17:53Z","closed_at":"2023-12-06T09:17:53Z","author_association":"MEMBER","active_lock_reason":null,"body":"See: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/7104781624\/job\/19340572394\r\n```\r\nFAILED tests\/test_load.py::test_loading_from_the_datasets_hub - NotADirectoryError: [WinError 267] The directory name is invalid: 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfcnps56i\\\\hf-internal-testing___dataset_with_script\\\\default\\\\0.0.0\\\\c240e2be3370bdbd\\\\dataset_with_script-train.arrow'\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6476\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6475","id":2027373734,"node_id":"I_kwDODunzps5410Sm","number":6475,"title":"laion2B-en failed to load on Windows with PrefetchVirtualMemory failed","user":{"login":"doctorpangloss","id":2229300,"node_id":"MDQ6VXNlcjIyMjkzMDA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2229300?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/doctorpangloss","html_url":"https:\/\/github.com\/doctorpangloss","followers_url":"https:\/\/api.github.com\/users\/doctorpangloss\/followers","following_url":"https:\/\/api.github.com\/users\/doctorpangloss\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/doctorpangloss\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/doctorpangloss\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/doctorpangloss\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/doctorpangloss\/orgs","repos_url":"https:\/\/api.github.com\/users\/doctorpangloss\/repos","events_url":"https:\/\/api.github.com\/users\/doctorpangloss\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/doctorpangloss\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-12-06T00:07:34Z","updated_at":"2023-12-06T23:26:23Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI have downloaded laion2B-en, and I'm receiving the following error trying to load it:\r\n```\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128\/128 [00:00<00:00, 1173.79it\/s]\r\nTraceback (most recent call last):\r\n  File \"D:\\Art-Workspace\\src\\artworkspace\\tokeneval\\compute_frequencies.py\", line 31, in <module>\r\n    count = compute_frequencies()\r\n            ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Art-Workspace\\src\\artworkspace\\tokeneval\\compute_frequencies.py\", line 17, in compute_frequencies\r\n    laion2b_dataset = load_dataset(\"laion\/laion2B-en\", split=\"train\", cache_dir=_CACHE_DIR, keep_in_memory=False)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\load.py\", line 2165, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\builder.py\", line 1187, in as_dataset\r\n    datasets = map_nested(\r\n               ^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\utils\\py_utils.py\", line 456, in map_nested\r\n    return function(data_struct)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\builder.py\", line 1217, in _build_single_dataset\r\n    ds = self._as_dataset(\r\n         ^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\builder.py\", line 1291, in _as_dataset\r\n    dataset_kwargs = ArrowReader(cache_dir, self.info).read(\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 244, in read\r\n    return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 265, in read_files\r\n    pa_table = self._read_files(files, in_memory=in_memory)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 200, in _read_files\r\n    pa_table: Table = self._get_table_from_filename(f_dict, in_memory=in_memory)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 336, in _get_table_from_filename\r\n    table = ArrowReader.read_table(filename, in_memory=in_memory)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 357, in read_table\r\n    return table_cls.from_file(filename)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\table.py\", line 1059, in from_file\r\n    table = _memory_mapped_arrow_table_from_file(filename)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\table.py\", line 66, in _memory_mapped_arrow_table_from_file\r\n    pa_table = opened_stream.read_all()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow\\ipc.pxi\", line 757, in pyarrow.lib.RecordBatchReader.read_all\r\n  File \"pyarrow\\error.pxi\", line 91, in pyarrow.lib.check_status\r\nOSError: [WinError 8] PrefetchVirtualMemory failed. Detail: [Windows error 8] Not enough memory resources are available to process this command.\r\n```\r\n\r\nThis error is probably a red herring: https:\/\/stackoverflow.com\/questions\/50263929\/numpy-memmap-returns-not-enough-memory-while-there-are-plenty-available In other words, the issue is related to asking for a memory mapping of length N > M the length of the file on Windows. This gracefully succeeds on Linux.\r\n\r\nI have 1024 arrow files in my cache instead of 128 like in the repository for it. Probably related. I don't know why `datasets` reorganized\/rewrote the dataset in my cache to be 1024 slices instead of the original 128.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\n# as a huggingface developer, you may already have laion2B-en somewhere\r\n_CACHE_DIR = \".\"\r\n\r\nfrom datasets import load_dataset\r\nload_dataset(\"laion\/laion2B-en\", split=\"train\", cache_dir=_CACHE_DIR, keep_in_memory=False)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThis should correctly load as a memory mapped Arrow dataset.\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: Windows-10-10.0.20348-SP0 (this is windows 2022)\r\n- Python version: 3.11.4\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.2\r\n- `fsspec` version: 2023.10.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6475\/timeline","performed_via_github_app":null,"state_reason":"reopened","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6474","id":2027006715,"node_id":"PR_kwDODunzps5hONZc","number":6474,"title":"Deprecate Beam API and download from HF GCS bucket","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-05T19:51:33Z","updated_at":"2023-12-10T17:55:50Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Deprecate the Beam API and download from the HF GCS bucked. \r\n\r\nTODO:\r\n- [ ] Deprecate the Beam-based [`wikipedia`](https:\/\/huggingface.co\/datasets\/wikipedia) in favor of [`wikimedia\/wikipedia`](https:\/\/huggingface.co\/datasets\/wikimedia\/wikipedia) ([Hub PR](https:\/\/huggingface.co\/datasets\/wikipedia\/discussions\/19))\r\n- [ ] Make [`natural_questions`](https:\/\/huggingface.co\/datasets\/natural_questions) a no-code dataset\r\n- [ ] Make [`wiki40b`](https:\/\/huggingface.co\/datasets\/wiki40b) a no-code dataset\r\n- [ ] Make [`wiki_dpr`](https:\/\/huggingface.co\/datasets\/wiki_dpr) an Arrow-based dataset","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6474\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6474","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6474","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6474.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6474.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6473","id":2026495084,"node_id":"PR_kwDODunzps5hMbvz","number":6473,"title":"Fix CI quality","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-05T15:36:23Z","updated_at":"2023-12-05T18:14:50Z","closed_at":"2023-12-05T18:08:41Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix #6472.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6473\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6473","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6473","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6473.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6473.patch","merged_at":"2023-12-05T18:08:41Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6472","id":2026493439,"node_id":"I_kwDODunzps54ydX_","number":6472,"title":"CI quality is broken","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":4296013012,"node_id":"LA_kwDODunzps8AAAABAA_01A","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/maintenance","name":"maintenance","color":"d4c5f9","default":false,"description":"Maintenance tasks"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-12-05T15:35:34Z","updated_at":"2023-12-06T08:17:34Z","closed_at":"2023-12-05T18:08:43Z","author_association":"MEMBER","active_lock_reason":null,"body":"See: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/7100835633\/job\/19327734359\r\n```\r\nWould reformat: src\/datasets\/features\/image.py\r\n1 file would be reformatted, 253 files left unchanged\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6472\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6471","id":2026100761,"node_id":"PR_kwDODunzps5hLEni","number":6471,"title":"Remove delete doc CI","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-05T12:37:50Z","updated_at":"2023-12-05T12:44:59Z","closed_at":"2023-12-05T12:38:50Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6471\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6471","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6471","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6471.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6471.patch","merged_at":"2023-12-05T12:38:50Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6470","id":2024724319,"node_id":"I_kwDODunzps54rtdf","number":6470,"title":"If an image in a dataset is corrupted, we get unescapable error","user":{"login":"chigozienri","id":14337872,"node_id":"MDQ6VXNlcjE0MzM3ODcy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14337872?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/chigozienri","html_url":"https:\/\/github.com\/chigozienri","followers_url":"https:\/\/api.github.com\/users\/chigozienri\/followers","following_url":"https:\/\/api.github.com\/users\/chigozienri\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/chigozienri\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/chigozienri\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/chigozienri\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/chigozienri\/orgs","repos_url":"https:\/\/api.github.com\/users\/chigozienri\/repos","events_url":"https:\/\/api.github.com\/users\/chigozienri\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/chigozienri\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-12-04T20:58:49Z","updated_at":"2023-12-04T20:58:49Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nExample discussed in detail here: https:\/\/huggingface.co\/datasets\/sasha\/birdsnap\/discussions\/1\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset, VerificationMode\r\ndataset = load_dataset(\r\n    'sasha\/birdsnap',\r\n    split=\"train\",\r\n    verification_mode=VerificationMode.ALL_CHECKS,\r\n    streaming=True # I recommend using streaming=True when reproducing, as this dataset is large\r\n)\r\nfor idx, row in enumerate(dataset):\r\n    # Iterating to 9287 took 7 minutes for me\r\n    # If you already have the data locally cached and set streaming=False, you see the same error just by with dataset[9287]\r\n    pass\r\n    # error at 9287 OSError: image file is truncated (45 bytes not processed)\r\n    # note that we can't avoid the error using a try\/except + continue inside the loop\r\n```\n\n### Expected behavior\n\nAble to escape errors in casting to Image() without killing the whole loop\n\n### Environment info\n\n- `datasets` version: 2.15.0\r\n- Platform: Linux-5.15.0-84-generic-x86_64-with-glibc2.31\r\n- Python version: 3.11.5\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.3\r\n- `fsspec` version: 2023.10.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6470\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6469","id":2023695839,"node_id":"PR_kwDODunzps5hC6xf","number":6469,"title":"Don't expand_info in HF glob","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-12-04T12:00:37Z","updated_at":"2023-12-15T13:18:37Z","closed_at":"2023-12-15T13:12:30Z","author_association":"MEMBER","active_lock_reason":null,"body":"Finally fix https:\/\/github.com\/huggingface\/datasets\/issues\/5537","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6469\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6469","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6469","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6469.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6469.patch","merged_at":"2023-12-15T13:12:30Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6468","id":2023617877,"node_id":"PR_kwDODunzps5hCpbN","number":6468,"title":"Use auth to get parquet export","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-04T11:18:27Z","updated_at":"2023-12-04T17:21:22Z","closed_at":"2023-12-04T17:15:11Z","author_association":"MEMBER","active_lock_reason":null,"body":"added `token` to the `_datasets_server` functions","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6468\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6468","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6468","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6468.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6468.patch","merged_at":"2023-12-04T17:15:11Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6467","id":2023174233,"node_id":"I_kwDODunzps54lzBZ","number":6467,"title":"New version release request","user":{"login":"LZHgrla","id":36994684,"node_id":"MDQ6VXNlcjM2OTk0Njg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36994684?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/LZHgrla","html_url":"https:\/\/github.com\/LZHgrla","followers_url":"https:\/\/api.github.com\/users\/LZHgrla\/followers","following_url":"https:\/\/api.github.com\/users\/LZHgrla\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/LZHgrla\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/LZHgrla\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/LZHgrla\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/LZHgrla\/orgs","repos_url":"https:\/\/api.github.com\/users\/LZHgrla\/repos","events_url":"https:\/\/api.github.com\/users\/LZHgrla\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/LZHgrla\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-04T07:08:26Z","updated_at":"2023-12-04T15:42:22Z","closed_at":"2023-12-04T15:42:22Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\r\n\r\nHi!\r\nI am using `datasets` in library `xtuner` and am highly interested in the features introduced since v2.15.0.\r\n\r\nTo avoid installation from source in our pypi wheels, we are eagerly waiting for the new release. So, Does your team have a new release plan for v2.15.1 and could you please share it with us?\r\n\r\nThanks very much!\r\n\r\n\r\n### Motivation\r\n\r\n.\r\n\r\n### Your contribution\r\n\r\n.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6467\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6466","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6466\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6466\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6466\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6466","id":2022601176,"node_id":"I_kwDODunzps54jnHY","number":6466,"title":"Can't align optional features of struct","user":{"login":"Dref360","id":8976546,"node_id":"MDQ6VXNlcjg5NzY1NDY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8976546?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Dref360","html_url":"https:\/\/github.com\/Dref360","followers_url":"https:\/\/api.github.com\/users\/Dref360\/followers","following_url":"https:\/\/api.github.com\/users\/Dref360\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Dref360\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Dref360\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Dref360\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Dref360\/orgs","repos_url":"https:\/\/api.github.com\/users\/Dref360\/repos","events_url":"https:\/\/api.github.com\/users\/Dref360\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Dref360\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-03T15:57:07Z","updated_at":"2023-12-11T14:38:34Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nHello!\r\n\r\nI'm currently experiencing an issue where I can't concatenate datasets if an inner field of a Feature is Optional.\r\n\r\nI have a column named `speaker`, and this holds some information about a speaker.\r\n\r\n```python\r\n@dataclass\r\nclass Speaker:\r\n   name: str\r\n   email: Optional[str]\r\n```\r\n\r\nIf I have two datasets, one happens to have `email` always None, then I get `The features can't be aligned because the key email of features`\r\n\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nYou can run the following script:\r\n\r\n```python\r\n\r\nds = Dataset.from_dict({'speaker': [{'name': 'Ben', 'email': None}]})\r\nds2 = Dataset.from_dict({'speaker': [{'name': 'Fred', 'email': 'abc@aol.com'}]})\r\nconcatenate_datasets([ds, ds2])\r\n\r\n>>>The features can't be aligned because the key speaker of features {'speaker': {'email': Value(dtype='string', id=None), 'name': Value(dtype='string', id=None)}} has unexpected type - {'email': Value(dtype='string', id=None), 'name': Value(dtype='string', id=None)} (expected either {'email': Value(dtype='null', id=None), 'name': Value(dtype='string', id=None)} or Value(\"null\").\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nI think this should work; if two top-level columns were in the same situation it would properly cast to `string`.\r\n\r\n```python\r\nds = Dataset.from_dict({'email': [None, None]})\r\nds2 = Dataset.from_dict({'email': ['abc@aol.com', 'one@yahoo.com']})\r\nconcatenate_datasets([ds, ds2])\r\n>>> # Works!\r\n```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.1.dev0\r\n- Platform: Linux-5.15.0-89-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.13\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.4.4\r\n- `fsspec` version: 2023.6.0\r\n\r\nI would be happy to fix this issue.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6466\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6466\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6465","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6465\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6465\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6465\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6465","id":2022212468,"node_id":"I_kwDODunzps54iIN0","number":6465,"title":"`load_dataset` uses out-of-date cache instead of re-downloading a changed dataset","user":{"login":"mnoukhov","id":3391297,"node_id":"MDQ6VXNlcjMzOTEyOTc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3391297?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mnoukhov","html_url":"https:\/\/github.com\/mnoukhov","followers_url":"https:\/\/api.github.com\/users\/mnoukhov\/followers","following_url":"https:\/\/api.github.com\/users\/mnoukhov\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mnoukhov\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mnoukhov\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mnoukhov\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mnoukhov\/orgs","repos_url":"https:\/\/api.github.com\/users\/mnoukhov\/repos","events_url":"https:\/\/api.github.com\/users\/mnoukhov\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mnoukhov\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-12-02T21:35:17Z","updated_at":"2023-12-04T16:13:10Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen a dataset is updated on the hub, using `load_dataset` will load the locally cached dataset instead of re-downloading the updated dataset\n\n### Steps to reproduce the bug\n\nHere is a minimal example script to\r\n1. create an initial dataset and upload\r\n2. download it so it is stored in cache\r\n3. change the dataset and re-upload\r\n4. redownload \r\n\r\n```python\r\nimport time                                                                                                                                                    \r\n                                                                                                                                                               \r\nfrom datasets import Dataset, DatasetDict, DownloadMode, load_dataset                                                                                 \r\n       \r\nusername = \"YOUR_USERNAME_HERE\"                                                                                                                                          \r\n                                                                                                                                                               \r\ninitial = Dataset.from_dict({\"foo\": [1, 2, 3]})                                                                                                                \r\nprint(f\"Intial {initial['foo']}\")                                                                                                                              \r\ninitial_ds = DatasetDict({\"train\": initial})                                                                                                                   \r\ninitial_ds.push_to_hub(\"test\")                                                                                                                                 \r\n                                                                                                                                                               \r\ntime.sleep(1)                                                                                                                                                  \r\n                                                                                                                                                               \r\ndownload = load_dataset(f\"{username}\/test\", split=\"train\")                                                                                                     \r\nchanged = download.map(lambda x: {\"foo\": x[\"foo\"] + 1})                                                                                                        \r\nprint(f\"Changed {changed['foo']}\")                                                                                                                             \r\nchanged.push_to_hub(\"test\")                                                                                                                                    \r\n                                                                                                                                                               \r\ntime.sleep(1)                                                                                                                                                  \r\n                                                                                                                                                               \r\ndownload_again = load_dataset(f\"{username}\/test\", split=\"train\")                                                                                               \r\nprint(f\"Download Changed {download_again['foo']}\")                                                                                                             \r\n# >>> gives the out-dated [1,2,3] when it should be changed [2,3,4]                                                                                                                                                                               \r\n```\r\n\r\nThe redownloaded dataset should be the changed dataset but it is actually the cached, initial dataset. Force-redownloading gives the correct dataset\r\n\r\n```python\r\ndownload_again_force = load_dataset(f\"{username}\/test\", split=\"train\", download_mode=DownloadMode.FORCE_REDOWNLOAD)                                            \r\nprint(f\"Force Download Changed {download_again_force['foo']}\")                                                                                                 \r\n# >>> [2,3,4]                                                                                                                                                                              \r\n```\r\n\r\n \n\n### Expected behavior\n\nI assumed there should be some sort of hashing that should check for changes in the dataset and re-download if the hashes don't match\n\n### Environment info\n\n- `datasets` version: 2.15.0                                                   \u2502\r\n- Platform: Linux-5.15.0-1028-nvidia-x86_64-with-glibc2.17                     \u2502\r\n- Python version: 3.8.17                                                       \u2502\r\n- `huggingface_hub` version: 0.19.4                                            \u2502\r\n- PyArrow version: 13.0.0                                                      \u2502\r\n- Pandas version: 2.0.3                                                        \u2502\r\n- `fsspec` version: 2023.6.0    ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6465\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6465\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6464","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6464\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6464\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6464\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6464","id":2020860462,"node_id":"PR_kwDODunzps5g5djo","number":6464,"title":"Add concurrent loading of shards to datasets.load_from_disk ","user":{"login":"kkoutini","id":51880718,"node_id":"MDQ6VXNlcjUxODgwNzE4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/51880718?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kkoutini","html_url":"https:\/\/github.com\/kkoutini","followers_url":"https:\/\/api.github.com\/users\/kkoutini\/followers","following_url":"https:\/\/api.github.com\/users\/kkoutini\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kkoutini\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kkoutini\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kkoutini\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kkoutini\/orgs","repos_url":"https:\/\/api.github.com\/users\/kkoutini\/repos","events_url":"https:\/\/api.github.com\/users\/kkoutini\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kkoutini\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-12-01T13:13:53Z","updated_at":"2023-12-07T12:47:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"In some file systems (like luster), memory mapping arrow files takes time. This can be accelerated by performing the mmap in parallel on processes or threads.\r\n\r\n- Threads seem to be faster than processes when gathering the list of tables from the workers (see https:\/\/github.com\/huggingface\/datasets\/issues\/2252).\r\n- I'm not sure if using threads would respect the\u00a0`IN_MEMORY_MAX_SIZE` config.\r\n- I'm not sure if we need to expose\u00a0num_proc\u00a0from\u00a0`BaseReader.read`\u00a0to\u00a0`DatasetBuilder.as_dataset`. Since `\u00a0DatasetBuilder.as_dataset` is used in many places beside `load_dataset`.\r\n\r\n### Tests on luster file system (on a shared partial node):\r\nLoading 1231 shards of ~2GBs.\r\nThe files were pre-loaded in another process before the script runs (couldn't get a fresh node).\r\n```python\r\nimport logging\r\nfrom time import perf_counter\r\n\r\nimport datasets\r\nlogger = datasets.logging.get_logger(__name__)\r\ndatasets.logging.set_verbosity_info()\r\nlogging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\r\n\r\nclass catchtime:\r\n    # context to measure loading time: https:\/\/stackoverflow.com\/questions\/33987060\/python-context-manager-that-measures-time\r\n    def __init__(self, debug_print=\"Time\", logger=logger):\r\n        self.debug_print = debug_print\r\n        self.logger = logger\r\n\r\n    def __enter__(self):\r\n        self.start = perf_counter()\r\n        return self\r\n\r\n    def __exit__(self, type, value, traceback):\r\n        self.time = perf_counter() - self.start\r\n        readout = f\"{self.debug_print}: {self.time:.3f} seconds\"\r\n        self.logger.info(readout)\r\n\r\ndataset_path=\"\"\r\n\r\n# warmup\r\nwith catchtime(\"Loading in parallel\", logger=logger):\r\n   ds = datasets.load_from_disk(dataset_path,num_proc=16)\r\n\r\n# num_proc=16\r\nwith catchtime(\"Loading in parallel\", logger=logger):\r\n   ds = datasets.load_from_disk(dataset_path,num_proc=16)\r\n# num_proc=32\r\nwith catchtime(\"Loading in parallel\", logger=logger):\r\n   ds = datasets.load_from_disk(dataset_path,num_proc=32)\r\n# num_proc=1\r\nwith catchtime(\"Loading in conseq\", logger=logger):\r\n   ds = datasets.load_from_disk(dataset_path,num_proc=1)\r\n\r\n```\r\n\r\n#### Run 1\r\n```\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [01:28<00:00, 13.96shards\/s]\r\nLoading in parallel: 88.690 seconds\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [01:48<00:00, 11.31shards\/s]\r\nLoading in parallel: 109.339 seconds\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk using 32 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [01:06<00:00, 18.56shards\/s]\r\nLoading in parallel: 66.931 seconds\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [05:09<00:00,  3.98shards\/s]\r\nLoading in conseq: 309.792 seconds\r\n```\r\n#### Run 2\r\n\r\n```\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [01:38<00:00, 12.53shards\/s]\r\nLoading in parallel: 98.831 seconds\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [02:01<00:00, 10.16shards\/s]\r\nLoading in parallel: 121.669 seconds\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk using 32 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [01:07<00:00, 18.18shards\/s]\r\nLoading in parallel: 68.192 seconds\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [05:19<00:00,  3.86shards\/s]\r\nLoading in conseq: 319.759 seconds\r\n```\r\n#### Run 3\r\n```\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [01:36<00:00, 12.74shards\/s]\r\nLoading in parallel: 96.936 seconds\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [02:00<00:00, 10.24shards\/s]\r\nLoading in parallel: 120.761 seconds\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk using 32 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [01:08<00:00, 18.04shards\/s]\r\nLoading in parallel: 68.666 seconds\r\nopen file: ...\/dataset_dict.json\r\nLoading the dataset from disk: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231\/1231 [05:35<00:00,  3.67shards\/s]\r\nLoading in conseq: 335.777 seconds\r\n```\r\n\r\n\r\nfix #2252\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6464\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6464\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6464","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6464","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6464.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6464.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6463","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6463\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6463\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6463\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6463","id":2020702967,"node_id":"PR_kwDODunzps5g46_4","number":6463,"title":"Disable benchmarks in PRs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-12-01T11:35:30Z","updated_at":"2023-12-01T12:09:09Z","closed_at":"2023-12-01T12:03:04Z","author_association":"MEMBER","active_lock_reason":null,"body":"In order to keep PR pages less spammy \/ more readable.\r\nHaving the benchmarks on commits on `main` is enough imo","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6463\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6463\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6463","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6463","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6463.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6463.patch","merged_at":"2023-12-01T12:03:04Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6462","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6462\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6462\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6462\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6462","id":2019238388,"node_id":"PR_kwDODunzps5gz68T","number":6462,"title":"Missing DatasetNotFoundError","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-30T18:09:43Z","updated_at":"2023-11-30T18:36:40Z","closed_at":"2023-11-30T18:30:30Z","author_association":"MEMBER","active_lock_reason":null,"body":"continuation of https:\/\/github.com\/huggingface\/datasets\/pull\/6431\r\n\r\nthis should fix the CI in https:\/\/github.com\/huggingface\/datasets\/pull\/6458 too","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6462\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6462\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6462","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6462","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6462.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6462.patch","merged_at":"2023-11-30T18:30:30Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6461","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6461\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6461\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6461\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6461","id":2018850731,"node_id":"PR_kwDODunzps5gykvO","number":6461,"title":"Fix shard retry mechanism in `push_to_hub`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-11-30T14:57:14Z","updated_at":"2023-12-01T17:57:39Z","closed_at":"2023-12-01T17:51:33Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"When it fails, `preupload_lfs_files` throws a [`RuntimeError`](https:\/\/github.com\/huggingface\/huggingface_hub\/blob\/5eefebee2c150a2df950ab710db350e96c711433\/src\/huggingface_hub\/_commit_api.py#L402) error and chains the original HTTP error. This PR modifies the retry mechanism's error handling to account for that.\r\n\r\nFix https:\/\/github.com\/huggingface\/datasets\/issues\/6392","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6461\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6461\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6461","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6461","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6461.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6461.patch","merged_at":"2023-12-01T17:51:33Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6460","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6460\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6460\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6460\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6460","id":2017433899,"node_id":"I_kwDODunzps54P5kr","number":6460,"title":"jsonlines files don't load with `load_dataset`","user":{"login":"serenalotreck","id":41377532,"node_id":"MDQ6VXNlcjQxMzc3NTMy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/41377532?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/serenalotreck","html_url":"https:\/\/github.com\/serenalotreck","followers_url":"https:\/\/api.github.com\/users\/serenalotreck\/followers","following_url":"https:\/\/api.github.com\/users\/serenalotreck\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/serenalotreck\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/serenalotreck\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/serenalotreck\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/serenalotreck\/orgs","repos_url":"https:\/\/api.github.com\/users\/serenalotreck\/repos","events_url":"https:\/\/api.github.com\/users\/serenalotreck\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/serenalotreck\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-11-29T21:20:11Z","updated_at":"2023-12-29T02:58:29Z","closed_at":"2023-12-05T13:30:53Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhile [the docs](https:\/\/huggingface.co\/docs\/datasets\/upload_dataset#upload-dataset) seem to state that `.jsonl` is a supported extension for `datasets`, loading the dataset results in a `JSONDecodeError`.\n\n### Steps to reproduce the bug\n\nCode:\r\n```\r\nfrom datasets import load_dataset\r\ndset = load_dataset('slotreck\/pickle')\r\n```\r\n\r\nTraceback:\r\n```\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 925\/925 [00:00<00:00, 3.11MB\/s]\r\nDownloading and preparing dataset json\/slotreck--pickle to \/mnt\/home\/lotrecks\/.cache\/huggingface\/datasets\/slotreck___json\/slotreck--pickle-0c311f36ed032b04\/0.0.0\/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 589k\/589k [00:00<00:00, 18.9MB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 104k\/104k [00:00<00:00, 4.61MB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 170k\/170k [00:00<00:00, 7.71MB\/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:00<00:00,  3.77it\/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:00<00:00, 523.92it\/s]\r\nGenerating train split: 0 examples [00:00, ? examples\/s]Failed to read file '\/mnt\/home\/lotrecks\/.cache\/huggingface\/datasets\/downloads\/6ec07bb2f279c9377036af6948532513fa8f48244c672d2644a2d7018ee5c9cb' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column(\/ner\/[]\/[]\/[]) changed from number to string in row 0\r\nTraceback (most recent call last):                      \r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/site-packages\/datasets\/packaged_modules\/json\/json.py\", line 144, in _generate_tables\r\n    dataset = json.load(f)\r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/json\/__init__.py\", line 296, in load\r\n    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/json\/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/json\/decoder.py\", line 340, in decode\r\n    raise JSONDecodeError(\"Extra data\", s, end)\r\njson.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 3086)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/site-packages\/datasets\/builder.py\", line 1879, in _prepare_split_single\r\n    for _, table in generator:\r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/site-packages\/datasets\/packaged_modules\/json\/json.py\", line 147, in _generate_tables\r\n    raise e\r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/site-packages\/datasets\/packaged_modules\/json\/json.py\", line 122, in _generate_tables\r\n    io.BytesIO(batch), read_options=paj.ReadOptions(block_size=block_size)\r\n  File \"pyarrow\/_json.pyx\", line 259, in pyarrow._json.read_json\r\n  File \"pyarrow\/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: JSON parse error: Column(\/ner\/[]\/[]\/[]) changed from number to string in row 0\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/site-packages\/datasets\/load.py\", line 1815, in load_dataset\r\n    storage_options=storage_options,\r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/site-packages\/datasets\/builder.py\", line 913, in download_and_prepare\r\n    **download_and_prepare_kwargs,\r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/site-packages\/datasets\/builder.py\", line 1004, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/site-packages\/datasets\/builder.py\", line 1768, in _prepare_split\r\n    gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n  File \"\/mnt\/home\/lotrecks\/anaconda3\/envs\/pickle\/lib\/python3.7\/site-packages\/datasets\/builder.py\", line 1912, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\n\n### Expected behavior\n\nFor the dataset to be loaded without error.\n\n### Environment info\n\n- `datasets` version: 2.13.1\r\n- Platform: Linux-3.10.0-1160.80.1.el7.x86_64-x86_64-with-centos-7.9.2009-Core\r\n- Python version: 3.7.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.3.5","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6460\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6460\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6459","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6459\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6459\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6459\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6459","id":2017029380,"node_id":"PR_kwDODunzps5gsWlz","number":6459,"title":"Retrieve cached datasets that were pushed to hub when offline","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-29T16:56:15Z","updated_at":"2023-12-13T13:54:48Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"I drafted the logic to retrieve a no-script dataset in the cache.\r\nFor example it can reload datasets that were pushed to hub if they exist in the cache.\r\n\r\nexample:\r\n\r\n```python\r\n>>> Dataset.from_dict({\"a\": [1, 2]}).push_to_hub(\"lhoestq\/tmp\")\r\n>>> load_dataset(\"lhoestq\/tmp\")\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['a'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\nand later, without connection:\r\n\r\n```python\r\n>>> load_dataset(\"lhoestq\/tmp\")\r\nUsing the latest cached version of the dataset from \/Users\/quentinlhoest\/.cache\/huggingface\/datasets\/lhoestq___tmp\/*\/*\/0b3caccda1725efb(last modified on Wed Nov 29 16:50:27 2023) since it couldn't be found locally at lhoestq\/tmp.\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['a'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\nfix https:\/\/github.com\/huggingface\/datasets\/issues\/3547\r\n\r\n## Implementation details (EDITED)\r\n\r\nI continued in https:\/\/github.com\/huggingface\/datasets\/pull\/6493, see the changes there\r\n\r\nTODO:\r\n- [x] tests\r\n- [ ] compatible with https:\/\/github.com\/huggingface\/datasets\/pull\/6458","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6459\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6459\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6459","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6459","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6459.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6459.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6458","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6458\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6458\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6458\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6458","id":2016577761,"node_id":"PR_kwDODunzps5gqy4M","number":6458,"title":"Lazy data files resolution","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":20,"created_at":"2023-11-29T13:18:44Z","updated_at":"2023-12-12T23:30:33Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Related to discussion at https:\/\/github.com\/huggingface\/datasets\/pull\/6255\r\n\r\nthis makes this code run in 2sec instead of >10sec\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"glue\", \"sst2\", streaming=True, trust_remote_code=False)\r\n```\r\n\r\nFor some datasets with many configs and files it can be up to 100x faster.\r\nThis is particularly important now that some datasets will be loaded from the Parquet export instead of the scripts.\r\n\r\nThe data files are only resolved in the builder `__init__`. To do so I added DataFilesPatternsList and DataFilesPatternsDict that have `.resolve()` to return resolved DataFilesList and DataFilesDict","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6458\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6458\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6458","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6458","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6458.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6458.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6457","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6457\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6457\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6457\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6457","id":2015650563,"node_id":"I_kwDODunzps54JGMD","number":6457,"title":"`TypeError`: huggingface_hub.hf_file_system.HfFileSystem.find() got multiple values for keyword argument 'maxdepth'","user":{"login":"wasertech","id":79070834,"node_id":"MDQ6VXNlcjc5MDcwODM0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/79070834?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wasertech","html_url":"https:\/\/github.com\/wasertech","followers_url":"https:\/\/api.github.com\/users\/wasertech\/followers","following_url":"https:\/\/api.github.com\/users\/wasertech\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wasertech\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wasertech\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wasertech\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wasertech\/orgs","repos_url":"https:\/\/api.github.com\/users\/wasertech\/repos","events_url":"https:\/\/api.github.com\/users\/wasertech\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wasertech\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-11-29T01:57:36Z","updated_at":"2023-11-29T15:39:03Z","closed_at":"2023-11-29T02:02:38Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nPlease see https:\/\/github.com\/huggingface\/huggingface_hub\/issues\/1872\n\n### Steps to reproduce the bug\n\nPlease see https:\/\/github.com\/huggingface\/huggingface_hub\/issues\/1872\n\n### Expected behavior\n\nPlease see https:\/\/github.com\/huggingface\/huggingface_hub\/issues\/1872\n\n### Environment info\n\nPlease see https:\/\/github.com\/huggingface\/huggingface_hub\/issues\/1872","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6457\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6457\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6456","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6456\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6456\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6456\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6456","id":2015186090,"node_id":"PR_kwDODunzps5gmDJY","number":6456,"title":"Don't require trust_remote_code in inspect_dataset","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-28T19:47:07Z","updated_at":"2023-11-30T10:40:23Z","closed_at":"2023-11-30T10:34:12Z","author_association":"MEMBER","active_lock_reason":null,"body":"don't require `trust_remote_code` in (deprecated) `inspect_dataset` (it defeats its purpose)\r\n\r\n(not super important but we might as well keep it until the next major release)\r\n\r\nthis is needed to fix the tests in https:\/\/github.com\/huggingface\/datasets\/pull\/6448","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6456\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6456\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6456","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6456","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6456.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6456.patch","merged_at":"2023-11-30T10:34:12Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6454","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6454\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6454\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6454\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6454","id":2013001584,"node_id":"PR_kwDODunzps5gej3H","number":6454,"title":"Refactor `dill` logic","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-11-27T20:01:25Z","updated_at":"2023-11-28T16:29:58Z","closed_at":"2023-11-28T16:29:31Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Refactor the `dill` logic to make it easier to maintain (and fix some issues along the way)\r\n\r\nIt makes the following improvements to the serialization API:\r\n* consistent order of a `dict`'s keys\r\n* support for hashing `torch.compile`-ed modules and functions\r\n* deprecates `datasets.fingerprint.hashregister` as the `hashregister`-ed reducers are never invoked anyways (does not support nested data as `pickle`\/`dill` do)\r\n\r\n\r\n~~TODO: optimize hashing of `pa.Table` and `datasets.table.Table`~~ The `pa_array.to_string` approach is faster for large arrays because it outputs the first 10 and last 10 elements (by default). The problem is that this can produce identical hashes for non-identical arrays if their differing elements get ellipsed...\r\n \r\nFix https:\/\/github.com\/huggingface\/datasets\/issues\/6440, fix https:\/\/github.com\/huggingface\/datasets\/issues\/5839","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6454\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6454\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6454","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6454","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6454.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6454.patch","merged_at":"2023-11-28T16:29:31Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6453","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6453\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6453\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6453\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6453","id":2011907787,"node_id":"PR_kwDODunzps5ga0rv","number":6453,"title":"Update hub-docs reference","user":{"login":"mishig25","id":11827707,"node_id":"MDQ6VXNlcjExODI3NzA3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11827707?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mishig25","html_url":"https:\/\/github.com\/mishig25","followers_url":"https:\/\/api.github.com\/users\/mishig25\/followers","following_url":"https:\/\/api.github.com\/users\/mishig25\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mishig25\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mishig25\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mishig25\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mishig25\/orgs","repos_url":"https:\/\/api.github.com\/users\/mishig25\/repos","events_url":"https:\/\/api.github.com\/users\/mishig25\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mishig25\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-27T09:57:20Z","updated_at":"2023-11-27T10:23:44Z","closed_at":"2023-11-27T10:17:34Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Follow up to huggingface\/huggingface.js#296","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6453\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6453\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6453","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6453","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6453.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6453.patch","merged_at":"2023-11-27T10:17:34Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6452","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6452\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6452\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6452\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6452","id":2011632708,"node_id":"PR_kwDODunzps5gZ5oe","number":6452,"title":"Praveen_repo_pull_req","user":{"login":"Praveenhh","id":151713216,"node_id":"U_kgDOCQr1wA","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/151713216?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Praveenhh","html_url":"https:\/\/github.com\/Praveenhh","followers_url":"https:\/\/api.github.com\/users\/Praveenhh\/followers","following_url":"https:\/\/api.github.com\/users\/Praveenhh\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Praveenhh\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Praveenhh\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Praveenhh\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Praveenhh\/orgs","repos_url":"https:\/\/api.github.com\/users\/Praveenhh\/repos","events_url":"https:\/\/api.github.com\/users\/Praveenhh\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Praveenhh\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-27T07:07:50Z","updated_at":"2023-11-27T09:28:00Z","closed_at":"2023-11-27T09:28:00Z","author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6452\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6452\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6452","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6452","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6452.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6452.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6451","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6451\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6451\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6451\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6451","id":2010693912,"node_id":"I_kwDODunzps532MEY","number":6451,"title":"Unable to read \"marsyas\/gtzan\" data","user":{"login":"gerald-wrona","id":32300890,"node_id":"MDQ6VXNlcjMyMzAwODkw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32300890?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/gerald-wrona","html_url":"https:\/\/github.com\/gerald-wrona","followers_url":"https:\/\/api.github.com\/users\/gerald-wrona\/followers","following_url":"https:\/\/api.github.com\/users\/gerald-wrona\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/gerald-wrona\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/gerald-wrona\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/gerald-wrona\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/gerald-wrona\/orgs","repos_url":"https:\/\/api.github.com\/users\/gerald-wrona\/repos","events_url":"https:\/\/api.github.com\/users\/gerald-wrona\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/gerald-wrona\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-25T15:13:17Z","updated_at":"2023-12-01T12:53:46Z","closed_at":"2023-11-27T09:36:25Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, this is my code and the error:\r\n```\r\nfrom datasets import load_dataset\r\ngtzan = load_dataset(\"marsyas\/gtzan\", \"all\")\r\n```\r\n[error_trace.txt](https:\/\/github.com\/huggingface\/datasets\/files\/13464397\/error_trace.txt)\r\n[audio_yml.txt](https:\/\/github.com\/huggingface\/datasets\/files\/13464410\/audio_yml.txt)\r\nPython 3.11.5\r\nJupyter Notebook 6.5.4\r\nWindows 10\r\n\r\nI'm able to download and work with other datasets, but not this one. For example, both these below work fine:\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"facebook\/voxpopuli\", \"pl\", split=\"train\", streaming=True)\r\nminds = load_dataset(\"PolyAI\/minds14\", name=\"en-US\", split=\"train\")\r\n```\r\nThanks for your help\r\n\r\nhttps:\/\/huggingface.co\/datasets\/marsyas\/gtzan\/tree\/main","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6451\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6451\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6450","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6450\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6450\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6450\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6450","id":2009491386,"node_id":"I_kwDODunzps53xme6","number":6450,"title":"Support multiple image\/audio columns in ImageFolder\/AudioFolder","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892865,"node_id":"MDU6TGFiZWwxOTM1ODkyODY1","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/duplicate","name":"duplicate","color":"cfd3d7","default":true,"description":"This issue or pull request already exists"},{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-24T10:34:09Z","updated_at":"2023-11-28T11:07:17Z","closed_at":"2023-11-24T17:24:38Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nHave a metadata.csv file with multiple columns that point to relative image or audio files.\n\n### Motivation\n\nCurrently, ImageFolder allows one column, called `file_name`, pointing to relative image files. On the same model, AudioFolder allows one column, called `file_name`, pointing to relative audio files.\r\n\r\nBut it's not possible to have two image columns, or to have two audio column, or to have one audio column and one image column.\n\n### Your contribution\n\nno specific contribution","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6450\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6450\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6449","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6449\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6449\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6449\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6449","id":2008617992,"node_id":"PR_kwDODunzps5gQCVZ","number":6449,"title":"Fix metadata file resolution when inferred pattern is `**`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-11-23T17:35:02Z","updated_at":"2023-11-27T10:02:56Z","closed_at":"2023-11-24T17:13:02Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Refetch metadata files in case they were dropped by `filter_extensions` in the previous step. \r\n\r\nFix #6442 \r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6449\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6449\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6449","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6449","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6449.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6449.patch","merged_at":"2023-11-24T17:13:02Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6448","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6448\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6448\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6448\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6448","id":2008614985,"node_id":"PR_kwDODunzps5gQBsE","number":6448,"title":"Use parquet export if possible","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":24,"created_at":"2023-11-23T17:31:57Z","updated_at":"2023-12-01T17:57:17Z","closed_at":"2023-12-01T17:50:59Z","author_association":"MEMBER","active_lock_reason":null,"body":"The idea is to make this code work for datasets with scripts if they have a Parquet export\r\n\r\n```python\r\nds = load_dataset(\"squad\", trust_remote_code=False)\r\n```\r\n\r\nAnd more generally, it means we use the Parquet export whenever it's possible (it's safer and faster than dataset scripts).\r\n\r\nI also added a `config.USE_PARQUET_EXPORT` variable to use in the datasets-server parquet conversion job\r\n\r\n- [x] Needs https:\/\/github.com\/huggingface\/datasets\/pull\/6429 to be merged first\r\n\r\ncc @severo I use the \/parquet and \/info endpoints from datasets-server","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6448\/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":2,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6448\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6448","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6448","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6448.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6448.patch","merged_at":"2023-12-01T17:50:59Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6447","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6447\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6447\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6447\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6447","id":2008195298,"node_id":"I_kwDODunzps53sqDi","number":6447,"title":"Support one dataset loader per config when using YAML","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-23T13:03:07Z","updated_at":"2023-11-23T13:03:07Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nSee https:\/\/huggingface.co\/datasets\/datasets-examples\/doc-unsupported-1\r\n\r\nI would like to use CSV loader for the \"csv\" config, JSONL loader for the \"jsonl\" config, etc.\n\n### Motivation\n\nIt would be more flexible for the users\n\n### Your contribution\n\nNo specific contribution","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6447\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6447\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6446","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6446\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6446\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6446\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6446","id":2007092708,"node_id":"I_kwDODunzps53oc3k","number":6446,"title":"Speech Commands v2 dataset doesn't match AST-v2 config","user":{"login":"vymao","id":18024303,"node_id":"MDQ6VXNlcjE4MDI0MzAz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/18024303?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vymao","html_url":"https:\/\/github.com\/vymao","followers_url":"https:\/\/api.github.com\/users\/vymao\/followers","following_url":"https:\/\/api.github.com\/users\/vymao\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vymao\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vymao\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vymao\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vymao\/orgs","repos_url":"https:\/\/api.github.com\/users\/vymao\/repos","events_url":"https:\/\/api.github.com\/users\/vymao\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vymao\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-22T20:46:36Z","updated_at":"2023-11-28T14:46:08Z","closed_at":"2023-11-28T14:46:08Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n[According](https:\/\/huggingface.co\/MIT\/ast-finetuned-speech-commands-v2) to `MIT\/ast-finetuned-speech-commands-v2`, the model was trained on the Speech Commands v2 dataset. However, while the model config says the model should have 35 class labels, the dataset itself has 36 class labels. Moreover, the class labels themselves don't match between the model config and the dataset. It is difficult to reproduce the data used to fine tune `MIT\/ast-finetuned-speech-commands-v2`.\n\n### Steps to reproduce the bug\n\n```\r\n>>> model = ASTForAudioClassification.from_pretrained(\"MIT\/ast-finetuned-speech-commands-v2\")\r\n>>> model.config.id2label\r\n{0: 'backward', 1: 'follow', 2: 'five', 3: 'bed', 4: 'zero', 5: 'on', 6: 'learn', 7: 'two', 8: 'house', 9: 'tree', 10: 'dog', 11: 'stop', 12: 'seven', 13: 'eight', 14: 'down', 15: 'six', 16: 'forward', 17: 'cat', 18: 'right', 19: 'visual', 20: 'four', 21: 'wow', 22: 'no', 23: 'nine', 24: 'off', 25: 'three', 26: 'left', 27: 'marvin', 28: 'yes', 29: 'up', 30: 'sheila', 31: 'happy', 32: 'bird', 33: 'go', 34: 'one'}\r\n\r\n>>> dataset = load_dataset(\"speech_commands\", \"v0.02\", split=\"test\")\r\n>>> torch.unique(torch.Tensor(dataset['label']))\r\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\r\n        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\r\n        28., 29., 30., 31., 32., 33., 34., 35.])\r\n```\r\nIf you try to explore the [dataset itself](https:\/\/huggingface.co\/datasets\/speech_commands\/viewer\/v0.02\/test), you can see that the id to label does not match what is provided by `model.config.id2label`.\r\n\n\n### Expected behavior\n\nThe labels should match completely and there should be the same number of label classes between the model config and the dataset itself.\n\n### Environment info\n\ndatasets = 2.14.6, transformers =  4.33.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6446\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6446\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6445","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6445\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6445\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6445\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6445","id":2006958595,"node_id":"PR_kwDODunzps5gKg2d","number":6445,"title":"Use `filelock` package for file locking","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-11-22T19:04:45Z","updated_at":"2023-11-23T18:47:30Z","closed_at":"2023-11-23T18:41:23Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Use the `filelock` package instead of `datasets.utils.filelock` for file locking to be consistent with `huggingface_hub` and not to be responsible for improving the `filelock` capabilities \ud83d\ude42. \r\n\r\n(Reverts https:\/\/github.com\/huggingface\/datasets\/pull\/859, but these `INFO` logs are not printed by default (anymore?), so this should be okay)\r\n\r\n ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6445\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6445\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6445","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6445","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6445.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6445.patch","merged_at":"2023-11-23T18:41:22Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6444","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6444\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6444\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6444\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6444","id":2006842179,"node_id":"PR_kwDODunzps5gKG_e","number":6444,"title":"Remove `Table.__getstate__` and `Table.__setstate__`","user":{"login":"LZHgrla","id":36994684,"node_id":"MDQ6VXNlcjM2OTk0Njg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36994684?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/LZHgrla","html_url":"https:\/\/github.com\/LZHgrla","followers_url":"https:\/\/api.github.com\/users\/LZHgrla\/followers","following_url":"https:\/\/api.github.com\/users\/LZHgrla\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/LZHgrla\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/LZHgrla\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/LZHgrla\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/LZHgrla\/orgs","repos_url":"https:\/\/api.github.com\/users\/LZHgrla\/repos","events_url":"https:\/\/api.github.com\/users\/LZHgrla\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/LZHgrla\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-11-22T17:55:10Z","updated_at":"2023-11-23T15:19:43Z","closed_at":"2023-11-23T15:13:28Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"When using distributed training, the code of `os.remove(filename)` may be executed separately by each rank, leading to `FileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmprxxxxxxx.arrow'`\r\n\r\n```python\r\nfrom torch import distributed as dist\r\n\r\nif dist.get_rank() == 0:\r\n    dataset = process_dataset(*args, **kwargs)\r\n    objects = [dataset]\r\nelse:\r\n    objects = [None]\r\ndist.broadcast_object_list(objects, src=0)\r\ndataset = objects[0]\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6444\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6444\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6444","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6444","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6444.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6444.patch","merged_at":"2023-11-23T15:13:28Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6443","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6443\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6443\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6443\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6443","id":2006568368,"node_id":"I_kwDODunzps53mc2w","number":6443,"title":"Trouble loading files defined in YAML explicitly","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-22T15:18:10Z","updated_at":"2023-11-23T09:06:20Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Look at https:\/\/huggingface.co\/datasets\/severo\/doc-yaml-2\r\n\r\nIt's a reproduction of the example given in the docs at https:\/\/huggingface.co\/docs\/hub\/datasets-manual-configuration\r\n\r\n```\r\nYou can select multiple files per split using a list of paths:\r\n\r\nmy_dataset_repository\/\r\n\u251c\u2500\u2500 README.md\r\n\u251c\u2500\u2500 data\/\r\n\u2502   \u251c\u2500\u2500 abc.csv\r\n\u2502   \u2514\u2500\u2500 def.csv\r\n\u2514\u2500\u2500 holdout\/\r\n    \u2514\u2500\u2500 ghi.csv\r\n\r\n\r\n---\r\nconfigs:\r\n- config_name: default\r\n  data_files:\r\n  - split: train\r\n    path:\r\n    - \"data\/abc.csv\"\r\n    - \"data\/def.csv\"\r\n  - split: test\r\n    path: \"holdout\/ghi.csv\"\r\n---\r\n```\r\n\r\nIt raises the following error:\r\n\r\n```\r\nError code:   ConfigNamesError\r\nException:    FileNotFoundError\r\nMessage:      Couldn't find a dataset script at \/src\/services\/worker\/severo\/doc-yaml-2\/doc-yaml-2.py or any data file in the same directory. Couldn't find 'severo\/doc-yaml-2' on the Hugging Face Hub either: FileNotFoundError: Unable to find 'hf:\/\/datasets\/severo\/doc-yaml-2@938a0578fb4c6bc9da7d80b06a3ba39c2834b0c2\/data\/def.csv' with any supported extension ['.csv', '.tsv', '.json', '.jsonl', '.parquet', '.arrow', '.txt', '.blp', '.bmp', '.dib', '.bufr', '.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', '.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', '.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', '.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', '.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', '.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', '.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', '.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', '.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', '.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', '.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', '.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k', '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', '.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', '.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', '.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', '.zip']\r\nTraceback:    Traceback (most recent call last):\r\n                File \"\/src\/services\/worker\/src\/worker\/job_runners\/dataset\/config_names.py\", line 65, in compute_config_names_response\r\n                  for config in sorted(get_dataset_config_names(path=dataset, token=hf_token))\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 351, in get_dataset_config_names\r\n                  dataset_module = dataset_module_factory(\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1507, in dataset_module_factory\r\n                  raise FileNotFoundError(\r\n              FileNotFoundError: Couldn't find a dataset script at \/src\/services\/worker\/severo\/doc-yaml-2\/doc-yaml-2.py or any data file in the same directory. Couldn't find 'severo\/doc-yaml-2' on the Hugging Face Hub either: FileNotFoundError: Unable to find 'hf:\/\/datasets\/severo\/doc-yaml-2@938a0578fb4c6bc9da7d80b06a3ba39c2834b0c2\/data\/def.csv' with any supported extension ['.csv', '.tsv', '.json', '.jsonl', '.parquet', '.arrow', '.txt', '.blp', '.bmp', '.dib', '.bufr', '.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', '.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', '.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', '.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', '.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', '.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', '.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', '.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', '.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', '.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', '.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', '.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k', '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', '.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', '.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', '.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', '.zip']\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6443\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6443\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6442","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6442\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6442\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6442\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6442","id":2006086907,"node_id":"I_kwDODunzps53knT7","number":6442,"title":"Trouble loading image folder with additional features -  metadata file ignored","user":{"login":"linoytsaban","id":57615435,"node_id":"MDQ6VXNlcjU3NjE1NDM1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57615435?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/linoytsaban","html_url":"https:\/\/github.com\/linoytsaban","followers_url":"https:\/\/api.github.com\/users\/linoytsaban\/followers","following_url":"https:\/\/api.github.com\/users\/linoytsaban\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/linoytsaban\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/linoytsaban\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/linoytsaban\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/linoytsaban\/orgs","repos_url":"https:\/\/api.github.com\/users\/linoytsaban\/repos","events_url":"https:\/\/api.github.com\/users\/linoytsaban\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/linoytsaban\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-22T11:01:35Z","updated_at":"2023-11-24T17:13:03Z","closed_at":"2023-11-24T17:13:03Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nLoading image folder with a caption column using `load_dataset(<image_folder_path>)` doesn't load the captions. \r\n\r\nWhen loading a local image folder with captions using `datasets==2.13.0`\r\n```\r\nfrom datasets import load_dataset\r\ndata = load_dataset(<image_folder_path>)\r\ndata.column_names\r\n```\r\nyields \r\n`{'train': ['image', 'prompt']}`\r\n\r\nbut when using `datasets==2.15.0`\r\nyeilds \r\n`{'train': ['image']}`\r\n\r\nPutting the images and `metadata.jsonl` file into a nested `train` folder **or** loading with `load_dataset(\"imagefolder\", data_dir=<image_folder_path>)` solves the issue and\r\nyields \r\n`{'train': ['image', 'prompt']}`\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. create a folder `<image_folder_path>` that contains images and a metadata file with additional features- e.g. \"prompt\"\r\n2. run:\r\n```\r\nfrom datasets import load_dataset\r\ndata = load_dataset(\"<image_folder_path>\")\r\ndata.column_names\r\n```\r\n\r\n### Expected behavior\r\n\r\n`{'train': ['image', 'prompt']}`\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6442\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6442\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6441","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6441\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6441\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6441\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6441","id":2004985857,"node_id":"I_kwDODunzps53gagB","number":6441,"title":"Trouble Loading a Gated Dataset For User with Granted Permission","user":{"login":"e-trop","id":124715309,"node_id":"U_kgDOB28BLQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/124715309?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/e-trop","html_url":"https:\/\/github.com\/e-trop","followers_url":"https:\/\/api.github.com\/users\/e-trop\/followers","following_url":"https:\/\/api.github.com\/users\/e-trop\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/e-trop\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/e-trop\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/e-trop\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/e-trop\/orgs","repos_url":"https:\/\/api.github.com\/users\/e-trop\/repos","events_url":"https:\/\/api.github.com\/users\/e-trop\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/e-trop\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-21T19:24:36Z","updated_at":"2023-12-13T08:27:16Z","closed_at":"2023-12-13T08:27:16Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI have granted permissions to several users to access a gated huggingface dataset. The users accepted the invite and when trying to load the dataset using their access token they get \r\n`FileNotFoundError: Couldn't find a dataset script at .....` . Also when they try to click the url link for the dataset they get a 404 error.\n\n### Steps to reproduce the bug\n\n1. Grant access to gated dataset for specific users \r\n2. Users accept invitation\r\n3. Users login to hugging face hub using cli login\r\n4. Users run load_dataset\n\n### Expected behavior\n\nDataset is loaded normally for users who were granted access to the gated dataset.\n\n### Environment info\n\ndatasets==2.15.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6441\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6441\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6440","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6440\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6440\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6440\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6440","id":2004509301,"node_id":"I_kwDODunzps53emJ1","number":6440,"title":"`.map` not hashing under python 3.9","user":{"login":"changyeli","id":9058204,"node_id":"MDQ6VXNlcjkwNTgyMDQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9058204?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/changyeli","html_url":"https:\/\/github.com\/changyeli","followers_url":"https:\/\/api.github.com\/users\/changyeli\/followers","following_url":"https:\/\/api.github.com\/users\/changyeli\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/changyeli\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/changyeli\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/changyeli\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/changyeli\/orgs","repos_url":"https:\/\/api.github.com\/users\/changyeli\/repos","events_url":"https:\/\/api.github.com\/users\/changyeli\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/changyeli\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-21T15:14:54Z","updated_at":"2023-11-28T16:29:33Z","closed_at":"2023-11-28T16:29:33Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThe `.map` function cannot hash under python 3.9. Tried to use [the solution here](https:\/\/github.com\/huggingface\/datasets\/issues\/4521#issuecomment-1205166653), but still get the same message: \r\n\r\n\r\n`Parameter 'function'=<function map_to_pred at 0x7fa0b49ead30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.`\n\n### Steps to reproduce the bug\n\n```python\r\ndef map_to_pred(batch):\r\n    \"\"\"\r\n    Perform inference on an audio batch\r\n\r\n    Parameters:\r\n        batch (dict): A dictionary containing audio data and other related information.\r\n\r\n    Returns:\r\n        dict: The input batch dictionary with added prediction and transcription fields.\r\n    \"\"\"\r\n    audio = batch['audio']\r\n    input_features = processor(\r\n        audio['array'], sampling_rate=audio['sampling_rate'], return_tensors=\"pt\").input_features\r\n    input_features = input_features.to('cuda')\r\n    with torch.no_grad():\r\n        predicted_ids = model.generate(input_features)\r\n    preds = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\r\n    batch['prediction'] = processor.tokenizer._normalize(preds)\r\n    batch[\"transcription\"] = processor.tokenizer._normalize(batch['transcription'])\r\n    return batch\r\n\r\nMODEL_CARD = \"openai\/whisper-small\"\r\nMODEL_NAME = MODEL_CARD.rsplit('\/', maxsplit=1)[-1]\r\nmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_CARD)\r\nprocessor = AutoProcessor.from_pretrained(\r\nMODEL_CARD, language=\"english\", task=\"transcribe\")\r\nmodel = torch.compile(model)\r\ndt = load_dataset(\"audiofolder\", data_dir=config['DATA']['dataset'], split=\"test\")\r\ndt = dt.cast_column(\"audio\", Audio(sampling_rate=16000))\r\nresult = coraal_dt.map(map_to_pred, num_proc=16)\r\n```\n\n### Expected behavior\n\nHashed and cached dataset starts inferencing\n\n### Environment info\n\n- `transformers` version: 4.35.0\r\n- Platform: Linux-5.14.0-284.30.1.el9_2.x86_64-x86_64-with-glibc2.34\r\n- Python version: 3.9.18\r\n- Huggingface_hub version: 0.17.3\r\n- Safetensors version: 0.4.0\r\n- Accelerate version: 0.24.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.1.0 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: yes\r\n- Using distributed or parallel set-up in script?: no","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6440\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6440\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6439","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6439\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6439\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6439\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6439","id":2002916514,"node_id":"I_kwDODunzps53YhSi","number":6439,"title":"Download + preparation speed of datasets.load_dataset is 20x slower than huggingface hub snapshot and manual loding","user":{"login":"AntreasAntoniou","id":10792502,"node_id":"MDQ6VXNlcjEwNzkyNTAy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10792502?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/AntreasAntoniou","html_url":"https:\/\/github.com\/AntreasAntoniou","followers_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/followers","following_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/orgs","repos_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/repos","events_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-20T20:07:23Z","updated_at":"2023-11-20T20:07:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am working with a dataset I am trying to publish. \r\n\r\nThe path is Antreas\/TALI. \r\n\r\nIt's a fairly large dataset, and contains images, video, audio and text. \r\n\r\nI have been having multiple problems when the dataset is being downloaded using the load_dataset function -- even with 64 workers taking more than 7 days to process. \r\n\r\nWith snapshot download it takes 12 hours, and that includes the dataset preparation done using load_dataset and passing the dataset parquet file paths. \r\n\r\nFind the script I am using below:\r\n\r\n```python\r\n\r\nimport multiprocessing as mp\r\nimport pathlib\r\nfrom typing import Optional\r\n\r\nimport datasets\r\nfrom rich import print\r\nfrom tqdm import tqdm\r\n\r\n\r\ndef download_dataset_via_hub(\r\n    dataset_name: str,\r\n    dataset_download_path: pathlib.Path,\r\n    num_download_workers: int = mp.cpu_count(),\r\n):\r\n    import huggingface_hub as hf_hub\r\n\r\n    download_folder = hf_hub.snapshot_download(\r\n        repo_id=dataset_name,\r\n        repo_type=\"dataset\",\r\n        cache_dir=dataset_download_path,\r\n        resume_download=True,\r\n        max_workers=num_download_workers,\r\n        ignore_patterns=[],\r\n    )\r\n\r\n    return pathlib.Path(download_folder) \/ \"data\"\r\n\r\n\r\ndef load_dataset_via_hub(\r\n    dataset_download_path: pathlib.Path,\r\n    num_download_workers: int = mp.cpu_count(),\r\n    dataset_name: Optional[str] = None,\r\n):\r\n    from dataclasses import dataclass, field\r\n\r\n    from datasets import ClassLabel, Features, Image, Sequence, Value\r\n\r\n    dataset_path = download_dataset_via_hub(\r\n        dataset_download_path=dataset_download_path,\r\n        num_download_workers=num_download_workers,\r\n        dataset_name=dataset_name,\r\n    )\r\n    # Building a list of file paths for validation set\r\n\r\n    train_files = [\r\n        file.as_posix()\r\n        for file in pathlib.Path(dataset_path).glob(\"*.parquet\")\r\n        if \"train\" in file.as_posix()\r\n    ]\r\n    val_files = [\r\n        file.as_posix()\r\n        for file in pathlib.Path(dataset_path).glob(\"*.parquet\")\r\n        if \"val\" in file.as_posix()\r\n    ]\r\n    test_files = [\r\n        file.as_posix()\r\n        for file in pathlib.Path(dataset_path).glob(\"*.parquet\")\r\n        if \"test\" in file.as_posix()\r\n    ]\r\n    print(\r\n        f\"Found {len(test_files)} files for testing set, {len(train_files)} for training set and {len(val_files)} for validation set\"\r\n    )\r\n    data_files = {\r\n        \"test\": test_files,\r\n        \"val\": val_files,\r\n        \"train\": train_files,\r\n    }\r\n\r\n    features = Features(\r\n        {\r\n            \"image\": Image(\r\n                decode=True\r\n            ),  # Set `decode=True` if you want to decode the images, otherwise `decode=False`\r\n            \"image_url\": Value(\"string\"),\r\n            \"item_idx\": Value(\"int64\"),\r\n            \"wit_features\": Sequence(\r\n                {\r\n                    \"attribution_passes_lang_id\": Value(\"bool\"),\r\n                    \"caption_alt_text_description\": Value(\"string\"),\r\n                    \"caption_reference_description\": Value(\"string\"),\r\n                    \"caption_title_and_reference_description\": Value(\"string\"),\r\n                    \"context_page_description\": Value(\"string\"),\r\n                    \"context_section_description\": Value(\"string\"),\r\n                    \"hierarchical_section_title\": Value(\"string\"),\r\n                    \"is_main_image\": Value(\"bool\"),\r\n                    \"language\": Value(\"string\"),\r\n                    \"page_changed_recently\": Value(\"bool\"),\r\n                    \"page_title\": Value(\"string\"),\r\n                    \"page_url\": Value(\"string\"),\r\n                    \"section_title\": Value(\"string\"),\r\n                }\r\n            ),\r\n            \"wit_idx\": Value(\"int64\"),\r\n            \"youtube_title_text\": Value(\"string\"),\r\n            \"youtube_description_text\": Value(\"string\"),\r\n            \"youtube_video_content\": Value(\"binary\"),\r\n            \"youtube_video_starting_time\": Value(\"string\"),\r\n            \"youtube_subtitle_text\": Value(\"string\"),\r\n            \"youtube_video_size\": Value(\"int64\"),\r\n            \"youtube_video_file_path\": Value(\"string\"),\r\n        }\r\n    )\r\n\r\n    dataset = datasets.load_dataset(\r\n        \"parquet\" if dataset_name is None else dataset_name,\r\n        data_files=data_files,\r\n        features=features,\r\n        num_proc=1,\r\n        cache_dir=dataset_download_path \/ \"cache\",\r\n    )\r\n    return dataset\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    dataset_cache = pathlib.Path(\"\/disk\/scratch_fast0\/tali\/\")\r\n    dataset = load_dataset_via_hub(dataset_cache, dataset_name=\"Antreas\/TALI\")[\r\n        \"test\"\r\n    ]\r\n\r\n    for sample in tqdm(dataset):\r\n        print(list(sample.keys()))\r\n```\r\n\r\nAlso, streaming this dataset has been a very painfully slow process. Streaming the train set takes 15m to start, and streaming the test and val sets takes 3 hours to start! \n\n### Steps to reproduce the bug\n\n1. Run the code I provided to get a sense of how fast snapshot + manual is\r\n2. Run datasets.load_dataset(\"Antreas\/TALI\") to get a sense of the speed of that OP. \r\n3. You should now have an appreciation of how long these things take.\r\n\r\n\n\n### Expected behavior\n\nThe load dataset function should be at least as fast as the huggingface snapshot download function in terms of downloading dataset files. Not 20 times slower. \n\n### Environment info\n\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.13\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.1.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6439\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6439\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6438","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6438\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6438\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6438\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6438","id":2002032804,"node_id":"I_kwDODunzps53VJik","number":6438,"title":"Support GeoParquet","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-11-20T11:54:58Z","updated_at":"2023-12-30T03:38:51Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nSupport the GeoParquet format\n\n### Motivation\n\nGeoParquet (https:\/\/geoparquet.org\/) is a common format for sharing vectorial geospatial data on the cloud, along with \"traditional\" data columns.\r\n\r\nIt would be nice to be able to load this format with datasets, and more generally, in the Datasets Hub (see https:\/\/huggingface.co\/datasets\/joshuasundance\/govgis_nov2023-slim-spatial\/discussions\/1).\n\n### Your contribution\n\nI would be happy to help work on a PR (but I don't think I can do one on my own).\r\n\r\nAlso, we have to define what we want to support:\r\n- load all the columns, but get the \"geospatial\" column in text-only mode for now\r\n- or, fully support the spatial features, maybe taking inspiration from (or depending upon) https:\/\/geopandas.org\/en\/stable\/index.html (which itself depends on https:\/\/fiona.readthedocs.io\/en\/stable\/, which requires a local install of https:\/\/gdal.org\/)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6438\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6438\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6437","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6437\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6437\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6437\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6437","id":2001272606,"node_id":"I_kwDODunzps53SP8e","number":6437,"title":"Problem in training iterable dataset","user":{"login":"21Timothy","id":38107672,"node_id":"MDQ6VXNlcjM4MTA3Njcy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/38107672?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/21Timothy","html_url":"https:\/\/github.com\/21Timothy","followers_url":"https:\/\/api.github.com\/users\/21Timothy\/followers","following_url":"https:\/\/api.github.com\/users\/21Timothy\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/21Timothy\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/21Timothy\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/21Timothy\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/21Timothy\/orgs","repos_url":"https:\/\/api.github.com\/users\/21Timothy\/repos","events_url":"https:\/\/api.github.com\/users\/21Timothy\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/21Timothy\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-20T03:04:02Z","updated_at":"2023-11-29T11:11:15Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI am using PyTorch DDP (Distributed Data Parallel) to train my model. Since the data is too large to load into memory at once, I am using load_dataset to read the data as an iterable dataset. I have used datasets.distributed.split_dataset_by_node to distribute the dataset. However, I have noticed that this distribution results in different processes having different amounts of data to train on. As a result, when the earliest process finishes training and starts predicting on the test set, other processes are still training, causing the overall training speed to be very slow.\r\n\r\n### Steps to reproduce the bug\r\n```\r\ndef train(args, model, device, train_loader, optimizer, criterion, epoch, length):\r\n    model.train()\r\n    idx_length = 0\r\n    for batch_idx, data in enumerate(train_loader):\r\n        s_time = time.time()\r\n        X = data['X']\r\n        target = data['y'].reshape(-1, 28)\r\n        X, target = X.to(device), target.to(device)\r\n        optimizer.zero_grad()\r\n        output = model(X)\r\n        loss = criterion(output, target)\r\n        loss.backward()\r\n        optimizer.step()\r\n        idx_length += 1\r\n        if batch_idx % args.log_interval == 0:\r\n            # print('Train Epoch: {} Batch_idx: {} Process: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n            #     epoch, batch_idx, torch.distributed.get_rank(), batch_idx * len(X), length \/ torch.distributed.get_world_size(),\r\n            #                                          100. * batch_idx * len(\r\n            #                                              X) * torch.distributed.get_world_size() \/ length, loss.item()))\r\n            print('Train Epoch: {} Batch_idx: {} Process: {} [{}\/{} ({:.0f}%)]\\t'.format(\r\n                epoch, batch_idx, torch.distributed.get_rank(), batch_idx * len(X), length \/ torch.distributed.get_world_size(),\r\n                                                                100. * batch_idx * len(\r\n                                                                    X) * torch.distributed.get_world_size() \/ length))\r\n            if args.dry_run:\r\n                break\r\n    print('Process %s length: %s time: %s' % (torch.distributed.get_rank(), idx_length, datetime.datetime.now()))\r\n\r\ntrain_iterable_dataset = load_dataset(\"parquet\", data_files=data_files, split=\"train\", streaming=True)\r\ntest_iterable_dataset = load_dataset(\"parquet\", data_files=data_files, split=\"test\", streaming=True)\r\ntrain_iterable_dataset = train_iterable_dataset.map(process_fn)\r\ntest_iterable_dataset = test_iterable_dataset.map(process_fn)\r\ntrain_iterable_dataset = train_iterable_dataset.map(scale)\r\ntest_iterable_dataset = test_iterable_dataset.map(scale)\r\n\r\ntrain_iterable_dataset = datasets.distributed.split_dataset_by_node(train_iterable_dataset,\r\n                                                                    world_size=world_size, rank=local_rank).shuffle(seed=1234)\r\ntest_iterable_dataset = datasets.distributed.split_dataset_by_node(test_iterable_dataset,\r\n                                                                   world_size=world_size, rank=local_rank).shuffle(seed=1234)\r\nprint(torch.distributed.get_rank(), train_iterable_dataset.n_shards, test_iterable_dataset.n_shards)\r\n\r\ntrain_kwargs = {'batch_size': args.batch_size}\r\ntest_kwargs = {'batch_size': args.test_batch_size}\r\nif use_cuda:\r\n    cuda_kwargs = {'num_workers': 3,#ngpus_per_node,\r\n                   'pin_memory': True,\r\n                   'shuffle': False}\r\n    train_kwargs.update(cuda_kwargs)\r\n    test_kwargs.update(cuda_kwargs)\r\ntrain_loader = torch.utils.data.DataLoader(train_iterable_dataset, **train_kwargs,\r\n                                           # sampler=torch.utils.data.distributed.DistributedSampler(\r\n                                           #     train_iterable_dataset,\r\n                                           #     num_replicas=ngpus_per_node,\r\n                                           #     rank=0)\r\n                                           )\r\ntest_loader = torch.utils.data.DataLoader(test_iterable_dataset, **test_kwargs,\r\n                                          # sampler=torch.utils.data.distributed.DistributedSampler(\r\n                                          #     test_iterable_dataset,\r\n                                          #     num_replicas=ngpus_per_node,\r\n                                          #     rank=0)\r\n                                          )\r\nfor epoch in range(1, args.epochs + 1):\r\n    start_time = time.time()\r\n    train_iterable_dataset.set_epoch(epoch)\r\n    test_iterable_dataset.set_epoch(epoch)\r\n    train(args, model, device, train_loader, optimizer, criterion, epoch, train_len)\r\n    test(args, model, device, criterion2, test_loader)\r\n\r\n```\r\n\r\n\r\n\r\nAnd here\u2019s the part of output:\r\n```\r\nTrain Epoch: 1 Batch_idx: 5000 Process: 0 [320000\/4710975.0 (7%)]\t\r\nTrain Epoch: 1 Batch_idx: 5000 Process: 1 [320000\/4710975.0 (7%)]\t\r\nTrain Epoch: 1 Batch_idx: 5000 Process: 2 [320000\/4710975.0 (7%)]\t\r\nTrain Epoch: 1 Batch_idx: 5862 Process: 3 Data_length: 12 coststime: 0.04095172882080078\r\nTrain Epoch: 1 Batch_idx: 5862 Process: 0 Data_length: 3 coststime: 0.0751960277557373\r\nTrain Epoch: 1 Batch_idx: 5867 Process: 3 Data_length: 49 coststime: 0.0032558441162109375\r\nTrain Epoch: 1 Batch_idx: 5872 Process: 1 Data_length: 2 coststime: 0.022842884063720703\r\nTrain Epoch: 1 Batch_idx: 5876 Process: 3 Data_length: 63 coststime: 0.002694845199584961\r\nProcess 3 length: 5877 time: 2023-11-17 17:03:26.582317\r\nTrain epoch 1 costTime: 241.72063446044922s . Process 3 Start to test.\r\n3 0 tensor(45508.8516, device='cuda:3')\r\n3 100 tensor(45309.0469, device='cuda:3')\r\n3 200 tensor(45675.3047, device='cuda:3')\r\n3 300 tensor(45263.0273, device='cuda:3')\r\nProcess 3 Reduce metrics.\r\nTrain Epoch: 2 Batch_idx: 0 Process: 3 [0\/4710975.0 (0%)]\t\r\nTrain Epoch: 1 Batch_idx: 5882 Process: 1 Data_length: 63 coststime: 0.05185818672180176\r\nTrain Epoch: 1 Batch_idx: 5887 Process: 1 Data_length: 12 coststime: 0.006895303726196289\r\nProcess 1 length: 5888 time: 2023-11-17 17:20:48.578204\r\nTrain epoch 1 costTime: 1285.7279663085938s . Process 1 Start to test.\r\n1 0 tensor(45265.9141, device='cuda:1')\r\n```\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nI'd like to know how to fix this problem.\r\n\r\n### Environment info\r\n```\r\ntorch==2.0\r\ndatasets==2.14.0\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6437\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6437\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6436","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6436\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6436\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6436\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6436","id":2000844474,"node_id":"I_kwDODunzps53Qna6","number":6436,"title":"TypeError: <lambda>() takes 0 positional arguments but 1 was given","user":{"login":"ahmadmustafaanis","id":47111429,"node_id":"MDQ6VXNlcjQ3MTExNDI5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47111429?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ahmadmustafaanis","html_url":"https:\/\/github.com\/ahmadmustafaanis","followers_url":"https:\/\/api.github.com\/users\/ahmadmustafaanis\/followers","following_url":"https:\/\/api.github.com\/users\/ahmadmustafaanis\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ahmadmustafaanis\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ahmadmustafaanis\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ahmadmustafaanis\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ahmadmustafaanis\/orgs","repos_url":"https:\/\/api.github.com\/users\/ahmadmustafaanis\/repos","events_url":"https:\/\/api.github.com\/users\/ahmadmustafaanis\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ahmadmustafaanis\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-19T13:10:20Z","updated_at":"2023-11-29T16:28:34Z","closed_at":"2023-11-29T16:28:34Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-35-7b6becee3685>](https:\/\/localhost:8080\/#) in <cell line: 1>()\r\n----> 1 from datasets import Dataset\r\n\r\n9 frames\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/__init__.py](https:\/\/localhost:8080\/#) in <module>\r\n     20 __version__ = \"2.15.0\"\r\n     21 \r\n---> 22 from .arrow_dataset import Dataset\r\n     23 from .arrow_reader import ReadInstruction\r\n     24 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in <module>\r\n     61 import pyarrow.compute as pc\r\n     62 from huggingface_hub import CommitOperationAdd, CommitOperationDelete, DatasetCard, DatasetCardData, HfApi\r\n---> 63 from multiprocess import Pool\r\n     64 from requests import HTTPError\r\n     65 \r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/__init__.py](https:\/\/localhost:8080\/#) in <module>\r\n     31 \r\n     32 import sys\r\n---> 33 from . import context\r\n     34 \r\n     35 #\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/context.py](https:\/\/localhost:8080\/#) in <module>\r\n      4 \r\n      5 from . import process\r\n----> 6 from . import reduction\r\n      7 \r\n      8 __all__ = ()\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/reduction.py](https:\/\/localhost:8080\/#) in <module>\r\n     14 import os\r\n     15 try:\r\n---> 16     import dill as pickle\r\n     17 except ImportError:\r\n     18     import pickle\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/dill\/__init__.py](https:\/\/localhost:8080\/#) in <module>\r\n     24 \r\n     25 \r\n---> 26 from ._dill import (\r\n     27     dump, dumps, load, loads, copy,\r\n     28     Pickler, Unpickler, register, pickle, pickles, check,\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/dill\/_dill.py](https:\/\/localhost:8080\/#) in <module>\r\n    166 try:\r\n    167     from _pyio import open as _open\r\n--> 168     PyTextWrapperType = get_file_type('r', buffering=-1, open=_open)\r\n    169     PyBufferedRandomType = get_file_type('r+b', buffering=-1, open=_open)\r\n    170     PyBufferedReaderType = get_file_type('rb', buffering=-1, open=_open)\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/dill\/_dill.py](https:\/\/localhost:8080\/#) in get_file_type(*args, **kwargs)\r\n    154 def get_file_type(*args, **kwargs):\r\n    155     open = kwargs.pop(\"open\", __builtin__.open)\r\n--> 156     f = open(os.devnull, *args, **kwargs)\r\n    157     t = type(f)\r\n    158     f.close()\r\n\r\n[\/usr\/lib\/python3.10\/_pyio.py](https:\/\/localhost:8080\/#) in open(file, mode, buffering, encoding, errors, newline, closefd, opener)\r\n    280             return result\r\n    281         encoding = text_encoding(encoding)\r\n--> 282         text = TextIOWrapper(buffer, encoding, errors, newline, line_buffering)\r\n    283         result = text\r\n    284         text.mode = mode\r\n\r\n[\/usr\/lib\/python3.10\/_pyio.py](https:\/\/localhost:8080\/#) in __init__(self, buffer, encoding, errors, newline, line_buffering, write_through)\r\n   2043                 encoding = \"utf-8\"\r\n   2044             else:\r\n-> 2045                 encoding = locale.getpreferredencoding(False)\r\n   2046 \r\n   2047         if not isinstance(encoding, str):\r\n\r\nTypeError: <lambda>() takes 0 positional arguments but 1 was given\r\n```\r\n\r\nor \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-36-652e886d387f>](https:\/\/localhost:8080\/#) in <cell line: 1>()\r\n----> 1 import datasets\r\n\r\n9 frames\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/__init__.py](https:\/\/localhost:8080\/#) in <module>\r\n     20 __version__ = \"2.15.0\"\r\n     21 \r\n---> 22 from .arrow_dataset import Dataset\r\n     23 from .arrow_reader import ReadInstruction\r\n     24 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in <module>\r\n     61 import pyarrow.compute as pc\r\n     62 from huggingface_hub import CommitOperationAdd, CommitOperationDelete, DatasetCard, DatasetCardData, HfApi\r\n---> 63 from multiprocess import Pool\r\n     64 from requests import HTTPError\r\n     65 \r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/__init__.py](https:\/\/localhost:8080\/#) in <module>\r\n     31 \r\n     32 import sys\r\n---> 33 from . import context\r\n     34 \r\n     35 #\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/context.py](https:\/\/localhost:8080\/#) in <module>\r\n      4 \r\n      5 from . import process\r\n----> 6 from . import reduction\r\n      7 \r\n      8 __all__ = ()\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/reduction.py](https:\/\/localhost:8080\/#) in <module>\r\n     14 import os\r\n     15 try:\r\n---> 16     import dill as pickle\r\n     17 except ImportError:\r\n     18     import pickle\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/dill\/__init__.py](https:\/\/localhost:8080\/#) in <module>\r\n     24 \r\n     25 \r\n---> 26 from ._dill import (\r\n     27     dump, dumps, load, loads, copy,\r\n     28     Pickler, Unpickler, register, pickle, pickles, check,\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/dill\/_dill.py](https:\/\/localhost:8080\/#) in <module>\r\n    166 try:\r\n    167     from _pyio import open as _open\r\n--> 168     PyTextWrapperType = get_file_type('r', buffering=-1, open=_open)\r\n    169     PyBufferedRandomType = get_file_type('r+b', buffering=-1, open=_open)\r\n    170     PyBufferedReaderType = get_file_type('rb', buffering=-1, open=_open)\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/dill\/_dill.py](https:\/\/localhost:8080\/#) in get_file_type(*args, **kwargs)\r\n    154 def get_file_type(*args, **kwargs):\r\n    155     open = kwargs.pop(\"open\", __builtin__.open)\r\n--> 156     f = open(os.devnull, *args, **kwargs)\r\n    157     t = type(f)\r\n    158     f.close()\r\n\r\n[\/usr\/lib\/python3.10\/_pyio.py](https:\/\/localhost:8080\/#) in open(file, mode, buffering, encoding, errors, newline, closefd, opener)\r\n    280             return result\r\n    281         encoding = text_encoding(encoding)\r\n--> 282         text = TextIOWrapper(buffer, encoding, errors, newline, line_buffering)\r\n    283         result = text\r\n    284         text.mode = mode\r\n\r\n[\/usr\/lib\/python3.10\/_pyio.py](https:\/\/localhost:8080\/#) in __init__(self, buffer, encoding, errors, newline, line_buffering, write_through)\r\n   2043                 encoding = \"utf-8\"\r\n   2044             else:\r\n-> 2045                 encoding = locale.getpreferredencoding(False)\r\n   2046 \r\n   2047         if not isinstance(encoding, str):\r\n\r\nTypeError: <lambda>() takes 0 positional arguments but 1 was given\r\n```\n\n### Steps to reproduce the bug\n\n`import datasets` on colab\n\n### Expected behavior\n\nwork fine\n\n### Environment info\n\ncolab\r\n\r\n`!pip install datasets`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6436\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6436\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6435","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6435\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6435\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6435\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6435","id":2000690513,"node_id":"I_kwDODunzps53QB1R","number":6435,"title":"Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method","user":{"login":"kopyl","id":17604849,"node_id":"MDQ6VXNlcjE3NjA0ODQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17604849?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kopyl","html_url":"https:\/\/github.com\/kopyl","followers_url":"https:\/\/api.github.com\/users\/kopyl\/followers","following_url":"https:\/\/api.github.com\/users\/kopyl\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kopyl\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kopyl\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kopyl\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kopyl\/orgs","repos_url":"https:\/\/api.github.com\/users\/kopyl\/repos","events_url":"https:\/\/api.github.com\/users\/kopyl\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kopyl\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-19T04:21:16Z","updated_at":"2024-01-06T23:44:43Z","closed_at":"2023-12-04T16:57:43Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n1. I ran dataset mapping with `num_proc=6` in it and got this error:\r\n`RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method`\r\n\r\nI can't actually find a way to run multi-GPU dataset mapping. Can you help?\n\n### Steps to reproduce the bug\n\n1. Rund SDXL training with `num_proc=6`: https:\/\/github.com\/huggingface\/diffusers\/blob\/main\/examples\/text_to_image\/train_text_to_image_sdxl.py\n\n### Expected behavior\n\nShould work well\n\n### Environment info\n\n6x A100 SXM, Linux","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6435\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6435\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6434","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6434\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6434\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6434\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6434","id":1999554915,"node_id":"PR_kwDODunzps5fxgUO","number":6434,"title":"Use `ruff` for formatting","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-17T16:53:22Z","updated_at":"2023-11-21T14:19:21Z","closed_at":"2023-11-21T14:13:13Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Use `ruff` instead of `black` for formatting to be consistent with `transformers` ([PR](https:\/\/github.com\/huggingface\/transformers\/pull\/27144)) and `huggingface_hub` ([PR 1](https:\/\/github.com\/huggingface\/huggingface_hub\/pull\/1783) and [PR 2](https:\/\/github.com\/huggingface\/huggingface_hub\/pull\/1789)).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6434\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6434\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6434","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6434","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6434.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6434.patch","merged_at":"2023-11-21T14:13:13Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6433","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6433\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6433\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6433\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6433","id":1999419105,"node_id":"PR_kwDODunzps5fxDoG","number":6433,"title":"Better `tqdm` wrapper","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-11-17T15:45:15Z","updated_at":"2023-11-22T16:48:18Z","closed_at":"2023-11-22T16:42:08Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This PR aligns the `tqdm` logic with `huggingface_hub` (without introducing breaking changes), as the current one is error-prone.\r\n\r\nAdditionally, it improves the doc page about the `datasets`' utilities, and the handling of local `fsspec` paths in `cached_path`.\r\n \r\nFix #6409 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6433\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6433\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6433","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6433","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6433.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6433.patch","merged_at":"2023-11-22T16:42:08Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6432","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6432\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6432\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6432\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6432","id":1999258140,"node_id":"I_kwDODunzps53KkIc","number":6432,"title":"load_dataset does not load all of the data in my input file","user":{"login":"demongolem-biz2","id":121301001,"node_id":"U_kgDOBzroCQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121301001?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/demongolem-biz2","html_url":"https:\/\/github.com\/demongolem-biz2","followers_url":"https:\/\/api.github.com\/users\/demongolem-biz2\/followers","following_url":"https:\/\/api.github.com\/users\/demongolem-biz2\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/demongolem-biz2\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/demongolem-biz2\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/demongolem-biz2\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/demongolem-biz2\/orgs","repos_url":"https:\/\/api.github.com\/users\/demongolem-biz2\/repos","events_url":"https:\/\/api.github.com\/users\/demongolem-biz2\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/demongolem-biz2\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-17T14:28:50Z","updated_at":"2023-11-22T17:34:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI have 127 elements in my input dataset.  When I do a len on the dataset after loaded, it is only 124 elements.\n\n### Steps to reproduce the bug\n\n    train_dataset = nlp.load_dataset(data_args.dataset_path, name=data_args.qg_format, split=nlp.Split.TRAIN)\r\n    valid_dataset = nlp.load_dataset(data_args.dataset_path, name=data_args.qg_format, split=nlp.Split.VALIDATION)\r\n    logger.info(len(train_dataset))\r\n    logger.info(len(valid_dataset))\r\n\r\nBoth train and valid input are 127 items.  However, they both only load 124 items.  The input format is in json.  At the end of the day, I am trying to create .pt files.  \n\n### Expected behavior\n\nI see all 127 elements in my dataset when performing len\n\n### Environment info\n\nPython 3.10.  CentOS operating system.  nlp==0.40, datasets==2.14.5, transformers==4.26.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6432\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6432\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6431","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6431\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6431\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6431\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6431","id":1997202770,"node_id":"PR_kwDODunzps5fpfos","number":6431,"title":"Create DatasetNotFoundError and DataFilesNotFoundError","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2023-11-16T16:02:55Z","updated_at":"2023-11-22T15:18:51Z","closed_at":"2023-11-22T15:12:33Z","author_association":"MEMBER","active_lock_reason":null,"body":"Create `DatasetNotFoundError` and `DataFilesNotFoundError`.\r\n\r\nFix #6397.\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6431\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6431\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6431","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6431","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6431.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6431.patch","merged_at":"2023-11-22T15:12:33Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6429","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6429\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6429\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6429\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6429","id":1996723698,"node_id":"PR_kwDODunzps5fn1r_","number":6429,"title":"Add trust_remote_code argument","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":14,"created_at":"2023-11-16T12:12:54Z","updated_at":"2023-11-28T16:10:39Z","closed_at":"2023-11-28T16:03:43Z","author_association":"MEMBER","active_lock_reason":null,"body":"Draft about adding `trust_remote_code` to `load_dataset`.\r\n\r\n```python\r\nds = load_dataset(..., trust_remote_code=True)  # run remote code (current default)\r\n```\r\n\r\nIt would default to `True` (current behavior) and in the next major release it will prompt the user to check the code before running it (we'll communicate on this before doing it of course).\r\n\r\n```python\r\n# in the future\r\nds = load_dataset(...)  # prompt the user to check the code before running it (future default)\r\nds = load_dataset(..., trust_remote_code=True)  # run remote code\r\nds = load_dataset(..., trust_remote_code=False)  # disallow remote code\r\n```\r\n\r\nRelated to https:\/\/github.com\/huggingface\/datasets\/issues\/6400\r\n\r\nWill do a separate PR to use the parquet export when possible","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6429\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6429\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6429","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6429","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6429.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6429.patch","merged_at":"2023-11-28T16:03:43Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6428","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6428\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6428\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6428\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6428","id":1996306394,"node_id":"PR_kwDODunzps5fmakS","number":6428,"title":"Set dev version","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-16T08:12:55Z","updated_at":"2023-11-16T08:19:39Z","closed_at":"2023-11-16T08:13:28Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6428\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6428\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6428","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6428","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6428.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6428.patch","merged_at":"2023-11-16T08:13:28Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6427","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6427\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6427\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6427\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6427","id":1996248605,"node_id":"PR_kwDODunzps5fmN1_","number":6427,"title":"Release: 2.15.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-11-16T07:37:20Z","updated_at":"2023-11-16T08:12:12Z","closed_at":"2023-11-16T07:43:05Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6427\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6427\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6427","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6427","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6427.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6427.patch","merged_at":"2023-11-16T07:43:05Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6426","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6426\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6426\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6426\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6426","id":1995363264,"node_id":"PR_kwDODunzps5fjOEK","number":6426,"title":"More robust temporary directory deletion","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-11-15T19:06:42Z","updated_at":"2023-12-01T15:37:32Z","closed_at":"2023-12-01T15:31:19Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"While fixing the Windows errors in #6362, I noticed that `PermissionError` can still easily be thrown on the session exit by the temporary cache directory's finalizer (we would also have to keep track of intermediate datasets, copies, etc.). ~~Due to the low usage of `datasets` on Windows, this PR takes a simpler approach to the issue than https:\/\/github.com\/huggingface\/datasets\/pull\/2403 - it tries to delete the temporary cache directory, and if this fails, logs a warning message about using a `delete-temp-cache` CLI command to delete it manually. The problematic references are freed after the session exits, so the CLI command should then succeed.~~ This PR implements `Dataset.__setstate__` to register datasets with temporary cache files for deletion. \r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6426\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6426\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6426","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6426","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6426.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6426.patch","merged_at":"2023-12-01T15:31:19Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6425","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6425\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6425\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6425\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6425","id":1995269382,"node_id":"PR_kwDODunzps5fi5ye","number":6425,"title":"Fix deprecation warning when building conda package","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-15T18:00:11Z","updated_at":"2023-12-13T14:22:30Z","closed_at":"2023-12-13T14:16:00Z","author_association":"MEMBER","active_lock_reason":null,"body":"When building\/releasing conda package, we get this deprecation warning:\r\n```\r\n\/usr\/share\/miniconda\/envs\/build-datasets\/bin\/conda-build:11: DeprecationWarning: conda_build.cli.main_build.main is deprecated and will be removed in 4.0.0. Use `conda build` instead.\r\n```\r\n\r\nThis PR fixes the deprecation warning by using `conda build` instead.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6425\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6425\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6425","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6425","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6425.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6425.patch","merged_at":"2023-12-13T14:16:00Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6424","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6424\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6424\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6424\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6424","id":1995224516,"node_id":"PR_kwDODunzps5fiwDC","number":6424,"title":"[docs] troubleshooting guide","user":{"login":"MKhalusova","id":1065417,"node_id":"MDQ6VXNlcjEwNjU0MTc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1065417?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/MKhalusova","html_url":"https:\/\/github.com\/MKhalusova","followers_url":"https:\/\/api.github.com\/users\/MKhalusova\/followers","following_url":"https:\/\/api.github.com\/users\/MKhalusova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/MKhalusova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/MKhalusova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/MKhalusova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/MKhalusova\/orgs","repos_url":"https:\/\/api.github.com\/users\/MKhalusova\/repos","events_url":"https:\/\/api.github.com\/users\/MKhalusova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/MKhalusova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-15T17:28:14Z","updated_at":"2023-11-30T17:29:55Z","closed_at":"2023-11-30T17:23:46Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Hi all! This is a PR adding a troubleshooting guide for Datasets docs. \r\nI went through the library's GitHub Issues and Forum questions and identified a few issues that are common enough that I think it would be valuable to include them in the troubleshooting guide. These are: \r\n- creating a dataset from a folder and not following the required format\r\n- authentication issues when using `push_to_hub`\r\n- `Too Many Requests` with `push_to_hub`\r\n- Pickling issues when using Dataset.from_generator()\r\n\r\nThere's also a section on asking for help. Please let me know if there are other common issues or advice that we can include here. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6424\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6424\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6424","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6424","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6424.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6424.patch","merged_at":"2023-11-30T17:23:46Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6423","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6423\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6423\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6423\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6423","id":1994946847,"node_id":"PR_kwDODunzps5fhzD6","number":6423,"title":"Fix conda release by adding pyarrow-hotfix dependency","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-11-15T14:57:12Z","updated_at":"2023-11-15T17:15:33Z","closed_at":"2023-11-15T17:09:24Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix conda release by adding pyarrow-hotfix dependency.\r\n\r\nNote that conda release failed in latest 2.14.7 release: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/6874667214\/job\/18696761723\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/usr\/share\/miniconda\/envs\/build-datasets\/conda-bld\/datasets_1700036460222\/test_tmp\/run_test.py\", line 2, in <module>\r\n    import datasets\r\n  File \"\/usr\/share\/miniconda\/envs\/build-datasets\/conda-bld\/datasets_1700036460222\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold\/lib\/python3.12\/site-packages\/datasets\/__init__.py\", line 22, in <module>\r\n    from .arrow_dataset import Dataset\r\n  File \"\/usr\/share\/miniconda\/envs\/build-datasets\/conda-bld\/datasets_1700036460222\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold\/lib\/python3.12\/site-packages\/datasets\/arrow_dataset.py\", line 67, in <module>\r\n    from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n  File \"\/usr\/share\/miniconda\/envs\/build-datasets\/conda-bld\/datasets_1700036460222\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold\/lib\/python3.12\/site-packages\/datasets\/arrow_writer.py\", line 27, in <module>\r\n    from .features import Features, Image, Value\r\n  File \"\/usr\/share\/miniconda\/envs\/build-datasets\/conda-bld\/datasets_1700036460222\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold\/lib\/python3.12\/site-packages\/datasets\/features\/__init__.py\", line 18, in <module>\r\n    from .features import Array2D, Array3D, Array4D, Array5D, ClassLabel, Features, Sequence, Value\r\n  File \"\/usr\/share\/miniconda\/envs\/build-datasets\/conda-bld\/datasets_1700036460222\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold\/lib\/python3.12\/site-packages\/datasets\/features\/features.py\", line 34, in <module>\r\n    import pyarrow_hotfix  # noqa: F401  # to fix vulnerability on pyarrow<14.0.1\r\n    ^^^^^^^^^^^^^^^^^^^^^\r\nModuleNotFoundError: No module named 'pyarrow_hotfix'\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6423\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6423\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6423","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6423","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6423.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6423.patch","merged_at":"2023-11-15T17:09:24Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6422","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6422\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6422\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6422\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6422","id":1994579267,"node_id":"I_kwDODunzps524t1D","number":6422,"title":"Allow to choose the `writer_batch_size` when using `save_to_disk`","user":{"login":"NathanGodey","id":38216711,"node_id":"MDQ6VXNlcjM4MjE2NzEx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/38216711?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NathanGodey","html_url":"https:\/\/github.com\/NathanGodey","followers_url":"https:\/\/api.github.com\/users\/NathanGodey\/followers","following_url":"https:\/\/api.github.com\/users\/NathanGodey\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NathanGodey\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NathanGodey\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NathanGodey\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NathanGodey\/orgs","repos_url":"https:\/\/api.github.com\/users\/NathanGodey\/repos","events_url":"https:\/\/api.github.com\/users\/NathanGodey\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NathanGodey\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-15T11:18:34Z","updated_at":"2023-11-16T10:00:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nAdd an argument in `save_to_disk` regarding batch size, which would be passed to `shard` and other methods.\n\n### Motivation\n\nThe `Dataset.save_to_disk` method currently calls `shard` without passing a `writer_batch_size` argument, thus implicitly using the default value (1000). This can result in RAM saturation when using a lot of processes on long text sequences or other modalities, or for specific IO configs.\n\n### Your contribution\n\nI would be glad to submit a PR, as long as it does not imply extensive tests refactoring.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6422\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6422\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6421","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6421\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6421\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6421\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6421","id":1994451553,"node_id":"PR_kwDODunzps5fgG1h","number":6421,"title":"Add pyarrow-hotfix to release docs","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":4296013012,"node_id":"LA_kwDODunzps8AAAABAA_01A","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/maintenance","name":"maintenance","color":"d4c5f9","default":false,"description":"Maintenance tasks"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-15T10:06:44Z","updated_at":"2023-11-15T13:49:55Z","closed_at":"2023-11-15T13:38:22Z","author_association":"MEMBER","active_lock_reason":null,"body":"Add `pyarrow-hotfix` to release docs.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6421\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6421\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6421","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6421","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6421.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6421.patch","merged_at":"2023-11-15T13:38:22Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6420","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6420\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6420\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6420\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6420","id":1994278903,"node_id":"PR_kwDODunzps5ffhdi","number":6420,"title":"Set dev version","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-15T08:22:19Z","updated_at":"2023-11-15T08:33:36Z","closed_at":"2023-11-15T08:22:33Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6420\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6420\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6420","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6420","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6420.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6420.patch","merged_at":"2023-11-15T08:22:33Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6419","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6419\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6419\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6419\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6419","id":1994257873,"node_id":"PR_kwDODunzps5ffc7d","number":6419,"title":"Release: 2.14.7","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-11-15T08:07:37Z","updated_at":"2023-11-15T17:35:30Z","closed_at":"2023-11-15T08:12:59Z","author_association":"MEMBER","active_lock_reason":null,"body":"Release 2.14.7.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6419\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6419\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6419","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6419","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6419.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6419.patch","merged_at":"2023-11-15T08:12:59Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6418","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6418\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6418\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6418\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6418","id":1993224629,"node_id":"PR_kwDODunzps5fb7lu","number":6418,"title":"Remove token value from warnings","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-14T17:34:06Z","updated_at":"2023-11-14T22:26:04Z","closed_at":"2023-11-14T22:19:45Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6412","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6418\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6418\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6418","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6418","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6418.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6418.patch","merged_at":"2023-11-14T22:19:45Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6417","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6417\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6417\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6417\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6417","id":1993149416,"node_id":"I_kwDODunzps52zQvo","number":6417,"title":" Bug: LayoutLMv3 finetuning on FUNSD Notebook; Arrow Error","user":{"login":"Davo00","id":57496007,"node_id":"MDQ6VXNlcjU3NDk2MDA3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57496007?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Davo00","html_url":"https:\/\/github.com\/Davo00","followers_url":"https:\/\/api.github.com\/users\/Davo00\/followers","following_url":"https:\/\/api.github.com\/users\/Davo00\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Davo00\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Davo00\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Davo00\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Davo00\/orgs","repos_url":"https:\/\/api.github.com\/users\/Davo00\/repos","events_url":"https:\/\/api.github.com\/users\/Davo00\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Davo00\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-14T16:53:20Z","updated_at":"2023-11-16T20:23:41Z","closed_at":"2023-11-16T20:23:41Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nArrow issues when running the example Notebook laptop locally on Mac with M1. Works on Google Collab.\r\n\r\n**Notebook**: https:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/LayoutLMv3\/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb\r\n\r\n**Error**: `ValueError: Arrow type extension<arrow.py_extension_type<pyarrow.lib.UnknownExtensionType>> does not have a datasets dtype equivalent.`\r\n\r\n**Caused by**:\r\n\r\n```\r\n# we need to define custom features for `set_format` (used later on) to work properly\r\nfeatures = Features({\r\n    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\r\n    'input_ids': Sequence(feature=Value(dtype='int64')),\r\n    'attention_mask': Sequence(Value(dtype='int64')),\r\n    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\r\n    'labels': Sequence(feature=Value(dtype='int64')),\r\n})\r\n```\n\n### Steps to reproduce the bug\n\nRun the notebook provided, locally. If possible also on M1.\n\n### Expected behavior\n\nThe cell where features are mapped to Array2D and Array3D should work without any issues.\n\n### Environment info\n\nTried with Python 3.9 and 3.10 conda envs. Running Mac M1.\r\n\r\n`pip show datasets`\r\n\r\n> Name: datasets\r\nVersion: 2.14.6\r\nSummary: HuggingFace community-driven open-source library of datasets\r\n\r\n`pip list`\r\n\r\n> Package                   Version\r\n> ------------------------- ------------\r\n> accelerate                0.24.1\r\n> aiohttp                   3.8.6\r\n> aiosignal                 1.3.1\r\n> anyio                     3.5.0\r\n> appnope                   0.1.2\r\n> argon2-cffi               21.3.0\r\n> argon2-cffi-bindings      21.2.0\r\n> asttokens                 2.0.5\r\n> async-timeout             4.0.3\r\n> attrs                     23.1.0\r\n> backcall                  0.2.0\r\n> beautifulsoup4            4.12.2\r\n> bleach                    4.1.0\r\n> certifi                   2023.7.22\r\n> cffi                      1.15.1\r\n> charset-normalizer        3.3.2\r\n> comm                      0.1.2\r\n> datasets                  2.14.6\r\n> debugpy                   1.6.7\r\n> decorator                 5.1.1\r\n> defusedxml                0.7.1\r\n> dill                      0.3.7\r\n> entrypoints               0.4\r\n> exceptiongroup            1.0.4\r\n> executing                 0.8.3\r\n> fastjsonschema            2.16.2\r\n> filelock                  3.13.1\r\n> frozenlist                1.4.0\r\n> fsspec                    2023.10.0\r\n> huggingface-hub           0.17.3\r\n> idna                      3.4\r\n> importlib-metadata        6.0.0\r\n> IProgress                 0.4\r\n> ipykernel                 6.25.0\r\n> ipython                   8.15.0\r\n> ipython-genutils          0.2.0\r\n> jedi                      0.18.1\r\n> Jinja2                    3.1.2\r\n> joblib                    1.3.2\r\n> jsonschema                4.19.2\r\n> jsonschema-specifications 2023.7.1\r\n> jupyter_client            7.4.9\r\n> jupyter_core              5.5.0\r\n> jupyter-server            1.23.4\r\n> jupyterlab-pygments       0.1.2\r\n> MarkupSafe                2.1.1\r\n> matplotlib-inline         0.1.6\r\n> mistune                   2.0.4\r\n> mpmath                    1.3.0\r\n> multidict                 6.0.4\r\n> multiprocess              0.70.15\r\n> nbclassic                 1.0.0\r\n> nbclient                  0.8.0\r\n> nbconvert                 7.10.0\r\n> nbformat                  5.9.2\r\n> nest-asyncio              1.5.6\r\n> networkx                  3.2.1\r\n> notebook                  6.5.4\r\n> notebook_shim             0.2.3\r\n> numpy                     1.26.1\r\n> packaging                 23.1\r\n> pandas                    2.1.3\r\n> pandocfilters             1.5.0\r\n> parso                     0.8.3\r\n> pexpect                   4.8.0\r\n> pickleshare               0.7.5\r\n> Pillow                    10.1.0\r\n> pip                       23.3\r\n> platformdirs              3.10.0\r\n> prometheus-client         0.14.1\r\n> prompt-toolkit            3.0.36\r\n> psutil                    5.9.0\r\n> ptyprocess                0.7.0\r\n> pure-eval                 0.2.2\r\n> pyarrow                   14.0.1\r\n> pycparser                 2.21\r\n> Pygments                  2.15.1\r\n> python-dateutil           2.8.2\r\n> pytz                      2023.3.post1\r\n> PyYAML                    6.0.1\r\n> pyzmq                     23.2.0\r\n> referencing               0.30.2\r\n> regex                     2023.10.3\r\n> requests                  2.31.0\r\n> rpds-py                   0.10.6\r\n> safetensors               0.4.0\r\n> scikit-learn              1.3.2\r\n> scipy                     1.11.3\r\n> Send2Trash                1.8.2\r\n> seqeval                   1.2.2\r\n> setuptools                68.0.0\r\n> six                       1.16.0\r\n> sniffio                   1.2.0\r\n> soupsieve                 2.5\r\n> stack-data                0.2.0\r\n> sympy                     1.12\r\n> terminado                 0.17.1\r\n> threadpoolctl             3.2.0\r\n> tinycss2                  1.2.1\r\n> tokenizers                0.14.1\r\n> torch                     2.1.0\r\n> tornado                   6.3.3\r\n> tqdm                      4.66.1\r\n> traitlets                 5.7.1\r\n> transformers              4.36.0.dev0\r\n> typing_extensions         4.7.1\r\n> tzdata                    2023.3\r\n> urllib3                   2.0.7\r\n> wcwidth                   0.2.5\r\n> webencodings              0.5.1\r\n> websocket-client          0.58.0\r\n> wheel                     0.41.2\r\n> xxhash                    3.4.1\r\n> yarl                      1.9.2\r\n> zipp                      3.11.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6417\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6417\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6416","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6416\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6416\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6416\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6416","id":1992954723,"node_id":"PR_kwDODunzps5fbA4H","number":6416,"title":"Rename audio_classificiation.py to audio_classification.py","user":{"login":"carlthome","id":1595907,"node_id":"MDQ6VXNlcjE1OTU5MDc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1595907?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/carlthome","html_url":"https:\/\/github.com\/carlthome","followers_url":"https:\/\/api.github.com\/users\/carlthome\/followers","following_url":"https:\/\/api.github.com\/users\/carlthome\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/carlthome\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/carlthome\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/carlthome\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/carlthome\/orgs","repos_url":"https:\/\/api.github.com\/users\/carlthome\/repos","events_url":"https:\/\/api.github.com\/users\/carlthome\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/carlthome\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-11-14T15:15:29Z","updated_at":"2023-11-15T11:59:32Z","closed_at":"2023-11-15T11:53:20Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6416\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6416\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6416","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6416","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6416.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6416.patch","merged_at":"2023-11-15T11:53:20Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6415","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6415\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6415\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6415\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6415","id":1992917248,"node_id":"PR_kwDODunzps5fa4n7","number":6415,"title":"Fix multi gpu map example","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2023-11-14T14:57:18Z","updated_at":"2024-01-02T15:13:09Z","closed_at":"2023-11-22T15:42:19Z","author_association":"MEMBER","active_lock_reason":null,"body":"- use `orch.cuda.set_device` instead of `CUDA_VISIBLE_DEVICES `\r\n- add `if __name__ == \"__main__\"`\r\n\r\nfix https:\/\/github.com\/huggingface\/datasets\/issues\/6186","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6415\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6415\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6415","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6415","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6415.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6415.patch","merged_at":"2023-11-22T15:42:19Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6414","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6414\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6414\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6414\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6414","id":1992482491,"node_id":"PR_kwDODunzps5fZZ2l","number":6414,"title":"Set `usedforsecurity=False` in hashlib methods (FIPS compliance)","user":{"login":"Wauplin","id":11801849,"node_id":"MDQ6VXNlcjExODAxODQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11801849?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Wauplin","html_url":"https:\/\/github.com\/Wauplin","followers_url":"https:\/\/api.github.com\/users\/Wauplin\/followers","following_url":"https:\/\/api.github.com\/users\/Wauplin\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Wauplin\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Wauplin\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Wauplin\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Wauplin\/orgs","repos_url":"https:\/\/api.github.com\/users\/Wauplin\/repos","events_url":"https:\/\/api.github.com\/users\/Wauplin\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Wauplin\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2023-11-14T10:47:09Z","updated_at":"2023-11-17T14:23:20Z","closed_at":"2023-11-17T14:17:00Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Related to https:\/\/github.com\/huggingface\/transformers\/issues\/27034 and https:\/\/github.com\/huggingface\/huggingface_hub\/pull\/1782.\r\n\r\n**TL;DR:** `hashlib` is not a secure library for cryptography-related stuff. We are only using `hashlib` for non-security-related purposes in `datasets` so it's fine. From Python 3.9 we set can `usedforsecurity=False` in any `hashlib` method which is mandatory for companies that forbid the use of `hashlib` for security purposes. This PR fixes that.\r\n\r\n**Note:** before merging this we need to release a new tokenizers version that would allow the newest `huggingface_hub` version (see https:\/\/github.com\/huggingface\/tokenizers\/pull\/1385). Otherwise it might create friction to users that want to install `datasets` + `tokenizers` at the same time.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6414\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6414\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6414","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6414","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6414.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6414.patch","merged_at":"2023-11-17T14:17:00Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6412","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6412\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6412\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6412\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6412","id":1992401594,"node_id":"I_kwDODunzps52waK6","number":6412,"title":"User token is printed out!","user":{"login":"mohsen-goodarzi","id":25702692,"node_id":"MDQ6VXNlcjI1NzAyNjky","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/25702692?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mohsen-goodarzi","html_url":"https:\/\/github.com\/mohsen-goodarzi","followers_url":"https:\/\/api.github.com\/users\/mohsen-goodarzi\/followers","following_url":"https:\/\/api.github.com\/users\/mohsen-goodarzi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mohsen-goodarzi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mohsen-goodarzi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mohsen-goodarzi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mohsen-goodarzi\/orgs","repos_url":"https:\/\/api.github.com\/users\/mohsen-goodarzi\/repos","events_url":"https:\/\/api.github.com\/users\/mohsen-goodarzi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mohsen-goodarzi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-14T10:01:34Z","updated_at":"2023-11-14T22:19:46Z","closed_at":"2023-11-14T22:19:46Z","author_association":"NONE","active_lock_reason":null,"body":"This line prints user token on command line! Is it safe?\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/12ebe695b4748c5a26e08b44ed51955f74f5801d\/src\/datasets\/load.py#L2091","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6412\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6412\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6411","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6411\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6411\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6411\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6411","id":1992386630,"node_id":"PR_kwDODunzps5fZE9F","number":6411,"title":"Fix dependency conflict within CI build documentation","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-14T09:52:51Z","updated_at":"2023-11-14T10:05:59Z","closed_at":"2023-11-14T10:05:35Z","author_association":"MEMBER","active_lock_reason":null,"body":"Manually fix dependency conflict on `typing-extensions` version originated by `apache-beam` + `pydantic` (now a dependency of `huggingface-hub`).\r\nThis is a temporary hot fix of our CI build documentation until we stop using `apache-beam`.\r\nFix #6406.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6411\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6411\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6411","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6411","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6411.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6411.patch","merged_at":"2023-11-14T10:05:34Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6410","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6410\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6410\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6410\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6410","id":1992100209,"node_id":"I_kwDODunzps52vQlx","number":6410,"title":"Datasets does not load HuggingFace Repository properly","user":{"login":"MikeDoes","id":40600201,"node_id":"MDQ6VXNlcjQwNjAwMjAx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/40600201?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/MikeDoes","html_url":"https:\/\/github.com\/MikeDoes","followers_url":"https:\/\/api.github.com\/users\/MikeDoes\/followers","following_url":"https:\/\/api.github.com\/users\/MikeDoes\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/MikeDoes\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/MikeDoes\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/MikeDoes\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/MikeDoes\/orgs","repos_url":"https:\/\/api.github.com\/users\/MikeDoes\/repos","events_url":"https:\/\/api.github.com\/users\/MikeDoes\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/MikeDoes\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-14T06:50:49Z","updated_at":"2023-11-16T06:54:36Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nDear Datasets team,\r\n\r\nWe just have published a dataset on Huggingface:\r\nhttps:\/\/huggingface.co\/ai4privacy\r\n\r\nHowever, when trying to read it using the Dataset library we get an error. As I understand jsonl files are compatible, could you please clarify how we can solve the issue? Please let me know and we would be more than happy to adapt the structure of the repository or meta data so it works easier:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"ai4privacy\/pii-masking-200k\")\r\n```\r\n\r\n```\r\nDownloading readme: 100%\r\n11.8k\/11.8k [00:00<00:00, 512kB\/s]\r\nDownloading data files: 100%\r\n1\/1 [00:11<00:00, 11.16s\/it]\r\nDownloading data: 100%\r\n64.3M\/64.3M [00:02<00:00, 32.9MB\/s]\r\nDownloading data: 100%\r\n113M\/113M [00:03<00:00, 35.0MB\/s]\r\nDownloading data: 100%\r\n97.7M\/97.7M [00:02<00:00, 46.1MB\/s]\r\nDownloading data: 100%\r\n90.8M\/90.8M [00:02<00:00, 44.9MB\/s]\r\nDownloading data: 100%\r\n7.63k\/7.63k [00:00<00:00, 41.0kB\/s]\r\nDownloading data: 100%\r\n1.03k\/1.03k [00:00<00:00, 9.44kB\/s]\r\nExtracting data files: 100%\r\n1\/1 [00:00<00:00, 29.26it\/s]\r\nGenerating train split:\r\n209261\/0 [00:05<00:00, 41201.25 examples\/s]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/builder.py](https:\/\/localhost:8080\/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1939                         )\r\n-> 1940                     writer.write_table(table)\r\n   1941                     num_examples_progress_update += len(table)\r\n\r\n8 frames\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_writer.py](https:\/\/localhost:8080\/#) in write_table(self, pa_table, writer_batch_size)\r\n    571         pa_table = pa_table.combine_chunks()\r\n--> 572         pa_table = table_cast(pa_table, self._schema)\r\n    573         if self.embed_local_files:\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/table.py](https:\/\/localhost:8080\/#) in table_cast(table, schema)\r\n   2327     if table.schema != schema:\r\n-> 2328         return cast_table_to_schema(table, schema)\r\n   2329     elif table.schema.metadata != schema.metadata:\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/table.py](https:\/\/localhost:8080\/#) in cast_table_to_schema(table, schema)\r\n   2285     if sorted(table.column_names) != sorted(features):\r\n-> 2286         raise ValueError(f\"Couldn't cast\\n{table.schema}\\nto\\n{features}\\nbecause column names don't match\")\r\n   2287     arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n\r\nValueError: Couldn't cast\r\nJOBTYPE: int64\r\nPHONEIMEI: int64\r\nACCOUNTNAME: int64\r\nVEHICLEVIN: int64\r\nGENDER: int64\r\nCURRENCYCODE: int64\r\nCREDITCARDISSUER: int64\r\nJOBTITLE: int64\r\nSEX: int64\r\nCURRENCYSYMBOL: int64\r\nIP: int64\r\nEYECOLOR: int64\r\nMASKEDNUMBER: int64\r\nSECONDARYADDRESS: int64\r\nJOBAREA: int64\r\nACCOUNTNUMBER: int64\r\nlanguage: string\r\nBITCOINADDRESS: int64\r\nMAC: int64\r\nSSN: int64\r\nEMAIL: int64\r\nETHEREUMADDRESS: int64\r\nDOB: int64\r\nVEHICLEVRM: int64\r\nIPV6: int64\r\nAMOUNT: int64\r\nURL: int64\r\nPHONENUMBER: int64\r\nPIN: int64\r\nTIME: int64\r\nCREDITCARDNUMBER: int64\r\nFIRSTNAME: int64\r\nIBAN: int64\r\nBIC: int64\r\nCOUNTY: int64\r\nSTATE: int64\r\nLASTNAME: int64\r\nZIPCODE: int64\r\nHEIGHT: int64\r\nORDINALDIRECTION: int64\r\nMIDDLENAME: int64\r\nSTREET: int64\r\nUSERNAME: int64\r\nCURRENCY: int64\r\nPREFIX: int64\r\nUSERAGENT: int64\r\nCURRENCYNAME: int64\r\nLITECOINADDRESS: int64\r\nCREDITCARDCVV: int64\r\nAGE: int64\r\nCITY: int64\r\nPASSWORD: int64\r\nBUILDINGNUMBER: int64\r\nIPV4: int64\r\nNEARBYGPSCOORDINATE: int64\r\nDATE: int64\r\nCOMPANYNAME: int64\r\nto\r\n{'masked_text': Value(dtype='string', id=None), 'unmasked_text': Value(dtype='string', id=None), 'privacy_mask': Value(dtype='string', id=None), 'span_labels': Value(dtype='string', id=None), 'bio_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'tokenised_text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\n[<ipython-input-2-f1c6811e9c83>](https:\/\/localhost:8080\/#) in <cell line: 3>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 dataset = load_dataset(\"ai4privacy\/pii-masking-200k\")\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py](https:\/\/localhost:8080\/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2151 \r\n   2152     # Download and prepare data\r\n-> 2153     builder_instance.download_and_prepare(\r\n   2154         download_config=download_config,\r\n   2155         download_mode=download_mode,\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/builder.py](https:\/\/localhost:8080\/#) in download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    952                         if num_proc is not None:\r\n    953                             prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 954                         self._download_and_prepare(\r\n    955                             dl_manager=dl_manager,\r\n    956                             verification_mode=verification_mode,\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/builder.py](https:\/\/localhost:8080\/#) in _download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   1047             try:\r\n   1048                 # Prepare split will record examples associated to the split\r\n-> 1049                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n   1050             except OSError as e:\r\n   1051                 raise OSError(\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/builder.py](https:\/\/localhost:8080\/#) in _prepare_split(self, split_generator, file_format, num_proc, max_shard_size)\r\n   1811             job_id = 0\r\n   1812             with pbar:\r\n-> 1813                 for job_id, done, content in self._prepare_split_single(\r\n   1814                     gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n   1815                 ):\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/builder.py](https:\/\/localhost:8080\/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1956             if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\r\n   1957                 e = e.__context__\r\n-> 1958             raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1959 \r\n   1960         yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\nThank you and have a great day ahead\r\n\r\n### Steps to reproduce the bug\r\n\r\nOpen Google Colab Notebook:\r\n\r\nRun command:\r\n!pip3 install datasets\r\n\r\nRun code:\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"ai4privacy\/pii-masking-200k\")\r\n\r\n### Expected behavior\r\n\r\nDownload the dataset successfully from HuggingFace to the notebook so that we can start working with it\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.6\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.19.1\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6410\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6410\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6409","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6409\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6409\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6409\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6409","id":1991960865,"node_id":"I_kwDODunzps52uukh","number":6409,"title":"using DownloadManager to download from local filesystem and disable_progress_bar, there will be an exception ","user":{"login":"neiblegy","id":16574677,"node_id":"MDQ6VXNlcjE2NTc0Njc3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16574677?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/neiblegy","html_url":"https:\/\/github.com\/neiblegy","followers_url":"https:\/\/api.github.com\/users\/neiblegy\/followers","following_url":"https:\/\/api.github.com\/users\/neiblegy\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/neiblegy\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/neiblegy\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/neiblegy\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/neiblegy\/orgs","repos_url":"https:\/\/api.github.com\/users\/neiblegy\/repos","events_url":"https:\/\/api.github.com\/users\/neiblegy\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/neiblegy\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-14T04:21:01Z","updated_at":"2023-11-22T16:42:09Z","closed_at":"2023-11-22T16:42:09Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\ni'm using datasets.download.download_manager.DownloadManager to download files like \"file:\/\/\/a\/b\/c.txt\", and i disable_progress_bar() to disable bar. there will be an exception as follows:\r\n\r\n`AttributeError: 'function' object has no attribute 'close'\r\nException ignored in: <function TqdmCallback.__del__ at 0x7fa8683d84c0>\r\nTraceback (most recent call last):\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/fsspec\/callbacks.py\", line 233, in __del__\r\n    self.tqdm.close()`\r\n\r\ni check your source code in datasets\/utils\/file_utils.py:348 you define TqdmCallback derive from fsspec.callbacks.TqdmCallback\r\n\r\nbut in the newest fsspec code [https:\/\/github.com\/fsspec\/filesystem_spec\/blob\/master\/fsspec\/callbacks.py](url) , line 146, in this case, _DEFAULT_CALLBACK will take effect, but in line 234, it calls \"close()\" function which _DEFAULT_CALLBACK don't have such thing.\r\n\r\nso i think the class \"TqdmCallback\" in datasets\/utils\/file_utils.py may override \"__del__\" function or report this bug to fsspec.\r\n\r\n\n\n### Steps to reproduce the bug\n\nas i said\n\n### Expected behavior\n\nno exception\n\n### Environment info\n\ndatasets: 2.14.4\r\npython: 3.9\r\nplatform: x86_64","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6409\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6409\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6408","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6408\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6408\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6408\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6408","id":1991902972,"node_id":"I_kwDODunzps52ugb8","number":6408,"title":"`IterableDataset` lost but not keep columns when map function adding columns with names in `remove_columns`","user":{"login":"shmily326","id":24571857,"node_id":"MDQ6VXNlcjI0NTcxODU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24571857?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shmily326","html_url":"https:\/\/github.com\/shmily326","followers_url":"https:\/\/api.github.com\/users\/shmily326\/followers","following_url":"https:\/\/api.github.com\/users\/shmily326\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shmily326\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shmily326\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shmily326\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shmily326\/orgs","repos_url":"https:\/\/api.github.com\/users\/shmily326\/repos","events_url":"https:\/\/api.github.com\/users\/shmily326\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shmily326\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-14T03:12:08Z","updated_at":"2023-11-16T06:24:10Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nIterableDataset lost but not keep columns when map function adding columns with names in remove_columns,\r\nDataset not.\r\n\r\nMay be related to the code below:\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/06c3ffb8d068b6307b247164b10f7c7311cefed4\/src\/datasets\/iterable_dataset.py#L750-L756\n\n### Steps to reproduce the bug\n\n```python\r\ndataset: IterableDataset = load_dataset(\"Anthropic\/hh-rlhf\", streaming=True, split=\"train\")\r\ncolumn_names = list(next(iter(dataset)).keys())  #  ['chosen', 'rejected']\r\n# map_fn will return dict {\"chosen\": xxx, \"rejected\": xxx, \"prompt\": xxx, \"history\": xxxx}\r\ndataset = dataset.map(map_fn, batched=True, remove_columns=column_names)\r\nnext(iter(dataset))\r\n# output\r\n# {'prompt': 'xxx, 'history': xxx}   \r\n```\r\n\r\n```python\r\n# when load_dataset with streaming=False, the column_names are kept: \r\ndataset: Dataset = load_dataset(\"Anthropic\/hh-rlhf\", streaming=False, split=\"train\")\r\ncolumn_names = list(next(iter(dataset)).keys())  #  ['chosen', 'rejected']\r\n# map_fn will return dict {\"chosen\": xxx, \"rejected\": xxx, \"prompt\": xxx, \"history\": xxxx}\r\ndataset = dataset.map(map_fn, batched=True, remove_columns=column_names)\r\nnext(iter(dataset))\r\n# output\r\n# {'prompt': 'xxx, 'history': xxx, \"chosen\": xxx, \"rejected\": xxx}   \r\n```\r\n\n\n### Expected behavior\n\nIterableDataset keep columns when map function adding columns with names in remove_columns\n\n### Environment info\n\ndatasets==2.14.6","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6408\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6408\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6407","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6407\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6407\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6407\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6407","id":1991514079,"node_id":"I_kwDODunzps52tBff","number":6407,"title":"Loading the dataset from private S3 bucket gives \"TypeError: cannot pickle '_contextvars.Context' object\"","user":{"login":"eawer","id":1741779,"node_id":"MDQ6VXNlcjE3NDE3Nzk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1741779?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/eawer","html_url":"https:\/\/github.com\/eawer","followers_url":"https:\/\/api.github.com\/users\/eawer\/followers","following_url":"https:\/\/api.github.com\/users\/eawer\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/eawer\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/eawer\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/eawer\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/eawer\/orgs","repos_url":"https:\/\/api.github.com\/users\/eawer\/repos","events_url":"https:\/\/api.github.com\/users\/eawer\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/eawer\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-13T21:27:43Z","updated_at":"2023-11-13T21:27:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm trying to read the parquet file from the private s3 bucket using the `load_dataset` function, but I receive `TypeError: cannot pickle '_contextvars.Context' object` error\r\n\r\nI'm working on a machine with `~\/.aws\/credentials` file. I can't give credentials and the path to a file in a private bucket for obvious reasons, but I'll try to give all possible outputs. \n\n### Steps to reproduce the bug\n\n```python\r\nimport s3fs\r\nfrom datasets import load_dataset\r\nfrom aiobotocore.session import get_session\r\n\r\n\r\nDATA_PATH = \"s3:\/\/bucket_name\/path\/validation.parquet\"\r\nfs = s3fs.S3FileSystem(session=get_session())\r\n```\r\n`fs.stat` returns the data, so we can say that fs is working and we have all permissions\r\n```python\r\nfs.stat(DATA_PATH)\r\n# Returns:\r\n# {'ETag': '\"123123a-19\"',\r\n# 'LastModified': datetime.datetime(2023, 11, 1, 10, 16, 57, tzinfo=tzutc()),\r\n# 'size': 312237170,\r\n# 'name': 'bucket_name\/path\/validation.parquet',\r\n# 'type': 'file',\r\n# 'StorageClass': 'STANDARD',\r\n# 'VersionId': 'Abc.HtmsC9h.as',\r\n# 'ContentType': 'binary\/octet-stream'}\r\n```\r\n\r\n```python\r\nfs.storage_options\r\n# Returns:\r\n# {'session': <aiobotocore.session.AioSession at 0x7f9193fa53c0>}\r\n```\r\n\r\n```python\r\nds = load_dataset(\"parquet\", data_files={\"train\": DATA_PATH}, storage_options=fs.storage_options)\r\n```\r\n\r\n<details>\r\n  <summary>Returns such error (expandable)<\/summary>\r\n  \r\n```python\r\n  ---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[88], line 1\r\n----> 1 ds = load_dataset(\"parquet\", data_files={\"train\": DATA_PATH}, storage_options=fs.storage_options)\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/site-packages\/datasets\/load.py:2153, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2150 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\r\n   2152 # Download and prepare data\r\n-> 2153 builder_instance.download_and_prepare(\r\n   2154     download_config=download_config,\r\n   2155     download_mode=download_mode,\r\n   2156     verification_mode=verification_mode,\r\n   2157     try_from_hf_gcs=try_from_hf_gcs,\r\n   2158     num_proc=num_proc,\r\n   2159     storage_options=storage_options,\r\n   2160 )\r\n   2162 # Build dataset for splits\r\n   2163 keep_in_memory = (\r\n   2164     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   2165 )\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/site-packages\/datasets\/builder.py:954, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    952     if num_proc is not None:\r\n    953         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 954     self._download_and_prepare(\r\n    955         dl_manager=dl_manager,\r\n    956         verification_mode=verification_mode,\r\n    957         **prepare_split_kwargs,\r\n    958         **download_and_prepare_kwargs,\r\n    959     )\r\n    960 # Sync info\r\n    961 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/site-packages\/datasets\/builder.py:1027, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   1025 split_dict = SplitDict(dataset_name=self.dataset_name)\r\n   1026 split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n-> 1027 split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n   1029 # Checksums verification\r\n   1030 if verification_mode == VerificationMode.ALL_CHECKS and dl_manager.record_checksums:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/parquet\/parquet.py:34, in Parquet._split_generators(self, dl_manager)\r\n     32 if not self.config.data_files:\r\n     33     raise ValueError(f\"At least one data file must be specified, but got data_files={self.config.data_files}\")\r\n---> 34 data_files = dl_manager.download_and_extract(self.config.data_files)\r\n     35 if isinstance(data_files, (str, list, tuple)):\r\n     36     files = data_files\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/site-packages\/datasets\/download\/download_manager.py:565, in DownloadManager.download_and_extract(self, url_or_urls)\r\n    549 def download_and_extract(self, url_or_urls):\r\n    550     \"\"\"Download and extract given `url_or_urls`.\r\n    551 \r\n    552     Is roughly equivalent to:\r\n   (...)\r\n    563         extracted_path(s): `str`, extracted paths of given URL(s).\r\n    564     \"\"\"\r\n--> 565     return self.extract(self.download(url_or_urls))\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/site-packages\/datasets\/download\/download_manager.py:420, in DownloadManager.download(self, url_or_urls)\r\n    401 def download(self, url_or_urls):\r\n    402     \"\"\"Download given URL(s).\r\n    403 \r\n    404     By default, only one process is used for download. Pass customized `download_config.num_proc` to change this behavior.\r\n   (...)\r\n    418     ```\r\n    419     \"\"\"\r\n--> 420     download_config = self.download_config.copy()\r\n    421     download_config.extract_compressed_file = False\r\n    422     if download_config.download_desc is None:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/site-packages\/datasets\/download\/download_config.py:94, in DownloadConfig.copy(self)\r\n     93 def copy(self) -> \"DownloadConfig\":\r\n---> 94     return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/site-packages\/datasets\/download\/download_config.py:94, in <dictcomp>(.0)\r\n     93 def copy(self) -> \"DownloadConfig\":\r\n---> 94     return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\n    [... skipping similar frames: _deepcopy_dict at line 231 (2 times), deepcopy at line 146 (2 times)]\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\n    [... skipping similar frames: deepcopy at line 146 (1 times)]\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:206, in _deepcopy_list(x, memo, deepcopy)\r\n    204 append = y.append\r\n    205 for a in x:\r\n--> 206     append(deepcopy(a, memo))\r\n    207 return y\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:238, in _deepcopy_method(x, memo)\r\n    237 def _deepcopy_method(x, memo): # Copy instance methods\r\n--> 238     return type(x)(x.__func__, deepcopy(x.__self__, memo))\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\n    [... skipping similar frames: _deepcopy_dict at line 231 (3 times), deepcopy at line 146 (3 times), deepcopy at line 172 (3 times), _reconstruct at line 271 (2 times)]\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\n    [... skipping similar frames: _deepcopy_dict at line 231 (1 times), deepcopy at line 146 (1 times)]\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:265, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    263 if deep and args:\r\n    264     args = (deepcopy(arg, memo) for arg in args)\r\n--> 265 y = func(*args)\r\n    266 if deep:\r\n    267     memo[id(x)] = y\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:264, in <genexpr>(.0)\r\n    262 deep = memo is not None\r\n    263 if deep and args:\r\n--> 264     args = (deepcopy(arg, memo) for arg in args)\r\n    265 y = func(*args)\r\n    266 if deep:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:211, in _deepcopy_tuple(x, memo, deepcopy)\r\n    210 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 211     y = [deepcopy(a, memo) for a in x]\r\n    212     # We're not going to put the tuple in the memo, but it's still important we\r\n    213     # check for it, in case the tuple contains recursive mutable structures.\r\n    214     try:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:211, in <listcomp>(.0)\r\n    210 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 211     y = [deepcopy(a, memo) for a in x]\r\n    212     # We're not going to put the tuple in the memo, but it's still important we\r\n    213     # check for it, in case the tuple contains recursive mutable structures.\r\n    214     try:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:211, in _deepcopy_tuple(x, memo, deepcopy)\r\n    210 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 211     y = [deepcopy(a, memo) for a in x]\r\n    212     # We're not going to put the tuple in the memo, but it's still important we\r\n    213     # check for it, in case the tuple contains recursive mutable structures.\r\n    214     try:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:211, in <listcomp>(.0)\r\n    210 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 211     y = [deepcopy(a, memo) for a in x]\r\n    212     # We're not going to put the tuple in the memo, but it's still important we\r\n    213     # check for it, in case the tuple contains recursive mutable structures.\r\n    214     try:\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~\/miniconda3\/envs\/test-env\/lib\/python3.10\/copy.py:161, in deepcopy(x, memo, _nil)\r\n    159 reductor = getattr(x, \"__reduce_ex__\", None)\r\n    160 if reductor is not None:\r\n--> 161     rv = reductor(4)\r\n    162 else:\r\n    163     reductor = getattr(x, \"__reduce__\", None)\r\n\r\nTypeError: cannot pickle '_contextvars.Context' object\r\n```\r\n<\/details>\r\n\n\n### Expected behavior\n\nIf I choose to load the file from the public bucket with `anon=True` passed - everything works, so I expected loading from the private bucket to work as well\n\n### Environment info\n\n- `datasets` version: 2.14.6\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.10.13\r\n- Huggingface_hub version: 0.19.1\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 1.5.3\r\n- s3fs version: 2023.10.0\r\n- fsspec version: 2023.10.0 \r\n- aiobotocore version: 2.7.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6407\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6407\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6406","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6406\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6406\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6406\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6406","id":1990469045,"node_id":"I_kwDODunzps52pCW1","number":6406,"title":"CI Build PR Documentation is broken: ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-13T11:36:10Z","updated_at":"2023-11-14T10:05:36Z","closed_at":"2023-11-14T10:05:36Z","author_association":"MEMBER","active_lock_reason":null,"body":"Our CI Build PR Documentation is broken. See: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/6799554060\/job\/18486828777?pr=6390\r\n```\r\nImportError: cannot import name 'TypeAliasType' from 'typing_extensions'\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6406\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6406\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6405","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6405\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6405\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6405\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6405","id":1990358743,"node_id":"I_kwDODunzps52onbX","number":6405,"title":"ConfigNamesError on a simple CSV file","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-13T10:28:29Z","updated_at":"2023-11-13T20:01:24Z","closed_at":"2023-11-13T20:01:24Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"See https:\/\/huggingface.co\/datasets\/Nguyendo1999\/mmath\/discussions\/1\r\n\r\n```\r\nError code:   ConfigNamesError\r\nException:    TypeError\r\nMessage:      __init__() missing 1 required positional argument: 'dtype'\r\nTraceback:    Traceback (most recent call last):\r\n                File \"\/src\/services\/worker\/src\/worker\/job_runners\/dataset\/config_names.py\", line 65, in compute_config_names_response\r\n                  for config in sorted(get_dataset_config_names(path=dataset, token=hf_token))\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 351, in get_dataset_config_names\r\n                  dataset_module = dataset_module_factory(\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1512, in dataset_module_factory\r\n                  raise e1 from None\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1489, in dataset_module_factory\r\n                  return HubDatasetModuleFactoryWithoutScript(\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1039, in get_module\r\n                  dataset_infos = DatasetInfosDict.from_dataset_card_data(dataset_card_data)\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/info.py\", line 468, in from_dataset_card_data\r\n                  dataset_info = DatasetInfo._from_yaml_dict(dataset_card_data[\"dataset_info\"])\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/info.py\", line 399, in _from_yaml_dict\r\n                  yaml_data[\"features\"] = Features._from_yaml_list(yaml_data[\"features\"])\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 1838, in _from_yaml_list\r\n                  return cls.from_dict(from_yaml_inner(yaml_data))\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 1690, in from_dict\r\n                  obj = generate_from_dict(dic)\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 1345, in generate_from_dict\r\n                  return {key: generate_from_dict(value) for key, value in obj.items()}\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 1345, in <dictcomp>\r\n                  return {key: generate_from_dict(value) for key, value in obj.items()}\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 1353, in generate_from_dict\r\n                  return class_type(**{k: v for k, v in obj.items() if k in field_names})\r\n              TypeError: __init__() missing 1 required positional argument: 'dtype'\r\n```\r\n\r\nThis is the CSV file: https:\/\/huggingface.co\/datasets\/Nguyendo1999\/mmath\/blob\/dbcdd7c2c6fc447f852ec136a7532292802bb46f\/math_train.csv","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6405\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6405\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6404","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6404\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6404\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6404\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6404","id":1990211901,"node_id":"PR_kwDODunzps5fRrJ-","number":6404,"title":"Support pyarrow 14.0.1 and fix vulnerability CVE-2023-47248","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":15,"created_at":"2023-11-13T09:15:39Z","updated_at":"2023-11-14T10:29:48Z","closed_at":"2023-11-14T10:23:29Z","author_association":"MEMBER","active_lock_reason":null,"body":"Support `pyarrow` 14.0.1 and fix vulnerability [CVE-2023-47248](https:\/\/github.com\/advisories\/GHSA-5wvp-7f3h-6wmm).\r\n\r\nFix #6396.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6404\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6404\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6404","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6404","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6404.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6404.patch","merged_at":"2023-11-14T10:23:29Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6403","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6403\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6403\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6403\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6403","id":1990098817,"node_id":"I_kwDODunzps52nn-B","number":6403,"title":"Cannot import datasets on google colab (python 3.10.12)","user":{"login":"nabilaannisa","id":15389235,"node_id":"MDQ6VXNlcjE1Mzg5MjM1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/15389235?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nabilaannisa","html_url":"https:\/\/github.com\/nabilaannisa","followers_url":"https:\/\/api.github.com\/users\/nabilaannisa\/followers","following_url":"https:\/\/api.github.com\/users\/nabilaannisa\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nabilaannisa\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nabilaannisa\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nabilaannisa\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nabilaannisa\/orgs","repos_url":"https:\/\/api.github.com\/users\/nabilaannisa\/repos","events_url":"https:\/\/api.github.com\/users\/nabilaannisa\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nabilaannisa\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-13T08:14:43Z","updated_at":"2023-11-16T05:04:22Z","closed_at":"2023-11-16T05:04:21Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm trying A full colab demo notebook of zero-shot-distillation from https:\/\/github.com\/huggingface\/transformers\/tree\/main\/examples\/research_projects\/zero-shot-distillation but i got this type of error when importing  datasets on my google colab (python version is 3.10.12)\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/15389235\/6f7758a2-681d-4436-87d0-5e557838e368)\r\n\r\nI found the same problem that have been solved in [#3326 ] but it seem still error on the google colab. I can't try on my local using jupyter notebook because of my laptop resource doesn't fulfill the requirements. \r\n\r\nPlease can anyone help me solve this problem. Thank you \ud83d\ude05 \n\n### Steps to reproduce the bug\n\nError:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n[<ipython-input-8-b6e092f83978>](https:\/\/localhost:8080\/#) in <cell line: 1>()\r\n----> 1 from datasets import load_dataset\r\n      2 \r\n      3 # Print all the available datasets\r\n      4 from huggingface_hub import list_datasets\r\n      5 print([dataset.id for dataset in list_datasets()])\r\n\r\n6 frames\r\n[\/usr\/lib\/python3.10\/functools.py](https:\/\/localhost:8080\/#) in update_wrapper(wrapper, wrapped, assigned, updated)\r\n     59     # Issue #17482: set __wrapped__ last so we don't inadvertently copy it\r\n     60     # from the wrapped function when updating __dict__\r\n---> 61     wrapper.__wrapped__ = wrapped\r\n     62     # Return the wrapper so this can be used as a decorator via partial()\r\n     63     return wrapper\r\n\r\nAttributeError: readonly attribute\r\n```\n\n### Expected behavior\n\nRun success on Google Colab (free)\n\n### Environment info\n\nWindows 11 x64, Google Colab free","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6403\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6403\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6402","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6402\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6402\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6402\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6402","id":1989094542,"node_id":"PR_kwDODunzps5fOBdK","number":6402,"title":"Update torch_formatter.py","user":{"login":"VarunNSrivastava","id":32204417,"node_id":"MDQ6VXNlcjMyMjA0NDE3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32204417?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/VarunNSrivastava","html_url":"https:\/\/github.com\/VarunNSrivastava","followers_url":"https:\/\/api.github.com\/users\/VarunNSrivastava\/followers","following_url":"https:\/\/api.github.com\/users\/VarunNSrivastava\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/VarunNSrivastava\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/VarunNSrivastava\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/VarunNSrivastava\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/VarunNSrivastava\/orgs","repos_url":"https:\/\/api.github.com\/users\/VarunNSrivastava\/repos","events_url":"https:\/\/api.github.com\/users\/VarunNSrivastava\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/VarunNSrivastava\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-11T19:40:41Z","updated_at":"2023-11-11T19:41:53Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Ensure PyTorch images are converted to (C, H, W) instead of (H, W, C). See #6394 for motivation. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6402\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6402\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6402","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6402","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6402.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6402.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6401","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6401\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6401\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6401\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6401","id":1988710061,"node_id":"I_kwDODunzps52iU6t","number":6401,"title":"dataset = load_dataset(\"Hyperspace-Technologies\/scp-wiki-text\") not working","user":{"login":"userbox020","id":47074021,"node_id":"MDQ6VXNlcjQ3MDc0MDIx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47074021?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/userbox020","html_url":"https:\/\/github.com\/userbox020","followers_url":"https:\/\/api.github.com\/users\/userbox020\/followers","following_url":"https:\/\/api.github.com\/users\/userbox020\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/userbox020\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/userbox020\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/userbox020\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/userbox020\/orgs","repos_url":"https:\/\/api.github.com\/users\/userbox020\/repos","events_url":"https:\/\/api.github.com\/users\/userbox020\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/userbox020\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-11T04:09:07Z","updated_at":"2023-11-20T17:45:20Z","closed_at":"2023-11-20T17:45:20Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\n```\r\n(datasets) mruserbox@guru-X99:\/media\/10TB_HHD\/_LLM_DATASETS$ python dataset.py\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 360\/360 [00:00<00:00, 2.16MB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 65.1M\/65.1M [00:19<00:00, 3.38MB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.35k\/6.35k [00:00<00:00, 20.7kB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.29M\/7.29M [00:01<00:00, 3.99MB\/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:21<00:00,  7.14s\/it]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:00<00:00, 1624.23it\/s]\r\nGenerating train split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 314294\/314294 [00:00<00:00, 668186.58 examples\/s]\r\nGenerating validation split: 120 examples [00:00, 100422.28 examples\/s]\r\nGenerating test split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 34922\/34922 [00:00<00:00, 754683.41 examples\/s]\r\nTraceback (most recent call last):\r\n  File \"\/media\/10TB_HHD\/_LLM_DATASETS\/dataset.py\", line 3, in <module>\r\n    dataset = load_dataset(\"Hyperspace-Technologies\/scp-wiki-text\")\r\n  File \"\/home\/mruserbox\/miniconda3\/envs\/datasets\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 2153, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/mruserbox\/miniconda3\/envs\/datasets\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 954, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/mruserbox\/miniconda3\/envs\/datasets\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1067, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"\/home\/mruserbox\/miniconda3\/envs\/datasets\/lib\/python3.10\/site-packages\/datasets\/utils\/info_utils.py\", line 93, in verify_splits\r\n    raise UnexpectedSplits(str(set(recorded_splits) - set(expected_splits)))\r\ndatasets.utils.info_utils.UnexpectedSplits: {'validation'}\r\n```\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nName:\r\n`dataset.py`\r\n\r\nCode:\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"Hyperspace-Technologies\/scp-wiki-text\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nRun without errors\r\n\r\n### Environment info\r\n\r\n```\r\nname: datasets\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=5.1=1_gnu\r\n  - bzip2=1.0.8=h7b6447c_0\r\n  - ca-certificates=2023.08.22=h06a4308_0\r\n  - ld_impl_linux-64=2.38=h1181459_1\r\n  - libffi=3.4.4=h6a678d5_0\r\n  - libgcc-ng=11.2.0=h1234567_1\r\n  - libgomp=11.2.0=h1234567_1\r\n  - libstdcxx-ng=11.2.0=h1234567_1\r\n  - libuuid=1.41.5=h5eee18b_0\r\n  - ncurses=6.4=h6a678d5_0\r\n  - openssl=3.0.12=h7f8727e_0\r\n  - python=3.10.13=h955ad1f_0\r\n  - readline=8.2=h5eee18b_0\r\n  - setuptools=68.0.0=py310h06a4308_0\r\n  - sqlite=3.41.2=h5eee18b_0\r\n  - tk=8.6.12=h1ccaba5_0\r\n  - wheel=0.41.2=py310h06a4308_0\r\n  - xz=5.4.2=h5eee18b_0\r\n  - zlib=1.2.13=h5eee18b_0\r\n  - pip:\r\n      - aiohttp==3.8.6\r\n      - aiosignal==1.3.1\r\n      - async-timeout==4.0.3\r\n      - attrs==23.1.0\r\n      - certifi==2023.7.22\r\n      - charset-normalizer==3.3.2\r\n      - click==8.1.7\r\n      - datasets==2.14.6\r\n      - dill==0.3.7\r\n      - filelock==3.13.1\r\n      - frozenlist==1.4.0\r\n      - fsspec==2023.10.0\r\n      - huggingface-hub==0.19.0\r\n      - idna==3.4\r\n      - multidict==6.0.4\r\n      - multiprocess==0.70.15\r\n      - numpy==1.26.1\r\n      - openai==0.27.8\r\n      - packaging==23.2\r\n      - pandas==2.1.3\r\n      - pip==23.3.1\r\n      - platformdirs==4.0.0\r\n      - pyarrow==14.0.1\r\n      - python-dateutil==2.8.2\r\n      - pytz==2023.3.post1\r\n      - pyyaml==6.0.1\r\n      - requests==2.31.0\r\n      - six==1.16.0\r\n      - tomli==2.0.1\r\n      - tqdm==4.66.1\r\n      - typer==0.9.0\r\n      - typing-extensions==4.8.0\r\n      - tzdata==2023.3\r\n      - urllib3==2.0.7\r\n      - xxhash==3.4.1\r\n      - yarl==1.9.2\r\nprefix: \/home\/mruserbox\/miniconda3\/envs\/datasets\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6401\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6401\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6400","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6400\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6400\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6400\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6400","id":1988571317,"node_id":"I_kwDODunzps52hzC1","number":6400,"title":"Safely load datasets by disabling execution of dataset loading script","user":{"login":"irenedea","id":14367635,"node_id":"MDQ6VXNlcjE0MzY3NjM1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14367635?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/irenedea","html_url":"https:\/\/github.com\/irenedea","followers_url":"https:\/\/api.github.com\/users\/irenedea\/followers","following_url":"https:\/\/api.github.com\/users\/irenedea\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/irenedea\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/irenedea\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/irenedea\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/irenedea\/orgs","repos_url":"https:\/\/api.github.com\/users\/irenedea\/repos","events_url":"https:\/\/api.github.com\/users\/irenedea\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/irenedea\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2023-11-10T23:48:29Z","updated_at":"2024-01-02T18:18:09Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nIs there a way to disable execution of dataset loading script using `load_dataset`? This is a security vulnerability that could lead to arbitrary code execution. \r\n\r\nAny suggested workarounds are welcome as well. \n\n### Motivation\n\nThis is a security vulnerability that could lead to arbitrary code execution. \n\n### Your contribution\n\nn\/a","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6400\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6400\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6399","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6399\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6399\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6399\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6399","id":1988368503,"node_id":"I_kwDODunzps52hBh3","number":6399,"title":"TypeError: Cannot convert pyarrow.lib.ChunkedArray to pyarrow.lib.Array","user":{"login":"y-hwang","id":76236359,"node_id":"MDQ6VXNlcjc2MjM2MzU5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/76236359?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/y-hwang","html_url":"https:\/\/github.com\/y-hwang","followers_url":"https:\/\/api.github.com\/users\/y-hwang\/followers","following_url":"https:\/\/api.github.com\/users\/y-hwang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/y-hwang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/y-hwang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/y-hwang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/y-hwang\/orgs","repos_url":"https:\/\/api.github.com\/users\/y-hwang\/repos","events_url":"https:\/\/api.github.com\/users\/y-hwang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/y-hwang\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-10T20:48:46Z","updated_at":"2023-11-10T20:48:46Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi, I am preprocessing a large custom dataset with numpy arrays. I am running into this TypeError during writing in a dataset.map() function. I've tried decreasing writer batch size, but this error persists. This error does not occur for smaller datasets. \r\n\r\n\r\n\r\nThank you!\r\n\r\n\n\n### Steps to reproduce the bug\n\nTraceback (most recent call last):\r\n  File \"\/n\/home12\/yhwang\/.conda\/envs\/lib\/python3.10\/site-packages\/multiprocess\/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"\/n\/home12\/yhwang\/.conda\/envs\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 1354, in _write_generator_to_queue\r\n    for i, result in enumerate(func(**kwargs)):\r\n  File \"\/n\/home12\/yhwang\/.conda\/envs\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 3493, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"\/n\/home12\/yhwang\/.conda\/envs\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 555, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow\/array.pxi\", line 243, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"\/n\/home12\/yhwang\/.conda\/envs\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 184, in __arrow_array__\r\n    out = numpy_to_pyarrow_listarray(data)\r\n  File \"\/n\/home12\/yhwang\/.conda\/envs\/lib\/python3.10\/site-packages\/datasets\/features\/features.py\", line 1394, in numpy_to_pyarrow_listarray\r\n    values = pa.ListArray.from_arrays(offsets, values)\r\n  File \"pyarrow\/array.pxi\", line 2004, in pyarrow.lib.ListArray.from_arrays\r\nTypeError: Cannot convert pyarrow.lib.ChunkedArray to pyarrow.lib.Array\n\n### Expected behavior\n\nType should not be a ChunkedArray\n\n### Environment info\n\ndatasets v2.14.5\r\narrow v1.2.3\r\npyarrow v12.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6399\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6399\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6398","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6398\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6398\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6398\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6398","id":1987786446,"node_id":"PR_kwDODunzps5fJlP7","number":6398,"title":"Remove redundant condition in builders","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-10T14:56:43Z","updated_at":"2023-11-14T10:49:15Z","closed_at":"2023-11-14T10:43:00Z","author_association":"MEMBER","active_lock_reason":null,"body":"Minor refactoring to remove redundant condition.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6398\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6398\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6398","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6398","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6398.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6398.patch","merged_at":"2023-11-14T10:43:00Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6397","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6397\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6397\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6397\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6397","id":1987622152,"node_id":"I_kwDODunzps52eLUI","number":6397,"title":"Raise a different exception for inexisting dataset vs files without known extension","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-10T13:22:14Z","updated_at":"2023-11-22T15:12:34Z","closed_at":"2023-11-22T15:12:34Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"See https:\/\/github.com\/huggingface\/datasets-server\/issues\/2082#issuecomment-1805716557\r\n\r\nWe have the same error for:\r\n- https:\/\/huggingface.co\/datasets\/severo\/a_dataset_that_does_not_exist: a dataset that does not exist\r\n- https:\/\/huggingface.co\/datasets\/severo\/test_files_without_extension: a dataset with files without a known extension\r\n\r\n```\r\n>>> import datasets\r\n>>> datasets.get_dataset_config_names('severo\/a_dataset_that_does_not_exist')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-server\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 351, in get_dataset_config_names\r\n    dataset_module = dataset_module_factory(\r\n  File \"\/home\/slesage\/hf\/datasets-server\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1508, in dataset_module_factory\r\n    raise FileNotFoundError(\r\nFileNotFoundError: Couldn't find a dataset script at \/home\/slesage\/hf\/datasets-server\/services\/worker\/severo\/a_dataset_that_does_not_exist\/a_dataset_that_does_not_exist.py or any data file in the same directory. Couldn't find 'severo\/a_dataset_that_does_not_exist' on the Hugging Face Hub either: FileNotFoundError: Dataset 'severo\/a_dataset_that_does_not_exist' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.\r\n>>> datasets.get_dataset_config_names('severo\/test_files_without_extension')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-server\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 351, in get_dataset_config_names\r\n    dataset_module = dataset_module_factory(\r\n  File \"\/home\/slesage\/hf\/datasets-server\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1508, in dataset_module_factory\r\n    raise FileNotFoundError(\r\nFileNotFoundError: Couldn't find a dataset script at \/home\/slesage\/hf\/datasets-server\/services\/worker\/severo\/test_files_without_extension\/test_files_without_extension.py or any data file in the same directory. Couldn't find 'severo\/test_files_without_extension' on the Hugging Face Hub either: FileNotFoundError: No (supported) data files or dataset script found in severo\/test_files_without_extension.\r\n```\r\n\r\nTo differentiate, we must parse the error message (only the end is different). We should have a different exception for these two errors.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6397\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6397\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6396","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6396\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6396\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6396\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6396","id":1987308077,"node_id":"I_kwDODunzps52c-ot","number":6396,"title":"Issue with pyarrow 14.0.1","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-11-10T10:02:12Z","updated_at":"2023-11-14T10:23:30Z","closed_at":"2023-11-14T10:23:30Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"See https:\/\/github.com\/huggingface\/datasets-server\/pull\/2089 for reference\r\n\r\n```\r\nfrom datasets import (Array2D, Dataset, Features)\r\nfeature_type = Array2D(shape=(2, 2), dtype=\"float32\")\r\ncontent = [[0.0, 0.0], [0.0, 0.0]]\r\nfeatures = Features({\"col\": feature_type})\r\ndataset = Dataset.from_dict({\"col\": [content]}, features=features)\r\n```\r\n\r\ngenerates\r\n\r\n```\r\n\/home\/slesage\/hf\/datasets-server\/libs\/libcommon\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py:648: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\r\n  pa.PyExtensionType.__init__(self, self.storage_dtype)\r\n\/home\/slesage\/hf\/datasets-server\/libs\/libcommon\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py:1661: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\r\nIn the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\r\n\r\n  obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\n\/home\/slesage\/hf\/datasets-server\/libs\/libcommon\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py:1661: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\r\n  obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/hf\/datasets-server\/libs\/libcommon\/.venv\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 924, in from_dict\r\n    return cls(pa_table, info=info, split=split)\r\n  File \"\/home\/slesage\/hf\/datasets-server\/libs\/libcommon\/.venv\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 693, in __init__\r\n    inferred_features = Features.from_arrow_schema(arrow_table.schema)\r\n  File \"\/home\/slesage\/hf\/datasets-server\/libs\/libcommon\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 1661, in from_arrow_schema\r\n    obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\n  File \"\/home\/slesage\/hf\/datasets-server\/libs\/libcommon\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 1661, in <dictcomp>\r\n    obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\n  File \"\/home\/slesage\/hf\/datasets-server\/libs\/libcommon\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 1381, in generate_from_arrow_type\r\n    return Value(dtype=_arrow_to_datasets_dtype(pa_type))\r\n  File \"\/home\/slesage\/hf\/datasets-server\/libs\/libcommon\/.venv\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 111, in _arrow_to_datasets_dtype\r\n    raise ValueError(f\"Arrow type {arrow_type} does not have a datasets dtype equivalent.\")\r\nValueError: Arrow type extension<arrow.py_extension_type<pyarrow.lib.UnknownExtensionType>> does not have a datasets dtype equivalent.\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6396\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6396\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6395","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6395\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6395\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6395\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6395","id":1986484124,"node_id":"I_kwDODunzps52Z1ec","number":6395,"title":"Add ability to set lock type","user":{"login":"leoleoasd","id":37735580,"node_id":"MDQ6VXNlcjM3NzM1NTgw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/37735580?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leoleoasd","html_url":"https:\/\/github.com\/leoleoasd","followers_url":"https:\/\/api.github.com\/users\/leoleoasd\/followers","following_url":"https:\/\/api.github.com\/users\/leoleoasd\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leoleoasd\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leoleoasd\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leoleoasd\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leoleoasd\/orgs","repos_url":"https:\/\/api.github.com\/users\/leoleoasd\/repos","events_url":"https:\/\/api.github.com\/users\/leoleoasd\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leoleoasd\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-09T22:12:30Z","updated_at":"2023-11-23T18:50:00Z","closed_at":"2023-11-23T18:50:00Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nAllow setting file lock type, maybe from an environment variable\r\n\r\nCurrently, it only depends on whether fnctl is available:\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/12ebe695b4748c5a26e08b44ed51955f74f5801d\/src\/datasets\/utils\/filelock.py#L463-L470C16\r\n\r\n### Motivation\r\n\r\nIn my environment, flock isn't supported on a network attached drive\r\n\r\n### Your contribution\r\n\r\nI'll be happy to submit a pr.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6395\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6395\/timeline","performed_via_github_app":null,"state_reason":"not_planned","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6394","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6394\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6394\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6394\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6394","id":1985947116,"node_id":"I_kwDODunzps52XyXs","number":6394,"title":"TorchFormatter images (H, W, C) instead of (C, H, W) format","user":{"login":"Modexus","id":37351874,"node_id":"MDQ6VXNlcjM3MzUxODc0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/37351874?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Modexus","html_url":"https:\/\/github.com\/Modexus","followers_url":"https:\/\/api.github.com\/users\/Modexus\/followers","following_url":"https:\/\/api.github.com\/users\/Modexus\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Modexus\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Modexus\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Modexus\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Modexus\/orgs","repos_url":"https:\/\/api.github.com\/users\/Modexus\/repos","events_url":"https:\/\/api.github.com\/users\/Modexus\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Modexus\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-09T16:02:15Z","updated_at":"2023-11-11T19:41:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nUsing .set_format(\"torch\") leads to images having shape (H, W, C), the same as in numpy. \r\nHowever, pytorch normally uses (C, H, W) format.\r\n\r\nMaybe I'm missing something but this makes the format a lot less useful as I then have to permute it anyways.\r\nIf not using the format it is possible to directly use torchvision transforms but any non-transformed value will not be a tensor.\r\n\r\nIs there a reason for this choice?\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import Dataset, Features, Audio, Image\r\nimages = [\"path\/to\/image.png\"] * 10\r\nfeatures = Features({\"image\": Image()})\r\nds = Dataset.from_dict({\"image\": images}, features=features) \r\nds = ds.with_format(\"torch\")\r\nds[0][\"image\"].shape\r\n```\r\n```python\r\ntorch.Size([512, 512, 4])\r\n```\r\n\r\n### Expected behavior\r\n\r\n```python\r\nfrom datasets import Dataset, Features, Audio, Image\r\nimages = [\"path\/to\/image.png\"] * 10\r\nfeatures = Features({\"image\": Image()})\r\nds = Dataset.from_dict({\"image\": images}, features=features) \r\nds = ds.with_format(\"torch\")\r\nds[0][\"image\"].shape\r\n```\r\n```python\r\ntorch.Size([4, 512, 512])\r\n```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.6\r\n- Platform: Linux-6.5.9-100.fc37.x86_64-x86_64-with-glibc2.31\r\n- Python version: 3.11.6\r\n- Huggingface_hub version: 0.18.0\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6394\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6394\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6393","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6393\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6393\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6393\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6393","id":1984913259,"node_id":"I_kwDODunzps52T19r","number":6393,"title":"Filter occasionally hangs","user":{"login":"dakinggg","id":43149077,"node_id":"MDQ6VXNlcjQzMTQ5MDc3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/43149077?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dakinggg","html_url":"https:\/\/github.com\/dakinggg","followers_url":"https:\/\/api.github.com\/users\/dakinggg\/followers","following_url":"https:\/\/api.github.com\/users\/dakinggg\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dakinggg\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dakinggg\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dakinggg\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dakinggg\/orgs","repos_url":"https:\/\/api.github.com\/users\/dakinggg\/repos","events_url":"https:\/\/api.github.com\/users\/dakinggg\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dakinggg\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-11-09T06:18:30Z","updated_at":"2023-11-21T17:39:26Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nA call to `.filter` occasionally hangs (after the filter is complete, according to tqdm)\r\n\r\nThere is a trace produced\r\n```\r\nException ignored in: <function Dataset.__del__ at 0x7efb48130c10>\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3\/dist-packages\/datasets\/arrow_dataset.py\", line 1366, in __del__\r\n    if hasattr(self, \"_indices\"):\r\n  File \"\/usr\/lib\/python3\/dist-packages\/composer\/core\/engine.py\", line 123, in sigterm_handler\r\n    sys.exit(128 + signal)\r\nSystemExit: 143\r\n```\r\n\r\nbut I'm not sure if the trace is actually from `datasets`, or from surrounding code that is trying to clean up after datasets gets stuck.\r\n\r\nUnfortunately I can't reproduce this issue anywhere close to reliably. It happens infrequently when using `num_procs > 1`. Anecdotally I started seeing it when using larger datasets (~10M samples).\r\n\r\n### Steps to reproduce the bug\r\n\r\nN\/A see description\r\n\r\n### Expected behavior\r\n\r\nmap\/filter calls always complete sucessfully\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.6\r\n- Platform: Linux-5.4.0-137-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.1.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6393\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6393\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6392","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6392\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6392\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6392\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6392","id":1984369545,"node_id":"I_kwDODunzps52RxOJ","number":6392,"title":"`push_to_hub` is not robust to hub closing connection","user":{"login":"msis","id":577139,"node_id":"MDQ6VXNlcjU3NzEzOQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/577139?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/msis","html_url":"https:\/\/github.com\/msis","followers_url":"https:\/\/api.github.com\/users\/msis\/followers","following_url":"https:\/\/api.github.com\/users\/msis\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/msis\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/msis\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/msis\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/msis\/orgs","repos_url":"https:\/\/api.github.com\/users\/msis\/repos","events_url":"https:\/\/api.github.com\/users\/msis\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/msis\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":12,"created_at":"2023-11-08T20:44:53Z","updated_at":"2023-12-20T07:28:24Z","closed_at":"2023-12-01T17:51:34Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nLike to #6172, `push_to_hub` will crash if Hub resets the connection and raise the following error:\r\n```\r\nPushing dataset shards to the dataset hub:  32%|\u2588\u2588\u2588\u258f      | 54\/171 [06:38<14:23,  7.38s\/it]\r\nTraceback (most recent call last):\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/urllib3\/connectionpool.py\", line 715, in urlopen\r\n    httplib_response = self._make_request(\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/urllib3\/connectionpool.py\", line 467, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/urllib3\/connectionpool.py\", line 462, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"\/usr\/lib\/python3.8\/http\/client.py\", line 1348, in getresponse\r\n    response.begin()\r\n  File \"\/usr\/lib\/python3.8\/http\/client.py\", line 316, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"\/usr\/lib\/python3.8\/http\/client.py\", line 285, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nhttp.client.RemoteDisconnected: Remote end closed connection without response\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/requests\/adapters.py\", line 486, in send\r\n    resp = conn.urlopen(\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/urllib3\/connectionpool.py\", line 799, in urlopen\r\n    retries = retries.increment(\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/urllib3\/util\/retry.py\", line 550, in increment\r\n    raise six.reraise(type(error), error, _stacktrace)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/urllib3\/packages\/six.py\", line 769, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/urllib3\/connectionpool.py\", line 715, in urlopen\r\n    httplib_response = self._make_request(\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/urllib3\/connectionpool.py\", line 467, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/urllib3\/connectionpool.py\", line 462, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"\/usr\/lib\/python3.8\/http\/client.py\", line 1348, in getresponse\r\n    response.begin()\r\n  File \"\/usr\/lib\/python3.8\/http\/client.py\", line 316, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"\/usr\/lib\/python3.8\/http\/client.py\", line 285, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/_commit_api.py\", line 383, in _wrapped_lfs_upload\r\n    lfs_upload(operation=operation, lfs_batch_action=batch_action, token=token)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/lfs.py\", line 223, in lfs_upload\r\n    _upload_multi_part(operation=operation, header=header, chunk_size=chunk_size, upload_url=upload_action[\"href\"])\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/lfs.py\", line 319, in _upload_multi_part\r\n    else _upload_parts_iteratively(operation=operation, sorted_parts_urls=sorted_parts_urls, chunk_size=chunk_size)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/lfs.py\", line 375, in _upload_parts_iteratively\r\n    part_upload_res = http_backoff(\"PUT\", part_upload_url, data=fileobj_slice)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/utils\/_http.py\", line 258, in http_backoff\r\n    response = session.request(method=method, url=url, **kwargs)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/requests\/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/requests\/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/utils\/_http.py\", line 63, in send\r\n    return super().send(request, *args, **kwargs)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/requests\/adapters.py\", line 501, in send\r\n    raise ConnectionError(err, request=request)\r\nrequests.exceptions.ConnectionError: (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 2bab8c06-b701-4266-aead-fe2e0dc0e3ed)')\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"convert_to_hf.py\", line 116, in <module>\r\n    main()\r\n  File \"convert_to_hf.py\", line 108, in main\r\n    audio_dataset.push_to_hub(\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/datasets\/dataset_dict.py\", line 1641, in push_to_hub\r\n    repo_id, split, uploaded_size, dataset_nbytes, _, _ = self[split]._push_parquet_shards_to_hub(\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 5308, in _push_parquet_shards_to_hub\r\n    _retry(\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/datasets\/utils\/file_utils.py\", line 290, in _retry\r\n    return func(*func_args, **func_kwargs)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/utils\/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/hf_api.py\", line 828, in _inner\r\n    return fn(self, *args, **kwargs)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/hf_api.py\", line 3221, in upload_file\r\n    commit_info = self.create_commit(\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/utils\/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/hf_api.py\", line 828, in _inner\r\n    return fn(self, *args, **kwargs)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/hf_api.py\", line 2695, in create_commit\r\n    upload_lfs_files(\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/utils\/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/_commit_api.py\", line 393, in upload_lfs_files\r\n    _wrapped_lfs_upload(filtered_actions[0])\r\n  File \"\/admin\/home-piraka9011\/.virtualenvs\/w2v2\/lib\/python3.8\/site-packages\/huggingface_hub\/_commit_api.py\", line 385, in _wrapped_lfs_upload\r\n    raise RuntimeError(f\"Error while uploading '{operation.path_in_repo}' to the Hub.\") from exc\r\nRuntimeError: Error while uploading 'batch_19\/train-00054-of-00171-932beb4082c034bf.parquet' to the Hub.\r\n```\r\n\r\nThe function should retry if the operations fails, or at least offer a way to recover after such a failure. \r\nRight now, calling the function again will start sending all the parquets files leading to duplicates in the repository, with no guarantee that it will actually  be pushed.\r\n\r\nPreviously, it would crash with an error 400 #4677 .\n\n### Steps to reproduce the bug\n\nAny large dataset pushed the hub:\r\n\r\n```py\r\naudio_dataset.push_to_hub(\r\n        repo_id=\"org\/dataset\",\r\n    )\r\n```\n\n### Expected behavior\n\n`push_to_hub` should have an option for max retries or resume.\n\n### Environment info\n\n- `datasets` version: 2.14.6\r\n- Platform: Linux-5.15.0-1044-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6392\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6392\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6391","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6391\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6391\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6391\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6391","id":1984091776,"node_id":"PR_kwDODunzps5e9BDO","number":6391,"title":"Webdataset dataset builder","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-11-08T17:31:59Z","updated_at":"2023-11-28T16:33:33Z","closed_at":"2023-11-28T16:33:10Z","author_association":"MEMBER","active_lock_reason":null,"body":"Allow `load_dataset` to support the Webdataset format.\r\n\r\nIt allows users to download\/stream data from local files or from the Hugging Face Hub.\r\n\r\nMoreover it will enable the Dataset Viewer for Webdataset datasets on HF.\r\n\r\n## Implementation details\r\n\r\n- I added a new Webdataset builder\r\n- dataset with TAR files are now read using the Webdataset builder\r\n- Basic decoding from `webdataset` is used by default, except unsafe ones like pickle\r\n- HF authentication support is done by registering a `webdataset.gopen` reader\r\n- `webdataset` uses buffering when reading files, so I had to add buffering support in `xopen`\r\n\r\n## TODOS\r\n\r\n- [x] tests\r\n- [x] docs","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6391\/reactions","total_count":3,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":3,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6391\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6391","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6391","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6391.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6391.patch","merged_at":"2023-11-28T16:33:10Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6390","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6390\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6390\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6390\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6390","id":1983725707,"node_id":"PR_kwDODunzps5e7xQ3","number":6390,"title":"handle future deprecation argument","user":{"login":"winglian","id":381258,"node_id":"MDQ6VXNlcjM4MTI1OA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/381258?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/winglian","html_url":"https:\/\/github.com\/winglian","followers_url":"https:\/\/api.github.com\/users\/winglian\/followers","following_url":"https:\/\/api.github.com\/users\/winglian\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/winglian\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/winglian\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/winglian\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/winglian\/orgs","repos_url":"https:\/\/api.github.com\/users\/winglian\/repos","events_url":"https:\/\/api.github.com\/users\/winglian\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/winglian\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-08T14:21:25Z","updated_at":"2023-11-21T02:10:24Z","closed_at":"2023-11-14T15:15:59Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"getting this error:\r\n```\r\n\/root\/miniconda3\/envs\/py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\r\n  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\r\n```\r\n\r\nSince datasets supports arrow greater than 8.0.0, we need to handle both cases.\r\n\r\n[Arrow v14 docs](https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.concat_tables.html)\r\n[Arrow v13 docs](https:\/\/arrow.apache.org\/docs\/13.0\/python\/generated\/pyarrow.concat_tables.html)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6390\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6390\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6390","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6390","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6390.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6390.patch","merged_at":"2023-11-14T15:15:59Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6389","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6389\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6389\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6389\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6389","id":1983545744,"node_id":"I_kwDODunzps52OoGQ","number":6389,"title":"Index 339 out of range for dataset of size 339 <-- save_to_file()","user":{"login":"jaggzh","id":20318973,"node_id":"MDQ6VXNlcjIwMzE4OTcz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20318973?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jaggzh","html_url":"https:\/\/github.com\/jaggzh","followers_url":"https:\/\/api.github.com\/users\/jaggzh\/followers","following_url":"https:\/\/api.github.com\/users\/jaggzh\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jaggzh\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jaggzh\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jaggzh\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jaggzh\/orgs","repos_url":"https:\/\/api.github.com\/users\/jaggzh\/repos","events_url":"https:\/\/api.github.com\/users\/jaggzh\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jaggzh\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-08T12:52:09Z","updated_at":"2023-11-24T09:14:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen saving out some Audio() data.\r\nThe data is audio recordings with associated 'sentences'.\r\n(They use the audio 'bytes' approach because they're clips within audio files).\r\nCode is below the traceback (I can't upload the voice audio\/text (it's not even me)).\r\n\r\n```\r\nTraceback (most recent call last):\r\nFile \"\/mnt\/ddrive\/prj\/voice\/voice-training-dataset-create\/.\/dataset.py\", line 156, in <module>\r\ncreate_dataset(args)\r\nFile \"\/mnt\/ddrive\/prj\/voice\/voice-training-dataset-create\/.\/dataset.py\", line 138, in create_dataset\r\nhf_dataset.save_to_disk(args.outds, max_shard_size='50MB')\r\nFile \"\/home\/j\/src\/py\/datasets\/src\/datasets\/arrow_dataset.py\", line 1531, in save_to_disk\r\nfor kwargs in kwargs_per_job:\r\nFile \"\/home\/j\/src\/py\/datasets\/src\/datasets\/arrow_dataset.py\", line 1508, in <genexpr>\r\n\"shard\": self.shard(num_shards=num_shards, index=shard_idx, contiguous=True),\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"\/home\/j\/src\/py\/datasets\/src\/datasets\/arrow_dataset.py\", line 4609, in shard\r\nreturn self.select(\r\n^^^^^^^^^^^^\r\nFile \"\/home\/j\/src\/py\/datasets\/src\/datasets\/arrow_dataset.py\", line 556, in wrapper\r\nout: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"\/home\/j\/src\/py\/datasets\/src\/datasets\/fingerprint.py\", line 511, in wrapper\r\nout = func(dataset, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"\/home\/j\/src\/py\/datasets\/src\/datasets\/arrow_dataset.py\", line 3797, in select\r\nreturn self._select_contiguous(start, length, new_fingerprint=new_fingerprint)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"\/home\/j\/src\/py\/datasets\/src\/datasets\/arrow_dataset.py\", line 556, in wrapper\r\nout: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"\/home\/j\/src\/py\/datasets\/src\/datasets\/fingerprint.py\", line 511, in wrapper\r\nout = func(dataset, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"\/home\/j\/src\/py\/datasets\/src\/datasets\/arrow_dataset.py\", line 3857, in _select_contiguous\r\n_check_valid_indices_value(start, len(self))\r\nFile \"\/home\/j\/src\/py\/datasets\/src\/datasets\/arrow_dataset.py\", line 648, in _check_valid_indices_value\r\nraise IndexError(f\"Index {index} out of range for dataset of size {size}.\")\r\nIndexError: Index 339 out of range for dataset of size 339.\r\n```\r\n\r\n\n\n### Steps to reproduce the bug\n\n(I had to set the default max batch size down due to a different bug... or maybe it's related: https:\/\/github.com\/huggingface\/datasets\/issues\/5717)\r\n\r\n```python3\r\n#!\/usr\/bin\/env python3\r\nimport argparse\r\nimport os\r\nfrom pathlib import Path\r\nimport soundfile as sf\r\n\r\nimport datasets\r\ndatasets.config.DEFAULT_MAX_BATCH_SIZE=35\r\nfrom datasets import Features, Array2D, Value, Dataset, Sequence, Audio\r\n\r\nimport numpy as np\r\nimport librosa\r\nimport sys\r\nimport soundfile as sf\r\nimport io\r\nimport logging\r\n\r\nlogging.basicConfig(level=logging.DEBUG, filename='debug.log', filemode='w',\r\n                    format='%(name)s - %(levelname)s - %(message)s')\r\n\r\n# Define the arguments for the command-line interface\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description=\"Create a Huggingface dataset from labeled audio files.\")\r\n    parser.add_argument(\"--indir_labeled\", action=\"append\", help=\"Directory containing labeled audio files.\", required=True)\r\n    parser.add_argument(\"--outds\", help=\"Path to save the dataset file.\", required=True)\r\n    parser.add_argument(\"--max_clips\", type=int, help=\"Max count of audio samples to add to the dataset.\", default=None)\r\n    parser.add_argument(\"-r\", \"--sr\", type=int, help=\"Sample rate for the audio files.\", default=16000)\r\n    parser.add_argument(\"--no-resample\", action=\"store_true\", help=\"Disable resampling of the audio files.\")\r\n    parser.add_argument(\"--max_clip_secs\", type=float, help=\"Max length of audio clips in seconds.\", default=3.0)\r\n    parser.add_argument(\"-v\", \"--verbose\", action='count', default=1, help=\"Increase verbosity\")\r\n    return parser.parse_args()\r\n\r\n# Convert the NumPy arrays to audio bytes in WAV format\r\ndef numpy_to_bytes(audio_array, sampling_rate=16000):\r\n    with io.BytesIO() as bytes_io:\r\n        sf.write(bytes_io, audio_array, samplerate=sampling_rate,\r\n                 format='wav', subtype='FLOAT') # float32\r\n        return bytes_io.getvalue()\r\n\r\n# Function to find audio and label files in a directory\r\ndef find_audio_label_pairs(indir_labeled):\r\n    audio_label_pairs = []\r\n    for root, _, files in os.walk(indir_labeled):\r\n        for file in files:\r\n            if file.endswith(('.mp3', '.wav', '.aac', '.flac')):\r\n                audio_path = Path(root) \/ file\r\n                if args.verbose>1:\r\n                    print(f'File: {audio_path}')\r\n                label_path = audio_path.with_suffix('.labels.txt')\r\n                if label_path.exists():\r\n                    if args.verbose>0:\r\n                        print(f'  Pair: {audio_path}')\r\n                    audio_label_pairs.append((audio_path, label_path))\r\n    return audio_label_pairs\r\n\r\ndef process_audio_label_pair(audio_path, label_path, sampling_rate, no_resample, max_clip_secs):\r\n    # Read the label file\r\n    with open(label_path, 'r') as label_file:\r\n        labels = label_file.readlines()\r\n\r\n    # Load the full audio file\r\n    full_audio, current_sr = sf.read(audio_path)\r\n    if not no_resample and current_sr != sampling_rate:\r\n        # You can use librosa.resample here if librosa is available\r\n        full_audio = librosa.resample(full_audio, orig_sr=current_sr, target_sr=sampling_rate)\r\n\r\n    audio_segments = []\r\n    sentences = []\r\n\r\n    # Process each label\r\n    for label in labels:\r\n        start_secs, end_secs, label_text = label.strip().split('\\t')\r\n        start_sample = int(float(start_secs) * sampling_rate)\r\n        end_sample = int(float(end_secs) * sampling_rate)\r\n\r\n        # Extract segment and truncate or pad to max_clip_secs\r\n        audio_segment = full_audio[start_sample:end_sample]\r\n        max_samples = int(max_clip_secs * sampling_rate)\r\n        if len(audio_segment) > max_samples: # Truncate\r\n            audio_segment = audio_segment[:max_samples]\r\n        elif len(audio_segment) < max_samples: # Pad\r\n            padding = np.zeros(max_samples - len(audio_segment), dtype=audio_segment.dtype)\r\n            audio_segment = np.concatenate((audio_segment, padding))\r\n\r\n        audio_segment = numpy_to_bytes(audio_segment)\r\n\r\n        audio_data = {\r\n            'path': str(audio_path),\r\n            'bytes': audio_segment,\r\n        }\r\n\r\n        audio_segments.append(audio_data)\r\n        sentences.append(label_text)\r\n\r\n    return audio_segments, sentences\r\n\r\n# Main function to create the dataset\r\ndef create_dataset(args):\r\n    audio_label_pairs = []\r\n    for indir in args.indir_labeled:\r\n        audio_label_pairs.extend(find_audio_label_pairs(indir))\r\n\r\n    # Initialize our dataset data\r\n    dataset_data = {\r\n        'path': [],        # This will be a list of strings\r\n        'audio': [],       # This will be a list of dictionaries\r\n        'sentence': [],    # This will be a list of strings\r\n    }\r\n\r\n    # Process each audio-label pair and add the data to the dataset\r\n    for audio_path, label_path in audio_label_pairs[:args.max_clips]:\r\n        audio_segments, sentences = process_audio_label_pair(audio_path, label_path, args.sr, args.no_resample, args.max_clip_secs)\r\n        if audio_segments and sentences:\r\n            for audio_data, sentence in zip(audio_segments, sentences):\r\n                if args.verbose>1:\r\n                    print(f'Appending {audio_data[\"path\"]}')\r\n                dataset_data['path'].append(audio_data['path'])\r\n                dataset_data['audio'].append({\r\n                    'path': audio_data['path'],\r\n                    'bytes': audio_data['bytes'],\r\n                })\r\n                dataset_data['sentence'].append(sentence)\r\n\r\n    features = Features({\r\n      'path': Value('string'), # Path is redundant in common voice set also\r\n      'audio': Audio(sampling_rate=16000),\r\n      'sentence': Value('string'),\r\n    })\r\n    hf_dataset = Dataset.from_dict(dataset_data, features=features)\r\n\r\n    for key in dataset_data:\r\n        for i, item in enumerate(dataset_data[key]):\r\n            if item is None or (isinstance(item, bytes) and len(item) == 0):\r\n                logging.error(f\"Invalid {key} at index {i}: {item}\")\r\n                import ipdb; ipdb.set_trace(context=16); pass\r\n\r\n    hf_dataset.save_to_disk(args.outds, max_shard_size='50MB')\r\n    # try:\r\n    #     hf_dataset.save_to_disk(args.outds)\r\n    # except TypeError as e:\r\n    #     # If there's a TypeError, log the exception and the dataset data that might have caused it\r\n    #     logging.exception(\"An error occurred while saving the dataset.\")\r\n    #     import ipdb; ipdb.set_trace(context=16); pass\r\n    #     for key in dataset_data:\r\n    #         logging.debug(f\"{key} length: {len(dataset_data[key])}\")\r\n    #         if key == 'audio':\r\n    #             # Log the first 100 bytes of the audio data to avoid huge log files\r\n    #             for i, audio in enumerate(dataset_data[key]):\r\n    #                 logging.debug(f\"Audio {i}: {audio['bytes'][:100]}\")\r\n    #     raise\r\n\r\n# Run the script\r\nif __name__ == \"__main__\":\r\n    args = parse_args()\r\n    create_dataset(args)\r\n```\n\n### Expected behavior\n\nIt shouldn't fail.\n\n### Environment info\n\n- `datasets` version: 2.14.7.dev0\r\n- Platform: Linux-6.1.0-13-amd64-x86_64-with-glibc2.36\r\n- Python version: 3.11.2\r\n- `huggingface_hub` version: 0.17.3\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.1.2\r\n- `fsspec` version: 2023.9.2\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6389\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6389\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6388","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6388\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6388\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6388\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6388","id":1981136093,"node_id":"I_kwDODunzps52Fbzd","number":6388,"title":"How to create 3d medical imgae dataset?","user":{"login":"QingYunA","id":41177312,"node_id":"MDQ6VXNlcjQxMTc3MzEy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/41177312?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/QingYunA","html_url":"https:\/\/github.com\/QingYunA","followers_url":"https:\/\/api.github.com\/users\/QingYunA\/followers","following_url":"https:\/\/api.github.com\/users\/QingYunA\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/QingYunA\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/QingYunA\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/QingYunA\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/QingYunA\/orgs","repos_url":"https:\/\/api.github.com\/users\/QingYunA\/repos","events_url":"https:\/\/api.github.com\/users\/QingYunA\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/QingYunA\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-07T11:27:36Z","updated_at":"2023-11-07T11:28:53Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nI am newer to huggingface, after i look up `datasets` docs, I can't find how to create the dataset contains 3d medical image (ends with '.mhd', '.dcm', '.nii')\r\n\r\n### Motivation\r\n\r\nhelp us to upload 3d medical dataset to huggingface!\r\n\r\n### Your contribution\r\n\r\nI'll submit a PR if I find a way to add this feature","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6388\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6388\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6387","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6387\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6387\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6387\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6387","id":1980224020,"node_id":"I_kwDODunzps52B9IU","number":6387,"title":"How to load existing downloaded dataset ?","user":{"login":"liming-ai","id":73068772,"node_id":"MDQ6VXNlcjczMDY4Nzcy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/73068772?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/liming-ai","html_url":"https:\/\/github.com\/liming-ai","followers_url":"https:\/\/api.github.com\/users\/liming-ai\/followers","following_url":"https:\/\/api.github.com\/users\/liming-ai\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/liming-ai\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/liming-ai\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/liming-ai\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/liming-ai\/orgs","repos_url":"https:\/\/api.github.com\/users\/liming-ai\/repos","events_url":"https:\/\/api.github.com\/users\/liming-ai\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/liming-ai\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-06T22:51:44Z","updated_at":"2023-11-16T18:07:01Z","closed_at":"2023-11-16T18:07:01Z","author_association":"NONE","active_lock_reason":null,"body":"Hi @mariosasko @lhoestq  @katielink \r\n\r\nThanks for your contribution and hard work.\r\n\r\n### Feature request\r\n\r\nFirst, I download a dataset as normal by:\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('username\/data_name', cache_dir='data')\r\n```\r\n\r\nThe dataset format in `data` directory will be:\r\n```\r\n-data\r\n  |-data_name\r\n    |-test-00000-of-00001-bf4c733542e35fcb.parquet\r\n    |-train-00000-of-00001-2a1df75c6bce91ab.parquet\r\n```\r\n\r\n\r\nThen I use SCP to clone this dataset into another machine, and then try:\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('data\/data_name')  # load from local path\r\n```\r\n\r\nThis leads to re-generating training and validation split for each time, and the disk quota will be duplicated occupation.\r\n\r\nHow can I just load the dataset without generating and saving these splits again?\r\n\r\n### Motivation\r\n\r\nI do not want to download the same dataset in two machines, scp is much faster and better than HuggingFace API. I hope we can directly load the downloaded datasets (.parquest)\r\n\r\n### Your contribution\r\n\r\nPlease refer to the feature","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6387\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6387\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6386","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6386\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6386\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6386\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6386","id":1979878014,"node_id":"I_kwDODunzps52Aop-","number":6386,"title":"Formatting overhead","user":{"login":"d-miketa","id":320321,"node_id":"MDQ6VXNlcjMyMDMyMQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/320321?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/d-miketa","html_url":"https:\/\/github.com\/d-miketa","followers_url":"https:\/\/api.github.com\/users\/d-miketa\/followers","following_url":"https:\/\/api.github.com\/users\/d-miketa\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/d-miketa\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/d-miketa\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/d-miketa\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/d-miketa\/orgs","repos_url":"https:\/\/api.github.com\/users\/d-miketa\/repos","events_url":"https:\/\/api.github.com\/users\/d-miketa\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/d-miketa\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-06T19:06:38Z","updated_at":"2023-11-06T23:56:12Z","closed_at":"2023-11-06T23:56:12Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi! I very recently noticed that my training time is dominated by batch formatting. Using Lightning's profilers, I located the bottleneck within `datasets.formatting.formatting` and then narrowed it down with `line-profiler`. It turns out that almost all of the overhead is due to creating new instances of `self.python_arrow_extractor`. I admit I'm confused why that could be the case - as far as I can tell there's no complex `__init__` logic to execute.\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/320321\/5e022e0b-0d21-43d0-8e6f-9e641142e96b)\n\n### Steps to reproduce the bug\n\n1. Set up a dataset `ds` with potentially several (4+) columns (not sure if this is necessary, but it did at one point of the investigation make overhead worse)\r\n2. Process it using a custom transform, `ds = ds.with_transform(transform_func)`\r\n3. Decorate this function https:\/\/github.com\/huggingface\/datasets\/blob\/main\/src\/datasets\/formatting\/formatting.py#L512 with `@profile` from https:\/\/pypi.org\/project\/line-profiler\/\r\n4. Profile with `$ kernprof -l script_to_profile.py`\n\n### Expected behavior\n\nBatch formatting should have acceptable overhead.\n\n### Environment info\n\n```\r\ndatasets=2.14.6\r\npyarrow=14.0.0\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6386\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6386\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6385","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6385\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6385\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6385\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6385","id":1979308338,"node_id":"I_kwDODunzps51-dky","number":6385,"title":"Get an error when i try to concatenate the squad dataset with my own dataset","user":{"login":"CCDXDX","id":149378500,"node_id":"U_kgDOCOdVxA","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/149378500?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/CCDXDX","html_url":"https:\/\/github.com\/CCDXDX","followers_url":"https:\/\/api.github.com\/users\/CCDXDX\/followers","following_url":"https:\/\/api.github.com\/users\/CCDXDX\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/CCDXDX\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/CCDXDX\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/CCDXDX\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/CCDXDX\/orgs","repos_url":"https:\/\/api.github.com\/users\/CCDXDX\/repos","events_url":"https:\/\/api.github.com\/users\/CCDXDX\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/CCDXDX\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-06T14:29:22Z","updated_at":"2023-11-06T16:50:45Z","closed_at":"2023-11-06T16:50:45Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nHello,\r\n\r\nI'm new here and I need to concatenate the squad dataset with my own dataset i created. I find the following error when i try to do it: Traceback (most recent call last):\r\n\r\n  Cell In[9], line 1\r\n    concatenated_dataset = concatenate_datasets([train_dataset, dataset1])\r\n\r\n  File ~\\anaconda3\\Lib\\site-packages\\datasets\\combine.py:213 in concatenate_datasets\r\n    return _concatenate_map_style_datasets(dsets, info=info, split=split, axis=axis)\r\n\r\n  File ~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:6002 in _concatenate_map_style_datasets\r\n    _check_if_features_can_be_aligned([dset.features for dset in dsets])\r\n\r\n  File ~\\anaconda3\\Lib\\site-packages\\datasets\\features\\features.py:2122 in _check_if_features_can_be_aligned\r\n    raise ValueError(\r\n\r\nValueError: The features can't be aligned because the key answers of features {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)} has unexpected type - Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None) (expected either {'answer_start': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'text': Value(dtype='string', id=None)} or Value(\"null\").\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\n\r\nfrom huggingface_hub import notebook_login\r\nfrom datasets import load_dataset\r\n\r\n\r\n\r\nnotebook_login(\"mymailadresse\", \"mypassword\")\r\nsquad = load_dataset(\"squad\", split=\"train[:5000]\")\r\nsquad = squad.train_test_split(test_size=0.2)\r\ndataset1 = squad[\"train\"]\r\n\r\n\r\n\r\n\r\nimport json\r\n\r\nmybase = [\r\n    {\r\n        \"id\": \"1\",\r\n        \"context\": \"She lives in Nantes\",\r\n        \"question\": \"Where does she live?\",\r\n        \"answers\": {\r\n            \"text\": \"Nantes\",\r\n            \"answer_start\": [13],\r\n        }\r\n    }\r\n]\r\n\r\n\r\n\r\n\r\n# Save the data to a JSON file\r\njson_file_path = r\"C:\\Users\\mypath\\thefile.json\"\r\nwith open(json_file_path, \"w\", encoding= \"utf-8\") as json_file:\r\n    json.dump(mybase, json_file, indent=4)\r\n\r\n\r\n\r\n\r\n# Load the JSON file as a dataset\r\ncustom_dataset = load_dataset(\"json\", data_files=json_file_path)\r\n\r\n\r\n# Access the train split\r\ntrain_dataset = custom_dataset[\"train\"]\r\n\r\n\r\nfrom datasets import concatenate_datasets\r\n\r\n\r\n# Concatenate the datasets\r\nconcatenated_dataset = concatenate_datasets([train_dataset, dataset1])\r\n```\r\n\r\n### Expected behavior\r\n\r\nI would expect the two datasets to be concatenated without error. The len(dataset1) is equal to 4000 and the len(train_dataset) is equal to 1 so I would exepect concatenated_dataset to be created and having lenght 4001.\r\n\r\n### Environment info\r\n\r\nPython 3.11.4 and using windows\r\n\r\nThank you for your help","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6385\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6385\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6384","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6384\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6384\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6384\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6384","id":1979117069,"node_id":"I_kwDODunzps519u4N","number":6384,"title":"Load the local dataset folder from other place","user":{"login":"OrangeSodahub","id":54439582,"node_id":"MDQ6VXNlcjU0NDM5NTgy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54439582?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/OrangeSodahub","html_url":"https:\/\/github.com\/OrangeSodahub","followers_url":"https:\/\/api.github.com\/users\/OrangeSodahub\/followers","following_url":"https:\/\/api.github.com\/users\/OrangeSodahub\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/OrangeSodahub\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/OrangeSodahub\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/OrangeSodahub\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/OrangeSodahub\/orgs","repos_url":"https:\/\/api.github.com\/users\/OrangeSodahub\/repos","events_url":"https:\/\/api.github.com\/users\/OrangeSodahub\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/OrangeSodahub\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-06T13:07:04Z","updated_at":"2023-11-19T05:42:06Z","closed_at":"2023-11-19T05:42:05Z","author_association":"NONE","active_lock_reason":null,"body":"This is from https:\/\/github.com\/huggingface\/diffusers\/issues\/5573\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6384\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6384\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6383","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6383\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6383\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6383\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6383","id":1978189389,"node_id":"I_kwDODunzps516MZN","number":6383,"title":"imagenet-1k downloads over and over","user":{"login":"seann999","id":6847529,"node_id":"MDQ6VXNlcjY4NDc1Mjk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6847529?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/seann999","html_url":"https:\/\/github.com\/seann999","followers_url":"https:\/\/api.github.com\/users\/seann999\/followers","following_url":"https:\/\/api.github.com\/users\/seann999\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/seann999\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/seann999\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/seann999\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/seann999\/orgs","repos_url":"https:\/\/api.github.com\/users\/seann999\/repos","events_url":"https:\/\/api.github.com\/users\/seann999\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/seann999\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-06T02:58:58Z","updated_at":"2023-11-06T06:02:39Z","closed_at":"2023-11-06T06:02:39Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhat could be causing this?\r\n```\r\n$ python3\r\nPython 3.8.13 (default, Mar 28 2022, 11:38:47) \r\n[GCC 7.5.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from datasets import load_dataset\r\n>>> load_dataset(\"imagenet-1k\")\r\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.72k\/4.72k [00:00<00:00, 7.51MB\/s]\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 85.4k\/85.4k [00:00<00:00, 510kB\/s]\r\nDownloading extra modules: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46.4k\/46.4k [00:00<00:00, 300kB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.1G\/29.1G [19:36<00:00, 24.8MB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.3G\/29.3G [08:38<00:00, 56.5MB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.0G\/29.0G [09:26<00:00, 51.2MB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.2G\/29.2G [09:38<00:00, 50.6MB\/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29.2G\/29.2G [09:37<00:00, 44.1MB\/s^Downloading data:   0%|                     | 106M\/29.1G [00:05<23:49, 20.3MB\/s]\r\n```\n\n### Steps to reproduce the bug\n\nSee above commands\/code\n\n### Expected behavior\n\nimagenet-1k is downloaded\n\n### Environment info\n\n- `datasets` version: 2.14.6\r\n- Platform: Linux-6.2.0-34-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.13\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 14.0.0\r\n- Pandas version: 1.5.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6383\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6383\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6382","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6382\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6382\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6382\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6382","id":1977400799,"node_id":"I_kwDODunzps513L3f","number":6382,"title":"Add CheXpert dataset for vision","user":{"login":"SauravMaheshkar","id":61241031,"node_id":"MDQ6VXNlcjYxMjQxMDMx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/61241031?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/SauravMaheshkar","html_url":"https:\/\/github.com\/SauravMaheshkar","followers_url":"https:\/\/api.github.com\/users\/SauravMaheshkar\/followers","following_url":"https:\/\/api.github.com\/users\/SauravMaheshkar\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/SauravMaheshkar\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/SauravMaheshkar\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/SauravMaheshkar\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/SauravMaheshkar\/orgs","repos_url":"https:\/\/api.github.com\/users\/SauravMaheshkar\/repos","events_url":"https:\/\/api.github.com\/users\/SauravMaheshkar\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/SauravMaheshkar\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2067376369,"node_id":"MDU6TGFiZWwyMDY3Mzc2MzY5","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset%20request","name":"dataset request","color":"e99695","default":false,"description":"Requesting to add a new dataset"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-04T15:36:11Z","updated_at":"2023-12-11T17:55:52Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\n### Name \r\n\r\n**CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison**\r\n\r\n### Paper\r\n\r\nhttps:\/\/arxiv.org\/abs\/1901.07031\r\n\r\n### Data\r\n\r\nhttps:\/\/stanfordaimi.azurewebsites.net\/datasets\/8cbd9ed4-2eb9-4565-affc-111cf4f7ebe2\n\n### Motivation\n\nCheXpert is one of the fundamental models in medical image classification and can serve as a viable pre-training dataset for radiology classification or low-scale ablation \/ exploratory studies.\r\n\r\nThis could also serve as a good pre-training dataset for Kaggle competitions.\n\n### Your contribution\n\nWould love to make a PR and pre-process \/ get this into \ud83e\udd17","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6382\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6382\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6381","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6381\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6381\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6381\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6381","id":1975028470,"node_id":"PR_kwDODunzps5eeYty","number":6381,"title":"Add my dataset","user":{"login":"keyur536","id":103646675,"node_id":"U_kgDOBi2F0w","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/103646675?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/keyur536","html_url":"https:\/\/github.com\/keyur536","followers_url":"https:\/\/api.github.com\/users\/keyur536\/followers","following_url":"https:\/\/api.github.com\/users\/keyur536\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/keyur536\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/keyur536\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/keyur536\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/keyur536\/orgs","repos_url":"https:\/\/api.github.com\/users\/keyur536\/repos","events_url":"https:\/\/api.github.com\/users\/keyur536\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/keyur536\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-02T20:59:52Z","updated_at":"2023-11-08T14:37:46Z","closed_at":"2023-11-06T15:50:14Z","author_association":"NONE","active_lock_reason":null,"body":"## medical data\r\n\r\n**Description:**\r\nThis dataset, named \"medical data,\" is a collection of text data from various sources, carefully curated and cleaned for use in natural language processing (NLP) tasks. It consists of a diverse range of text, including articles, books, and online content, covering topics from science to literature.\r\n\r\n**Citation:**\r\nIf applicable, please include a citation for this dataset to give credit to the original sources or contributors.\r\n\r\n**Key Features:**\r\n- Language: The text is primarily in English, but it may include content in other languages as well.\r\n- Use Cases: This dataset is suitable for text classification, language modeling, sentiment analysis, and other NLP tasks.\r\n\r\n**Usage:**\r\nTo access this dataset, use the `load_your_dataset` function provided in the `your_dataset.py` script within this repository. You can specify the dataset split you need, such as \"train,\" \"test,\" or \"validation,\" to get the data for your specific task.\r\n\r\n**Contributors:**\r\n- [Keyur Chaudhari]\r\n\r\n**Contact:**\r\nIf you have any questions or need assistance regarding this dataset, please feel free to contact [keyurchaudhari536@gmail.com].\r\n\r\nPlease note that this dataset is shared under a specific license, which can be found in the [LICENSE](link to your dataset's license) file. Make sure to review and adhere to the terms of the license when using this dataset for your projects.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6381\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6381\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6381","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6381","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6381.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6381.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6380","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6380\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6380\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6380\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6380","id":1974741221,"node_id":"PR_kwDODunzps5edaO6","number":6380,"title":"Fix for continuation behaviour on broken dataset archives due to starving download connections via HTTP-GET","user":{"login":"RuntimeRacer","id":49956579,"node_id":"MDQ6VXNlcjQ5OTU2NTc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49956579?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/RuntimeRacer","html_url":"https:\/\/github.com\/RuntimeRacer","followers_url":"https:\/\/api.github.com\/users\/RuntimeRacer\/followers","following_url":"https:\/\/api.github.com\/users\/RuntimeRacer\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/RuntimeRacer\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/RuntimeRacer\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/RuntimeRacer\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/RuntimeRacer\/orgs","repos_url":"https:\/\/api.github.com\/users\/RuntimeRacer\/repos","events_url":"https:\/\/api.github.com\/users\/RuntimeRacer\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/RuntimeRacer\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-11-02T17:28:23Z","updated_at":"2023-11-02T17:31:19Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"This PR proposes a (slightly hacky) fix for an Issue that can occur when downloading large dataset parts over unstable connections.\r\nThe underlying issue is also being discussed in https:\/\/github.com\/huggingface\/datasets\/issues\/5594.\r\n\r\nIssue Symptoms & Behaviour:\r\n- Download of a large archive file during dataset download via HTTP-GET fails.\r\n- An silent net exception (which I was unable to identify) is thrown within the `tqdm` download progress.\r\n- Due to missing exception catch code, the above process just continues processing, assuming `http_get` completed successfully.\r\n- Pending Archive file gets renamed to remove the `.incomplete` extension, despite not all data has been downloaded. \r\n- Also, for reasons I did not investigate, there seems to be no real integrity check for the downloaded files; or it does not detect this problem. This is especially problematic, since the downloader script won't retry downloading this archive after CRC-Checking, even if it is being manually restarted \/ executed again after running into errors on extraction.\r\n\r\nFix proposal: Adding a retry mechanic for HTTP-GET downloads, which adds the following behaviour:\r\n- Download Progress Thread checks for download size validity in case the HTTP connection starves mid download. If the check fails, a RuntimeError is thrown\r\n- Cache Downloader code with retry mechanic monitors for an exception thrown by the download progress thread, and retries download with updated `resume_size`.\r\n- Cache Downloader will not mark incomplete files which have thrown an exception during download, and exceeded retries, as complete.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6380\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6380\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6380","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6380","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6380.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6380.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6379","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6379\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6379\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6379\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6379","id":1974638850,"node_id":"PR_kwDODunzps5edDZL","number":6379,"title":"Avoid redundant warning when encoding NumPy array as `Image`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-11-02T16:37:58Z","updated_at":"2023-11-06T17:53:27Z","closed_at":"2023-11-02T17:08:07Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Avoid a redundant warning in `encode_np_array` by removing the identity check as NumPy `dtype`s can be equal without having identical `id`s.\r\n\r\nAdditionally, fix \"unreachable\" checks in `encode_np_array`.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6379\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6379\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6379","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6379","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6379.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6379.patch","merged_at":"2023-11-02T17:08:07Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6378","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6378\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6378\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6378\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6378","id":1973942770,"node_id":"PR_kwDODunzps5eaqhv","number":6378,"title":"Support pyarrow 14.0.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-02T10:25:10Z","updated_at":"2023-11-02T15:24:28Z","closed_at":"2023-11-02T15:15:44Z","author_association":"MEMBER","active_lock_reason":null,"body":"Support `pyarrow` 14.0.0.\r\n\r\nFix #6377 and fix #6374 (root cause).\r\n\r\nThis fix is analog to a previous one:\r\n- #6175","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6378\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6378\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6378","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6378","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6378.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6378.patch","merged_at":"2023-11-02T15:15:44Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6377","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6377\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6377\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6377\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6377","id":1973937612,"node_id":"I_kwDODunzps51p-XM","number":6377,"title":"Support pyarrow 14.0.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-11-02T10:22:08Z","updated_at":"2023-11-02T15:15:45Z","closed_at":"2023-11-02T15:15:45Z","author_association":"MEMBER","active_lock_reason":null,"body":"Support pyarrow 14.0.0 by fixing the root cause of:\r\n- #6374\r\n\r\nand revert:\r\n- #6375","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6377\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6377\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6376","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6376\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6376\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6376\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6376","id":1973927468,"node_id":"I_kwDODunzps51p74s","number":6376,"title":"Caching problem when deleting a dataset ","user":{"login":"clefourrier","id":22726840,"node_id":"MDQ6VXNlcjIyNzI2ODQw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22726840?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/clefourrier","html_url":"https:\/\/github.com\/clefourrier","followers_url":"https:\/\/api.github.com\/users\/clefourrier\/followers","following_url":"https:\/\/api.github.com\/users\/clefourrier\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/clefourrier\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/clefourrier\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/clefourrier\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/clefourrier\/orgs","repos_url":"https:\/\/api.github.com\/users\/clefourrier\/repos","events_url":"https:\/\/api.github.com\/users\/clefourrier\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/clefourrier\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-02T10:15:58Z","updated_at":"2023-12-04T16:53:34Z","closed_at":"2023-12-04T16:53:33Z","author_association":"MEMBER","active_lock_reason":null,"body":"### Describe the bug\n\nPushing a dataset with n + m features to a repo which was deleted, but contained n features, will fail.\n\n### Steps to reproduce the bug\n\n1. Create a dataset with n features per row\r\n2. `dataset.push_to_hub(YOUR_PATH, SPLIT, token=TOKEN)`\r\n3. Go on the hub, delete the repo at `YOUR_PATH`\r\n4. Update your local dataset to have n + m features per row\r\n5. `dataset.push_to_hub(YOUR_PATH, SPLIT, token=TOKEN)` will fail because of a mismatch in features number\r\n\n\n### Expected behavior\n\nStep 5 should work or display a message to indicate the cache has not been cleared\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.10\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6376\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6376\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6375","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6375\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6375\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6375\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6375","id":1973877879,"node_id":"PR_kwDODunzps5eacao","number":6375,"title":"Temporarily pin pyarrow < 14.0.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-02T09:48:58Z","updated_at":"2023-11-02T10:22:33Z","closed_at":"2023-11-02T10:11:19Z","author_association":"MEMBER","active_lock_reason":null,"body":"Temporarily pin `pyarrow` < 14.0.0 until permanent solution is found.\r\n\r\nHot fix #6374.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6375\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6375\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6375","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6375","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6375.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6375.patch","merged_at":"2023-11-02T10:11:19Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6374","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6374\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6374\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6374\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6374","id":1973857428,"node_id":"I_kwDODunzps51pqyU","number":6374,"title":"CI is broken: TypeError: Couldn't cast array","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-11-02T09:37:06Z","updated_at":"2023-11-02T10:11:20Z","closed_at":"2023-11-02T10:11:20Z","author_association":"MEMBER","active_lock_reason":null,"body":"See: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/6730567226\/job\/18293518039\r\n```\r\nFAILED tests\/test_table.py::test_cast_sliced_fixed_size_array_to_features - TypeError: Couldn't cast array of type\r\nfixed_size_list<item: int32>[3]\r\nto\r\nSequence(feature=Value(dtype='int64', id=None), length=3, id=None)\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6374\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6374\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6373","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6373\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6373\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6373\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6373","id":1973349695,"node_id":"PR_kwDODunzps5eYsZc","number":6373,"title":"Fix typo in `Dataset.map` docstring","user":{"login":"bryant1410","id":3905501,"node_id":"MDQ6VXNlcjM5MDU1MDE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3905501?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/bryant1410","html_url":"https:\/\/github.com\/bryant1410","followers_url":"https:\/\/api.github.com\/users\/bryant1410\/followers","following_url":"https:\/\/api.github.com\/users\/bryant1410\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/bryant1410\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/bryant1410\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/bryant1410\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/bryant1410\/orgs","repos_url":"https:\/\/api.github.com\/users\/bryant1410\/repos","events_url":"https:\/\/api.github.com\/users\/bryant1410\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/bryant1410\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-02T01:36:49Z","updated_at":"2023-11-02T15:18:22Z","closed_at":"2023-11-02T10:11:38Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6373\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6373\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6373","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6373","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6373.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6373.patch","merged_at":"2023-11-02T10:11:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6372","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6372\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6372\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6372\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6372","id":1972837794,"node_id":"PR_kwDODunzps5eW9kO","number":6372,"title":"do not try to download from HF GCS for generator","user":{"login":"yundai424","id":43726198,"node_id":"MDQ6VXNlcjQzNzI2MTk4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/43726198?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yundai424","html_url":"https:\/\/github.com\/yundai424","followers_url":"https:\/\/api.github.com\/users\/yundai424\/followers","following_url":"https:\/\/api.github.com\/users\/yundai424\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yundai424\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yundai424\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yundai424\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yundai424\/orgs","repos_url":"https:\/\/api.github.com\/users\/yundai424\/repos","events_url":"https:\/\/api.github.com\/users\/yundai424\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yundai424\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-01T17:57:11Z","updated_at":"2023-11-02T16:02:52Z","closed_at":"2023-11-02T15:52:09Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"attempt to fix https:\/\/github.com\/huggingface\/datasets\/issues\/6371","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6372\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6372\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6372","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6372","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6372.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6372.patch","merged_at":"2023-11-02T15:52:09Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6371","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6371\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6371\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6371\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6371","id":1972807579,"node_id":"I_kwDODunzps51lqeb","number":6371,"title":"`Dataset.from_generator` should not try to download from HF GCS","user":{"login":"yundai424","id":43726198,"node_id":"MDQ6VXNlcjQzNzI2MTk4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/43726198?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yundai424","html_url":"https:\/\/github.com\/yundai424","followers_url":"https:\/\/api.github.com\/users\/yundai424\/followers","following_url":"https:\/\/api.github.com\/users\/yundai424\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yundai424\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yundai424\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yundai424\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yundai424\/orgs","repos_url":"https:\/\/api.github.com\/users\/yundai424\/repos","events_url":"https:\/\/api.github.com\/users\/yundai424\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yundai424\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-11-01T17:36:17Z","updated_at":"2023-11-02T15:52:10Z","closed_at":"2023-11-02T15:52:10Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nWhen using [`Dataset.from_generator`](https:\/\/github.com\/huggingface\/datasets\/blob\/c9c1166e1cf81d38534020f9c167b326585339e5\/src\/datasets\/arrow_dataset.py#L1072) with `streaming=False`, the internal logic will call [`download_and_prepare`](https:\/\/github.com\/huggingface\/datasets\/blob\/main\/src\/datasets\/io\/generator.py#L47) which will attempt to download from HF GCS which is redundant, because user has already provided the generator from which the data should be drawn. \r\n\r\nIf someone attempts to call `Dataset.from_generator` from an environment that doesn't have external internet access (for example internal production machine) and doesn't set `HF_DATASETS_OFFLINE=1`, this will result in process being stuck at building connection.\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\ndef gen():\r\n    for _ in range(100):\r\n        yield {\"text\": \"dummy text\"}\r\n\r\ndataset = datasets.Dataset.from_generator(gen)\r\n```\r\n\r\nA minimum example executed on any environment that doesn't have access to HF GCS can result in the error\n\n### Expected behavior\n\n`try_from_hf_gcs` should be set to False here https:\/\/github.com\/huggingface\/datasets\/blob\/c9c1166e1cf81d38534020f9c167b326585339e5\/src\/datasets\/io\/generator.py#L51\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-3.10.0-1160.90.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.17.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6371\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6371\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6370","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6370\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6370\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6370\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6370","id":1972073909,"node_id":"I_kwDODunzps51i3W1","number":6370,"title":"TensorDataset format does not work with Trainer from transformers","user":{"login":"jinzzasol","id":49014051,"node_id":"MDQ6VXNlcjQ5MDE0MDUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49014051?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jinzzasol","html_url":"https:\/\/github.com\/jinzzasol","followers_url":"https:\/\/api.github.com\/users\/jinzzasol\/followers","following_url":"https:\/\/api.github.com\/users\/jinzzasol\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jinzzasol\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jinzzasol\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jinzzasol\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jinzzasol\/orgs","repos_url":"https:\/\/api.github.com\/users\/jinzzasol\/repos","events_url":"https:\/\/api.github.com\/users\/jinzzasol\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jinzzasol\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-11-01T10:09:54Z","updated_at":"2023-11-29T16:31:08Z","closed_at":"2023-11-29T16:31:08Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nThe model was built to do fine tunning on BERT model for relation extraction.\r\n\r\ntrainer.train() returns an error message ```TypeError: vars() argument must have __dict__ attribute``` when it has `train_dataset` generated from `torch.utils.data.TensorDataset`\r\n\r\nHowever, in the document, the required data format is `torch.utils.data.TensorDataset`. \r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/49014051\/36fa34ac-3127-4c64-9580-9ab736136d83)\r\n\r\nTransformers trainer is supposed to accept the train_dataset in the format of torch.utils.data.TensorDataset, but it returns error message *\"TypeError: vars() argument must have __dict__ attribute\"*\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-30-5df728c929a2> in <cell line: 1>()\r\n----> 1 trainer.train()\r\n      2 trainer.evaluate(test_dataset)\r\n\r\n9 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/data\/data_collator.py in <listcomp>(.0)\r\n    107 \r\n    108     if not isinstance(features[0], Mapping):\r\n--> 109         features = [vars(f) for f in features]\r\n    110     first = features[0]\r\n    111     batch = {}\r\n\r\nTypeError: vars() argument must have __dict__ attribute\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nCreate train_dataset using `torch.utils.data.TensorDataset`, for instance,\r\n\r\n```train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_masks, train_labels)```\r\n\r\nFeed this `train_dataset` to your trainer and run trainer.train\r\n\r\n```\r\ntrainer = Trainer(model,\r\n                  training_args,\r\n                  train_dataset=train_dataset,\r\n                  eval_dataset=dev_dataset,\r\n                  compute_metrics=compute_metrics,\r\n                  )\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer should start training\r\n\r\n### Environment info\r\n\r\nIt is running on Google Colab\r\n\r\n- `datasets` version: 2.14.6\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6370\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6370\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6369","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6369\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6369\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6369\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6369","id":1971794108,"node_id":"I_kwDODunzps51hzC8","number":6369,"title":"Multi process map did not load cache file correctly","user":{"login":"enze5088","id":14285786,"node_id":"MDQ6VXNlcjE0Mjg1Nzg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14285786?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/enze5088","html_url":"https:\/\/github.com\/enze5088","followers_url":"https:\/\/api.github.com\/users\/enze5088\/followers","following_url":"https:\/\/api.github.com\/users\/enze5088\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/enze5088\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/enze5088\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/enze5088\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/enze5088\/orgs","repos_url":"https:\/\/api.github.com\/users\/enze5088\/repos","events_url":"https:\/\/api.github.com\/users\/enze5088\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/enze5088\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-11-01T06:36:54Z","updated_at":"2023-11-30T16:04:46Z","closed_at":"2023-11-30T16:04:45Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I  was training model on Multiple GPUs by DDP, the dataset is tokenized multiple times after main process. \r\n![1698820541284](https:\/\/github.com\/huggingface\/datasets\/assets\/14285786\/0b2fe054-54d8-4e00-96e6-6ca5b69e662b)\r\n\r\n![1698820501568](https:\/\/github.com\/huggingface\/datasets\/assets\/14285786\/dd62bf6f-a58f-41bf-9848-ea4fb3b62b9b)\r\n\r\nCode is modified from  [run_clm.py](https:\/\/github.com\/huggingface\/transformers\/blob\/7d8ff3629b2725ec43ace99c1a6e87ac1978d433\/examples\/pytorch\/language-modeling\/run_clm.py#L484)\r\n\n\n### Steps to reproduce the bug\n\n```\r\nblock_size = data_args.block_size\r\nIGNORE_INDEX = -100\r\nIgnore_Input = False\r\n\r\ndef tokenize_function(examples):\r\n    sources = []\r\n    targets = []\r\n    for instruction, inputs, output in zip(examples['instruction'], examples['input'], examples['output']):\r\n\r\n        source = instruction + inputs \r\n\r\n        target = f\"{output}{tokenizer.eos_token}\"\r\n\r\n        sources.append(source)\r\n        targets.append(target)\r\n\r\n    tokenized_sources = tokenizer(sources, return_attention_mask=False)\r\n\r\n    tokenized_targets = tokenizer(targets, return_attention_mask=False,\r\n                                  add_special_tokens=False\r\n                                  )\r\n\r\n    all_input_ids = []\r\n    all_labels = []\r\n    for s, t in zip(tokenized_sources['input_ids'], tokenized_targets['input_ids']):\r\n        if len(s) > block_size and Ignore_Input == False:\r\n            # print(s)\r\n            continue\r\n        input_ids = torch.LongTensor(s + t)[:block_size]\r\n        if Ignore_Input:\r\n            labels = torch.LongTensor([IGNORE_INDEX] * len(s) + t)[:block_size]\r\n        else:\r\n            labels = input_ids\r\n        assert len(input_ids) == len(labels)\r\n        all_input_ids.append(input_ids)\r\n        all_labels.append(labels)\r\n\r\n    results = {\r\n        'input_ids': all_input_ids,\r\n        'labels': all_labels,\r\n\r\n    }\r\n    return results\r\n\r\n\r\n\r\nwith training_args.main_process_first(desc=\"dataset map tokenization \", local=False):\r\n    # print('local_rank',training_args.local_rank)\r\n    if not data_args.streaming:\r\n        tokenized_datasets = raw_datasets.map(\r\n            tokenize_function,\r\n            batched=True,\r\n            num_proc=data_args.preprocessing_num_workers,\r\n            remove_columns=column_names,\r\n            load_from_cache_file=not data_args.overwrite_cache,\r\n            desc=\"Running tokenizer on dataset \",\r\n        )\r\n    else:\r\n        tokenized_datasets = raw_datasets.map(\r\n            tokenize_function,\r\n            batched=True,\r\n            remove_columns=column_names,\r\n            desc=\"Running tokenizer on dataset \"\r\n        )\r\n```\n\n### Expected behavior\n\nThis code should only tokenize the dataset in the main process, and the other processes load the dataset after waiting\n\n### Environment info\n\ntransformers == 4.34.1\r\ndatasets == 2.14.5","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6369\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6369\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6368","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6368\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6368\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6368\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6368","id":1971193692,"node_id":"PR_kwDODunzps5eRZwQ","number":6368,"title":"Fix python formatting for complex types in `format_table`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-31T19:48:08Z","updated_at":"2023-11-02T14:42:28Z","closed_at":"2023-11-02T14:21:16Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6366","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6368\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6368\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6368","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6368","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6368.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6368.patch","merged_at":"2023-11-02T14:21:16Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6367","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6367\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6367\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6367\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6367","id":1971015861,"node_id":"PR_kwDODunzps5eQy1D","number":6367,"title":"Fix time measuring snippet in docs","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-31T17:57:17Z","updated_at":"2023-10-31T18:35:53Z","closed_at":"2023-10-31T18:24:02Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix https:\/\/discuss.huggingface.co\/t\/attributeerror-enter\/60509","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6367\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6367\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6367","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6367","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6367.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6367.patch","merged_at":"2023-10-31T18:24:02Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6366","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6366\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6366\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6366\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6366","id":1970213490,"node_id":"I_kwDODunzps51bxJy","number":6366,"title":"with_format() function returns bytes instead of PIL images even when image column is not part of \"columns\"","user":{"login":"leot13","id":17809020,"node_id":"MDQ6VXNlcjE3ODA5MDIw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17809020?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leot13","html_url":"https:\/\/github.com\/leot13","followers_url":"https:\/\/api.github.com\/users\/leot13\/followers","following_url":"https:\/\/api.github.com\/users\/leot13\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leot13\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leot13\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leot13\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leot13\/orgs","repos_url":"https:\/\/api.github.com\/users\/leot13\/repos","events_url":"https:\/\/api.github.com\/users\/leot13\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leot13\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-31T11:10:48Z","updated_at":"2023-11-02T14:21:17Z","closed_at":"2023-11-02T14:21:17Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen using the with_format() function on a dataset containing images, even if the image column is not part of the columns provided in the function, its type will be changed to bytes.\r\nHere is a minimal reproduction of the bug:\r\nhttps:\/\/colab.research.google.com\/drive\/1hyaOspgyhB41oiR1-tXE3k_gJCdJUQCf?usp=sharing\n\n### Steps to reproduce the bug\n\n1. Load the image dataset\r\n2. apply with_format(columns=[\"text\"])\r\n3. Check the type of images in the \"image\" column before and after applying with_format\n\n### Expected behavior\n\nThe type should stay the same, but it does not\n\n### Environment info\n\ndatasets==2.14.6\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6366\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6366\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6365","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6365\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6365\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6365\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6365","id":1970140392,"node_id":"I_kwDODunzps51bfTo","number":6365,"title":"Parquet size grows exponential for categorical data","user":{"login":"aseganti","id":82567957,"node_id":"MDQ6VXNlcjgyNTY3OTU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/82567957?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/aseganti","html_url":"https:\/\/github.com\/aseganti","followers_url":"https:\/\/api.github.com\/users\/aseganti\/followers","following_url":"https:\/\/api.github.com\/users\/aseganti\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/aseganti\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/aseganti\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/aseganti\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/aseganti\/orgs","repos_url":"https:\/\/api.github.com\/users\/aseganti\/repos","events_url":"https:\/\/api.github.com\/users\/aseganti\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/aseganti\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-31T10:29:02Z","updated_at":"2023-10-31T10:49:17Z","closed_at":"2023-10-31T10:49:17Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nIt seems that when saving a data frame with a categorical column inside the size can grow exponentially.\r\n\r\nThis seems to happen because when we save the categorical data to parquet, we are saving the data + all the categories existing in the original data. This happens even when the categories are not present in the original data.\r\n\n\n### Steps to reproduce the bug\n\nTo reproduce the bug, it is enough to run this script:\r\n\r\n```\r\nimport pandas as pd\r\nimport os\r\n\r\nif __name__ == \"__main__\":\r\n    for n in [10, 1e2, 1e3, 1e4, 1e5]:\r\n        for n_col in [1, 10, 100, 1000, 10000]:\r\n            input = pd.DataFrame([{\"{i}\": f\"{i}_cat\" for col in range(n_col)} for i in range(int(n))])\r\n            input.iloc[0:100].to_parquet(\"a.parquet\")\r\n            for col in input.columns:\r\n                input[col] = input[col].astype(\"category\")\r\n            input.iloc[0:100].to_parquet(\"b.parquet\")\r\n            a_size_mb = os.stat(\"a.parquet\").st_size \/ (1024 * 1024)\r\n            b_size_mb = os.stat(\"b.parquet\").st_size \/ (1024 * 1024)\r\n            print(f\"{n} {n_col} {a_size_mb} {b_size_mb} {100*b_size_mb\/a_size_mb:.2f}\")\r\n```\r\n\r\nThat produces this output:\r\n\r\n<img width=\"464\" alt=\"Screenshot 2023-10-31 at 11 25 25\" src=\"https:\/\/github.com\/huggingface\/datasets\/assets\/82567957\/2b8a9284-7f9e-4c10-a006-0a27236ebd15\">\r\n\r\n\n\n### Expected behavior\n\nIn my opinion either:\r\n\r\n1. The two file should have (almost) the same size\r\n2. There should be warning telling the user that such difference in size is possible\n\n### Environment info\n\nPython 3.8.18\r\npandas==2.0.3\r\nnumpy==1.24.4","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6365\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6365\/timeline","performed_via_github_app":null,"state_reason":"not_planned","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6364","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6364\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6364\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6364\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6364","id":1969136106,"node_id":"I_kwDODunzps51XqHq","number":6364,"title":"ArrowNotImplementedError: Unsupported cast from string to list using function cast_list","user":{"login":"divyakrishna-devisetty","id":32887094,"node_id":"MDQ6VXNlcjMyODg3MDk0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32887094?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/divyakrishna-devisetty","html_url":"https:\/\/github.com\/divyakrishna-devisetty","followers_url":"https:\/\/api.github.com\/users\/divyakrishna-devisetty\/followers","following_url":"https:\/\/api.github.com\/users\/divyakrishna-devisetty\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/divyakrishna-devisetty\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/divyakrishna-devisetty\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/divyakrishna-devisetty\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/divyakrishna-devisetty\/orgs","repos_url":"https:\/\/api.github.com\/users\/divyakrishna-devisetty\/repos","events_url":"https:\/\/api.github.com\/users\/divyakrishna-devisetty\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/divyakrishna-devisetty\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-30T20:14:01Z","updated_at":"2023-10-31T19:21:23Z","closed_at":"2023-10-31T19:21:23Z","author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\n\r\nI am trying to load a local csv dataset(similar to explodinggradients_fiqa) using load_dataset. When I try to pass features, I am facing the mentioned issue.\r\n\r\nCSV Data sample(golden_dataset.csv):\r\nQuestion        |          Context                      |         answer      |        groundtruth\r\n\"what is abc?\"   | \"abc is this and that\"     |    \"abc is this \"  |    \"abc is this and that\"\r\n\r\n```\r\nimport csv \r\n\r\n# built it based on https:\/\/huggingface.co\/datasets\/explodinggradients\/fiqa\/viewer\/ragas_eval?row=0\r\nmydict = [\r\n{'question' : \"what is abc?\", 'contexts': [\"abc is this and that\"], 'answer': \"abc is this \" , 'groundtruth':  [\"abc is this and that\"]},\r\n{'question' : \"what is abc?\", 'contexts': [\"abc is this and that\"], 'answer': \"abc is this \" , 'groundtruth':  [\"abc is this and that\"]},\r\n{'question' : \"what is abc?\", 'contexts': [\"abc is this and that\"], 'answer': \"abc is this \" , 'groundtruth':  [\"abc is this and that\"]}\r\n]\r\n         \r\nfields = ['question', 'contexts', 'answer', 'ground_truths'] \r\n\r\nwith open('golden_dataset.csv', 'w', newline='\\n') as file: \r\n    writer = csv.DictWriter(file, fieldnames = fields)\r\n    \r\n    writer.writeheader() \r\n    for row in mydict:\r\n        writer.writerow(row)\r\n```\r\n\r\nRetrieved dataset:\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['question', 'contexts', 'answer', 'ground_truths'],\r\n        num_rows: 1\r\n    })\r\n})\r\n\r\n\r\nCode to reproduce issue:\r\n\r\n\r\n```\r\nfrom datasets import load_dataset, Features, Sequence, Value\r\n\r\nencode_features = Features(\r\n    {\r\n        \"question\": Value(dtype='string', id=0),\r\n        \"contexts\": Sequence(feature=Value(dtype='string', id=1)),\r\n        \"answer\": Value(dtype='string', id=2),\r\n        \"ground_truths\": Sequence(feature=Value(dtype='string',id=3)),\r\n    }\r\n)\r\n\r\neval_dataset = load_dataset('csv', data_files='\/golden_dataset.csv', features = encode_features )\r\n```\r\n\r\n\r\nError trace:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nArrowNotImplementedError                  Traceback (most recent call last)\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/builder.py:1925, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1924 _time = time.time()\r\n-> 1925 for _, table in generator:\r\n   1926     if max_shard_size is not None and writer._num_bytes > max_shard_size:\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/csv\/csv.py:192, in Csv._generate_tables(self, files)\r\n    189         # Uncomment for debugging (will print the Arrow table size and elements)\r\n    190         # logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\r\n    191         # logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\r\n--> 192         yield (file_idx, batch_idx), self._cast_table(pa_table)\r\n    193 except ValueError as e:\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/csv\/csv.py:167, in Csv._cast_table(self, pa_table)\r\n    165 if all(not require_storage_cast(feature) for feature in self.config.features.values()):\r\n    166     # cheaper cast\r\n--> 167     pa_table = pa.Table.from_arrays([pa_table[field.name] for field in schema], schema=schema)\r\n    168 else:\r\n    169     # more expensive cast; allows str <-> int\/float or str to Audio for example\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/pyarrow\/table.pxi:3781, in pyarrow.lib.Table.from_arrays()\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/pyarrow\/table.pxi:1449, in pyarrow.lib._sanitize_arrays()\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/pyarrow\/array.pxi:354, in pyarrow.lib.asarray()\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/pyarrow\/table.pxi:551, in pyarrow.lib.ChunkedArray.cast()\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/pyarrow\/compute.py:400, in cast(arr, target_type, safe, options, memory_pool)\r\n    399         options = CastOptions.safe(target_type)\r\n--> 400 return call_function(\"cast\", [arr], options, memory_pool)\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/pyarrow\/_compute.pyx:572, in pyarrow._compute.call_function()\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/pyarrow\/_compute.pyx:367, in pyarrow._compute.Function.call()\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/pyarrow\/error.pxi:144, in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/pyarrow\/error.pxi:121, in pyarrow.lib.check_status()\r\n\r\nArrowNotImplementedError: Unsupported cast from string to list using function cast_list\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\nCell In[57], line 1\r\n----> 1 eval_dataset = load_dataset('csv', data_files='\/golden_dataset.csv', features = encode_features )\r\n     \r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/load.py:2153, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2150 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\r\n   2152 # Download and prepare data\r\n-> 2153 builder_instance.download_and_prepare(\r\n   2154     download_config=download_config,\r\n   2155     download_mode=download_mode,\r\n   2156     verification_mode=verification_mode,\r\n   2157     try_from_hf_gcs=try_from_hf_gcs,\r\n   2158     num_proc=num_proc,\r\n   2159     storage_options=storage_options,\r\n   2160 )\r\n   2162 # Build dataset for splits\r\n   2163 keep_in_memory = (\r\n   2164     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   2165 )\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/builder.py:954, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    952     if num_proc is not None:\r\n    953         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 954     self._download_and_prepare(\r\n    955         dl_manager=dl_manager,\r\n    956         verification_mode=verification_mode,\r\n    957         **prepare_split_kwargs,\r\n    958         **download_and_prepare_kwargs,\r\n    959     )\r\n    960 # Sync info\r\n    961 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/builder.py:1049, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   1045 split_dict.add(split_generator.split_info)\r\n   1047 try:\r\n   1048     # Prepare split will record examples associated to the split\r\n-> 1049     self._prepare_split(split_generator, **prepare_split_kwargs)\r\n   1050 except OSError as e:\r\n   1051     raise OSError(\r\n   1052         \"Cannot find data file. \"\r\n   1053         + (self.manual_download_instructions or \"\")\r\n   1054         + \"\\nOriginal error:\\n\"\r\n   1055         + str(e)\r\n   1056     ) from None\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/builder.py:1813, in ArrowBasedBuilder._prepare_split(self, split_generator, file_format, num_proc, max_shard_size)\r\n   1811 job_id = 0\r\n   1812 with pbar:\r\n-> 1813     for job_id, done, content in self._prepare_split_single(\r\n   1814         gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n   1815     ):\r\n   1816         if done:\r\n   1817             result = content\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/builder.py:1958, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1956     if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\r\n   1957         e = e.__context__\r\n-> 1958     raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1960 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset \r\n```\r\nEnvironment Info:\r\ndatasets version: 2.14.5\r\nPython version: 3.10.8\r\nPyArrow version: 12.0.1\r\nPandas version: 2.0.3\r\n\r\nI have also tried to load dataset first and then use cast_column, or save_to_disk and load_from_disk.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6364\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6364\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6363","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6363\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6363\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6363\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6363","id":1968891277,"node_id":"I_kwDODunzps51WuWN","number":6363,"title":"dataset.transform() hangs indefinitely while finetuning the stable diffusion XL","user":{"login":"bhosalems","id":10846405,"node_id":"MDQ6VXNlcjEwODQ2NDA1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10846405?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/bhosalems","html_url":"https:\/\/github.com\/bhosalems","followers_url":"https:\/\/api.github.com\/users\/bhosalems\/followers","following_url":"https:\/\/api.github.com\/users\/bhosalems\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/bhosalems\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/bhosalems\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/bhosalems\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/bhosalems\/orgs","repos_url":"https:\/\/api.github.com\/users\/bhosalems\/repos","events_url":"https:\/\/api.github.com\/users\/bhosalems\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/bhosalems\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-10-30T17:34:05Z","updated_at":"2023-11-22T00:29:21Z","closed_at":"2023-11-22T00:29:21Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nMulti-GPU fine-tuning the stable diffusion X by following https:\/\/github.com\/huggingface\/diffusers\/blob\/main\/examples\/text_to_image\/README_sdxl.md hangs indefinitely.\r\n\r\n### Steps to reproduce the bug\r\n\r\naccelerate launch train_text_to_image_sdxl.py --pretrained_model_name_or_path=$MODEL_NAME --pretrained_vae_model_name_or_path=$VAE_NAME --dataset_name=$DATASET_NAME --enable_xformers_memory_efficient_attention --resolution=512 --center_crop --random_flip --proportion_empty_prompts=0.2 --train_batch_size=1 --gradient_accumulation_steps=4 --gradient_checkpointing --max_train_steps=10000 --use_8bit_adam --learning_rate=1e-06 --lr_scheduler=\"constant\" --lr_warmup_steps=0 --mixed_precision=\"fp16\" --report_to=\"wandb\" --validation_prompt=\"a cute Sundar Pichai creature\" --validation_epochs 5 --checkpointing_steps=5000 --output_dir=\"sdxl-pokemon-model\"\r\n\r\n### Expected behavior\r\n\r\nIt should start the training as it does for the single GPU training. I opened the issue in diffusers **https:\/\/github.com\/huggingface\/diffusers\/issues\/5534 but it does seem to be an issue with the Pokemon dataset.\r\n\r\nI added some debug prints\r\n\r\n  ```\r\n  print(\"==========HERE3=============\")\r\n    with accelerator.main_process_first():\r\n        print(accelerator.is_main_process)\r\n        print(\"===========Here3.1===========\")\r\n        if args.max_train_samples is not None:\r\n            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\r\n        print(\"===========Here3.2===========\")\r\n        # Set the training transforms\r\n        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\r\n    print(\"==========HERE4=============\")\r\n\r\n\r\nCorresponding Output\r\n\r\nDetected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\r\n10\/25\/2023 21:18:04 - INFO - main - Distributed environment: MULTI_GPU Backend: nccl\r\nNum processes: 3\r\nProcess index: 1\r\nLocal process index: 1\r\nDevice: cuda:1\r\n\r\nMixed precision type: fp16\r\n\r\n10\/25\/2023 21:18:04 - INFO - main - Distributed environment: MULTI_GPU Backend: nccl\r\nNum processes: 3\r\nProcess index: 2\r\nLocal process index: 2\r\nDevice: cuda:2\r\n\r\nMixed precision type: fp16\r\n\r\n10\/25\/2023 21:18:04 - INFO - main - Distributed environment: MULTI_GPU Backend: nccl\r\nNum processes: 3\r\nProcess index: 0\r\nLocal process index: 0\r\nDevice: cuda:0\r\n\r\nMixed precision type: fp16\r\n\r\nYou are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\r\n{\u2018variance_type\u2019, \u2018clip_sample_range\u2019, \u2018thresholding\u2019, \u2018dynamic_thresholding_ratio\u2019} was not found in config. Values will be initialized to default values.\r\n{\u2018attention_type\u2019, \u2018reverse_transformer_layers_per_block\u2019, \u2018dropout\u2019} was not found in config. Values will be initialized to default values.\r\n==========HERE1=============\r\n==========HERE1=============\r\n==========HERE1=============\r\n==========HERE2=============\r\n==========HERE2=============\r\n==========HERE2=============\r\n==========HERE3=============\r\nTrue\r\n===========Here3.1===========\r\n===========Here3.2===========\r\n==========HERE3=============\r\n==========HERE3=========\r\n```\r\n\r\n### Environment info\r\n\r\n_libgcc_mutex 0.1 conda_forge conda-forge\r\n_openmp_mutex 4.5 2_kmp_llvm conda-forge\r\nabsl-py 2.0.0 pypi_0 pypi\r\naccelerate 0.24.0 pypi_0 pypi\r\naiohttp 3.8.6 pypi_0 pypi\r\naiosignal 1.3.1 pypi_0 pypi\r\nappdirs 1.4.4 pyh9f0ad1d_0 conda-forge\r\nasync-timeout 4.0.3 pypi_0 pypi\r\nattrs 23.1.0 pypi_0 pypi\r\nbitsandbytes 0.41.1 pypi_0 pypi\r\nblas 1.0 mkl\r\nblessings 1.7 py39h06a4308_1002\r\nbrotli-python 1.0.9 py39h6a678d5_7\r\nbzip2 1.0.8 h7b6447c_0\r\nca-certificates 2023.08.22 h06a4308_0\r\ncachetools 5.3.2 pypi_0 pypi\r\ncertifi 2023.7.22 py39h06a4308_0\r\ncffi 1.15.1 py39h5eee18b_3\r\ncharset-normalizer 2.0.4 pyhd3eb1b0_0\r\nclick 8.1.7 unix_pyh707e725_0 conda-forge\r\ncryptography 41.0.3 py39hdda0065_0\r\ncuda-cudart 11.7.99 0 nvidia\r\ncuda-cupti 11.7.101 0 nvidia\r\ncuda-libraries 11.7.1 0 nvidia\r\ncuda-nvrtc 11.7.99 0 nvidia\r\ncuda-nvtx 11.7.91 0 nvidia\r\ncuda-runtime 11.7.1 0 nvidia\r\ndatasets 2.14.6 pypi_0 pypi\r\ndiffusers 0.22.0.dev0 pypi_0 pypi\r\ndill 0.3.7 pypi_0 pypi\r\ndocker-pycreds 0.4.0 py_0 conda-forge\r\nffmpeg 4.3 hf484d3e_0 pytorch\r\nfilelock 3.12.4 pypi_0 pypi\r\nfreetype 2.12.1 h4a9f257_0\r\nfrozenlist 1.4.0 pypi_0 pypi\r\nfsspec 2023.10.0 pypi_0 pypi\r\nftfy 6.1.1 pypi_0 pypi\r\ngiflib 5.2.1 h5eee18b_3\r\ngitdb 4.0.11 pyhd8ed1ab_0 conda-forge\r\ngitpython 3.1.40 pyhd8ed1ab_0 conda-forge\r\ngmp 6.2.1 h295c915_3\r\ngnutls 3.6.15 he1e5248_0\r\ngoogle-auth 2.23.3 pypi_0 pypi\r\ngoogle-auth-oauthlib 1.1.0 pypi_0 pypi\r\ngpustat 0.6.0 pyhd3eb1b0_1\r\ngrpcio 1.59.0 pypi_0 pypi\r\nhuggingface-hub 0.17.3 pypi_0 pypi\r\nidna 3.4 py39h06a4308_0\r\nimportlib-metadata 6.8.0 pypi_0 pypi\r\nintel-openmp 2023.1.0 hdb19cb5_46305\r\njinja2 3.1.2 pypi_0 pypi\r\njpeg 9e h5eee18b_1\r\nlame 3.100 h7b6447c_0\r\nlcms2 2.12 h3be6417_0\r\nld_impl_linux-64 2.38 h1181459_1\r\nlerc 3.0 h295c915_0\r\nlibcublas 11.10.3.66 0 nvidia\r\nlibcufft 10.7.2.124 h4fbf590_0 nvidia\r\nlibcufile 1.8.0.34 0 nvidia\r\nlibcurand 10.3.4.52 0 nvidia\r\nlibcusolver 11.4.0.1 0 nvidia\r\nlibcusparse 11.7.4.91 0 nvidia\r\nlibdeflate 1.17 h5eee18b_1\r\nlibffi 3.4.4 h6a678d5_0\r\nlibgcc-ng 13.2.0 h807b86a_2 conda-forge\r\nlibgfortran-ng 13.2.0 h69a702a_2 conda-forge\r\nlibgfortran5 13.2.0 ha4646dd_2 conda-forge\r\nlibiconv 1.16 h7f8727e_2\r\nlibidn2 2.3.4 h5eee18b_0\r\nlibnpp 11.7.4.75 0 nvidia\r\nlibnvjpeg 11.8.0.2 0 nvidia\r\nlibpng 1.6.39 h5eee18b_0\r\nlibprotobuf 3.20.3 he621ea3_0\r\nlibstdcxx-ng 13.2.0 h7e041cc_2 conda-forge\r\nlibtasn1 4.19.0 h5eee18b_0\r\nlibtiff 4.5.1 h6a678d5_0\r\nlibunistring 0.9.10 h27cfd23_0\r\nlibwebp 1.3.2 h11a3e52_0\r\nlibwebp-base 1.3.2 h5eee18b_0\r\nllvm-openmp 14.0.6 h9e868ea_0\r\nlz4-c 1.9.4 h6a678d5_0\r\nmarkdown 3.5 pypi_0 pypi\r\nmarkupsafe 2.1.3 pypi_0 pypi\r\nmkl 2023.1.0 h213fc3f_46343\r\nmkl-service 2.4.0 py39h5eee18b_1\r\nmkl_fft 1.3.8 py39h5eee18b_0\r\nmkl_random 1.2.4 py39hdb19cb5_0\r\nmultidict 6.0.4 pypi_0 pypi\r\nmultiprocess 0.70.15 pypi_0 pypi\r\nncurses 6.4 h6a678d5_0\r\nnettle 3.7.3 hbbd107a_1\r\nnumpy 1.26.0 py39h5f9d8c6_0\r\nnumpy-base 1.26.0 py39hb5e798b_0\r\nnvidia-ml 7.352.0 pyhd3eb1b0_0\r\noauthlib 3.2.2 pypi_0 pypi\r\nopenh264 2.1.1 h4ff587b_0\r\nopenjpeg 2.4.0 h3ad879b_0\r\nopenssl 3.0.11 h7f8727e_2\r\npackaging 23.2 pypi_0 pypi\r\npandas 2.1.1 pypi_0 pypi\r\npathtools 0.1.2 py_1 conda-forge\r\npillow 10.0.1 py39ha6cbd5a_0\r\npip 23.3 py39h06a4308_0\r\nprotobuf 4.23.4 pypi_0 pypi\r\npsutil 5.9.6 pypi_0 pypi\r\npyarrow 13.0.0 pypi_0 pypi\r\npyasn1 0.5.0 pypi_0 pypi\r\npyasn1-modules 0.3.0 pypi_0 pypi\r\npycparser 2.21 pyhd3eb1b0_0\r\npyopenssl 23.2.0 py39h06a4308_0\r\npysocks 1.7.1 py39h06a4308_0\r\npython 3.9.18 h955ad1f_0\r\npython-dateutil 2.8.2 pypi_0 pypi\r\npython_abi 3.9 2_cp39 conda-forge\r\npytorch 1.13.1 py3.9_cuda11.7_cudnn8.5.0_0 pytorch\r\npytorch-cuda 11.7 h778d358_5 pytorch\r\npytorch-mutex 1.0 cuda pytorch\r\npytz 2023.3.post1 pypi_0 pypi\r\npyyaml 6.0.1 pypi_0 pypi\r\nreadline 8.2 h5eee18b_0\r\nregex 2023.10.3 pypi_0 pypi\r\nrequests 2.31.0 py39h06a4308_0\r\nrequests-oauthlib 1.3.1 pypi_0 pypi\r\nrsa 4.9 pypi_0 pypi\r\nsafetensors 0.4.0 pypi_0 pypi\r\nscipy 1.11.3 py39h5f9d8c6_0\r\nsentry-sdk 1.32.0 pyhd8ed1ab_0 conda-forge\r\nsetproctitle 1.1.10 py39h3811e60_1004 conda-forge\r\nsetuptools 68.0.0 py39h06a4308_0\r\nsix 1.16.0 pyh6c4a22f_0 conda-forge\r\nsmmap 5.0.0 pyhd8ed1ab_0 conda-forge\r\nsqlite 3.41.2 h5eee18b_0\r\ntbb 2021.8.0 hdb19cb5_0\r\ntensorboard 2.15.0 pypi_0 pypi\r\ntensorboard-data-server 0.7.2 pypi_0 pypi\r\ntk 8.6.12 h1ccaba5_0\r\ntokenizers 0.14.1 pypi_0 pypi\r\ntorchaudio 0.13.1 py39_cu117 pytorch\r\ntorchtriton 2.1.0 py39 pytorch\r\ntorchvision 0.14.1 py39_cu117 pytorch\r\ntqdm 4.66.1 pypi_0 pypi\r\ntransformers 4.34.1 pypi_0 pypi\r\ntyping_extensions 4.7.1 py39h06a4308_0\r\ntzdata 2023.3 pypi_0 pypi\r\nurllib3 1.26.18 py39h06a4308_0\r\nwandb 0.15.12 pyhd8ed1ab_0 conda-forge\r\nwcwidth 0.2.8 pypi_0 pypi\r\nwerkzeug 3.0.1 pypi_0 pypi\r\nwheel 0.41.2 py39h06a4308_0\r\nxformers 0.0.22.post7 py39_cu11.7.1_pyt1.13.1 xformers\r\nxxhash 3.4.1 pypi_0 pypi\r\nxz 5.4.2 h5eee18b_0\r\nyaml 0.2.5 h7f98852_2 conda-forge\r\nyarl 1.9.2 pypi_0 pypi\r\nzipp 3.17.0 pypi_0 pypi\r\nzlib 1.2.13 h5eee18b_0\r\nzstd 1.5.5 hc292b87_0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6363\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6363\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6362","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6362\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6362\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6362\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6362","id":1965794569,"node_id":"PR_kwDODunzps5d_MxD","number":6362,"title":"Simplify filesystem logic","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2023-10-27T15:54:18Z","updated_at":"2023-11-15T14:08:29Z","closed_at":"2023-11-15T14:02:02Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Simplifies the existing filesystem logic (e.g., to avoid unnecessary if-else as mentioned in https:\/\/github.com\/huggingface\/datasets\/pull\/6098#issue-1827655071)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6362\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6362\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6362","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6362","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6362.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6362.patch","merged_at":"2023-11-15T14:02:02Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6360","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6360\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6360\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6360\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6360","id":1965672950,"node_id":"I_kwDODunzps51Kcn2","number":6360,"title":" Add support for `Sequence(Audio\/Image)` feature in `push_to_hub`","user":{"login":"Laurent2916","id":21087104,"node_id":"MDQ6VXNlcjIxMDg3MTA0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/21087104?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Laurent2916","html_url":"https:\/\/github.com\/Laurent2916","followers_url":"https:\/\/api.github.com\/users\/Laurent2916\/followers","following_url":"https:\/\/api.github.com\/users\/Laurent2916\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Laurent2916\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Laurent2916\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Laurent2916\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Laurent2916\/orgs","repos_url":"https:\/\/api.github.com\/users\/Laurent2916\/repos","events_url":"https:\/\/api.github.com\/users\/Laurent2916\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Laurent2916\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-10-27T14:39:57Z","updated_at":"2023-11-02T17:49:28Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nAllow for `Sequence` of `Image` (or `Audio`) to be embedded inside the shards.\n\n### Motivation\n\nCurrently, thanks to #3685, when `embed_external_files` is set to True (which is the default) in `push_to_hub`, features of type `Image` and `Audio` are embedded inside the arrow\/parquet shards, instead of only storing paths to the files.\r\n\r\nI've noticed that this behavior does not extend to `Sequence` of `Image`, when working with a [dataset of timelapse images](https:\/\/huggingface.co\/datasets\/1aurent\/Human-Embryo-Timelapse).\n\n### Your contribution\n\nI'll submit a PR if I find a way to add this feature","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6360\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6360\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6359","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6359\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6359\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6359\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6359","id":1965378583,"node_id":"I_kwDODunzps51JUwX","number":6359,"title":"Stuck in \"Resolving data files...\"","user":{"login":"Luciennnnnnn","id":20135317,"node_id":"MDQ6VXNlcjIwMTM1MzE3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20135317?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Luciennnnnnn","html_url":"https:\/\/github.com\/Luciennnnnnn","followers_url":"https:\/\/api.github.com\/users\/Luciennnnnnn\/followers","following_url":"https:\/\/api.github.com\/users\/Luciennnnnnn\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Luciennnnnnn\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Luciennnnnnn\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Luciennnnnnn\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Luciennnnnnn\/orgs","repos_url":"https:\/\/api.github.com\/users\/Luciennnnnnn\/repos","events_url":"https:\/\/api.github.com\/users\/Luciennnnnnn\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Luciennnnnnn\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-27T12:01:51Z","updated_at":"2023-10-28T01:38:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI have an image dataset with 300k images, the size of image is 768 * 768.\r\n\r\nWhen I run `dataset = load_dataset(\"imagefolder\", data_dir=\"\/path\/to\/img_dir\", split='train')` in second time, it takes 50 minutes to finish \"Resolving data files\" part, what's going on in this part?\r\n\r\nFrom my understand, after Arrow files been created in the first run, the second run should not take time longer than one or two minutes.\r\n\r\n### Steps to reproduce the bug\r\n\r\n# Run following code two times\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"\/path\/to\/img_dir\", split='train')\r\n\r\n### Expected behavior\r\n\r\nFast dataset building\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.0-60-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6359\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6359\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6358","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6358\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6358\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6358\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6358","id":1965014595,"node_id":"I_kwDODunzps51H75D","number":6358,"title":"Mounting datasets cache fails due to absolute paths.","user":{"login":"charliebudd","id":72921588,"node_id":"MDQ6VXNlcjcyOTIxNTg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/72921588?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/charliebudd","html_url":"https:\/\/github.com\/charliebudd","followers_url":"https:\/\/api.github.com\/users\/charliebudd\/followers","following_url":"https:\/\/api.github.com\/users\/charliebudd\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/charliebudd\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/charliebudd\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/charliebudd\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/charliebudd\/orgs","repos_url":"https:\/\/api.github.com\/users\/charliebudd\/repos","events_url":"https:\/\/api.github.com\/users\/charliebudd\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/charliebudd\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-27T08:20:27Z","updated_at":"2023-11-28T14:47:12Z","closed_at":"2023-11-28T14:47:12Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nCreating a datasets cache and mounting this into, for example, a docker container, renders the data unreadable due to absolute paths written into the cache.\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Create a datasets cache by downloading some data\r\n2. Mount the dataset folder into a docker container or remote system.\r\n3. (Edit) Set `HF_HOME` or `HF_DATASET_CACHE` to point to the mounted cache.\r\n4. Attempt to access the data from within the docker container.\r\n5. An error is thrown saying no file exists at \\<absolute path to original cache location\\>\r\n\r\n### Expected behavior\r\n\r\nThe data is loaded without error\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.4.0-162-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6358\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6358\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6357","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6357\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6357\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6357\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6357","id":1964653995,"node_id":"I_kwDODunzps51Gj2r","number":6357,"title":"Allow passing a multiprocessing context to functions that support `num_proc`","user":{"login":"bryant1410","id":3905501,"node_id":"MDQ6VXNlcjM5MDU1MDE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3905501?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/bryant1410","html_url":"https:\/\/github.com\/bryant1410","followers_url":"https:\/\/api.github.com\/users\/bryant1410\/followers","following_url":"https:\/\/api.github.com\/users\/bryant1410\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/bryant1410\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/bryant1410\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/bryant1410\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/bryant1410\/orgs","repos_url":"https:\/\/api.github.com\/users\/bryant1410\/repos","events_url":"https:\/\/api.github.com\/users\/bryant1410\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/bryant1410\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-10-27T02:31:16Z","updated_at":"2023-10-27T02:31:16Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nAllow specifying [a multiprocessing context](https:\/\/docs.python.org\/3\/library\/multiprocessing.html#contexts-and-start-methods) to functions that support `num_proc` or use multiprocessing pools. For example, the following could be done:\r\n\r\n```python\r\ndataset = dataset.map(_func, num_proc=2, mp_context=multiprocess.get_context(\"spawn\"))\r\n```\r\n\r\nOr at least the multiprocessing start method (\"fork\", \"spawn\", \"fork_server\" or `None`):\r\n\r\n```python\r\ndataset = dataset.map(_func, num_proc=2, mp_start_method=\"spawn\")\r\n```\r\n\r\nAnother option could be passing the `pool` as an argument.\n\n### Motivation\n\nBy default, `multiprocess` (the `multiprocessing`-fork library that this repo uses) uses the \"fork\" start method for multiprocessing pools (for the default context). It could be changed by using `set_start_method`. However, this conditions the multiprocessing start method from all processing in a Python program that uses the default context, because [you can't call that function more than once](https:\/\/docs.python.org\/3\/library\/multiprocessing.html#contexts-and-start-methods:~:text=set_start_method()%20should%20not%20be%20used%20more%20than%20once%20in%20the%20program.). My proposal is to allow using a different multiprocessing context, not to condition the whole Python program.\r\n\r\nOne reason to change the start method is that \"fork\" (the default) makes child processes likely deadlock if thread pools were created before (and also this is not supported by POSIX). For example, this happens when using PyTorch because OpenMP threads are used for CPU intra-op parallelism, which is enabled by default (e.g., for context see [`torch.set_num_threads`](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.set_num_threads.html)). This can also be fixed by setting `torch.set_num_threads(1)` (or similarly by other methods) but this conditions the whole Python program as it can only be set once to guarantee its behavior (similarly to). There are noticeable performance differences when setting this number to 1 even when using GPU(s). Using, e.g., a \"spawn\" start method would solve this issue.\r\n\r\nFor more context, see:\r\n\r\n* https:\/\/discuss.huggingface.co\/t\/dataset-map-stuck-with-torch-set-num-threads-set-to-2-or-larger\/37984\r\n* https:\/\/discuss.huggingface.co\/t\/using-num-proc-1-in-dataset-map-hangs\/44310\n\n### Your contribution\n\nI'd be happy to review a PR that makes such a change. And if you really don't have the bandwidth for it, I'd consider creating one.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6357\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6357\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6356","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6356\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6356\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6356\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6356","id":1964015802,"node_id":"PR_kwDODunzps5d5Jri","number":6356,"title":"Add `fsspec` version to the `datasets-cli env` command output","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-26T17:19:25Z","updated_at":"2023-10-26T18:42:56Z","closed_at":"2023-10-26T18:32:21Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"... to make debugging issues easier, as `fsspec`'s releases often introduce breaking changes.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6356\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6356\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6356","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6356","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6356.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6356.patch","merged_at":"2023-10-26T18:32:21Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6355","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6355\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6355\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6355\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6355","id":1963979896,"node_id":"PR_kwDODunzps5d5B2B","number":6355,"title":"More hub centric docs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-26T16:54:46Z","updated_at":"2023-10-30T17:33:32Z","closed_at":"2023-10-30T17:32:57Z","author_association":"MEMBER","active_lock_reason":null,"body":"Let's have more hub-centric documentation in the datasets docs\r\n\r\nTutorials\r\n- Add \u201cConfigure the dataset viewer\u201d page\r\n- Change order:\r\n  - Overview\r\n    - and more focused on the Hub rather than the library\r\n  - Then all the hub related things\r\n    - and mention how to read\/write with other tools like pandas\r\n  - Then all the datasets lib related things in a subsection\r\n\r\nAlso:\r\n- Rename \u201cknow your dataset\u201d page to \u201cExplore your dataset\u201d\r\n- Remove \u201cEvaluate Predictions\u201d page since it's 'evaluate' stuff (or move to legacy section ?)\r\n\r\nTODO:\r\n- [ ] write the \u201cConfigure the dataset viewer\u201d page","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6355\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6355\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6355","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6355","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6355.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6355.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6354","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6354\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6354\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6354\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6354","id":1963483324,"node_id":"I_kwDODunzps51CGC8","number":6354,"title":"`IterableDataset.from_spark` does not support multiple workers in pytorch `Dataloader`","user":{"login":"NazyS","id":50199774,"node_id":"MDQ6VXNlcjUwMTk5Nzc0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/50199774?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NazyS","html_url":"https:\/\/github.com\/NazyS","followers_url":"https:\/\/api.github.com\/users\/NazyS\/followers","following_url":"https:\/\/api.github.com\/users\/NazyS\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NazyS\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NazyS\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NazyS\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NazyS\/orgs","repos_url":"https:\/\/api.github.com\/users\/NazyS\/repos","events_url":"https:\/\/api.github.com\/users\/NazyS\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NazyS\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-26T12:43:36Z","updated_at":"2023-11-14T18:46:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nLooks like `IterableDataset.from_spark` does not support multiple workers in pytorch `Dataloader` if I'm not missing anything.\r\n\r\nAlso, returns not consistent error messages, which probably depend on the nondeterministic order of worker executions\r\n\r\nSome exampes I've encountered:\r\n\r\n```\r\nFile \"\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/spark\/spark.py\", line 79, in __iter__\r\n    yield from self.generate_examples_fn()\r\n  File \"\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/spark\/spark.py\", line 49, in generate_fn\r\n    df_with_partition_id = df.select(\"*\", pyspark.sql.functions.spark_partition_id().alias(\"part_id\"))\r\n  File \"\/databricks\/spark\/python\/pyspark\/instrumentation_utils.py\", line 54, in wrapper\r\n    logger.log_failure(\r\n  File \"\/databricks\/spark\/python\/pyspark\/databricks\/usage_logger.py\", line 70, in log_failure\r\n    self.logger.recordFunctionCallFailureEvent(\r\n  File \"\/databricks\/spark\/python\/lib\/py4j-0.10.9.7-src.zip\/py4j\/java_gateway.py\", line 1322, in __call__\r\n    return_value = get_return_value(\r\n  File \"\/databricks\/spark\/python\/pyspark\/errors\/exceptions\/captured.py\", line 188, in deco\r\n    return f(*a, **kw)\r\n  File \"\/databricks\/spark\/python\/lib\/py4j-0.10.9.7-src.zip\/py4j\/protocol.py\", line 342, in get_return_value\r\n    return OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\nKeyError: 'c'\r\n```\r\n\r\n```\r\n  File \"\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/spark\/spark.py\", line 79, in __iter__\r\n    yield from self.generate_examples_fn()\r\n  File \"\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/spark\/spark.py\", line 49, in generate_fn\r\n    df_with_partition_id = df.select(\"*\", pyspark.sql.functions.spark_partition_id().alias(\"part_id\"))\r\n  File \"\/databricks\/spark\/python\/pyspark\/sql\/utils.py\", line 162, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"\/databricks\/spark\/python\/pyspark\/sql\/functions.py\", line 4893, in spark_partition_id\r\n    return _invoke_function(\"spark_partition_id\")\r\n  File \"\/databricks\/spark\/python\/pyspark\/sql\/functions.py\", line 98, in _invoke_function\r\n    return Column(jf(*args))\r\n  File \"\/databricks\/spark\/python\/lib\/py4j-0.10.9.7-src.zip\/py4j\/java_gateway.py\", line 1322, in __call__\r\n    return_value = get_return_value(\r\n  File \"\/databricks\/spark\/python\/pyspark\/errors\/exceptions\/captured.py\", line 188, in deco\r\n    return f(*a, **kw)\r\n  File \"\/databricks\/spark\/python\/lib\/py4j-0.10.9.7-src.zip\/py4j\/protocol.py\", line 342, in get_return_value\r\n    return OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\nKeyError: 'm'\r\n```\r\n\r\n```\r\nFile \"\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/spark\/spark.py\", line 79, in __iter__\r\n    yield from self.generate_examples_fn()\r\n  File \"\/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/spark\/spark.py\", line 49, in generate_fn\r\n    df_with_partition_id = df.select(\"*\", pyspark.sql.functions.spark_partition_id().alias(\"part_id\"))\r\n  File \"\/databricks\/spark\/python\/pyspark\/sql\/utils.py\", line 162, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"\/databricks\/spark\/python\/pyspark\/sql\/functions.py\", line 4893, in spark_partition_id\r\n    return _invoke_function(\"spark_partition_id\")\r\n  File \"\/databricks\/spark\/python\/pyspark\/sql\/functions.py\", line 97, in _invoke_function\r\n    jf = _get_jvm_function(name, SparkContext._active_spark_context)\r\n  File \"\/databricks\/spark\/python\/pyspark\/sql\/functions.py\", line 88, in _get_jvm_function\r\n    return getattr(sc._jvm.functions, name)\r\n  File \"\/databricks\/spark\/python\/lib\/py4j-0.10.9.7-src.zip\/py4j\/java_gateway.py\", line 1725, in __getattr__\r\n    raise Py4JError(message)\r\npy4j.protocol.Py4JError: functions does not exist in the JVM\r\n```\n\n### Steps to reproduce the bug\n\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nbatch_size = 16\r\n\r\npdf = pd.DataFrame({\r\n    key: np.random.rand(16*100) for key in ['feature', 'target']\r\n})\r\ntest_df = spark.createDataFrame(pdf)\r\n\r\nfrom datasets import IterableDataset\r\nfrom torch.utils.data import DataLoader\r\n\r\nids = IterableDataset.from_spark(test_df)\r\nfor batch in DataLoader(ids, batch_size=16, num_workers=4):\r\n    for k, b in batch.items():\r\n        print(k, b.shape, sep='\\t')\r\n    print('\\n')\r\n```\n\n### Expected behavior\n\nFor `num_workers` equal to 0 or 1 works fine as expected:\r\n\r\n```\r\nfeature\ttorch.Size([16])\r\ntarget\ttorch.Size([16])\r\n\r\n\r\nfeature\ttorch.Size([16])\r\ntarget\ttorch.Size([16])\r\n....\r\n```\r\n\r\nExpected to support workers >1.\n\n### Environment info\n\nDatabricks 13.3 LTS ML runtime - Spark 3.4.1\r\npyspark==3.4.1\r\npy4j==0.10.9.7\r\ndatasets==2.13.1 and also tested with datasets==2.14.6","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6354\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6354\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6353","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6353\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6353\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6353\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6353","id":1962646450,"node_id":"I_kwDODunzps50-5uy","number":6353,"title":"load_dataset save_to_disk load_from_disk error","user":{"login":"brisker","id":13804492,"node_id":"MDQ6VXNlcjEzODA0NDky","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/13804492?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/brisker","html_url":"https:\/\/github.com\/brisker","followers_url":"https:\/\/api.github.com\/users\/brisker\/followers","following_url":"https:\/\/api.github.com\/users\/brisker\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/brisker\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/brisker\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/brisker\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/brisker\/orgs","repos_url":"https:\/\/api.github.com\/users\/brisker\/repos","events_url":"https:\/\/api.github.com\/users\/brisker\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/brisker\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-26T03:47:06Z","updated_at":"2023-11-22T09:57:17Z","closed_at":"2023-10-26T10:18:04Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\ndatasets version\uff1a 2.10.1\r\nI `load_dataset `and `save_to_disk` sucessfully on windows10( **and I `load_from_disk(\/LLM\/data\/wiki)` succcesfully on windows10**), and I copy the dataset `\/LLM\/data\/wiki`\r\ninto a ubuntu system, but when I `load_from_disk(\/LLM\/data\/wiki)` on ubuntu,  something weird happens:\r\n\r\n\r\n```\r\nload_from_disk('\/LLM\/data\/wiki')\r\n  File \"\/usr\/local\/miniconda3\/lib\/python3.8\/site-packages\/datasets\/load.py\", line 1874, in load_from_disk\r\n    return DatasetDict.load_from_disk(dataset_path, keep_in_memory=keep_in_memory, storage_options=storage_options)\r\n  File \"\/usr\/local\/miniconda3\/lib\/python3.8\/site-packages\/datasets\/dataset_dict.py\", line 1309, in load_from_disk\r\n    dataset_dict[k] = Dataset.load_from_disk(\r\n  File \"\/usr\/local\/miniconda3\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 1543, in load_from_disk\r\n    fs_token_paths = fsspec.get_fs_token_paths(dataset_path, storage_options=storage_options)\r\n  File \"\/usr\/local\/miniconda3\/lib\/python3.8\/site-packages\/fsspec\/core.py\", line 610, in get_fs_token_paths\r\n    chain = _un_chain(urlpath0, storage_options or {})\r\n  File \"\/usr\/local\/miniconda3\/lib\/python3.8\/site-packages\/fsspec\/core.py\", line 325, in _un_chain\r\n    cls = get_filesystem_class(protocol)\r\n  File \"\/usr\/local\/miniconda3\/lib\/python3.8\/site-packages\/fsspec\/registry.py\", line 232, in get_filesystem_class\r\n    raise ValueError(f\"Protocol not known: {protocol}\")\r\nValueError: Protocol not known: \/LLM\/data\/wiki\r\n```\r\nIt seems that something went wrong on the arrow file?\r\nHow can I solve this , since currently I can not save_to_disk on ubuntu system\r\n\r\n### Steps to reproduce the bug\r\n\r\ndatasets version\uff1a 2.10.1\r\n\r\n### Expected behavior\r\n\r\ndatasets version\uff1a 2.10.1\r\n\r\n### Environment info\r\n\r\ndatasets version\uff1a 2.10.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6353\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6353\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6352","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6352\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6352\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6352\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6352","id":1962296057,"node_id":"I_kwDODunzps509kL5","number":6352,"title":"Error loading wikitext data    raise NotImplementedError(f\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\")","user":{"login":"Ahmed-Roushdy","id":68569076,"node_id":"MDQ6VXNlcjY4NTY5MDc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/68569076?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Ahmed-Roushdy","html_url":"https:\/\/github.com\/Ahmed-Roushdy","followers_url":"https:\/\/api.github.com\/users\/Ahmed-Roushdy\/followers","following_url":"https:\/\/api.github.com\/users\/Ahmed-Roushdy\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Ahmed-Roushdy\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Ahmed-Roushdy\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Ahmed-Roushdy\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Ahmed-Roushdy\/orgs","repos_url":"https:\/\/api.github.com\/users\/Ahmed-Roushdy\/repos","events_url":"https:\/\/api.github.com\/users\/Ahmed-Roushdy\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Ahmed-Roushdy\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-10-25T21:55:31Z","updated_at":"2023-11-07T07:26:54Z","closed_at":"2023-11-07T07:26:54Z","author_association":"NONE","active_lock_reason":null,"body":"I was trying to load  the wiki dataset, but i got this error \r\n\r\n    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\r\n  File \"\/home\/aelkordy\/.conda\/envs\/prune_llm\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1804, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n  File \"\/home\/aelkordy\/.conda\/envs\/prune_llm\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 1108, in as_dataset\r\n    raise NotImplementedError(f\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\")\r\nNotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6352\/reactions","total_count":4,"+1":4,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6352\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6351","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6351\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6351\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6351\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6351","id":1961982988,"node_id":"PR_kwDODunzps5dyMvh","number":6351,"title":"Fix use_dataset.mdx","user":{"login":"angel-luis","id":17672548,"node_id":"MDQ6VXNlcjE3NjcyNTQ4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17672548?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/angel-luis","html_url":"https:\/\/github.com\/angel-luis","followers_url":"https:\/\/api.github.com\/users\/angel-luis\/followers","following_url":"https:\/\/api.github.com\/users\/angel-luis\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/angel-luis\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/angel-luis\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/angel-luis\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/angel-luis\/orgs","repos_url":"https:\/\/api.github.com\/users\/angel-luis\/repos","events_url":"https:\/\/api.github.com\/users\/angel-luis\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/angel-luis\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-25T18:21:08Z","updated_at":"2023-10-26T17:19:49Z","closed_at":"2023-10-26T17:10:27Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The current example isn't working because it can't find `labels` inside the Dataset object. So I've added an extra step to the process. Tested and working in Colab.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6351\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6351\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6351","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6351","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6351.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6351.patch","merged_at":"2023-10-26T17:10:27Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6350","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6350\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6350\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6350\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6350","id":1961869203,"node_id":"I_kwDODunzps5077-T","number":6350,"title":"Different objects are returned from calls that should be returning the same kind of object.","user":{"login":"phalexo","id":4603365,"node_id":"MDQ6VXNlcjQ2MDMzNjU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/4603365?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/phalexo","html_url":"https:\/\/github.com\/phalexo","followers_url":"https:\/\/api.github.com\/users\/phalexo\/followers","following_url":"https:\/\/api.github.com\/users\/phalexo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/phalexo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/phalexo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/phalexo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/phalexo\/orgs","repos_url":"https:\/\/api.github.com\/users\/phalexo\/repos","events_url":"https:\/\/api.github.com\/users\/phalexo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/phalexo\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-25T17:08:39Z","updated_at":"2023-10-26T21:03:06Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n        1.  dataset = load_dataset(\"togethercomputer\/RedPajama-Data-1T-Sample\", cache_dir=training_args.cache_dir, split='train[:1%]')\r\n        2.  dataset = load_dataset(\"togethercomputer\/RedPajama-Data-1T-Sample\", cache_dir=training_args.cache_dir)\r\n\r\nThe only difference I would expect these calls to have is the size of the dataset.\r\n\r\nBut, while 2. returns a dictionary with \"train\" key in it, 1. returns a dataset WITHOUT any initial \"train\" keyword.\r\n\r\nBoth calls are to be used within exactly the same context. They should return identically structured datasets of different size.\n\n### Steps to reproduce the bug\n\nSee above.\n\n### Expected behavior\n\nExpect both calls to return the same structured Dataset structure but with different number of elements, i.e. call 1. should have 1% of the data of the call 2.0\n\n### Environment info\n\nUbuntu 20.04\r\ngcc 9.x.x.\r\n\r\nIt is really irrelevant.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6350\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6350\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6349","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6349\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6349\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6349\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6349","id":1961435673,"node_id":"I_kwDODunzps506SIZ","number":6349,"title":"Can't load ds = load_dataset(\"imdb\")","user":{"login":"vivianc2","id":86415736,"node_id":"MDQ6VXNlcjg2NDE1NzM2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/86415736?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vivianc2","html_url":"https:\/\/github.com\/vivianc2","followers_url":"https:\/\/api.github.com\/users\/vivianc2\/followers","following_url":"https:\/\/api.github.com\/users\/vivianc2\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vivianc2\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vivianc2\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vivianc2\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vivianc2\/orgs","repos_url":"https:\/\/api.github.com\/users\/vivianc2\/repos","events_url":"https:\/\/api.github.com\/users\/vivianc2\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vivianc2\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-25T13:29:51Z","updated_at":"2023-10-31T19:59:35Z","closed_at":"2023-10-31T19:59:35Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI did `from datasets import load_dataset, load_metric` and then `ds = load_dataset(\"imdb\")` and it gave me the error:\r\nExpectedMoreDownloadedFiles: {'http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/aclImdb_v1.tar.gz'}\r\n\r\nI tried doing `ds = load_dataset(\"imdb\",download_mode=\"force_redownload\")` as well as reinstalling dataset. I still face this problem.\n\n### Steps to reproduce the bug\n\n1. from datasets import load_dataset, load_metric\r\n2. ds = load_dataset(\"imdb\")\n\n### Expected behavior\n\nIt should load and give me this when I run `ds`\r\n\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['text', 'label'],\r\n        num_rows: 25000\r\n    })\r\n    test: Dataset({\r\n        features: ['text', 'label'],\r\n        num_rows: 25000\r\n    })\r\n    unsupervised: Dataset({\r\n        features: ['text', 'label'],\r\n        num_rows: 50000\r\n    })\r\n})\n\n### Environment info\n\n- `datasets` version: 2.14.6\r\n- Platform: Linux-5.4.0-164-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.18\r\n- Huggingface_hub version: 0.16.2\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.0.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6349\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6349\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6348","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6348\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6348\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6348\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6348","id":1961268504,"node_id":"I_kwDODunzps505pUY","number":6348,"title":"Parquet stream-conversion fails to embed images\/audio files from gated repos","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-10-25T12:12:44Z","updated_at":"2023-10-25T12:13:07Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"it seems to be an issue with datasets not passing the token to embed_table_storage when generating a dataset\r\n\r\nSee https:\/\/github.com\/huggingface\/datasets-server\/issues\/2010","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6348\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6348\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6347","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6347\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6347\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6347\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6347","id":1959004835,"node_id":"I_kwDODunzps50xAqj","number":6347,"title":"Incorrect example code in 'Create a dataset' docs","user":{"login":"rwood-97","id":72076688,"node_id":"MDQ6VXNlcjcyMDc2Njg4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/72076688?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rwood-97","html_url":"https:\/\/github.com\/rwood-97","followers_url":"https:\/\/api.github.com\/users\/rwood-97\/followers","following_url":"https:\/\/api.github.com\/users\/rwood-97\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rwood-97\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rwood-97\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rwood-97\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rwood-97\/orgs","repos_url":"https:\/\/api.github.com\/users\/rwood-97\/repos","events_url":"https:\/\/api.github.com\/users\/rwood-97\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rwood-97\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-24T11:01:21Z","updated_at":"2023-10-25T13:05:21Z","closed_at":"2023-10-25T13:05:21Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nOn [this](https:\/\/huggingface.co\/docs\/datasets\/create_dataset) page, the example code for loading in images and audio is incorrect.\r\n\r\nCurrently, examples are:\r\n\r\n``` python\r\nfrom datasets import ImageFolder\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"\/path\/to\/pokemon\")\r\n```\r\nand \r\n``` python\r\nfrom datasets import AudioFolder\r\ndataset = load_dataset(\"audiofolder\", data_dir=\"\/path\/to\/folder\")\r\n```\r\n\r\nI'm pretty sure the imports are wrong and should be:\r\n\r\n``` python \r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"audiofolder\", data_dir=\"\/path\/to\/folder\")\r\n```\r\n\r\nI am happy to update this if this is right but just wanted to check before making any changes.\n\n### Steps to reproduce the bug\n\nGo to https:\/\/huggingface.co\/docs\/datasets\/create_dataset\n\n### Expected behavior\n\nN\/A\n\n### Environment info\n\nN\/A","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6347\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6347\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6346","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6346\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6346\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6346\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6346","id":1958777076,"node_id":"PR_kwDODunzps5dnZM_","number":6346,"title":"Fix UnboundLocalError if preprocessing returns an empty list","user":{"login":"cwallenwein","id":40916592,"node_id":"MDQ6VXNlcjQwOTE2NTky","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/40916592?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/cwallenwein","html_url":"https:\/\/github.com\/cwallenwein","followers_url":"https:\/\/api.github.com\/users\/cwallenwein\/followers","following_url":"https:\/\/api.github.com\/users\/cwallenwein\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/cwallenwein\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/cwallenwein\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/cwallenwein\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/cwallenwein\/orgs","repos_url":"https:\/\/api.github.com\/users\/cwallenwein\/repos","events_url":"https:\/\/api.github.com\/users\/cwallenwein\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/cwallenwein\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-24T08:38:43Z","updated_at":"2023-10-25T17:39:17Z","closed_at":"2023-10-25T16:36:38Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"If this tokenization function is used with IterableDatasets and no sample is as big as the context length, `input_batch` will be an empty list.\r\n```\r\ndef tokenize(batch, tokenizer, context_length):\r\n    outputs = tokenizer(\r\n        batch[\"text\"],\r\n        truncation=True,\r\n        max_length=context_length,\r\n        return_overflowing_tokens=True,\r\n        return_length=True\r\n    )\r\n    input_batch = []\r\n    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\r\n        if length == context_length:\r\n            input_batch.append(input_ids)\r\n    return {\"input_ids\": input_batch}\r\n\r\ndataset.map(tokenize, batched=True, batch_size=batch_size, fn_kwargs={\"context_length\": context_length, \"tokenizer\": tokenizer}, remove_columns=dataset.column_names)\r\n```\r\n\r\nThis will throw the following error: UnboundLocalError: local variable 'batch_idx' referenced before assignment, because the for loop was not executed a single time\r\n\r\n```\r\nfor batch_idx, example in enumerate(_batch_to_examples(transformed_batch)):\r\n     yield new_key, example\r\ncurrent_idx += batch_idx + 1\r\n```\r\n\r\nSome of the possible solutions\r\n\r\n```\r\nfor batch_idx, example in enumerate(_batch_to_examples(transformed_batch)):\r\n     yield new_key, example\r\ntry:\r\n     current_idx += batch_idx + 1\r\nexcept:\r\n     current_idx += 1\r\n```\r\n\r\nor \r\n\r\n```\r\nbatch_idx = 0\r\nfor batch_idx, example in enumerate(_batch_to_examples(transformed_batch)):\r\n     yield new_key, example\r\ncurrent_idx += batch_idx + 1\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6346\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6346\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6346","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6346","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6346.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6346.patch","merged_at":"2023-10-25T16:36:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6345","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6345\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6345\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6345\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6345","id":1957707870,"node_id":"I_kwDODunzps50sEBe","number":6345,"title":"support squad structure datasets using a YAML parameter","user":{"login":"MajdTannous1","id":138524319,"node_id":"U_kgDOCEG2nw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/138524319?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/MajdTannous1","html_url":"https:\/\/github.com\/MajdTannous1","followers_url":"https:\/\/api.github.com\/users\/MajdTannous1\/followers","following_url":"https:\/\/api.github.com\/users\/MajdTannous1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/MajdTannous1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/MajdTannous1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/MajdTannous1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/MajdTannous1\/orgs","repos_url":"https:\/\/api.github.com\/users\/MajdTannous1\/repos","events_url":"https:\/\/api.github.com\/users\/MajdTannous1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/MajdTannous1\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-10-23T17:55:37Z","updated_at":"2023-10-23T17:55:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nSince the squad structure is widely used, I think it could be beneficial to support it using a YAML parameter.\r\ncould you implement automatic data loading of squad-like data using squad JSON format, to read it from JSON files and view it in the correct squad structure.\r\nThe dataset structure should be like this:\r\nhttps:\/\/huggingface.co\/datasets\/squad\r\nColumns:id,title,context,question,answers\n\n### Motivation\n\nDataset repo requires arbitrary Python code execution\n\n### Your contribution\n\nThe dataset structure should be like this:\r\nhttps:\/\/huggingface.co\/datasets\/squad\r\nColumns:id,title,context,question,answers\r\ntrain and dev sets in squad structure JSON files","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6345\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6345\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6344","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6344\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6344\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6344\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6344","id":1957412169,"node_id":"PR_kwDODunzps5diyd5","number":6344,"title":"set dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-23T15:13:28Z","updated_at":"2023-10-23T15:24:31Z","closed_at":"2023-10-23T15:13:38Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6344\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6344\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6344","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6344","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6344.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6344.patch","merged_at":"2023-10-23T15:13:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6343","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6343\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6343\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6343\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6343","id":1957370711,"node_id":"PR_kwDODunzps5dipeb","number":6343,"title":"Remove unused argument in `_get_data_files_patterns`","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-23T14:54:18Z","updated_at":"2023-11-16T09:09:42Z","closed_at":"2023-11-16T09:03:39Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6343\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6343\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6343","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6343","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6343.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6343.patch","merged_at":"2023-11-16T09:03:39Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6342","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6342\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6342\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6342\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6342","id":1957344445,"node_id":"PR_kwDODunzps5dijxt","number":6342,"title":"Release: 2.14.6","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-10-23T14:43:26Z","updated_at":"2023-10-23T15:21:54Z","closed_at":"2023-10-23T15:07:25Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6342\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6342\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6342","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6342","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6342.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6342.patch","merged_at":"2023-10-23T15:07:25Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6340","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6340\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6340\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6340\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6340","id":1956917893,"node_id":"PR_kwDODunzps5dhGpW","number":6340,"title":"Release 2.14.5","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-23T11:10:22Z","updated_at":"2023-10-23T14:20:46Z","closed_at":"2023-10-23T11:12:40Z","author_association":"MEMBER","active_lock_reason":null,"body":"(wrong release number - I was continuing the 2.14 branch but 2.14.5 was released from `main`)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6340\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6340\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6340","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6340","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6340.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6340.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6339","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6339\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6339\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6339\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6339","id":1956912627,"node_id":"PR_kwDODunzps5dhFfg","number":6339,"title":"minor release step improvement","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-23T11:07:04Z","updated_at":"2023-11-07T10:38:54Z","closed_at":"2023-11-07T10:32:41Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6339\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6339\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6339","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6339","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6339.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6339.patch","merged_at":"2023-11-07T10:32:41Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6338","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6338\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6338\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6338\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6338","id":1956886072,"node_id":"PR_kwDODunzps5dg_sb","number":6338,"title":"pin fsspec before it switches to glob.glob","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-23T10:50:54Z","updated_at":"2023-10-23T10:57:07Z","closed_at":"2023-10-23T10:51:52Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6338\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6338\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6338","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6338","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6338.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6338.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6337","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6337\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6337\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6337\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6337","id":1956875259,"node_id":"PR_kwDODunzps5dg9Uu","number":6337,"title":"Pin supported upper version of fsspec","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-10-23T10:44:16Z","updated_at":"2023-10-23T12:13:20Z","closed_at":"2023-10-23T12:04:36Z","author_association":"MEMBER","active_lock_reason":null,"body":"Pin upper version of `fsspec` to avoid disruptions introduced by breaking changes (and the need of urgent patch releases with hotfixes) on each release on their side. See:\r\n- #6331\r\n- #6210 \r\n- #5731\r\n- #5617\r\n- #5447\r\n\r\nI propose that we explicitly test, introduce fixes and support each new `fsspec` version release.\r\n\r\nCC: @LysandreJik ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6337\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6337\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6337","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6337","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6337.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6337.patch","merged_at":"2023-10-23T12:04:36Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6336","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6336\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6336\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6336\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6336","id":1956827232,"node_id":"PR_kwDODunzps5dgy0w","number":6336,"title":"unpin-fsspec","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-23T10:16:46Z","updated_at":"2023-10-23T10:28:46Z","closed_at":"2023-10-23T10:17:48Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6336\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6336\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6336","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6336","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6336.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6336.patch","merged_at":"2023-10-23T10:17:48Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6335","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6335\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6335\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6335\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6335","id":1956740818,"node_id":"PR_kwDODunzps5dggIV","number":6335,"title":"Support fsspec 2023.10.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-10-23T09:29:17Z","updated_at":"2023-11-14T14:18:12Z","closed_at":"2023-11-14T14:17:40Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix #6333.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6335\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6335\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6335","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6335","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6335.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6335.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6334","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6334\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6334\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6334\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6334","id":1956719774,"node_id":"PR_kwDODunzps5dgbpR","number":6334,"title":"datasets.filesystems: fix is_remote_filesystems","user":{"login":"ap--","id":1463443,"node_id":"MDQ6VXNlcjE0NjM0NDM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1463443?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ap--","html_url":"https:\/\/github.com\/ap--","followers_url":"https:\/\/api.github.com\/users\/ap--\/followers","following_url":"https:\/\/api.github.com\/users\/ap--\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ap--\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ap--\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ap--\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ap--\/orgs","repos_url":"https:\/\/api.github.com\/users\/ap--\/repos","events_url":"https:\/\/api.github.com\/users\/ap--\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ap--\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-23T09:17:54Z","updated_at":"2023-10-23T17:30:55Z","closed_at":"2023-10-23T10:14:10Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Close #6330\r\n\r\n`fsspec.implementations.LocalFilesystem.protocol`\r\nwas changed from `str` \"file\" to `tuple[str,...]` (\"file\", \"local\") in `fsspec>=2023.10.0`\r\n\r\nThis commit supports both styles.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6334\/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":2,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6334\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6334","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6334","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6334.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6334.patch","merged_at":"2023-10-23T10:14:10Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6333","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6333\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6333\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6333\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6333","id":1956714423,"node_id":"I_kwDODunzps50oRe3","number":6333,"title":"Support fsspec 2023.10.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-10-23T09:14:53Z","updated_at":"2023-10-23T09:15:10Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Once root issue is fixed, remove temporary pin of fsspec < 2023.10.0 introduced by:\r\n- #6331\r\n\r\nRelated to issue:\r\n- #6330\r\n\r\nAs @ZachNagengast suggested, the issue might be related to:\r\n- https:\/\/github.com\/fsspec\/filesystem_spec\/pull\/1381","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6333\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6333\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6332","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6332\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6332\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6332\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6332","id":1956697328,"node_id":"PR_kwDODunzps5dgW3w","number":6332,"title":"Replace deprecated license_file in setup.cfg","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-23T09:05:26Z","updated_at":"2023-11-07T08:23:10Z","closed_at":"2023-11-07T08:09:06Z","author_association":"MEMBER","active_lock_reason":null,"body":"Replace deprecated license_file in `setup.cfg`.\r\n\r\nSee: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/6610930650\/job\/17953825724?pr=6331\r\n```\r\n      \/tmp\/pip-build-env-a51hls20\/overlay\/lib\/python3.8\/site-packages\/setuptools\/config\/setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\r\n      !!\r\n      \r\n              ********************************************************************************\r\n              The license_file parameter is deprecated, use license_files instead.\r\n      \r\n              By 2023-Oct-30, you need to update your project and remove deprecated calls\r\n              or your builds will no longer be supported.\r\n      \r\n              See https:\/\/setuptools.pypa.io\/en\/latest\/userguide\/declarative_config.html for details.\r\n              ********************************************************************************\r\n      \r\n      !!\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6332\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6332\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6332","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6332","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6332.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6332.patch","merged_at":"2023-11-07T08:09:06Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6331","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6331\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6331\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6331\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6331","id":1956671256,"node_id":"PR_kwDODunzps5dgRQt","number":6331,"title":"Temporarily pin fsspec < 2023.10.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-23T08:51:50Z","updated_at":"2023-10-23T09:26:42Z","closed_at":"2023-10-23T09:17:55Z","author_association":"MEMBER","active_lock_reason":null,"body":"Temporarily pin fsspec < 2023.10.0 until permanent solution is found.\r\n\r\nHot fix #6330.\r\n\r\nSee: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/6610904287\/job\/17953774987\r\n```\r\n...\r\nERROR tests\/test_iterable_dataset.py::test_iterable_dataset_from_file - NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.\r\n= 373 failed, 2055 passed, 17 skipped, 8 warnings, 6 errors in 228.14s (0:03:48) =\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6331\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6331\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6331","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6331","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6331.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6331.patch","merged_at":"2023-10-23T09:17:55Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6330","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6330\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6330\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6330\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6330","id":1956053294,"node_id":"I_kwDODunzps50lwEu","number":6330,"title":"Latest fsspec==2023.10.0 issue with streaming datasets","user":{"login":"ZachNagengast","id":1981179,"node_id":"MDQ6VXNlcjE5ODExNzk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1981179?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ZachNagengast","html_url":"https:\/\/github.com\/ZachNagengast","followers_url":"https:\/\/api.github.com\/users\/ZachNagengast\/followers","following_url":"https:\/\/api.github.com\/users\/ZachNagengast\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ZachNagengast\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ZachNagengast\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ZachNagengast\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ZachNagengast\/orgs","repos_url":"https:\/\/api.github.com\/users\/ZachNagengast\/repos","events_url":"https:\/\/api.github.com\/users\/ZachNagengast\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ZachNagengast\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":6,"created_at":"2023-10-22T20:57:10Z","updated_at":"2023-11-07T10:02:14Z","closed_at":"2023-10-23T09:17:56Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nLoading a streaming dataset with this version of fsspec fails with the following error:\r\n\r\n`NotImplementedError: Loading a streaming dataset cached in a LocalFileSystem is not supported yet.`\r\n\r\nI suspect the issue is with this PR \r\n\r\nhttps:\/\/github.com\/fsspec\/filesystem_spec\/pull\/1381\n\n### Steps to reproduce the bug\n\n1. Upgrade fsspec to version `2023.10.0`\r\n2. Attempt to load a streaming dataset e.g. `load_dataset(\"laion\/gpt4v-emotion-dataset\", split=\"train\", streaming=True)`\r\n3. Observe the following exception:\r\n\r\n```\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.11.6\/x64\/lib\/python3.11\/site-packages\/datasets\/load.py\", line 2146, in load_dataset\r\n    return builder_instance.as_streaming_dataset(split=split)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/hostedtoolcache\/Python\/3.11.6\/x64\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1318, in as_streaming_dataset\r\n    raise NotImplementedError(\r\nNotImplementedError: Loading a streaming dataset cached in a LocalFileSystem is not supported yet.\r\n```\n\n### Expected behavior\n\nShould stream the dataset as normal.\n\n### Environment info\n\ndatasets@main\r\nfsspec==2023.10.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6330\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6330\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6329","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6329\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6329\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6329\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6329","id":1955858020,"node_id":"I_kwDODunzps50lAZk","number":6329,"title":"\u0634\u0628\u06a9\u0647 \u0647\u0627\u06cc \u0645\u062a\u0646 \u0628\u0647 \u06af\u0641\u062a\u0627\u0631 \u0627\u0628\u062a\u062f\u0627 \u0645\u062a\u0646 \u062f\u0627\u062f\u0647 \u0634\u062f\u0647 \u0631\u0627 \u0628\u0647 \u0628\u0627\u0632\u0646\u0645\u0627\u06cc\u06cc \u0645\u06cc\u0627\u0646\u06cc","user":{"login":"shabnam706","id":147399213,"node_id":"U_kgDOCMkiLQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/147399213?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shabnam706","html_url":"https:\/\/github.com\/shabnam706","followers_url":"https:\/\/api.github.com\/users\/shabnam706\/followers","following_url":"https:\/\/api.github.com\/users\/shabnam706\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shabnam706\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shabnam706\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shabnam706\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shabnam706\/orgs","repos_url":"https:\/\/api.github.com\/users\/shabnam706\/repos","events_url":"https:\/\/api.github.com\/users\/shabnam706\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shabnam706\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-10-22T11:07:46Z","updated_at":"2023-10-23T09:22:58Z","closed_at":"2023-10-23T09:22:58Z","author_association":"NONE","active_lock_reason":null,"body":"\u0634\u0628\u06a9\u0647 \u0647\u0627\u06cc \u0645\u062a\u0646 \u0628\u0647 \u06af\u0641\u062a\u0627\u0631 \u0627\u0628\u062a\u062f\u0627 \u0645\u062a\u0646 \u062f\u0627\u062f\u0647 \u0634\u062f\u0647 \u0631\u0627 \u0628\u0647 \u0628\u0627\u0632\u0646\u0645\u0627\u06cc\u06cc \u0645\u06cc\u0627\u0646\u06cc\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6329\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6329\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6328","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6328\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6328\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6328\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6328","id":1955857904,"node_id":"I_kwDODunzps50lAXw","number":6328,"title":"\u0634\u0628\u06a9\u0647 \u0647\u0627\u06cc \u0645\u062a\u0646 \u0628\u0647 \u06af\u0641\u062a\u0627\u0631 \u0627\u0628\u062a\u062f\u0627 \u0645\u062a\u0646 \u062f\u0627\u062f\u0647 \u0634\u062f\u0647 \u0631\u0627 \u0628\u0647 \u0628\u0627\u0632\u0646\u0645\u0627\u06cc\u06cc \u0645\u06cc\u0627\u0646\u06cc","user":{"login":"shabnam706","id":147399213,"node_id":"U_kgDOCMkiLQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/147399213?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shabnam706","html_url":"https:\/\/github.com\/shabnam706","followers_url":"https:\/\/api.github.com\/users\/shabnam706\/followers","following_url":"https:\/\/api.github.com\/users\/shabnam706\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shabnam706\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shabnam706\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shabnam706\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shabnam706\/orgs","repos_url":"https:\/\/api.github.com\/users\/shabnam706\/repos","events_url":"https:\/\/api.github.com\/users\/shabnam706\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shabnam706\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-22T11:07:21Z","updated_at":"2023-10-23T09:22:38Z","closed_at":"2023-10-23T09:22:38Z","author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6328\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6328\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6327","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6327\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6327\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6327\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6327","id":1955470755,"node_id":"I_kwDODunzps50jh2j","number":6327,"title":"FileNotFoundError when trying to load the downloaded dataset with `load_dataset(..., streaming=True)`","user":{"login":"yzhangcs","id":18402347,"node_id":"MDQ6VXNlcjE4NDAyMzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/18402347?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yzhangcs","html_url":"https:\/\/github.com\/yzhangcs","followers_url":"https:\/\/api.github.com\/users\/yzhangcs\/followers","following_url":"https:\/\/api.github.com\/users\/yzhangcs\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yzhangcs\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yzhangcs\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yzhangcs\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yzhangcs\/orgs","repos_url":"https:\/\/api.github.com\/users\/yzhangcs\/repos","events_url":"https:\/\/api.github.com\/users\/yzhangcs\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yzhangcs\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-21T12:27:03Z","updated_at":"2023-10-23T18:50:07Z","closed_at":"2023-10-23T18:50:07Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nHi, I'm trying to load the dataset `togethercomputer\/RedPajama-Data-1T-Sample` with `load_dataset` in streaming mode, i.e., `streaming=True`, but `FileNotFoundError` occurs.\r\n\r\n### Steps to reproduce the bug\r\n\r\nI've downloaded the dataset and save it to the cache dir in advance. My hope is loading the files in offline environment and without taking too much hours to prepross the entire data before running into the training process.\r\nSo I try the following code to load the files streamingly\r\n```py\r\ndataset = load_dataset('togethercomputer\/RedPajama-Data-1T-Sample', streaming=True)\r\nprint(next(iter(dataset['train'])))\r\n```\r\nSadly, it raises the following:\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: 'CURRENT_CODE_PATH\/arxiv_sample.jsonl'\r\n```\r\nI've noticed that the dataset can be properly found in the begining\r\n```\r\nUsing the latest cached version of the module from \/root\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/togethercomputer--RedPajama-Data-1T-Sample\/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039 (last modified on Sat Oct 21 20:12:57 2023) since it couldn't be found locally at togethercomputer\/RedPajama-Data-1T-Sample., or remotely on the Hugging Face Hub.\r\n```\r\nBut it seems that the paths couldn't be properly parsed when loading iteratively.\r\n \r\nHow should I fix this error. I've tried specifying `data_files` or `data_dir` as `...\/arxiv_sample.jsonl` but none of them works.\r\n\r\nThanks.\r\n\r\n### Expected behavior\r\n\r\nProperly load the dataset.\r\n\r\n### Environment info\r\n\r\n`datasets==2.14.5`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6327\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6327\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6326","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6326\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6326\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6326\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6326","id":1955420536,"node_id":"PR_kwDODunzps5dcSRa","number":6326,"title":"Create battery_analysis.py","user":{"login":"vinitkm","id":130216732,"node_id":"U_kgDOB8LzHA","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/130216732?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vinitkm","html_url":"https:\/\/github.com\/vinitkm","followers_url":"https:\/\/api.github.com\/users\/vinitkm\/followers","following_url":"https:\/\/api.github.com\/users\/vinitkm\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vinitkm\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vinitkm\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vinitkm\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vinitkm\/orgs","repos_url":"https:\/\/api.github.com\/users\/vinitkm\/repos","events_url":"https:\/\/api.github.com\/users\/vinitkm\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vinitkm\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-10-21T10:07:48Z","updated_at":"2023-10-23T14:56:20Z","closed_at":"2023-10-23T14:56:20Z","author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6326\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6326\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6326","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6326","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6326.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6326.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6325","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6325\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6325\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6325\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6325","id":1955420178,"node_id":"PR_kwDODunzps5dcSM3","number":6325,"title":"Create battery_analysis.py","user":{"login":"vinitkm","id":130216732,"node_id":"U_kgDOB8LzHA","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/130216732?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vinitkm","html_url":"https:\/\/github.com\/vinitkm","followers_url":"https:\/\/api.github.com\/users\/vinitkm\/followers","following_url":"https:\/\/api.github.com\/users\/vinitkm\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vinitkm\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vinitkm\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vinitkm\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vinitkm\/orgs","repos_url":"https:\/\/api.github.com\/users\/vinitkm\/repos","events_url":"https:\/\/api.github.com\/users\/vinitkm\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vinitkm\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-10-21T10:06:37Z","updated_at":"2023-10-23T14:55:58Z","closed_at":"2023-10-23T14:55:58Z","author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6325\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6325\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6325","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6325","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6325.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6325.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6324","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6324\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6324\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6324\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6324","id":1955126687,"node_id":"I_kwDODunzps50iN2f","number":6324,"title":"Conversion to Arrow fails due to wrong type heuristic","user":{"login":"jphme","id":2862336,"node_id":"MDQ6VXNlcjI4NjIzMzY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2862336?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jphme","html_url":"https:\/\/github.com\/jphme","followers_url":"https:\/\/api.github.com\/users\/jphme\/followers","following_url":"https:\/\/api.github.com\/users\/jphme\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jphme\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jphme\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jphme\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jphme\/orgs","repos_url":"https:\/\/api.github.com\/users\/jphme\/repos","events_url":"https:\/\/api.github.com\/users\/jphme\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jphme\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-20T23:20:58Z","updated_at":"2023-10-23T20:52:57Z","closed_at":"2023-10-23T20:52:57Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI have a list of dictionaries with valid\/JSON-serializable values. \r\n\r\nOne key is the denominator for a paragraph. In 99.9% of cases its a number, but there are some occurences of '1a', '2b' and so on.\r\n\r\nIf trying to convert this list to a dataset with `Dataset.from_list()`, I always get\r\n`ArrowInvalid: Could not convert '1' with type str: tried to convert to int64`, presumably because pyarrow tries to convert the keys to integers.\r\n\r\nIs there any way to circumvent this and fix dtypes? I didn't find anything in the documentation.\n\n### Steps to reproduce the bug\n\n* create a list of dicts with one key being a string of an integer for the first few thousand occurences and try to convert to dataset.\r\n\n\n### Expected behavior\n\nThere shouldn't be an error (e.g. some flag to turn off automatic str to numeric conversion).\n\n### Environment info\n\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.0-84-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.18\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.1.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6324\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6324\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6323","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6323\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6323\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6323\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6323","id":1954245980,"node_id":"I_kwDODunzps50e21c","number":6323,"title":"Loading dataset from large GCS bucket very slow since 2.14","user":{"login":"jbcdnr","id":6209990,"node_id":"MDQ6VXNlcjYyMDk5OTA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6209990?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jbcdnr","html_url":"https:\/\/github.com\/jbcdnr","followers_url":"https:\/\/api.github.com\/users\/jbcdnr\/followers","following_url":"https:\/\/api.github.com\/users\/jbcdnr\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jbcdnr\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jbcdnr\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jbcdnr\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jbcdnr\/orgs","repos_url":"https:\/\/api.github.com\/users\/jbcdnr\/repos","events_url":"https:\/\/api.github.com\/users\/jbcdnr\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jbcdnr\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-10-20T12:59:55Z","updated_at":"2023-10-20T12:59:55Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nSince updating to >2.14 we have very slow access to our parquet files on GCS when loading a dataset (>30 min vs 3s). Our GCS bucket has many objects and resolving globs is very slow. I could track down the problem to this change:\r\nhttps:\/\/github.com\/huggingface\/datasets\/blame\/bade7af74437347a760830466eb74f7a8ce0d799\/src\/datasets\/data_files.py#L348 \r\nThe underlying implementation with gcsfs is really slow. Could you go back to the old way if we are simply giving the parquet files and no glob pattern?\r\n\r\nThank you.\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\nLoad a dataset from a GCS bucket that has many files.\n\n### Expected behavior\n\nUsed to be fast (3s) in 2.13\n\n### Environment info\n\ndatasets==2.14.5","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6323\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6323\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6322","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6322\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6322\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6322\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6322","id":1952947461,"node_id":"PR_kwDODunzps5dT5vG","number":6322,"title":"Fix regex `get_data_files` formatting for base paths","user":{"login":"ZachNagengast","id":1981179,"node_id":"MDQ6VXNlcjE5ODExNzk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1981179?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ZachNagengast","html_url":"https:\/\/github.com\/ZachNagengast","followers_url":"https:\/\/api.github.com\/users\/ZachNagengast\/followers","following_url":"https:\/\/api.github.com\/users\/ZachNagengast\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ZachNagengast\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ZachNagengast\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ZachNagengast\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ZachNagengast\/orgs","repos_url":"https:\/\/api.github.com\/users\/ZachNagengast\/repos","events_url":"https:\/\/api.github.com\/users\/ZachNagengast\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ZachNagengast\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-19T19:45:10Z","updated_at":"2023-10-23T14:40:45Z","closed_at":"2023-10-23T14:31:21Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"With this pr https:\/\/github.com\/huggingface\/datasets\/pull\/6309, it is formatting the entire base path into regex, which results in the undesired formatting error `doesn't match the pattern` because of the line in `glob_pattern_to_regex`: `.replace(\"\/\/\", \"\/\")`:\r\n- Input: `hf:\/\/datasets\/...`\r\n- Output: `hf:\/datasets\/...`\r\n\r\nThis fix will only convert the `split_pattern` to regex and keep the `base_path` unchanged.\r\n\r\ncc @albertvillanova hopefully this still works with your implementation","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6322\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6322\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6322","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6322","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6322.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6322.patch","merged_at":"2023-10-23T14:31:21Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6321","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6321\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6321\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6321\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6321","id":1952643483,"node_id":"PR_kwDODunzps5dS3Mc","number":6321,"title":"Fix typos","user":{"login":"python273","id":3097956,"node_id":"MDQ6VXNlcjMwOTc5NTY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3097956?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/python273","html_url":"https:\/\/github.com\/python273","followers_url":"https:\/\/api.github.com\/users\/python273\/followers","following_url":"https:\/\/api.github.com\/users\/python273\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/python273\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/python273\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/python273\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/python273\/orgs","repos_url":"https:\/\/api.github.com\/users\/python273\/repos","events_url":"https:\/\/api.github.com\/users\/python273\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/python273\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-19T16:24:35Z","updated_at":"2023-10-19T17:18:00Z","closed_at":"2023-10-19T17:07:35Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6321\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6321\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6321","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6321","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6321.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6321.patch","merged_at":"2023-10-19T17:07:35Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6320","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6320\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6320\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6320\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6320","id":1952618316,"node_id":"I_kwDODunzps50YpdM","number":6320,"title":"Dataset slice splits can't load training and validation at the same time","user":{"login":"timlac","id":32488097,"node_id":"MDQ6VXNlcjMyNDg4MDk3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32488097?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/timlac","html_url":"https:\/\/github.com\/timlac","followers_url":"https:\/\/api.github.com\/users\/timlac\/followers","following_url":"https:\/\/api.github.com\/users\/timlac\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/timlac\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/timlac\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/timlac\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/timlac\/orgs","repos_url":"https:\/\/api.github.com\/users\/timlac\/repos","events_url":"https:\/\/api.github.com\/users\/timlac\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/timlac\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-19T16:09:22Z","updated_at":"2023-11-30T16:21:15Z","closed_at":"2023-11-30T16:21:15Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nAccording to the [documentation](https:\/\/huggingface.co\/docs\/datasets\/v2.14.5\/loading#slice-splits) is should be possible to run the following command:\r\n\r\n`train_test_ds = datasets.load_dataset(\"bookcorpus\", split=\"train+test\")`\r\n\r\nto load the train and test sets from the dataset. \r\n\r\nHowever executing the equivalent code:\r\n\r\n`speech_commands_v1 = load_dataset(\"superb\", \"ks\", split=\"train+test\")`\r\n\r\nonly yields the following output:\r\n\r\n> Dataset({\r\n>     features: ['file', 'audio', 'label'],\r\n>     num_rows: 54175\r\n> })\r\n\r\nWhere loading the dataset without the split argument yields:\r\n\r\n> DatasetDict({\r\n>     train: Dataset({\r\n>         features: ['file', 'audio', 'label'],\r\n>         num_rows: 51094\r\n>     })\r\n>     validation: Dataset({\r\n>         features: ['file', 'audio', 'label'],\r\n>         num_rows: 6798\r\n>     })\r\n>     test: Dataset({\r\n>         features: ['file', 'audio', 'label'],\r\n>         num_rows: 3081\r\n>     })\r\n> })\r\n\r\nThus, the API seems to be broken in this regard. \r\n\r\nThis is a bit annoying since I want to be able to use the split argument with `split=\"train[:10%]+test[:10%]\"` to have smaller dataset to work with when validating my model is working correctly. \r\n\r\n### Steps to reproduce the bug\r\n\r\n`speech_commands_v1 = load_dataset(\"superb\", \"ks\", split=\"train+test\")`\r\n\r\n### Expected behavior\r\n\r\n> DatasetDict({\r\n>     train: Dataset({\r\n>         features: ['file', 'audio', 'label'],\r\n>         num_rows: 51094\r\n>     })\r\n>     test: Dataset({\r\n>         features: ['file', 'audio', 'label'],\r\n>         num_rows: 3081\r\n>     })\r\n> })\r\n\r\n### Environment info\r\n\r\n```\r\nimport datasets\r\nprint(datasets.__version__)\r\n```\r\n> 2.14.5\r\n\r\n```\r\nimport sys\r\nprint(sys.version)\r\n```\r\n> 3.9.17 (main, Jul  5 2023, 20:41:20) \r\n> [GCC 11.2.0]","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6320\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6320\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6319","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6319\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6319\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6319\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6319","id":1952101717,"node_id":"I_kwDODunzps50WrVV","number":6319,"title":"Datasets.map is severely broken","user":{"login":"phalexo","id":4603365,"node_id":"MDQ6VXNlcjQ2MDMzNjU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/4603365?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/phalexo","html_url":"https:\/\/github.com\/phalexo","followers_url":"https:\/\/api.github.com\/users\/phalexo\/followers","following_url":"https:\/\/api.github.com\/users\/phalexo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/phalexo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/phalexo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/phalexo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/phalexo\/orgs","repos_url":"https:\/\/api.github.com\/users\/phalexo\/repos","events_url":"https:\/\/api.github.com\/users\/phalexo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/phalexo\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2023-10-19T12:19:33Z","updated_at":"2023-11-30T03:27:26Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nRegardless of how many cores I used, I have 16 or 32 threads, map slows down to a crawl at around 80% done, lingers maybe until 97% extremely slowly and NEVER finishes the job. It just hangs.\r\n\r\nAfter watching this for 27 hours I control-C out of it. Until the end one process appears to be doing something, but it never ends.\r\n\r\nI saw some comments about fast tokenizers using Rust and all and tried different variations. NOTHING works.\r\n\r\n\n\n### Steps to reproduce the bug\n\nRunning it without breaking the dataset into parts results in the same behavior. The loop was an attempt to see if this was a RAM issue. \r\n\r\nfor idx in range(100):\r\n    dataset = load_dataset(\"togethercomputer\/RedPajama-Data-1T-Sample\", cache_dir=cache_dir, split=f'train[{idx}%:{idx+1}%]')\r\n    dataset = dataset.map(partial(tokenize_fn, tokenizer), batched=False, num_proc=1, remove_columns=[\"text\", \"meta\"])\r\n    dataset.save_to_disk(training_args.cache_dir + f\"\/training_data_{idx}\")\r\n\n\n### Expected behavior\n\nI expect map to run at more or less the same speed it starts with and FINISH its processing. \n\n### Environment info\n\nPython 3.8, same with 3.10 makes no difference.\r\nUbuntu 20.04, ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6319\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6319\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6318","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6318\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6318\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6318\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6318","id":1952100706,"node_id":"PR_kwDODunzps5dRC9V","number":6318,"title":"Deterministic set hash","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-19T12:19:13Z","updated_at":"2023-10-19T16:27:20Z","closed_at":"2023-10-19T16:16:31Z","author_association":"MEMBER","active_lock_reason":null,"body":"Sort the items in a set according to their `datasets.fingerprint.Hasher.hash` hash to get a deterministic hash of sets.\r\n\r\nThis is useful to get deterministic hashes of tokenizers that use a trie based on python sets.\r\n\r\nreported in https:\/\/github.com\/huggingface\/datasets\/issues\/3847","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6318\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6318\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6318","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6318","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6318.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6318.patch","merged_at":"2023-10-19T16:16:31Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6317","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6317\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6317\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6317\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6317","id":1951965668,"node_id":"I_kwDODunzps50WKHk","number":6317,"title":"sentiment140 dataset unavailable","user":{"login":"AndreasKarasenko","id":52670382,"node_id":"MDQ6VXNlcjUyNjcwMzgy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/52670382?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/AndreasKarasenko","html_url":"https:\/\/github.com\/AndreasKarasenko","followers_url":"https:\/\/api.github.com\/users\/AndreasKarasenko\/followers","following_url":"https:\/\/api.github.com\/users\/AndreasKarasenko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/AndreasKarasenko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/AndreasKarasenko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/AndreasKarasenko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/AndreasKarasenko\/orgs","repos_url":"https:\/\/api.github.com\/users\/AndreasKarasenko\/repos","events_url":"https:\/\/api.github.com\/users\/AndreasKarasenko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/AndreasKarasenko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-10-19T11:25:21Z","updated_at":"2023-10-19T13:04:56Z","closed_at":"2023-10-19T13:04:56Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nloading the dataset using load_dataset(\"sentiment140\") returns the following error\r\nConnectionError: Couldn't reach http:\/\/cs.stanford.edu\/people\/alecmgo\/trainingandtestdata.zip (error 403)\r\n\r\n### Steps to reproduce the bug\r\n\r\nRun the following code (version should not matter).\r\n\r\n```\r\nfrom datasets import load_dataset\r\ndata = load_dataset(\"sentiment140\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe dataset should be loaded just like any other.\r\nThe main issue is that it is no longer hosted by stanford. It is still available from a [Google Drive Link](https:\/\/docs.google.com\/file\/d\/0B04GJPshIjmPRnZManQwWEdTZjg\/edit).\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.5\r\n- Platform: Windows-10-10.0.19045-SP0\r\n- Python version: 3.10.8\r\n- Huggingface_hub version: 0.17.3    \r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.1.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6317\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6317\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6316","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6316\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6316\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6316\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6316","id":1951819869,"node_id":"PR_kwDODunzps5dQGpg","number":6316,"title":"Fix loading Hub datasets with CSV metadata file","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-19T10:21:34Z","updated_at":"2023-10-20T06:23:21Z","closed_at":"2023-10-20T06:14:09Z","author_association":"MEMBER","active_lock_reason":null,"body":"Currently, the reading of the metadata file infers the file extension (.jsonl or .csv) from the passed filename. However, downloaded files from the Hub don't have file extension. For example:\r\n- the original file: `hf:\/\/datasets\/__DUMMY_TRANSFORMERS_USER__\/test-dataset-5916a4-16977085077831\/metadata.jsonl`\r\n- corresponds to the downloaded path: `\/tmp\/pytest-of-username\/pytest-46\/cache\/datasets\/downloads\/9f5374dbb470f711f6b89d66a5eec1f19cc96324b26bcbebe29138bda6cb20e6`, which does not have extension\r\n\r\nIn the case where the metadata file does not have an extension, the reader assumes it is a JSONL file, thus the reported error when trying to read a CSV file as a JSONL one: `ArrowInvalid: JSON parse error: Invalid value. in row 0`\r\n\r\nThis behavior was introduced by:\r\n- #4837\r\n\r\nThis PR extracts the metadata file extension from the original filename (instead of the downloaded one) and passes it as a parameter to the read_metadata function.\r\n\r\nFix #6315.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6316\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6316\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6316","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6316","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6316.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6316.patch","merged_at":"2023-10-20T06:14:09Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6315","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6315\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6315\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6315\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6315","id":1951800819,"node_id":"I_kwDODunzps50Vh3z","number":6315,"title":"Hub datasets with CSV metadata raise ArrowInvalid: JSON parse error: Invalid value. in row 0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-10-19T10:11:29Z","updated_at":"2023-10-20T06:14:10Z","closed_at":"2023-10-20T06:14:10Z","author_association":"MEMBER","active_lock_reason":null,"body":"When trying to load a Hub dataset that contains a CSV metadata file, it raises an `ArrowInvalid` error:\r\n```\r\nE   pyarrow.lib.ArrowInvalid: JSON parse error: Invalid value. in row 0\r\n\r\npyarrow\/error.pxi:100: ArrowInvalid\r\n```\r\n\r\nSee: https:\/\/huggingface.co\/datasets\/lukarape\/public_small_papers\/discussions\/1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6315\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6315\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6314","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6314\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6314\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6314\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6314","id":1951684763,"node_id":"PR_kwDODunzps5dPo25","number":6314,"title":"Support creating new branch in push_to_hub","user":{"login":"jmif","id":1000442,"node_id":"MDQ6VXNlcjEwMDA0NDI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1000442?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jmif","html_url":"https:\/\/github.com\/jmif","followers_url":"https:\/\/api.github.com\/users\/jmif\/followers","following_url":"https:\/\/api.github.com\/users\/jmif\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jmif\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jmif\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jmif\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jmif\/orgs","repos_url":"https:\/\/api.github.com\/users\/jmif\/repos","events_url":"https:\/\/api.github.com\/users\/jmif\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jmif\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-10-19T09:12:39Z","updated_at":"2023-10-19T09:20:06Z","closed_at":"2023-10-19T09:19:48Z","author_association":"NONE","active_lock_reason":null,"body":"This adds support for creating a new branch when pushing a dataset to the hub.  Tested both methods locally and branches are created.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6314\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6314\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6314","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6314","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6314.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6314.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6313","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6313\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6313\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6313\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6313","id":1951527712,"node_id":"PR_kwDODunzps5dPGmL","number":6313,"title":"Fix commit message formatting in multi-commit uploads","user":{"login":"qgallouedec","id":45557362,"node_id":"MDQ6VXNlcjQ1NTU3MzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45557362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/qgallouedec","html_url":"https:\/\/github.com\/qgallouedec","followers_url":"https:\/\/api.github.com\/users\/qgallouedec\/followers","following_url":"https:\/\/api.github.com\/users\/qgallouedec\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/qgallouedec\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/qgallouedec\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/qgallouedec\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/qgallouedec\/orgs","repos_url":"https:\/\/api.github.com\/users\/qgallouedec\/repos","events_url":"https:\/\/api.github.com\/users\/qgallouedec\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/qgallouedec\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-19T07:53:56Z","updated_at":"2023-10-20T14:06:13Z","closed_at":"2023-10-20T13:57:39Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Currently, the commit message keeps on adding:\r\n\r\n- `Upload dataset (part 00000-of-00002)`\r\n- `Upload dataset (part 00000-of-00002) (part 00001-of-00002)`\r\n\r\nIntroduced in https:\/\/github.com\/huggingface\/datasets\/pull\/6269\r\n\r\nThis PR fixes this issue to have\r\n\r\n- `Upload dataset (part 00000-of-00002)`\r\n- `Upload dataset (part 00001-of-00002)`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6313\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6313\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6313","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6313","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6313.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6313.patch","merged_at":"2023-10-20T13:57:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6312","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6312\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6312\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6312\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6312","id":1950128416,"node_id":"PR_kwDODunzps5dKWDF","number":6312,"title":"docs: resolving namespace conflict, refactored variable ","user":{"login":"smty2018","id":74114936,"node_id":"MDQ6VXNlcjc0MTE0OTM2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/74114936?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/smty2018","html_url":"https:\/\/github.com\/smty2018","followers_url":"https:\/\/api.github.com\/users\/smty2018\/followers","following_url":"https:\/\/api.github.com\/users\/smty2018\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/smty2018\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/smty2018\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/smty2018\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/smty2018\/orgs","repos_url":"https:\/\/api.github.com\/users\/smty2018\/repos","events_url":"https:\/\/api.github.com\/users\/smty2018\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/smty2018\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-18T16:10:59Z","updated_at":"2023-10-19T16:31:59Z","closed_at":"2023-10-19T16:23:07Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"In docs of about_arrow.md, in the below example code\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/74114936\/fc70e152-e15f-422e-949a-1c4c4c9aa116)\r\nThe variable name 'time' was being used in a way that could potentially lead to a namespace conflict with Python's built-in 'time' module. It is not a good convention and can lead to unintended variable shadowing for any user re-using the example code.\r\nTo ensure code clarity, and prevent potential naming conflicts renamed the variable 'time' to 'elapsed_time' in the example  code.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6312\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6312\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6312","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6312","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6312.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6312.patch","merged_at":"2023-10-19T16:23:07Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6311","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6311\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6311\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6311\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6311","id":1949304993,"node_id":"I_kwDODunzps50MAih","number":6311,"title":"cast_column to Sequence with length=4 occur exception raise in datasets\/table.py:2146","user":{"login":"neiblegy","id":16574677,"node_id":"MDQ6VXNlcjE2NTc0Njc3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16574677?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/neiblegy","html_url":"https:\/\/github.com\/neiblegy","followers_url":"https:\/\/api.github.com\/users\/neiblegy\/followers","following_url":"https:\/\/api.github.com\/users\/neiblegy\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/neiblegy\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/neiblegy\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/neiblegy\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/neiblegy\/orgs","repos_url":"https:\/\/api.github.com\/users\/neiblegy\/repos","events_url":"https:\/\/api.github.com\/users\/neiblegy\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/neiblegy\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-18T09:38:05Z","updated_at":"2023-10-20T10:17:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\ni load a dataset from local csv file which has 187383612 examples, then use `map` to generate new columns for test.\r\nhere is my code : \r\n\r\n```\r\nimport os\r\n\r\nfrom datasets import load_dataset\r\nfrom datasets.features import Sequence, Value\r\n\r\ndef add_new_path(example):\r\n\r\n    example[\"ais_bbox\"] = [100,100,200,200]\r\n    example[\"ais_image_path\"] = os.path.join(\"images\", example[\"image_path\"]) if example[\"image_path\"] else \"\"\r\n    return example\r\n\r\nais_dataset = load_dataset(\"\/data\/ryan.gao\/ais_dataset_cache\/raw\/1749\/\")\r\nhf_ds = ais_dataset.map(add_new_path, batched=False, num_proc=32)\r\nds = hf_ds.cast_column(\"ais_bbox\", Sequence(Value(\"int32\"), length=4))\r\n```\r\n\r\nand the `cast_column` raise an exception\r\n```\r\nCasting the dataset:   3%|\u2588\u2588\u2588\u2589\r\n...\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 2110, in cast_column\r\n    return self.cast(features)\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 2055, in cast\r\n    dataset = dataset.map(\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 592, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 3097, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 3474, in _map_single\r\n    batch = apply_function_on_filtered_inputs(\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 3353, in apply_function_on_filtered_inputs\r\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/table.py\", line 2329, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/table.py\", line 2288, in cast_table_to_schema\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/table.py\", line 2288, in <listcomp>\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/table.py\", line 1831, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/table.py\", line 1831, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"\/home\/protoss.gao\/.local\/lib\/python3.9\/site-packages\/datasets\/table.py\", line 2145, in cast_array_to_feature\r\n    raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\nTypeError: Couldn't cast array of type\r\nlist<item: int64>\r\nto\r\nSequence(feature=Value(dtype='int32', id=None), length=4, id=None)\r\n```\r\ni check the source code and make debug info:\r\nin datasets\/table.py:2092\r\n```\r\n2091             if feature.length > -1:\r\n2092                 if feature.length * len(array) == len(array.values):\r\n2093                     return pa.FixedSizeListArray.from_arrays(_c(array.values, feature.feature), feature.length)\r\n2094                 print(len(array))\r\n2095                 print(len(array.values))\r\n```\r\nmy feature.length is 4.  but feature.length * len(array) == len(array.values) is false.\r\nprint(len(array)) is 262\r\nprint(len(array.values)) is 4000\r\n\r\nthen I use \"for item in array\" to print each item then get 262 * [100,100,200,200]\r\nand use \"for item in array.values\" to print each item and get 4000 int32 which are 1000 * [100,100,200,200]\r\n\r\ni'm wondering the `chunk` in each `array.chunks`, the \"chunk.values\" may get all the chunks's value rather than single chunk? but i check the pyarrow's doc seems chunk.values is chunk's value not all.\r\n\r\n### Steps to reproduce the bug\r\n\r\ncode provided above.\r\n\r\n### Expected behavior\r\n\r\nfeature.length * len(array) == len(array.values) should be true. and there should not has Exception.\r\n\r\n### Environment info\r\n\r\npython3.9\r\nx86_64\r\ndatasets: 2.14.4\r\npyarrow: 13.0.0 or 10.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6311\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6311\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6310","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6310\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6310\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6310\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6310","id":1947457988,"node_id":"PR_kwDODunzps5dBPnY","number":6310,"title":"Add return_file_name in load_dataset","user":{"login":"juliendenize","id":40604584,"node_id":"MDQ6VXNlcjQwNjA0NTg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/40604584?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/juliendenize","html_url":"https:\/\/github.com\/juliendenize","followers_url":"https:\/\/api.github.com\/users\/juliendenize\/followers","following_url":"https:\/\/api.github.com\/users\/juliendenize\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/juliendenize\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/juliendenize\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/juliendenize\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/juliendenize\/orgs","repos_url":"https:\/\/api.github.com\/users\/juliendenize\/repos","events_url":"https:\/\/api.github.com\/users\/juliendenize\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/juliendenize\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-17T13:36:57Z","updated_at":"2023-11-27T21:11:14Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Proposition to fix #5806.\r\n\r\nAdded an optional parameter `return_file_name` in the dataset builder config. When set to `True`, the function will include the file name corresponding to the sample in the returned output.\r\n\r\nThere is a difference between arrow-based and folder-based datasets to return the file name:\r\n- for arrow-based: a column is concatenated after the table is cast.\r\n- for folder-based: `dataset.info.features` has the entry `file_name` and the original file name is passed to the `sample_metadata` dictionary. \r\n\r\nThe difference in behavior might be a concern, also I do not know whether the `file_name` should return the original file path or the downloaded one for folder-based datasets.\r\n\r\nI added some tests for the datasets that already had a test file.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6310\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6310\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6310","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6310","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6310.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6310.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6309","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6309\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6309\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6309\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6309","id":1946916969,"node_id":"PR_kwDODunzps5c_YcX","number":6309,"title":"Fix get_data_patterns for directories with the word data twice","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-10-17T09:00:39Z","updated_at":"2023-10-18T14:01:52Z","closed_at":"2023-10-18T13:50:35Z","author_association":"MEMBER","active_lock_reason":null,"body":"Before the fix, `get_data_patterns` inferred wrongly the split name for paths with the word \"data\" twice:\r\n- For the URL path: `hf:\/\/datasets\/piuba-bigdata\/articles_and_comments@f328d536425ae8fcac5d098c8408f437bffdd357\/data\/train-00001-of-00009.parquet` (note the org name `piuba-bigdata\/` ending with `data\/`)\r\n- The inferred split name was: `articles_and_comments@f328d536425ae8fcac5d098c8408f437bffdd357\/data\/train` instead of `train`\r\n\r\nThis PR fixes this issue by passing the `base_path` (`hf:\/\/datasets\/piuba-bigdata\/articles_and_comments@f328d536425ae8fcac5d098c8408f437bffdd357`) to `_get_data_files_patterns` and prepending it to the regex split pattern (`data\/{split}-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9].*\\\\..*`).\r\n\r\nFix #6305.\r\nFix https:\/\/huggingface.co\/datasets\/piuba-bigdata\/articles_and_comments\/discussions\/1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6309\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6309\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6309","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6309","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6309.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6309.patch","merged_at":"2023-10-18T13:50:35Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6308","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6308\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6308\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6308\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6308","id":1946810625,"node_id":"I_kwDODunzps50CfkB","number":6308,"title":"module 'resource' has no attribute 'error'","user":{"login":"NeoWang9999","id":48009681,"node_id":"MDQ6VXNlcjQ4MDA5Njgx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/48009681?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NeoWang9999","html_url":"https:\/\/github.com\/NeoWang9999","followers_url":"https:\/\/api.github.com\/users\/NeoWang9999\/followers","following_url":"https:\/\/api.github.com\/users\/NeoWang9999\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NeoWang9999\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NeoWang9999\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NeoWang9999\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NeoWang9999\/orgs","repos_url":"https:\/\/api.github.com\/users\/NeoWang9999\/repos","events_url":"https:\/\/api.github.com\/users\/NeoWang9999\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NeoWang9999\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-17T08:08:54Z","updated_at":"2023-10-25T17:09:22Z","closed_at":"2023-10-25T17:09:22Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\njust run import:\r\n`from datasets import load_dataset`\r\n\r\nand then:\r\n```\r\n\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\__init__.py\", line 22, in <module>\r\n    from .arrow_dataset import Dataset\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 66, in <module>\r\n    from .arrow_reader import ArrowReader\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\arrow_reader.py\", line 30, in <module>\r\n    from .download.download_config import DownloadConfig\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\download\\__init__.py\", line 10, in <module>\r\n    from .streaming_download_manager import StreamingDownloadManager\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py\", line 21, in <module>\r\n    from ..filesystems import COMPRESSION_FILESYSTEMS\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\filesystems\\__init__.py\", line 8, in <module>\r\n    import fsspec.asyn\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\fsspec\\asyn.py\", line 157, in <module>\r\n    ResourceEror = resource.error\r\nAttributeError: module 'resource' has no attribute 'error'\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n\r\nand the error codes are:\r\n```\r\ntry:\r\n    import resource\r\nexcept ImportError:\r\n    resource = None\r\n    ResourceError = OSError\r\nelse:\r\n    ResourceEror = resource.error\r\n\r\n```\r\n1. miss spelling : \"ResourceEror \" should be \"ResourceErorr\"\r\n2. module 'resource' has no attribute 'error'\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\nonly one step:\r\n\r\n`from datasets import load_dataset`\n\n### Expected behavior\n\nslove error: module 'resource' has no attribute 'error'\n\n### Environment info\n\npython=3.10\r\n\r\ndatasets==2.14.5\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6308\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6308\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6307","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6307\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6307\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6307\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6307","id":1946414808,"node_id":"PR_kwDODunzps5c9s0j","number":6307,"title":"Fix typo in code example in docs","user":{"login":"bryant1410","id":3905501,"node_id":"MDQ6VXNlcjM5MDU1MDE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3905501?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/bryant1410","html_url":"https:\/\/github.com\/bryant1410","followers_url":"https:\/\/api.github.com\/users\/bryant1410\/followers","following_url":"https:\/\/api.github.com\/users\/bryant1410\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/bryant1410\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/bryant1410\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/bryant1410\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/bryant1410\/orgs","repos_url":"https:\/\/api.github.com\/users\/bryant1410\/repos","events_url":"https:\/\/api.github.com\/users\/bryant1410\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/bryant1410\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-17T02:28:50Z","updated_at":"2023-10-17T12:59:26Z","closed_at":"2023-10-17T06:36:19Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6307\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6307\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6307","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6307","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6307.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6307.patch","merged_at":"2023-10-17T06:36:18Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6306","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6306\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6306\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6306\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6306","id":1946363452,"node_id":"I_kwDODunzps50AyY8","number":6306,"title":"pyinstaller : OSError: could not get source code","user":{"login":"dusk877647949","id":57702070,"node_id":"MDQ6VXNlcjU3NzAyMDcw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57702070?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dusk877647949","html_url":"https:\/\/github.com\/dusk877647949","followers_url":"https:\/\/api.github.com\/users\/dusk877647949\/followers","following_url":"https:\/\/api.github.com\/users\/dusk877647949\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dusk877647949\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dusk877647949\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dusk877647949\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dusk877647949\/orgs","repos_url":"https:\/\/api.github.com\/users\/dusk877647949\/repos","events_url":"https:\/\/api.github.com\/users\/dusk877647949\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dusk877647949\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-10-17T01:41:51Z","updated_at":"2023-11-02T07:24:51Z","closed_at":"2023-10-18T14:03:42Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI ran a package with pyinstaller and got the following error\uff1a\r\n\r\n\n\n### Steps to reproduce the bug\n\n```  \r\n  ...\r\n  File \"datasets\\__init__.py\", line 52, in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"PyInstaller\\loader\\pyimod02_importers.py\", line 499, in exec_module\r\n  File \"datasets\\inspect.py\", line 30, in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"PyInstaller\\loader\\pyimod02_importers.py\", line 499, in exec_module\r\n  File \"datasets\\load.py\", line 58, in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"PyInstaller\\loader\\pyimod02_importers.py\", line 499, in exec_module\r\n  File \"datasets\\packaged_modules\\__init__.py\", line 31, in <module>\r\n  File \"inspect.py\", line 1147, in getsource\r\n  File \"inspect.py\", line 1129, in getsourcelines\r\n  File \"inspect.py\", line 958, in findsource\r\nOSError: could not get source code\r\n```\n\n### Expected behavior\n\nI have looked up the relevant information, but I can't find a suitable reason\n\n### Environment info\n\n```python\r\npython 3.10\r\ndatasets 2.14.4\r\npyinstaller 5.6.2\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6306\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6306\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6305","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6305\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6305\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6305\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6305","id":1946010912,"node_id":"I_kwDODunzps5z_cUg","number":6305,"title":"Cannot load dataset with `2.14.5`: `FileNotFound` error","user":{"login":"finiteautomata","id":167943,"node_id":"MDQ6VXNlcjE2Nzk0Mw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/167943?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/finiteautomata","html_url":"https:\/\/github.com\/finiteautomata","followers_url":"https:\/\/api.github.com\/users\/finiteautomata\/followers","following_url":"https:\/\/api.github.com\/users\/finiteautomata\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/finiteautomata\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/finiteautomata\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/finiteautomata\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/finiteautomata\/orgs","repos_url":"https:\/\/api.github.com\/users\/finiteautomata\/repos","events_url":"https:\/\/api.github.com\/users\/finiteautomata\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/finiteautomata\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-10-16T20:11:27Z","updated_at":"2023-10-18T13:50:36Z","closed_at":"2023-10-18T13:50:36Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm trying to load [piuba-bigdata\/articles_and_comments] and I'm stumbling with this error on `2.14.5`. However, this works on  `2.10.0`.\r\n\n\n### Steps to reproduce the bug\n\n[Colab link](https:\/\/colab.research.google.com\/drive\/1SAftFMQnFE708ikRnJJHIXZV7R5IBOCE#scrollTo=r2R2ipCCDmsg)\r\n\r\n```python\r\nDownloading readme: 100%\r\n1.19k\/1.19k [00:00<00:00, 30.9kB\/s]\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n[<ipython-input-2-807c3583d297>](https:\/\/localhost:8080\/#) in <cell line: 3>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 load_dataset(\"piuba-bigdata\/articles_and_comments\", split=\"train\")\r\n\r\n2 frames\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py](https:\/\/localhost:8080\/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2127 \r\n   2128     # Create a dataset builder\r\n-> 2129     builder_instance = load_dataset_builder(\r\n   2130         path=path,\r\n   2131         name=name,\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py](https:\/\/localhost:8080\/#) in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\r\n   1813         download_config = download_config.copy() if download_config else DownloadConfig()\r\n   1814         download_config.storage_options.update(storage_options)\r\n-> 1815     dataset_module = dataset_module_factory(\r\n   1816         path,\r\n   1817         revision=revision,\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py](https:\/\/localhost:8080\/#) in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1506                     raise e1 from None\r\n   1507                 if isinstance(e1, FileNotFoundError):\r\n-> 1508                     raise FileNotFoundError(\r\n   1509                         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1510                         f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n\r\nFileNotFoundError: Couldn't find a dataset script at \/content\/piuba-bigdata\/articles_and_comments\/articles_and_comments.py or any data file in the same directory. Couldn't find 'piuba-bigdata\/articles_and_comments' on the Hugging Face Hub either: FileNotFoundError: No (supported) data files or dataset script found in piuba-bigdata\/articles_and_comments.\r\n```\n\n### Expected behavior\n\nIt should load normally.\n\n### Environment info\n\n```\r\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.18.0\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6305\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6305\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6304","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6304\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6304\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6304\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6304","id":1945913521,"node_id":"PR_kwDODunzps5c7-4q","number":6304,"title":"Update README.md","user":{"login":"smty2018","id":74114936,"node_id":"MDQ6VXNlcjc0MTE0OTM2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/74114936?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/smty2018","html_url":"https:\/\/github.com\/smty2018","followers_url":"https:\/\/api.github.com\/users\/smty2018\/followers","following_url":"https:\/\/api.github.com\/users\/smty2018\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/smty2018\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/smty2018\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/smty2018\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/smty2018\/orgs","repos_url":"https:\/\/api.github.com\/users\/smty2018\/repos","events_url":"https:\/\/api.github.com\/users\/smty2018\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/smty2018\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-16T19:10:39Z","updated_at":"2023-10-17T15:13:37Z","closed_at":"2023-10-17T15:04:52Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fixed typos in ReadMe and added punctuation marks\r\n\r\nTensorflow --> TensorFlow\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6304\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6304\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6304","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6304","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6304.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6304.patch","merged_at":"2023-10-17T15:04:52Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6303","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6303\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6303\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6303\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6303","id":1943466532,"node_id":"I_kwDODunzps5z1vIk","number":6303,"title":"Parquet uploads off-by-one naming scheme","user":{"login":"ZachNagengast","id":1981179,"node_id":"MDQ6VXNlcjE5ODExNzk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1981179?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ZachNagengast","html_url":"https:\/\/github.com\/ZachNagengast","followers_url":"https:\/\/api.github.com\/users\/ZachNagengast\/followers","following_url":"https:\/\/api.github.com\/users\/ZachNagengast\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ZachNagengast\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ZachNagengast\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ZachNagengast\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ZachNagengast\/orgs","repos_url":"https:\/\/api.github.com\/users\/ZachNagengast\/repos","events_url":"https:\/\/api.github.com\/users\/ZachNagengast\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ZachNagengast\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-14T18:31:03Z","updated_at":"2023-10-16T16:33:21Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nI noticed this numbering scheme not matching up in a different project and wanted to raise it as an issue for discussion, what is the actual proper way to have these stored?\r\n\r\n<img width=\"425\" alt=\"image\" src=\"https:\/\/github.com\/huggingface\/datasets\/assets\/1981179\/3ffa2144-7c9a-446f-b521-a5e9db71e7ce\">\r\n\r\nThe `-SSSSS-of-NNNNN` seems to be used widely across the codebase. The section that creates the part in my screenshot is here https:\/\/github.com\/huggingface\/datasets\/blob\/main\/src\/datasets\/arrow_dataset.py#L5287\r\nThere are also some edits to this section in the single commit branch.\n\n### Steps to reproduce the bug\n\n1. Upload a dataset that requires at least two parquet files in it\r\n2. Observe the naming scheme\n\n### Expected behavior\n\nThe couple options here are of course **1. keeping it as is**\r\n\r\n**2. Starting the index at 1:**\r\ntrain-00001-of-00002-{hash}.parquet\r\ntrain-00002-of-00002-{hash}.parquet\r\n\r\n**3. My preferred option** (which would solve my specific issue), dropping the total entirely:\r\ntrain-00000-{hash}.parquet\r\ntrain-00001-{hash}.parquet\r\n\r\nThis also solves an issue that will occur with an `append` variable for `push_to_hub` (see https:\/\/github.com\/huggingface\/datasets\/issues\/6290) where as you add a new parquet file, you need to rename everything in the repo as well. \r\n\r\nHowever, I know there are parts of the repo that use 0 as the starting file or may require the total, so raising the question for discussion.\r\n\n\n### Environment info\n\n- `datasets` version: 2.14.6.dev0\r\n- Platform: macOS-14.0-arm64-arm-64bit\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.18.0\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6303\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6303\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6302","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6302\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6302\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6302\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6302","id":1942096078,"node_id":"I_kwDODunzps5zwgjO","number":6302,"title":"ArrowWriter\/ParquetWriter `write` method does not increase `_num_bytes` and hence datasets not sharding at `max_shard_size`","user":{"login":"Rassibassi","id":2855550,"node_id":"MDQ6VXNlcjI4NTU1NTA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2855550?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Rassibassi","html_url":"https:\/\/github.com\/Rassibassi","followers_url":"https:\/\/api.github.com\/users\/Rassibassi\/followers","following_url":"https:\/\/api.github.com\/users\/Rassibassi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Rassibassi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Rassibassi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Rassibassi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Rassibassi\/orgs","repos_url":"https:\/\/api.github.com\/users\/Rassibassi\/repos","events_url":"https:\/\/api.github.com\/users\/Rassibassi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Rassibassi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-13T14:43:36Z","updated_at":"2023-10-17T06:52:12Z","closed_at":"2023-10-17T06:52:11Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nAn example from [1], does not work when limiting shards with `max_shard_size`.\r\n\r\nTry the following example with low `max_shard_size`, such as:\r\n\r\n```python\r\nbuilder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\", max_shard_size=\"10MB\")\r\n```\r\n\r\nThe reason for this is that, in line [2] `writer._num_bytes > max_shard_size` is never true, because the `write` method of `ArrowWriter` [3] does not increase `self._num_bytes`.\r\n\r\nSuch that respective Arrow\/Parquet shards are only written to file based on the `writer_batch_size` or `config.DEFAULT_MAX_BATCH_SIZE`, but not based on `max_shard_size`.\r\n\r\n\r\n[1] https:\/\/huggingface.co\/docs\/datasets\/filesystems#download-and-prepare-a-dataset-into-a-cloud-storage\r\n\r\n[2] https:\/\/github.com\/huggingface\/datasets\/blob\/3e8d420808718c9a1453a2e7ee3484ca12c9c70d\/src\/datasets\/builder.py#L1677\r\n\r\n[3] https:\/\/github.com\/huggingface\/datasets\/blob\/3e8d420808718c9a1453a2e7ee3484ca12c9c70d\/src\/datasets\/arrow_writer.py#L459\n\n### Steps to reproduce the bug\n\nGet example from: https:\/\/huggingface.co\/docs\/datasets\/filesystems#download-and-prepare-a-dataset-into-a-cloud-storage\r\n\r\nCall `builder.download_and_prepare` with low `max_shard_size` such as `10MB`, e.g.:\r\n```python\r\nbuilder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\", max_shard_size=\"10MB\")\r\n```\n\n### Expected behavior\n\nShards should be written based on `max_shard_size` instead of batch size.\n\n### Environment info\n\n```\r\n>>> import datasets\r\n>>> datasets.__version__\r\n'2.14.6.dev0\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6302\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6302\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6301","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6301\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6301\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6301\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6301","id":1940183999,"node_id":"PR_kwDODunzps5cpPVh","number":6301,"title":"Unpin `tensorflow` maximum version","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-12T14:58:07Z","updated_at":"2023-10-12T15:58:20Z","closed_at":"2023-10-12T15:49:54Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Removes the temporary pin introduced in #6264 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6301\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6301\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6301","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6301","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6301.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6301.patch","merged_at":"2023-10-12T15:49:54Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6300","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6300\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6300\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6300\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6300","id":1940153432,"node_id":"PR_kwDODunzps5cpIoG","number":6300,"title":"Unpin `jax` maximum version","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-10-12T14:42:40Z","updated_at":"2023-10-12T16:37:55Z","closed_at":"2023-10-12T16:28:57Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"fix #6299\r\nfix #6202","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6300\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6300\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6300","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6300","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6300.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6300.patch","merged_at":"2023-10-12T16:28:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6299","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6299\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6299\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6299\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6299","id":1939649238,"node_id":"I_kwDODunzps5znLLW","number":6299,"title":"Support for newer versions of JAX","user":{"login":"ddrous","id":25456859,"node_id":"MDQ6VXNlcjI1NDU2ODU5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/25456859?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ddrous","html_url":"https:\/\/github.com\/ddrous","followers_url":"https:\/\/api.github.com\/users\/ddrous\/followers","following_url":"https:\/\/api.github.com\/users\/ddrous\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ddrous\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ddrous\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ddrous\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ddrous\/orgs","repos_url":"https:\/\/api.github.com\/users\/ddrous\/repos","events_url":"https:\/\/api.github.com\/users\/ddrous\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ddrous\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-10-12T10:03:46Z","updated_at":"2023-10-12T16:28:59Z","closed_at":"2023-10-12T16:28:59Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nHi,\r\n\r\nI like your idea of adapting the datasets library to be usable with JAX. Thank you for that.\r\n\r\nHowever, in your [setup.py](https:\/\/github.com\/huggingface\/datasets\/blob\/main\/setup.py), you enforce old versions of JAX <= 0.3... It is very cumbersome !\r\n\r\nWhat is the rationale for such a limitation ? Can you remove it please ?\r\n\r\nThanks,\r\n\r\n### Motivation\r\n\r\nThis library is unusable with new versions of JAX ?\r\n\r\n### Your contribution\r\n\r\nYes.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6299\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6299\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6298","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6298\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6298\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6298\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6298","id":1938797389,"node_id":"PR_kwDODunzps5ckg6j","number":6298,"title":"Doc readme improvements","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-11T21:51:12Z","updated_at":"2023-10-12T12:47:15Z","closed_at":"2023-10-12T12:38:19Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Changes in the doc READMe:\r\n* adds two new sections (to be aligned with `transformers` and `hfh`): \"Previewing the documentation\" and \"Writing documentation examples\"\r\n* replaces the mentions of `transformers` with `datasets`\r\n* fixes some dead links","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6298\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6298\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6298","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6298","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6298.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6298.patch","merged_at":"2023-10-12T12:38:19Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6297","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6297\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6297\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6297\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6297","id":1938752707,"node_id":"PR_kwDODunzps5ckXBa","number":6297,"title":"Fix ArrayXD cast","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-11T21:14:59Z","updated_at":"2023-10-13T13:54:00Z","closed_at":"2023-10-13T13:45:30Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6291 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6297\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6297\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6297","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6297","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6297.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6297.patch","merged_at":"2023-10-13T13:45:30Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6296","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6296\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6296\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6296\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6296","id":1938453845,"node_id":"PR_kwDODunzps5cjUs1","number":6296,"title":"Move `exceptions.py` to `utils\/exceptions.py`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-11T18:28:00Z","updated_at":"2023-10-17T13:25:33Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I didn't notice the path while reviewing the PR yesterday :(","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6296\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6296\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6296","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6296","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6296.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6296.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6295","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6295\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6295\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6295\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6295","id":1937362102,"node_id":"PR_kwDODunzps5cfiW8","number":6295,"title":"Fix parquet columns argument in streaming mode","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-11T10:01:01Z","updated_at":"2023-10-11T16:30:24Z","closed_at":"2023-10-11T16:21:36Z","author_association":"MEMBER","active_lock_reason":null,"body":"It was failing when there's a DatasetInfo with non-None info.features from the YAML (therefore containing columns that should be ignored)\r\n\r\nFix https:\/\/github.com\/huggingface\/datasets\/issues\/6293","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6295\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6295\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6295","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6295","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6295.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6295.patch","merged_at":"2023-10-11T16:21:36Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6294","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6294\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6294\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6294\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6294","id":1937359605,"node_id":"I_kwDODunzps5zecL1","number":6294,"title":"IndexError: Invalid key is out of bounds for size 0 despite having a populated dataset","user":{"login":"ZYM66","id":61892155,"node_id":"MDQ6VXNlcjYxODkyMTU1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/61892155?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ZYM66","html_url":"https:\/\/github.com\/ZYM66","followers_url":"https:\/\/api.github.com\/users\/ZYM66\/followers","following_url":"https:\/\/api.github.com\/users\/ZYM66\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ZYM66\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ZYM66\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ZYM66\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ZYM66\/orgs","repos_url":"https:\/\/api.github.com\/users\/ZYM66\/repos","events_url":"https:\/\/api.github.com\/users\/ZYM66\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ZYM66\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-11T09:59:38Z","updated_at":"2023-10-17T11:24:06Z","closed_at":"2023-10-17T11:24:06Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am encountering an `IndexError` when trying to access data from a DataLoader which wraps around a dataset I've loaded using the `datasets` library. The error suggests that the dataset size is `0`, but when I check the length and print the dataset, it's clear that it has `1166` entries.\n\n### Steps to reproduce the bug\n\n1. Load a dataset with `1166` entries.\r\n2. Create a DataLoader using this dataset.\r\n3. Try iterating over the DataLoader.\r\n\r\ncode:\r\n```python\r\ndef get_train_dataloader(self) -> DataLoader:\r\n        if self.train_dataset is None:\r\n            raise ValueError(\"Trainer: training requires a train_dataset.\")\r\n\r\n        train_dataset = self.train_dataset\r\n        data_collator = self.data_collator\r\n        print(len(train_dataset))\r\n        print(train_dataset)\r\n        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\r\n            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\r\n        else:\r\n            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\r\n        train_sampler = self._get_train_sampler()\r\n        dl = DataLoader(\r\n            train_dataset,\r\n            batch_size=self._train_batch_size,\r\n            sampler=train_sampler,\r\n            collate_fn=data_collator,\r\n            drop_last=self.args.dataloader_drop_last,\r\n            num_workers=self.args.dataloader_num_workers,\r\n            pin_memory=self.args.dataloader_pin_memory,\r\n            worker_init_fn=seed_worker,\r\n        )\r\n        print(dl)\r\n        print(len(dl))\r\n        for i in dl:\r\n            print(i)\r\n            break\r\n        return dl\r\n```\r\n\r\noutput : \r\n```\r\n1166\r\nDataset({\r\n    features: ['input_ids', 'special_tokens_mask'],\r\n    num_rows: 1166\r\n})\r\n<torch.utils.data.dataloader.DataLoader object ...>\r\n146\r\n```\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/dl\/zym\/llamaJP\/TestUseContinuePretrainLlama.py\", line 266, in <module>\r\n    train()\r\n  File \"\/home\/dl\/zym\/llamaJP\/TestUseContinuePretrainLlama.py\", line 260, in train\r\n    trainer.train()\r\n  File \"\/root\/miniconda3\/envs\/LLM\/lib\/python3.10\/site-packages\/transformers\/trainer.py\", line 1506, in train\r\n    return inner_training_loop(\r\n  File \"\/root\/miniconda3\/envs\/LLM\/lib\/python3.10\/site-packages\/transformers\/trainer.py\", line 1520, in _inner_training_loop\r\n    train_dataloader = self.get_train_dataloader()\r\n  File \"\/home\/dl\/zym\/llamaJP\/TestUseContinuePretrainLlama.py\", line 80, in get_train_dataloader\r\n    for i in dl:\r\n  File \"\/root\/miniconda3\/envs\/LLM\/lib\/python3.10\/site-packages\/torch\/utils\/data\/dataloader.py\", line 630, in __next__\r\n    data = self._next_data()\r\n  File \"\/root\/miniconda3\/envs\/LLM\/lib\/python3.10\/site-packages\/torch\/utils\/data\/dataloader.py\", line 674, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"\/root\/miniconda3\/envs\/LLM\/lib\/python3.10\/site-packages\/torch\/utils\/data\/_utils\/fetch.py\", line 49, in fetch\r\n    data = self.dataset.__getitems__(possibly_batched_index)\r\n  File \"\/root\/miniconda3\/envs\/LLM\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2807, in __getitems__\r\n    batch = self.__getitem__(keys)\r\n  File \"\/root\/miniconda3\/envs\/LLM\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2803, in __getitem__\r\n    return self._getitem(key)\r\n  File \"\/root\/miniconda3\/envs\/LLM\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2787, in _getitem\r\n    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n  File \"\/root\/miniconda3\/envs\/LLM\/lib\/python3.10\/site-packages\/datasets\/formatting\/formatting.py\", line 583, in query_table\r\n    _check_valid_index_key(key, size)\r\n  File \"\/root\/miniconda3\/envs\/LLM\/lib\/python3.10\/site-packages\/datasets\/formatting\/formatting.py\", line 536, in _check_valid_index_key\r\n    _check_valid_index_key(int(max(key)), size=size)\r\n  File \"\/root\/miniconda3\/envs\/LLM\/lib\/python3.10\/site-packages\/datasets\/formatting\/formatting.py\", line 526, in _check_valid_index_key\r\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\r\nIndexError: Invalid key: 1116 is out of bounds for size 0\r\n```\n\n### Expected behavior\n\nI expect to be able to iterate over the DataLoader without encountering an IndexError since the dataset is populated.\n\n### Environment info\n\n- `datasets` library version: [2.14.5]\r\n- Platform: [Linux] \r\n- Python version: 3.10\r\n- Other libraries involved: HuggingFace Transformers","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6294\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6294\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6293","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6293\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6293\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6293\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6293","id":1937238047,"node_id":"I_kwDODunzps5zd-gf","number":6293,"title":"Choose columns to stream parquet data in streaming mode","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-10-11T08:59:36Z","updated_at":"2023-10-11T16:21:38Z","closed_at":"2023-10-11T16:21:38Z","author_association":"MEMBER","active_lock_reason":null,"body":"Currently passing columns= to load_dataset in streaming mode fails\r\n\r\n```\r\nTried to load parquet data with columns '['link']' with mismatching features '{'caption': Value(dtype='string', id=None), 'image': {'bytes': Value(dtype='binary', id=None), 'path': Value(dtype='null', id=None)}, 'link': Value(dtype='string', id=None), 'message_id': Value(dtype='string', id=None), 'timestamp': Value(dtype='string', id=None)}'\r\n```\r\n\r\nsimilar to https:\/\/github.com\/huggingface\/datasets\/issues\/6039\r\n\r\nreported at https:\/\/huggingface.co\/datasets\/laion\/dalle-3-dataset\/discussions\/3#65259a09617407d4520f4ad9","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6293\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6293\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6292","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6292\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6292\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6292\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6292","id":1937050470,"node_id":"I_kwDODunzps5zdQtm","number":6292,"title":"how to load the image of dtype float32 or float64","user":{"login":"wanglaofei","id":26437644,"node_id":"MDQ6VXNlcjI2NDM3NjQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26437644?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wanglaofei","html_url":"https:\/\/github.com\/wanglaofei","followers_url":"https:\/\/api.github.com\/users\/wanglaofei\/followers","following_url":"https:\/\/api.github.com\/users\/wanglaofei\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wanglaofei\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wanglaofei\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wanglaofei\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wanglaofei\/orgs","repos_url":"https:\/\/api.github.com\/users\/wanglaofei\/repos","events_url":"https:\/\/api.github.com\/users\/wanglaofei\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wanglaofei\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-11T07:27:16Z","updated_at":"2023-10-11T13:19:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"_FEATURES = datasets.Features(\r\n    {\r\n        \"image\": datasets.Image(),\r\n        \"text\": datasets.Value(\"string\"),\r\n    },\r\n)\r\nThe datasets builder seems only support the unit8 data. How to load the float dtype data? ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6292\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6292\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6291","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6291\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6291\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6291\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6291","id":1936129871,"node_id":"I_kwDODunzps5zZv9P","number":6291,"title":"Casting type from Array2D int to Array2D float crashes","user":{"login":"AlanBlanchet","id":22567306,"node_id":"MDQ6VXNlcjIyNTY3MzA2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22567306?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/AlanBlanchet","html_url":"https:\/\/github.com\/AlanBlanchet","followers_url":"https:\/\/api.github.com\/users\/AlanBlanchet\/followers","following_url":"https:\/\/api.github.com\/users\/AlanBlanchet\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/AlanBlanchet\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/AlanBlanchet\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/AlanBlanchet\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/AlanBlanchet\/orgs","repos_url":"https:\/\/api.github.com\/users\/AlanBlanchet\/repos","events_url":"https:\/\/api.github.com\/users\/AlanBlanchet\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/AlanBlanchet\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-10T20:10:10Z","updated_at":"2023-10-13T13:45:31Z","closed_at":"2023-10-13T13:45:31Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI am on a school project and the initial type for feature annotations are `Array2D(shape=(None, 4))`. I am trying to cast this type to a `float64` and pyarrow gives me this error : \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/alan\/dev\/ClassezDesImagesAvecDesAlgorithmesDeDeeplearning\/src\/sdd\/data\/dataset.py\", line 141, in <module>\r\n    dataset = StanfordDogsDataset(size, 5).original(True).demo()\r\n  File \"<attrs generated init __main__.StanfordDogsDataset>\", line 4, in __init__\r\n  File \"\/home\/alan\/dev\/ClassezDesImagesAvecDesAlgorithmesDeDeeplearning\/src\/sdd\/data\/dataset.py\", line 33, in __attrs_post_init__\r\n    self.dataset = self.dataset.cast_column(\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/fingerprint.py\", line 511, in wrapper\r\n    out = func(dataset, *args, **kwargs)\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2110, in cast_column\r\n    return self.cast(features)\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2055, in cast\r\n    dataset = dataset.map(\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 592, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 3097, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 3474, in _map_single\r\n    batch = apply_function_on_filtered_inputs(\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 3353, in apply_function_on_filtered_inputs\r\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 2328, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 2287, in cast_table_to_schema\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 2287, in <listcomp>\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1831, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1831, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 2143, in cast_array_to_feature\r\n    return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1833, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"\/home\/alan\/.cache\/pypoetry\/virtualenvs\/sdd-2XWLAjSi-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1967, in array_cast\r\n    return pa_type.wrap_array(array)\r\n  File \"pyarrow\/types.pxi\", line 1369, in pyarrow.lib.BaseExtensionType.wrap_array\r\nTypeError: Incompatible storage type for extension<arrow.py_extension_type<Array2DExtensionType>>: expected list<item: list<item: double>>, got list<item: list<item: int32>>\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\ndataset = datasets.load_dataset(\"Alanox\/stanford-dogs\", split=\"full\")\r\ndataset = dataset.cast_column(\"annotations\", Array2D((None, 4), \"float64\"))\r\n```\r\n\r\n### Expected behavior\r\n\r\nIt should simply cast the column feature type to a `float64` without error\r\n\r\n### Environment info\r\n\r\ndatasets == 2.14.5","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6291\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6291\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6290","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6290\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6290\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6290\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6290","id":1935629679,"node_id":"I_kwDODunzps5zX11v","number":6290,"title":"Incremental dataset (e.g. `.push_to_hub(..., append=True)`)","user":{"login":"Wauplin","id":11801849,"node_id":"MDQ6VXNlcjExODAxODQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11801849?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Wauplin","html_url":"https:\/\/github.com\/Wauplin","followers_url":"https:\/\/api.github.com\/users\/Wauplin\/followers","following_url":"https:\/\/api.github.com\/users\/Wauplin\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Wauplin\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Wauplin\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Wauplin\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Wauplin\/orgs","repos_url":"https:\/\/api.github.com\/users\/Wauplin\/repos","events_url":"https:\/\/api.github.com\/users\/Wauplin\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Wauplin\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-10T15:18:03Z","updated_at":"2023-10-13T16:05:26Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nHave the possibility to do `ds.push_to_hub(..., append=True)`.\n\n### Motivation\n\nRequested in this [comment](https:\/\/huggingface.co\/datasets\/laion\/dalle-3-dataset\/discussions\/3#65252597c4edc168202a5eaa) and \r\nthis [comment](https:\/\/huggingface.co\/datasets\/laion\/dalle-3-dataset\/discussions\/4#6524f675c9607bdffb208d8f). Discussed internally on [slack](https:\/\/huggingface.slack.com\/archives\/C02EMARJ65P\/p1696950642610639?thread_ts=1690554266.830949&cid=C02EMARJ65P).\r\n\n\n### Your contribution\n\nWhat I suggest to do for parquet datasets is to use `CommitOperationCopy` + `CommitOperationDelete` from `huggingface_hub`:\r\n1. list files\r\n2. copy files from parquet-0001-of-0004 to parquet-0001-of-0005\r\n3. delete files like parquet-0001-of-0004\r\n4. generate + add last parquet file parquet-0005-of-0005\r\n\r\n=> make a single commit with all commit operations at once\r\n\r\n\r\nI think it should be quite straightforward to implement. Happy to review a PR (maybe conflicting with the ongoing \"1 commit push_to_hub\" PR https:\/\/github.com\/huggingface\/datasets\/pull\/6269)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6290\/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6290\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6289","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6289\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6289\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6289\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6289","id":1935628506,"node_id":"PR_kwDODunzps5cZiay","number":6289,"title":"testing doc-builder","user":{"login":"mishig25","id":11827707,"node_id":"MDQ6VXNlcjExODI3NzA3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11827707?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mishig25","html_url":"https:\/\/github.com\/mishig25","followers_url":"https:\/\/api.github.com\/users\/mishig25\/followers","following_url":"https:\/\/api.github.com\/users\/mishig25\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mishig25\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mishig25\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mishig25\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mishig25\/orgs","repos_url":"https:\/\/api.github.com\/users\/mishig25\/repos","events_url":"https:\/\/api.github.com\/users\/mishig25\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mishig25\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-10T15:17:29Z","updated_at":"2023-10-13T08:57:14Z","closed_at":"2023-10-13T08:56:48Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"testing https:\/\/github.com\/huggingface\/doc-builder\/pull\/426","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6289\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6289\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6289","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6289","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6289.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6289.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6288","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6288\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6288\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6288\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6288","id":1935005457,"node_id":"I_kwDODunzps5zVdcR","number":6288,"title":"Dataset.from_pandas with a DataFrame of PIL.Images","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-10T10:29:16Z","updated_at":"2023-10-20T18:23:05Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Currently type inference doesn't know what to do with a Pandas Series of PIL.Image objects, though it would be nice to get a Dataset with the Image type this way","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6288\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6288\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6287","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6287\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6287\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6287\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6287","id":1932758192,"node_id":"I_kwDODunzps5zM4yw","number":6287,"title":"map() not recognizing \"text\"","user":{"login":"EngineerKhan","id":5688359,"node_id":"MDQ6VXNlcjU2ODgzNTk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5688359?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/EngineerKhan","html_url":"https:\/\/github.com\/EngineerKhan","followers_url":"https:\/\/api.github.com\/users\/EngineerKhan\/followers","following_url":"https:\/\/api.github.com\/users\/EngineerKhan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/EngineerKhan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/EngineerKhan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/EngineerKhan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/EngineerKhan\/orgs","repos_url":"https:\/\/api.github.com\/users\/EngineerKhan\/repos","events_url":"https:\/\/api.github.com\/users\/EngineerKhan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/EngineerKhan\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-09T10:27:30Z","updated_at":"2023-10-11T20:28:45Z","closed_at":"2023-10-11T20:28:45Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThe [map() documentation](https:\/\/huggingface.co\/docs\/datasets\/v2.14.5\/en\/package_reference\/main_classes#datasets.Dataset.map) reads:\r\n`\r\nds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)`\r\n\r\nI have been trying to reproduce it in my code as:\r\n\r\n`tokenizedDataset = dataset.map(lambda x: tokenizer(x['text']), batched=True)`\r\n\r\nBut it doesn't work as it throws the error:\r\n\r\n> KeyError: 'text'\r\n\r\nCan you please guide me on how to fix it?\r\n\r\n\n\n### Steps to reproduce the bug\n\n1. `from datasets import load_dataset\r\n\r\ndataset = load_dataset(\"amazon_reviews_multi\")`\r\n\r\n2. Then this code: `from transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")`\r\n3. The line I quoted above (which I have been trying)\n\n### Expected behavior\n\nAs mentioned in the documentation, it should run without any error and map the tokenization on the whole dataset.\n\n### Environment info\n\nPython 3.10.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6287\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6287\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6286","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6286\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6286\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6286\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6286","id":1932640128,"node_id":"PR_kwDODunzps5cPKNK","number":6286,"title":"Create DefunctDatasetError","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-09T09:23:23Z","updated_at":"2023-10-10T07:13:22Z","closed_at":"2023-10-10T07:03:04Z","author_association":"MEMBER","active_lock_reason":null,"body":"Create `DefunctDatasetError` as a specific error to be raised when a dataset is defunct and no longer accessible.\r\n\r\nSee Hub discussion: https:\/\/huggingface.co\/datasets\/the_pile_books3\/discussions\/7#6523c13a94f3a1a2092d251b","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6286\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6286\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6286","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6286","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6286.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6286.patch","merged_at":"2023-10-10T07:03:04Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6285","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6285\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6285\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6285\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6285","id":1932306325,"node_id":"I_kwDODunzps5zLKeV","number":6285,"title":"TypeError: expected str, bytes or os.PathLike object, not dict","user":{"login":"andysingal","id":20493493,"node_id":"MDQ6VXNlcjIwNDkzNDkz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20493493?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/andysingal","html_url":"https:\/\/github.com\/andysingal","followers_url":"https:\/\/api.github.com\/users\/andysingal\/followers","following_url":"https:\/\/api.github.com\/users\/andysingal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/andysingal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/andysingal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/andysingal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/andysingal\/orgs","repos_url":"https:\/\/api.github.com\/users\/andysingal\/repos","events_url":"https:\/\/api.github.com\/users\/andysingal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/andysingal\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-09T04:56:26Z","updated_at":"2023-10-10T13:17:33Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nmy dataset is in form : train- image \/n -labels\r\n\r\nand tried the code:\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndata_files = {\r\n    \"train\": \"\/content\/datasets\/PotholeDetectionYOLOv8-1\/train\/\",\r\n      \"validation\": \"\/content\/datasets\/PotholeDetectionYOLOv8-1\/valid\/\",\r\n      \"test\": \"\/content\/datasets\/PotholeDetectionYOLOv8-1\/test\/\"\r\n}\r\ndataset = load_dataset(\"imagefolder\", data_dir=data_files)\r\ndataset\r\n```\r\ngot error:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-29-2ef1926f73d9>](https:\/\/localhost:8080\/#) in <cell line: 8>()\r\n      6       \"test\": \"\/content\/datasets\/PotholeDetectionYOLOv8-1\/test\/\"\r\n      7 }\r\n----> 8 dataset = load_dataset(\"imagefolder\", data_dir=data_files)\r\n      9 dataset\r\n\r\n6 frames\r\n[\/usr\/lib\/python3.10\/pathlib.py](https:\/\/localhost:8080\/#) in _parse_args(cls, args)\r\n    576                 parts += a._parts\r\n    577             else:\r\n--> 578                 a = os.fspath(a)\r\n    579                 if isinstance(a, str):\r\n    580                     # Force-cast str subclasses to str (issue #21127)\r\n\r\nTypeError: expected str, bytes or os.PathLike object, not dict\r\n```\n\n### Steps to reproduce the bug\n\nas share above \n\n### Expected behavior\n\nload images and labels , but my dataset only uploads images \r\n- https:\/\/huggingface.co\/datasets\/Andyrasika\/potholes-dataset\n\n### Environment info\n\ncolab pro ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6285\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6285\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6284","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6284\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6284\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6284\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6284","id":1929551712,"node_id":"I_kwDODunzps5zAp9g","number":6284,"title":"Add Belebele multiple-choice machine reading comprehension (MRC) dataset","user":{"login":"rajveer43","id":64583161,"node_id":"MDQ6VXNlcjY0NTgzMTYx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/64583161?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rajveer43","html_url":"https:\/\/github.com\/rajveer43","followers_url":"https:\/\/api.github.com\/users\/rajveer43\/followers","following_url":"https:\/\/api.github.com\/users\/rajveer43\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rajveer43\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rajveer43\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rajveer43\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rajveer43\/orgs","repos_url":"https:\/\/api.github.com\/users\/rajveer43\/repos","events_url":"https:\/\/api.github.com\/users\/rajveer43\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rajveer43\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-06T06:58:03Z","updated_at":"2023-10-06T13:26:51Z","closed_at":"2023-10-06T13:26:51Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nBelebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. This dataset enables the evaluation of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the [FLORES-200](https:\/\/github.com\/facebookresearch\/flores\/tree\/main\/flores200) dataset. The human annotation procedure was carefully curated to create questions that discriminate between different levels of generalizable language comprehension and is reinforced by extensive quality checks. While all questions directly relate to the passage, the English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. Belebele opens up new avenues for evaluating and analyzing the multilingual abilities of language models and NLP systems.\r\n\r\nPlease refer to paper for more details, [The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants](https:\/\/arxiv.org\/abs\/2308.16884).\r\n\r\n\r\n## Composition\r\n\r\n- 900 questions per language variant\r\n- 488 distinct passages, there are 1-2 associated questions for each.\r\n- For each question, there is 4 multiple-choice answers, exactly 1 of which is correct.\r\n- 122 language\/language variants (including English).\r\n- 900 x 122 = 109,800 total questions.\r\n\n\n### Motivation\n\nofficial repo https:\/\/github.com\/facebookresearch\/belebele\n\n### Your contribution\n\n-","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6284\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6284\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6283","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6283\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6283\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6283\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6283","id":1928552257,"node_id":"PR_kwDODunzps5cBlKq","number":6283,"title":"Fix `array.values` handling in array cast\/embed","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-10-05T15:24:05Z","updated_at":"2023-12-21T20:00:10Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6280, fix #6311, fix #6360\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6283\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6283\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6283","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6283","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6283.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6283.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6282","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6282\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6282\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6282\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6282","id":1928473630,"node_id":"PR_kwDODunzps5cBT5p","number":6282,"title":"Drop data_files duplicates","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-05T14:43:08Z","updated_at":"2023-11-14T17:24:50Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"I just added drop_duplicates=True to `.from_patterns`. I used a dict to deduplicate and preserve the order\r\n\r\nclose https:\/\/github.com\/huggingface\/datasets\/issues\/6259\r\nclose https:\/\/github.com\/huggingface\/datasets\/issues\/6272\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6282\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6282\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6282","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6282","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6282.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6282.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6281","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6281\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6281\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6281\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6281","id":1928456959,"node_id":"PR_kwDODunzps5cBQPd","number":6281,"title":"Improve documentation of dataset.from_generator","user":{"login":"hartmans","id":53510,"node_id":"MDQ6VXNlcjUzNTEw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/53510?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/hartmans","html_url":"https:\/\/github.com\/hartmans","followers_url":"https:\/\/api.github.com\/users\/hartmans\/followers","following_url":"https:\/\/api.github.com\/users\/hartmans\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/hartmans\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/hartmans\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/hartmans\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/hartmans\/orgs","repos_url":"https:\/\/api.github.com\/users\/hartmans\/repos","events_url":"https:\/\/api.github.com\/users\/hartmans\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/hartmans\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-10-05T14:34:49Z","updated_at":"2023-10-05T19:09:07Z","closed_at":"2023-10-05T18:57:41Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Improve documentation to clarify sharding behavior (#6270)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6281\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6281\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6281","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6281","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6281.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6281.patch","merged_at":"2023-10-05T18:57:41Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6280","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6280\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6280\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6280\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6280","id":1928215278,"node_id":"I_kwDODunzps5y7jru","number":6280,"title":"Couldn't cast array of type fixed_size_list to Sequence(Value(float64))","user":{"login":"jmif","id":1000442,"node_id":"MDQ6VXNlcjEwMDA0NDI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1000442?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jmif","html_url":"https:\/\/github.com\/jmif","followers_url":"https:\/\/api.github.com\/users\/jmif\/followers","following_url":"https:\/\/api.github.com\/users\/jmif\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jmif\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jmif\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jmif\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jmif\/orgs","repos_url":"https:\/\/api.github.com\/users\/jmif\/repos","events_url":"https:\/\/api.github.com\/users\/jmif\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jmif\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-05T12:48:31Z","updated_at":"2023-10-13T09:41:54Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI have a dataset with an embedding column, when I try to map that dataset I get the following exception:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/jmif\/.virtualenvs\/llm-training\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 3189, in map\r\n    for rank, done, content in iflatmap_unordered(\r\n  File \"\/Users\/jmif\/.virtualenvs\/llm-training\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 1387, in iflatmap_unordered\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"\/Users\/jmif\/.virtualenvs\/llm-training\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 1387, in <listcomp>\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"\/Users\/jmif\/.virtualenvs\/llm-training\/lib\/python3.10\/site-packages\/multiprocess\/pool.py\", line 774, in get\r\n    raise self._value\r\nTypeError: Couldn't cast array of type\r\nfixed_size_list<item: float>[2]\r\nto\r\nSequence(feature=Value(dtype='float32', id=None), length=2, id=None)\r\n```\n\n### Steps to reproduce the bug\n\nHere's a simple repro script:\r\n\r\n```\r\nfrom datasets import Features, Value, Sequence, ClassLabel, Dataset\r\n\r\ndataset_features = Features({\r\n    'text': Value('string'),\r\n    'embedding': Sequence(Value('double'), length=2),\r\n    'categories': Sequence(ClassLabel(names=sorted([\r\n        'one',\r\n        'two',\r\n        'three'\r\n    ]))),\r\n})\r\n\r\ndataset = Dataset.from_dict(\r\n    {\r\n        'text': ['A'] * 10000,\r\n        'embedding': [[0.0, 0.1]] * 10000,\r\n        'categories': [[0]] * 10000,\r\n    },\r\n    features=dataset_features\r\n)\r\n\r\ndef test_mapper(r):\r\n    r['text'] = list(map(lambda t: t + ' b', r['text']))\r\n    return r\r\n\r\n\r\ndataset = dataset.map(test_mapper, batched=True, batch_size=10, features=dataset_features, num_proc=2)\r\n```\r\n\r\nRemoving the embedding column fixes the issue!\n\n### Expected behavior\n\nThe mapping completes successfully.\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: macOS-14.0-arm64-arm-64bit\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.17.1\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6280\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6280\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6279","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6279\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6279\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6279\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6279","id":1928028226,"node_id":"I_kwDODunzps5y62BC","number":6279,"title":"Batched IterableDataset","user":{"login":"lneukom","id":7010688,"node_id":"MDQ6VXNlcjcwMTA2ODg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7010688?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lneukom","html_url":"https:\/\/github.com\/lneukom","followers_url":"https:\/\/api.github.com\/users\/lneukom\/followers","following_url":"https:\/\/api.github.com\/users\/lneukom\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lneukom\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lneukom\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lneukom\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lneukom\/orgs","repos_url":"https:\/\/api.github.com\/users\/lneukom\/repos","events_url":"https:\/\/api.github.com\/users\/lneukom\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lneukom\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-05T11:12:49Z","updated_at":"2023-10-05T11:50:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nHi, \r\n\r\ncould you add an implementation of a batched `IterableDataset`. It already support an option to do batch iteration via `.iter(batch_size=...)` but this cannot be used in combination with a torch `DataLoader` since it just returns an iterator.\n\n### Motivation\n\nThe current implementation loads each element of a batch individually which can be very slow in cases of a big batch_size. I did some experiments [here](https:\/\/discuss.huggingface.co\/t\/slow-dataloader-with-big-batch-size\/57224) and using a batched iteration would speed up data loading significantly.\n\n### Your contribution\n\nN\/A","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6279\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6279\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6278","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6278\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6278\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6278\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6278","id":1927957877,"node_id":"PR_kwDODunzps5b_iKb","number":6278,"title":"No data files duplicates","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-10-05T10:31:58Z","updated_at":"2023-10-05T14:43:17Z","closed_at":"2023-10-05T14:43:17Z","author_association":"MEMBER","active_lock_reason":null,"body":"I added a new DataFilesSet class to disallow duplicate data files.\r\nI also deprecated DataFilesList.\r\n\r\nEDIT: actually I might just add drop_duplicates=True to `.from_patterns`\r\n\r\nclose https:\/\/github.com\/huggingface\/datasets\/issues\/6259\r\nclose https:\/\/github.com\/huggingface\/datasets\/issues\/6272\r\n\r\nTODO:\r\n- [ ] tests\r\n- [ ] preserve data files order","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6278\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6278\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6278","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6278","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6278.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6278.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6277","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6277\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6277\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6277\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6277","id":1927044546,"node_id":"I_kwDODunzps5y3F3C","number":6277,"title":"FileNotFoundError: Couldn't find a module script at \/content\/paws-x\/paws-x.py. Module 'paws-x' doesn't exist on the Hugging Face Hub either.","user":{"login":"diegogonzalezc","id":66733346,"node_id":"MDQ6VXNlcjY2NzMzMzQ2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/66733346?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/diegogonzalezc","html_url":"https:\/\/github.com\/diegogonzalezc","followers_url":"https:\/\/api.github.com\/users\/diegogonzalezc\/followers","following_url":"https:\/\/api.github.com\/users\/diegogonzalezc\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/diegogonzalezc\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/diegogonzalezc\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/diegogonzalezc\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/diegogonzalezc\/orgs","repos_url":"https:\/\/api.github.com\/users\/diegogonzalezc\/repos","events_url":"https:\/\/api.github.com\/users\/diegogonzalezc\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/diegogonzalezc\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-04T22:01:25Z","updated_at":"2023-10-08T17:05:46Z","closed_at":"2023-10-08T17:05:46Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm encountering a \"FileNotFoundError\" while attempting to use the \"paws-x\" dataset to retrain the DistilRoBERTa-base model. The error message is as follows: \r\n\r\nFileNotFoundError: Couldn't find a module script at \/content\/paws-x\/paws-x.py. Module 'paws-x' doesn't exist on the Hugging Face Hub either.\n\n### Steps to reproduce the bug\n\nhttps:\/\/colab.research.google.com\/drive\/11xUUFxloClpmqLvDy_Xxfmo3oUzjY5nx#scrollTo=kUn74FigzhHm\n\n### Expected behavior\n\nThe the trained model\n\n### Environment info\n\ncolab, \"paws-x\" dataset , DistilRoBERTa-base model","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6277\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6277\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6276","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6276\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6276\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6276\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6276","id":1925961878,"node_id":"I_kwDODunzps5yy9iW","number":6276,"title":"I'm trying to fine tune the openai\/whisper model from huggingface using jupyter notebook and i keep getting this error","user":{"login":"valaofficial","id":50768065,"node_id":"MDQ6VXNlcjUwNzY4MDY1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/50768065?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/valaofficial","html_url":"https:\/\/github.com\/valaofficial","followers_url":"https:\/\/api.github.com\/users\/valaofficial\/followers","following_url":"https:\/\/api.github.com\/users\/valaofficial\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/valaofficial\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/valaofficial\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/valaofficial\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/valaofficial\/orgs","repos_url":"https:\/\/api.github.com\/users\/valaofficial\/repos","events_url":"https:\/\/api.github.com\/users\/valaofficial\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/valaofficial\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-04T11:03:41Z","updated_at":"2023-11-27T10:39:16Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm trying to fine tune the openai\/whisper model from huggingface using jupyter notebook and i keep getting this error, i'm following the steps in this blog post\r\n\r\nhttps:\/\/huggingface.co\/blog\/fine-tune-whisper\r\n\r\nI tried google collab and it works but because I'm on the free version the training doesn't complete\r\n\r\nthe error comes in jupyter notebook when i run this line\r\n\r\n`common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)`\r\n\r\nhere is the error message\r\n\r\n```\r\nMap (num_proc=4): 0% 0\/2506 [00:52<?, ? examples\/s]\r\nThe above exception was the direct cause of the following exception:\r\n\r\nNameError Traceback (most recent call last) Cell In[19], line 1 ----> 1 common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\dataset_dict.py:853, in DatasetDict.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc) 850 if cache_file_names is None: 851 cache_file_names = {k: None for k in self} 852 return DatasetDict( --> 853 { 854 k: dataset.map( 855 function=function, 856 with_indices=with_indices, 857 with_rank=with_rank, 858 input_columns=input_columns, 859 batched=batched, 860 batch_size=batch_size, 861 drop_last_batch=drop_last_batch, 862 remove_columns=remove_columns, 863 keep_in_memory=keep_in_memory, 864 load_from_cache_file=load_from_cache_file, 865 cache_file_name=cache_file_names[k], 866 writer_batch_size=writer_batch_size, 867 features=features, 868 disable_nullable=disable_nullable, 869 fn_kwargs=fn_kwargs, 870 num_proc=num_proc, 871 desc=desc, 872 ) 873 for k, dataset in self.items() 874 } 875 )\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\dataset_dict.py:854, in <dictcomp>(.0) 850 if cache_file_names is None: 851 cache_file_names = {k: None for k in self} 852 return DatasetDict( 853 { --> 854 k: dataset.map( 855 function=function, 856 with_indices=with_indices, 857 with_rank=with_rank, 858 input_columns=input_columns, 859 batched=batched, 860 batch_size=batch_size, 861 drop_last_batch=drop_last_batch, 862 remove_columns=remove_columns, 863 keep_in_memory=keep_in_memory, 864 load_from_cache_file=load_from_cache_file, 865 cache_file_name=cache_file_names[k], 866 writer_batch_size=writer_batch_size, 867 features=features, 868 disable_nullable=disable_nullable, 869 fn_kwargs=fn_kwargs, 870 num_proc=num_proc, 871 desc=desc, 872 ) 873 for k, dataset in self.items() 874 } 875 )\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:592, in transmit_tasks.<locals>.wrapper(*args, **kwargs) 590 self: \"Dataset\" = kwargs.pop(\"self\") 591 # apply actual function --> 592 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs) 593 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out] 594 for dataset in datasets: 595 # Remove task templates if a column mapping of the template is no longer valid\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:557, in transmit_format.<locals>.wrapper(*args, **kwargs) 550 self_format = { 551 \"type\": self._format_type, 552 \"format_kwargs\": self._format_kwargs, 553 \"columns\": self._format_columns, 554 \"output_all_columns\": self._output_all_columns, 555 } 556 # apply actual function --> 557 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs) 558 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out] 559 # re-apply format to the output\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:3189, in Dataset.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc) 3182 logger.info(f\"Spawning {num_proc} processes\") 3183 with logging.tqdm( 3184 disable=not logging.is_progress_bar_enabled(), 3185 unit=\" examples\", 3186 total=pbar_total, 3187 desc=(desc or \"Map\") + f\" (num_proc={num_proc})\", 3188 ) as pbar: -> 3189 for rank, done, content in iflatmap_unordered( 3190 pool, Dataset._map_single, kwargs_iterable=kwargs_per_job 3191 ): 3192 if done: 3193 shards_done += 1\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\utils\\py_utils.py:1394, in iflatmap_unordered(pool, func, kwargs_iterable) 1391 finally: 1392 if not pool_changed: 1393 # we get the result in case there's an error to raise -> 1394 [async_result.get(timeout=0.05) for async_result in async_results]\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\utils\\py_utils.py:1394, in <listcomp>(.0) 1391 finally: 1392 if not pool_changed: 1393 # we get the result in case there's an error to raise -> 1394 [async_result.get(timeout=0.05) for async_result in async_results]\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\multiprocess\\pool.py:774, in ApplyResult.get(self, timeout) 772 return self._value 773 else: --> 774 raise self._value\r\n\r\nNameError: name 'feature_extractor' is not defined\r\n```\n\n### Steps to reproduce the bug\n\n1. follow the steps in this blog post \r\n     https:\/\/huggingface.co\/blog\/fine-tune-whisper\r\n\r\n2.  run this line of code\r\n    `common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)`\r\n\r\n3. I'm using jupyter notebook from anaconda\n\n### Expected behavior\n\nNo error message\n\n### Environment info\n\ndatasets version: 2.8.0\r\nPython version: 3.11\r\nWindows 10","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6276\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6276\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6275","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6275\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6275\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6275\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6275","id":1921354680,"node_id":"I_kwDODunzps5yhYu4","number":6275,"title":"Would like to Contribute a dataset","user":{"login":"vikas70607","id":97907750,"node_id":"U_kgDOBdX0Jg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/97907750?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vikas70607","html_url":"https:\/\/github.com\/vikas70607","followers_url":"https:\/\/api.github.com\/users\/vikas70607\/followers","following_url":"https:\/\/api.github.com\/users\/vikas70607\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vikas70607\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vikas70607\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vikas70607\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vikas70607\/orgs","repos_url":"https:\/\/api.github.com\/users\/vikas70607\/repos","events_url":"https:\/\/api.github.com\/users\/vikas70607\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vikas70607\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-02T07:00:21Z","updated_at":"2023-10-10T16:27:54Z","closed_at":"2023-10-10T16:27:54Z","author_association":"NONE","active_lock_reason":null,"body":"I have a dataset of 2500 images that can be used for color-blind machine-learning algorithms. Since , there was no dataset available online , I made this dataset myself and would like to contribute this now to community","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6275\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6275\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6274","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6274\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6274\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6274\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6274","id":1921036328,"node_id":"I_kwDODunzps5ygLAo","number":6274,"title":"FileNotFoundError for dataset with multiple builder config","user":{"login":"LouisChen15","id":97120485,"node_id":"U_kgDOBcnw5Q","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/97120485?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/LouisChen15","html_url":"https:\/\/github.com\/LouisChen15","followers_url":"https:\/\/api.github.com\/users\/LouisChen15\/followers","following_url":"https:\/\/api.github.com\/users\/LouisChen15\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/LouisChen15\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/LouisChen15\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/LouisChen15\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/LouisChen15\/orgs","repos_url":"https:\/\/api.github.com\/users\/LouisChen15\/repos","events_url":"https:\/\/api.github.com\/users\/LouisChen15\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/LouisChen15\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-10-01T23:45:56Z","updated_at":"2023-10-02T20:09:38Z","closed_at":"2023-10-02T20:09:38Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen there is only one config and only the dataset name is entered when using datasets.load_dataset(), it works fine. But if I create a second builder_config for my dataset and enter the config name when using datasets.load_dataset(), the following error will happen.\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\/Users\/chenx\/.cache\/huggingface\/datasets\/my_dataset\/0_shot_multiple_choice\/1.0.0\/97c3854a012cfd6b045e3be4c864739902af2d818bb9235b047baa94c302e9a2.incomplete\/my_dataset-test-00000-00000-of-NNNNN.arrow'\r\n\r\nThe \"XXX.incomplete folder\" in the cache folder of my dataset will disappear before \"generating test split\", which does not happen when config name is not entered and the config name is \"default\"\r\n\r\nC:\\Users\\chenx\\.cache\\huggingface\\datasets\\my_dataset\\0_shot_multiple_choice\\1.0.0    \r\nThe folder that is supposed to remain under the above directory will disappear, and the data generator will not have a place to generate data into. \n\n### Steps to reproduce the bug\n\ntest = load_dataset('my_dataset', '0_shot_multiple_choice')\n\n### Expected behavior\n\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\/Users\/chenx\/.cache\/huggingface\/datasets\/my_dataset\/0_shot_multiple_choice\/1.0.0\/97c3854a012cfd6b045e3be4c864739902af2d818bb9235b047baa94c302e9a2.incomplete\/my_dataset-test-00000-00000-of-NNNNN.arrow'\n\n### Environment info\n\ndatasets                  2.14.5\r\npython                    3.8.18","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6274\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6274\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6273","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6273\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6273\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6273\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6273","id":1920922260,"node_id":"I_kwDODunzps5yfvKU","number":6273,"title":"Broken Link to PubMed Abstracts dataset .","user":{"login":"sameemqureshi","id":100606327,"node_id":"U_kgDOBf8hdw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/100606327?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sameemqureshi","html_url":"https:\/\/github.com\/sameemqureshi","followers_url":"https:\/\/api.github.com\/users\/sameemqureshi\/followers","following_url":"https:\/\/api.github.com\/users\/sameemqureshi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sameemqureshi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sameemqureshi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sameemqureshi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sameemqureshi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sameemqureshi\/repos","events_url":"https:\/\/api.github.com\/users\/sameemqureshi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sameemqureshi\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-10-01T19:08:48Z","updated_at":"2023-10-02T16:40:18Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThe link provided for the dataset is broken,\r\ndata_files = \r\n[https:\/\/the-eye.eu\/public\/AI\/pile_preliminary_components\/PUBMED_title_abstracts_2019_baseline.jsonl.zst](url)\r\n\r\nThe \n\n### Steps to reproduce the bug\n\nSteps to reproduce:\r\n\r\n1) Head over to [https:\/\/huggingface.co\/learn\/nlp-course\/chapter5\/4?fw=pt#big-data-datasets-to-the-rescue](url)\r\n\r\n2) In the Section  \"What is the Pile?\", you can see a code snippet that contains the broken link.\n\n### Expected behavior\n\nThe link should Redirect to the  \"PubMed Abstracts dataset\" as expected .\n\n### Environment info\n\n.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6273\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6273\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6272","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6272\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6272\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6272\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6272","id":1920831487,"node_id":"I_kwDODunzps5yfY__","number":6272,"title":"Duplicate `data_files` when named `<split>\/<split>.parquet`","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-10-01T15:43:56Z","updated_at":"2023-10-05T10:32:27Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"e.g. with `u23429\/stock_1_minute_ticker`\r\n\r\n```ipython\r\nIn [1]: from datasets import *\r\n\r\nIn [2]: b = load_dataset_builder(\"u23429\/stock_1_minute_ticker\")\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 627\/627 [00:00<00:00, 246kB\/s]\r\n\r\nIn [3]: b.config.data_files\r\nOut[3]: \r\n{NamedSplit('train'): ['hf:\/\/datasets\/u23429\/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166\/train\/train.parquet',\r\n  'hf:\/\/datasets\/u23429\/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166\/train\/train.parquet'],\r\n NamedSplit('validation'): ['hf:\/\/datasets\/u23429\/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166\/validation\/validation.parquet',\r\n  'hf:\/\/datasets\/u23429\/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166\/validation\/validation.parquet'],\r\n NamedSplit('test'): ['hf:\/\/datasets\/u23429\/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166\/test\/test.parquet',\r\n  'hf:\/\/datasets\/u23429\/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166\/test\/test.parquet']}\r\n\r\n```\r\n\r\nThis bug issue is present in the current `datasets` 2.14.5 and also on `main` even after https:\/\/github.com\/huggingface\/datasets\/pull\/6244 cc @mariosasko ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6272\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6272\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6271","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6271\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6271\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6271\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6271","id":1920420295,"node_id":"I_kwDODunzps5yd0nH","number":6271,"title":"Overwriting Split overwrites data but not metadata, corrupting dataset","user":{"login":"govindrai","id":13859249,"node_id":"MDQ6VXNlcjEzODU5MjQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/13859249?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/govindrai","html_url":"https:\/\/github.com\/govindrai","followers_url":"https:\/\/api.github.com\/users\/govindrai\/followers","following_url":"https:\/\/api.github.com\/users\/govindrai\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/govindrai\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/govindrai\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/govindrai\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/govindrai\/orgs","repos_url":"https:\/\/api.github.com\/users\/govindrai\/repos","events_url":"https:\/\/api.github.com\/users\/govindrai\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/govindrai\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-09-30T22:37:31Z","updated_at":"2023-10-16T13:30:50Z","closed_at":"2023-10-16T13:30:50Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI want to be able to overwrite\/update\/delete splits in my dataset. Currently the only way to do is to manually go into the dataset and delete the split. If I try to overwrite programmatically I end up in an error state and (somewhat) corrupting the dataset. Read below.\r\n\r\n**Current Behavior**\r\nWhen I push to an existing split I get this error:\r\n`ValueError: Split complexRoofLocation_01Apr2023_to_31May2023test already present` \r\nThis seems to suggest that the library doesn't support overwriting splits. \r\n\r\n**Potential Bug**\r\nWhat\u2019s strange is that datasets, despite the operation erroring out with the ValueError above, does, in fact, overwrite the split:\r\n\r\n`Pushing dataset shards to the dataset hub: 100% [.....................] 1\/1 [00:00<00:00, 55.04it\/s]` \r\nEven though you got an error message and your code fails, your dataset is now changed. That seems like a bug. Either don't change the dataset, or don't throw the error and allow the script to proceed. \r\n\r\nAdditional Bug\r\nWhile it overwrites the split, it doesn\u2019t overwrite the split\u2019s information. Because of this when you pull down the dataset you may end up getting a `NonMatchingSplitsSizesError` if the size of the dataset during the overwrite is different. For example, my original split had 5 rows, but on my overwrite, I only had 4. Then when I try to download the dataset, I get a `NonMatchingSplitsSizesError` because the dataset's data.json states there\u2019s 5 but only 4 exist in the split.\r\n\r\nExpected Behavior\r\nThis corrupts the dataset rendering it unusable (until you take manual intervention). Either the library should let the overwrite happen (which it does but should also update the metadata) or it shouldn\u2019t do anything.\n\n### Steps to reproduce the bug\n\n[Colab Notebook](https:\/\/colab.research.google.com\/drive\/1bqVkD06Ngs9MQNdSk_ygCG6y1UqXA4pC?usp=sharing)\r\n\n\n### Expected behavior\n\nThe split should be overwritten and I should be able to use the new version of the dataset without issue. \n\n### Environment info\n\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6271\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6271\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6270","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6270\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6270\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6270\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6270","id":1920329373,"node_id":"I_kwDODunzps5ydead","number":6270,"title":"Dataset.from_generator raises with sharded gen_args","user":{"login":"hartmans","id":53510,"node_id":"MDQ6VXNlcjUzNTEw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/53510?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/hartmans","html_url":"https:\/\/github.com\/hartmans","followers_url":"https:\/\/api.github.com\/users\/hartmans\/followers","following_url":"https:\/\/api.github.com\/users\/hartmans\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/hartmans\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/hartmans\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/hartmans\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/hartmans\/orgs","repos_url":"https:\/\/api.github.com\/users\/hartmans\/repos","events_url":"https:\/\/api.github.com\/users\/hartmans\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/hartmans\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-09-30T16:50:06Z","updated_at":"2023-10-11T20:29:12Z","closed_at":"2023-10-11T20:29:11Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nAccording to the docs of Datasets.from_generator:\r\n```\r\n        gen_kwargs(`dict`, *optional*):\r\n            Keyword arguments to be passed to the `generator` callable.\r\n            You can define a sharded dataset by passing the list of shards in `gen_kwargs`.\r\n```\r\nSo I'd expect that if gen_kwargs was a list, then my generator would be called once for each element in the list with the dict in the list for that element.\r\nIt doesn't work that way though.\n\n### Steps to reproduce the bug\n\n```python\r\n#!\/usr\/bin\/python\r\n\r\nfrom pathlib import Path\r\nimport datasets\r\n\r\ndef process_yaml(file):\r\n    yield dict(example=42)\r\n\r\n\r\nif __name__ == '__main__':\r\n    import sys\r\n    dir = Path(sys.argv[0]).parent\r\n    ds = datasets.Dataset.from_generator(process_yaml, gen_kwargs=[{'file':f} for f in dir.glob('*.yml')],\r\n        )\r\n    ds.to_json('training.jsonl')\r\n    \r\n\r\n```\r\n```\r\nGenerating train split: 0 examples [00:00, ? examples\/s]\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/dataset_bug.py\", line 13, in <module>\r\n    ds = datasets.Dataset.from_generator(process_yaml, gen_kwargs=[{'file':f} for f in dir.glob('*.yml')],\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/hartmans\/ai\/venv\/lib\/python3.11\/site-packages\/datasets\/arrow_dataset.py\", line 1072, in from_generator\r\n    ).read()\r\n      ^^^^^^\r\n  File \"\/home\/hartmans\/ai\/venv\/lib\/python3.11\/site-packages\/datasets\/io\/generator.py\", line 47, in read\r\n    self.builder.download_and_prepare(\r\n  File \"\/home\/hartmans\/ai\/venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 954, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/hartmans\/ai\/venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1717, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"\/home\/hartmans\/ai\/venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1049, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/home\/hartmans\/ai\/venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1555, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"\/home\/hartmans\/ai\/venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1656, in _prepare_split_single\r\n    generator = self._generate_examples(**gen_kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: datasets.packaged_modules.generator.generator.Generator._generate_examples() argument after ** must be a ```\r\nmapping, not list\r\n\n\n### Expected behavior\n\nI would expect that process_yaml would be called once for each yaml file in the directory where the script is run.\r\nI also tried with the list being in gen_kwargs, but in that case process_yaml gets called with a list.\r\n\n\n### Environment info\n\n- `datasets` version: 2.14.6.dev0 (git commit 0cc77d7f45c7369; also tested with 2.14.0)\r\n- Platform: Linux-6.1.0-10-amd64-x86_64-with-glibc2.36\r\n- Python version: 3.11.2\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6270\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6270\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6269","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6269\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6269\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6269\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6269","id":1919572790,"node_id":"PR_kwDODunzps5bjbDc","number":6269,"title":"Reduce the number of commits in `push_to_hub`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":21,"created_at":"2023-09-29T16:22:31Z","updated_at":"2023-10-16T16:03:18Z","closed_at":"2023-10-16T13:30:46Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Reduces the number of commits in `push_to_hub` by using the `preupload` API from https:\/\/github.com\/huggingface\/huggingface_hub\/pull\/1699. Each commit contains a maximum of 50 uploaded files.\r\n\r\nA shard's fingerprint no longer needs to be added as a suffix to support resuming an upload, meaning the shards' naming scheme is the same as the initial one.\r\n\r\nAlso, it adds support for the following params: `create_pr`, `commit_message` and `revision` (`branch` deprecated; unlike the previous implementation, this one creates a branch if the branch does not exist to be consistent with `transformers`). \r\n\r\n(Nit) This implementation keeps the markdown section of the generated README.md empty to enable importing the card template (when the card is accessed on the Hub).\r\n\r\nFixes https:\/\/github.com\/huggingface\/datasets\/issues\/5492, fixes https:\/\/github.com\/huggingface\/datasets\/issues\/6257, fixes https:\/\/github.com\/huggingface\/datasets\/issues\/5045, fixes https:\/\/github.com\/huggingface\/datasets\/issues\/6271\r\n\r\nTODO:\r\n- [x] set the minimal version to the next `hfh` release (once it's published)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6269\/reactions","total_count":4,"+1":0,"-1":0,"laugh":0,"hooray":3,"confused":0,"heart":0,"rocket":1,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6269\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6269","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6269","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6269.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6269.patch","merged_at":"2023-10-16T13:30:46Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6268","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6268\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6268\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6268\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6268","id":1919010645,"node_id":"PR_kwDODunzps5bhgs7","number":6268,"title":"Add repo_id to DatasetInfo","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-09-29T10:24:55Z","updated_at":"2023-10-01T15:29:45Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"lhoestq\/demo1\", split=\"train\")\r\nds = ds.map(lambda x: {}, num_proc=2).filter(lambda x: True).remove_columns([\"id\"])\r\nprint(ds.repo_id)\r\n# lhoestq\/demo1\r\n```\r\n\r\n- repo_id is None when the dataset doesn't come from the Hub, e.g. from Dataset.from_dict\r\n- repo_id is set to None when concatenating datasets with different repo ids\r\n\r\nrelated to https:\/\/github.com\/huggingface\/datasets\/issues\/4129\r\n\r\nTODO:\r\n- [ ] discuss if it's ok for now\r\n- [ ] tests","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6268\/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":2,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6268\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6268","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6268","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6268.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6268.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6267","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6267\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6267\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6267\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6267","id":1916443262,"node_id":"I_kwDODunzps5yOpp-","number":6267,"title":"Multi label class encoding","user":{"login":"jmif","id":1000442,"node_id":"MDQ6VXNlcjEwMDA0NDI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1000442?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jmif","html_url":"https:\/\/github.com\/jmif","followers_url":"https:\/\/api.github.com\/users\/jmif\/followers","following_url":"https:\/\/api.github.com\/users\/jmif\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jmif\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jmif\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jmif\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jmif\/orgs","repos_url":"https:\/\/api.github.com\/users\/jmif\/repos","events_url":"https:\/\/api.github.com\/users\/jmif\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jmif\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-09-27T22:48:08Z","updated_at":"2023-10-26T18:46:08Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nI have a multi label dataset and I'd like to be able to class encode the column and store the mapping directly in the features just as I can with a single label column.  `class_encode_column` currently does not support multi labels.\r\n\r\nHere's an example of what I'd like to encode:\r\n\r\n```\r\ndata = {\r\n    'text': ['one', 'two', 'three', 'four'],\r\n    'labels': [['a', 'b'], ['b'], ['b', 'c'], ['a', 'd']]\r\n}\r\n\r\ndataset = Dataset.from_dict(data)\r\ndataset = dataset.class_encode_column('labels')\r\n```\r\n\r\nI did some digging into the code base to evaluate the feasibility of this (note I'm very new to this code base) and from what I noticed the `ClassLabel` feature is still stored as an underlying raw data type of int so I thought a `MultiLabel` feature could similarly be stored as a Sequence of ints, thus not requiring significant serialization \/ conversion work to \/ from arrow.\r\n\r\nI did a POC of this [here](https:\/\/github.com\/huggingface\/datasets\/commit\/15443098e9ce053943172f7ec6fce3769d7dff6e) and included a simple test case (please excuse all the commented out tests, going for speed of POC here and didn't want to fight IDE to debug a single test).   In the test I just assert that `num_classes` is the same to show that things are properly serializing, but if you break after loading from disk you'll see the dataset correct and the dataset feature is as expected.\r\n\r\nAfter digging more I did notice a few issues\r\n- After loading from disk I noticed type of the `labels` class is `Sequence` not `MultiLabel` (though the added `feature` attribute came through).  This doesn't happen for `ClassLabel` but I couldn't find the encode \/ decode code paths that handle this.\r\n- I subclass `Sequence` in `MultiLabel` to leverage existing serialization, but this does miss the custom encode logic that `ClassLabel` has.  I'm not sure of the best way to approach this as I haven't fully understood the encode \/ decode flow for datasets.  I suspect my simple implementation will need some improvement as it'll require a significant amount of repeated logic to mimic `ClassLabel` behavior.\r\n\r\n\n\n### Motivation\n\nSee above - would like to support multi label class encodings.\n\n### Your contribution\n\nThis would be a big help for us and we're open to contributing but I'll likely need some guidance on how to implement to fit the encode \/ decode flow.  Some suggestions on tests \/ would be great too, I'm guessing in addition to the class encode tests (that I'll need to expand) we'll need encode \/ decode tests.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6267\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6267\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6266","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6266\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6266\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6266\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6266","id":1916334394,"node_id":"PR_kwDODunzps5bYYb8","number":6266,"title":"Use LibYAML with PyYAML if available","user":{"login":"bryant1410","id":3905501,"node_id":"MDQ6VXNlcjM5MDU1MDE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3905501?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/bryant1410","html_url":"https:\/\/github.com\/bryant1410","followers_url":"https:\/\/api.github.com\/users\/bryant1410\/followers","following_url":"https:\/\/api.github.com\/users\/bryant1410\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/bryant1410\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/bryant1410\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/bryant1410\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/bryant1410\/orgs","repos_url":"https:\/\/api.github.com\/users\/bryant1410\/repos","events_url":"https:\/\/api.github.com\/users\/bryant1410\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/bryant1410\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-09-27T21:13:36Z","updated_at":"2023-09-28T14:29:24Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"PyYAML, the YAML framework used in this library, allows the use of LibYAML to accelerate the methods `load` and `dump`. To use it, a user would need to first install a PyYAML version that uses LibYAML (not available in PyPI; needs to be manually installed). Then, to actually use them, PyYAML suggests importing the LibYAML version of the `Loader` and `Dumper` and falling back to the default ones. This PR implements this change. See [PyYAML docs](https:\/\/pyyaml.org\/wiki\/PyYAMLDocumentation) for more info.\r\n\r\nThis change was motivated after trying to use any of [the SugarCREPE datasets in the Hub](https:\/\/huggingface.co\/datasets?search=sugarcrepe) provided by [the org HuggingFaceM4](https:\/\/huggingface.co\/datasets\/HuggingFaceM4). Such datasets save a lot of information (~1MB) in the YAML metadata from the `README.md` file and I noticed this slowed down the data loading process. BTW, I also noticed cache files for it is also slow because it tries to hash an instance of `DatasetInfo`, which in turn has all this metadata.\r\n\r\nAlso, I changed two list comprehensions into generator expressions to avoid allocating extra memory unnecessarily.\r\n\r\nAnd BTW, there's [an issue in PyYAML suggesting to make this automatic](https:\/\/github.com\/yaml\/pyyaml\/issues\/437).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6266\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6266\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6266","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6266","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6266.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6266.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6265","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6265\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6265\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6265\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6265","id":1915651566,"node_id":"PR_kwDODunzps5bWDfc","number":6265,"title":"Remove `apache_beam` import in `BeamBasedBuilder._save_info`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-09-27T13:56:34Z","updated_at":"2023-09-28T18:34:02Z","closed_at":"2023-09-28T18:23:35Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"... to avoid an `ImportError` raised in `BeamBasedBuilder._save_info` when `apache_beam` is not installed (e.g., when downloading the processed version of a dataset from the HF GCS)\r\n\r\nFix https:\/\/github.com\/huggingface\/datasets\/issues\/6260","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6265\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6265\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6265","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6265","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6265.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6265.patch","merged_at":"2023-09-28T18:23:35Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6264","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6264\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6264\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6264\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6264","id":1914958781,"node_id":"PR_kwDODunzps5bTvzh","number":6264,"title":"Temporarily pin tensorflow < 2.14.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-09-27T08:16:06Z","updated_at":"2023-09-27T08:45:24Z","closed_at":"2023-09-27T08:36:39Z","author_association":"MEMBER","active_lock_reason":null,"body":"Temporarily pin tensorflow < 2.14.0 until permanent solution is found.\r\n\r\nHot fix #6263.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6264\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6264\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6264","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6264","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6264.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6264.patch","merged_at":"2023-09-27T08:36:39Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6263","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6263\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6263\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6263\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6263","id":1914951043,"node_id":"I_kwDODunzps5yI9WD","number":6263,"title":"CI is broken: ImportError: cannot import name 'context' from 'tensorflow.python'","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-09-27T08:12:05Z","updated_at":"2023-09-27T08:36:40Z","closed_at":"2023-09-27T08:36:40Z","author_association":"MEMBER","active_lock_reason":null,"body":"Python 3.10 CI is broken for `test_py310`.\r\n\r\nSee: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/6322990957\/job\/17169678812?pr=6262\r\n\r\n```\r\nFAILED tests\/test_py_utils.py::TempSeedTest::test_tensorflow - ImportError: cannot import name 'context' from 'tensorflow.python' (\/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/lib\/python3.10\/site-packages\/tensorflow\/python\/__init__.py)\r\n```\r\n\r\n```\r\n_________________________ TempSeedTest.test_tensorflow _________________________\r\n[gw1] linux -- Python 3.10.13 \/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/bin\/python\r\n\r\nself = <tests.test_py_utils.TempSeedTest testMethod=test_tensorflow>\r\n\r\n    @require_tf\r\n    def test_tensorflow(self):\r\n        import tensorflow as tf\r\n        from tensorflow.keras import layers\r\n    \r\n        model = layers.Dense(2)\r\n    \r\n        def gen_random_output():\r\n            x = tf.random.uniform((1, 3))\r\n            return model(x).numpy()\r\n    \r\n>       with temp_seed(42, set_tensorflow=True):\r\n\r\ntests\/test_py_utils.py:155: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/lib\/python3.10\/contextlib.py:135: in __enter__\r\n    return next(self.gen)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nseed = 42, set_pytorch = False, set_tensorflow = True\r\n\r\n    @contextmanager\r\n    def temp_seed(seed: int, set_pytorch=False, set_tensorflow=False):\r\n        \"\"\"Temporarily set the random seed. This works for python numpy, pytorch and tensorflow.\"\"\"\r\n        np_state = np.random.get_state()\r\n        np.random.seed(seed)\r\n    \r\n        if set_pytorch and config.TORCH_AVAILABLE:\r\n            import torch\r\n    \r\n            torch_state = torch.random.get_rng_state()\r\n            torch.random.manual_seed(seed)\r\n    \r\n            if torch.cuda.is_available():\r\n                torch_cuda_states = torch.cuda.get_rng_state_all()\r\n                torch.cuda.manual_seed_all(seed)\r\n    \r\n        if set_tensorflow and config.TF_AVAILABLE:\r\n            import tensorflow as tf\r\n>           from tensorflow.python import context as tfpycontext\r\nE           ImportError: cannot import name 'context' from 'tensorflow.python' (\/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/lib\/python3.10\/site-packages\/tensorflow\/python\/__init__.py)\r\n\r\n\/opt\/hostedtoolcache\/Python\/3.10.13\/x64\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py:257: ImportError\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6263\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6263\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6262","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6262\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6262\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6262\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6262","id":1914895459,"node_id":"PR_kwDODunzps5bTh6H","number":6262,"title":"Fix CI 404 errors","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-09-27T07:40:18Z","updated_at":"2023-09-28T15:39:16Z","closed_at":"2023-09-28T15:30:40Z","author_association":"MEMBER","active_lock_reason":null,"body":"Currently our CI usually raises 404 errors when trying to delete temporary repositories. See, e.g.: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/6314980985\/job\/17146507884\r\n```\r\nFAILED tests\/test_upstream_hub.py::TestPushToHub::test_push_dataset_dict_to_hub_multiple_files_with_max_shard_size - huggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6512fb99-4a52c561752ece3d77eb6d57;2b61cae4-613d-4a73-bbb1-2faf9e32b02d)\r\nRepository Not Found for url: https:\/\/hub-ci.huggingface.co\/api\/repos\/delete.\r\nPlease make sure you specified the correct `repo_id` and `repo_type`.\r\nIf you are trying to access a private or gated repo, make sure you are authenticated.\r\n\r\nFAILED tests\/test_upstream_hub.py::TestPushToHub::test_push_dataset_to_hub_custom_features_audio - huggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6512fbb2-0333dd666d42f0e173c2bb68;dfdc4271-b49b-4008-8c49-f05cf7c1d53d)\r\nRepository Not Found for url: https:\/\/hub-ci.huggingface.co\/api\/repos\/delete.\r\nPlease make sure you specified the correct `repo_id` and `repo_type`.\r\nIf you are trying to access a private or gated repo, make sure you are authenticated.\r\n\r\nFAILED tests\/test_upstream_hub.py::TestPushToHub::test_push_dataset_dict_to_hub_custom_splits - huggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6512fbca-167690694f39770a5b3a444e;baeaa905-0a57-4585-ac97-9aaae12dd47d)\r\nRepository Not Found for url: https:\/\/hub-ci.huggingface.co\/api\/repos\/delete.\r\nPlease make sure you specified the correct `repo_id` and `repo_type`.\r\nIf you are trying to access a private or gated repo, make sure you are authenticated.\r\n```\r\n\r\nI think this can be caused by collisions in temporary repository IDs because we create them in multiprocessing:\r\n```python\r\nwith temporary_repo(f\"{CI_HUB_USER}\/test-{int(time.time() * 10e3)}\") as ds_name:\r\n```\r\nThis can also be caused when there is another issue that does not allow the creation of the repository, thus making it impossible to delete it.\r\n\r\nThis PR tries to fix this issue by increasing the precision of the number on the repository ID: `10e6` instead of `10e3`.\r\nAdditionally, this PR catches RepositoryNotFoundError.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6262\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6262\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6262","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6262","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6262.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6262.patch","merged_at":"2023-09-28T15:30:40Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6261","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6261\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6261\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6261\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6261","id":1913813178,"node_id":"I_kwDODunzps5yEni6","number":6261,"title":"Can't load a dataset","user":{"login":"joaopedrosdmm","id":37955817,"node_id":"MDQ6VXNlcjM3OTU1ODE3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/37955817?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/joaopedrosdmm","html_url":"https:\/\/github.com\/joaopedrosdmm","followers_url":"https:\/\/api.github.com\/users\/joaopedrosdmm\/followers","following_url":"https:\/\/api.github.com\/users\/joaopedrosdmm\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/joaopedrosdmm\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/joaopedrosdmm\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/joaopedrosdmm\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/joaopedrosdmm\/orgs","repos_url":"https:\/\/api.github.com\/users\/joaopedrosdmm\/repos","events_url":"https:\/\/api.github.com\/users\/joaopedrosdmm\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/joaopedrosdmm\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-09-26T15:46:25Z","updated_at":"2023-10-05T10:23:23Z","closed_at":"2023-10-05T10:23:22Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nCan't seem to load the JourneyDB dataset.\r\n\r\nIt throws the following error:\r\n\r\n ```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\nCell In[15], line 2\r\n      1 # If the dataset is gated\/private, make sure you have run huggingface-cli login\r\n----> 2 dataset = load_dataset(\"JourneyDB\/JourneyDB\", data_files=\"data\", use_auth_token=True)\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/load.py:1664, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1661 ignore_verifications = ignore_verifications or save_infos\r\n   1663 # Create a dataset builder\r\n-> 1664 builder_instance = load_dataset_builder(\r\n   1665     path=path,\r\n   1666     name=name,\r\n   1667     data_dir=data_dir,\r\n   1668     data_files=data_files,\r\n   1669     cache_dir=cache_dir,\r\n   1670     features=features,\r\n   1671     download_config=download_config,\r\n   1672     download_mode=download_mode,\r\n   1673     revision=revision,\r\n   1674     use_auth_token=use_auth_token,\r\n   1675     **config_kwargs,\r\n   1676 )\r\n   1678 # Return iterable dataset in case of streaming\r\n   1679 if streaming:\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/load.py:1490, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\r\n   1488     download_config = download_config.copy() if download_config else DownloadConfig()\r\n   1489     download_config.use_auth_token = use_auth_token\r\n-> 1490 dataset_module = dataset_module_factory(\r\n   1491     path,\r\n   1492     revision=revision,\r\n   1493     download_config=download_config,\r\n   1494     download_mode=download_mode,\r\n   1495     data_dir=data_dir,\r\n   1496     data_files=data_files,\r\n   1497 )\r\n   1499 # Get dataset builder class from the processing script\r\n   1500 builder_cls = import_main_class(dataset_module.module_path)\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/load.py:1238, in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1236                 raise ConnectionError(f\"Couln't reach the Hugging Face Hub for dataset '{path}': {e1}\") from None\r\n   1237             if isinstance(e1, FileNotFoundError):\r\n-> 1238                 raise FileNotFoundError(\r\n   1239                     f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1240                     f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n   1241                 ) from None\r\n   1242             raise e1 from None\r\n   1243 else:\r\n\r\nFileNotFoundError: Couldn't find a dataset script at \/kaggle\/working\/JourneyDB\/JourneyDB\/JourneyDB.py or any data file in the same directory. Couldn't find 'JourneyDB\/JourneyDB' on the Hugging Face Hub either: FileNotFoundError: Unable to find data in dataset repository JourneyDB\/JourneyDB with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'zip']\r\n\r\n```\n\n### Steps to reproduce the bug\n\n1) \r\n```\r\nfrom huggingface_hub import notebook_login\r\nnotebook_login()\r\n```\r\n\r\n2)\r\n```\r\n!pip install -q datasets\r\nfrom datasets import load_dataset\r\n```\r\n\r\n3)\r\n`dataset = load_dataset(\"JourneyDB\/JourneyDB\", data_files=\"data\", use_auth_token=True)`\n\n### Expected behavior\n\nLoad the dataset\n\n### Environment info\n\nNotebook","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6261\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6261\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6260","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6260\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6260\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6260\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6260","id":1912593466,"node_id":"I_kwDODunzps5x_9w6","number":6260,"title":"REUSE_DATASET_IF_EXISTS don't work ","user":{"login":"rangehow","id":88258534,"node_id":"MDQ6VXNlcjg4MjU4NTM0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/88258534?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rangehow","html_url":"https:\/\/github.com\/rangehow","followers_url":"https:\/\/api.github.com\/users\/rangehow\/followers","following_url":"https:\/\/api.github.com\/users\/rangehow\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rangehow\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rangehow\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rangehow\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rangehow\/orgs","repos_url":"https:\/\/api.github.com\/users\/rangehow\/repos","events_url":"https:\/\/api.github.com\/users\/rangehow\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rangehow\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-09-26T03:02:16Z","updated_at":"2023-09-28T18:23:36Z","closed_at":"2023-09-28T18:23:36Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI use the following code  to download natural_question dataset. Even though I have completely download it, the next time I run this code, the new download procedure will start and cover the original \/data\/lxy\/NQ\r\n\r\nconfig=datasets.DownloadConfig(resume_download=True,max_retries=100,cache_dir=r'\/data\/lxy\/NQ',download_desc='NQ')\r\n\r\ndata=datasets.load_dataset('natural_questions',cache_dir=r'\/data\/lxy\/NQ',download_config=config,download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS)\r\n\r\n---\r\nSince I don't have apache_beam installed, it throw a exception. After I pip install apache_beam ,the download restart..\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/88258534\/f28ce7fe-29ea-4348-b87f-e69182a8bd41)\r\n\n\n### Steps to reproduce the bug\n\nrun this two line code\r\n\r\nconfig=datasets.DownloadConfig(resume_download=True,max_retries=100,cache_dir=r'\/data\/lxy\/NQ',download_desc='NQ')\r\n\r\ndata=datasets.load_dataset('natural_questions',cache_dir=r'\/data\/lxy\/NQ',download_config=config,download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS)\n\n### Expected behavior\n\nDownload behavior can be correctly follow DownloadMode\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-3.10.0-1160.88.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.9.17\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6260\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6260\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6259","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6259\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6259\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6259\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6259","id":1911965758,"node_id":"I_kwDODunzps5x9kg-","number":6259,"title":"Duplicated Rows When Loading Parquet Files from Root Directory with Subdirectories","user":{"login":"MF-FOOM","id":141304309,"node_id":"U_kgDOCGwh9Q","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/141304309?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/MF-FOOM","html_url":"https:\/\/github.com\/MF-FOOM","followers_url":"https:\/\/api.github.com\/users\/MF-FOOM\/followers","following_url":"https:\/\/api.github.com\/users\/MF-FOOM\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/MF-FOOM\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/MF-FOOM\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/MF-FOOM\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/MF-FOOM\/orgs","repos_url":"https:\/\/api.github.com\/users\/MF-FOOM\/repos","events_url":"https:\/\/api.github.com\/users\/MF-FOOM\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/MF-FOOM\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-09-25T17:20:54Z","updated_at":"2023-09-26T17:54:08Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen parquet files are saved in \"train\" and \"val\" subdirectories under a root directory, and datasets are then loaded using `load_dataset(\"parquet\", data_dir=\"root_directory\")`, the resulting dataset has duplicated rows for both the training and validation sets.\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Create a root directory, e.g., \"testing123\".\r\n2. Under \"testing123\", create two subdirectories: \"train\" and \"val\".\r\n3. Create and save a parquet file with 3 unique rows in the \"train\" subdirectory.\r\n4. Create and save a parquet file with 4 unique rows in the \"val\" subdirectory.\r\n5. Load the datasets from the root directory using `load_dataset(\"parquet\", data_dir=\"testing123\")`\r\n6. Iterate through the datasets and print the rows\r\n\r\nHere's a collab reproducing these steps:\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/11NEdImnQ3OqJlwKSHRMhr7jCBesNdLY4?usp=sharing\r\n\r\n### Expected behavior\r\n\r\n- Training set should contain 3 unique rows.\r\n- Validation set should contain 4 unique rows.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.17.2\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6259\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6259\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6258","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6258\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6258\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6258\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6258","id":1911445373,"node_id":"PR_kwDODunzps5bHxHl","number":6258,"title":"[DOCS] Fix typo: Elasticsearch","user":{"login":"leemthompo","id":32779855,"node_id":"MDQ6VXNlcjMyNzc5ODU1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32779855?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leemthompo","html_url":"https:\/\/github.com\/leemthompo","followers_url":"https:\/\/api.github.com\/users\/leemthompo\/followers","following_url":"https:\/\/api.github.com\/users\/leemthompo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leemthompo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leemthompo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leemthompo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leemthompo\/orgs","repos_url":"https:\/\/api.github.com\/users\/leemthompo\/repos","events_url":"https:\/\/api.github.com\/users\/leemthompo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leemthompo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-09-25T12:50:59Z","updated_at":"2023-09-26T14:55:35Z","closed_at":"2023-09-26T13:36:40Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Not ElasticSearch :)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6258\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6258\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6258","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6258","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6258.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6258.patch","merged_at":"2023-09-26T13:36:40Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6257","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6257\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6257\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6257\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6257","id":1910741044,"node_id":"I_kwDODunzps5x45g0","number":6257,"title":"HfHubHTTPError - exceeded our hourly quotas for action: commit","user":{"login":"yuvalkirstain","id":57996478,"node_id":"MDQ6VXNlcjU3OTk2NDc4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57996478?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yuvalkirstain","html_url":"https:\/\/github.com\/yuvalkirstain","followers_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/followers","following_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/orgs","repos_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/repos","events_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-09-25T06:11:43Z","updated_at":"2023-10-16T13:30:49Z","closed_at":"2023-10-16T13:30:48Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI try to upload a very large dataset of images, and get the following error:\r\n```\r\nFile \/fsx-multigen\/yuvalkirstain\/miniconda\/envs\/pickapic\/lib\/python3.10\/site-packages\/huggingface_hub\/hf_api.py:2712, in HfApi.create_commit(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\r\n   2710 try:\r\n   2711     commit_resp = get_session().post(url=commit_url, headers=headers, data=data, params=params)\r\n-> 2712     hf_raise_for_status(commit_resp, endpoint_name=\"commit\")\r\n   2713 except RepositoryNotFoundError as e:\r\n   2714     e.append_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\r\n\r\nFile \/fsx-multigen\/yuvalkirstain\/miniconda\/envs\/pickapic\/lib\/python3.10\/site-packages\/huggingface_hub\/utils\/_errors.py:301, in hf_raise_for_status(response, endpoint_name)\r\n    297     raise BadRequestError(message, response=response) from e\r\n    299 # Convert `HTTPError` into a `HfHubHTTPError` to display request information\r\n    300 # as well (request id and\/or server error message)\r\n--> 301 raise HfHubHTTPError(str(e), response=response) from e\r\n\r\nHfHubHTTPError: 429 Client Error: Too Many Requests for url: https:\/\/huggingface.co\/api\/datasets\/yuvalkirstain\/pickapic_v2\/commit\/main (Request ID: Root=1-65112399-12d63f7d7f28bfa40a36a0fd)\r\n\r\nYou have exceeded our hourly quotas for action: commit. We invite you to retry later.\r\n```\r\n\r\nthis makes it much less convenient to host large datasets on HF hub.\n\n### Steps to reproduce the bug\n\nUpload a very large dataset of images\n\n### Expected behavior\n\nthe upload to work well\n\n### Environment info\n\n- `datasets` version: 2.13.1\r\n- Platform: Linux-5.15.0-1033-aws-x86_64-with-glibc2.31\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6257\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6257\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6256","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6256\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6256\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6256\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6256","id":1910275199,"node_id":"I_kwDODunzps5x3Hx_","number":6256,"title":"load_dataset() function's cache_dir does not seems to work","user":{"login":"andyzhu","id":171831,"node_id":"MDQ6VXNlcjE3MTgzMQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/171831?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/andyzhu","html_url":"https:\/\/github.com\/andyzhu","followers_url":"https:\/\/api.github.com\/users\/andyzhu\/followers","following_url":"https:\/\/api.github.com\/users\/andyzhu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/andyzhu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/andyzhu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/andyzhu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/andyzhu\/orgs","repos_url":"https:\/\/api.github.com\/users\/andyzhu\/repos","events_url":"https:\/\/api.github.com\/users\/andyzhu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/andyzhu\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-09-24T15:34:06Z","updated_at":"2023-09-27T13:40:45Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\ndatasets version: 2.14.5\r\n\r\nwhen trying to run the following command\r\ntrec = load_dataset('trec', split='train[:1000]', cache_dir='\/path\/to\/my\/dir')\r\n\r\nI keep getting error saying the command does not have permission to the default cache directory on my macbook pro machine. \r\n\r\nIt seems the cache_dir parameter cannot change the dataset saving directory from the default \r\n\r\nwhat ever explained in the https:\/\/huggingface.co\/docs\/datasets\/cache does not seem to work\r\n\n\n### Steps to reproduce the bug\n\ndatasets version: 2.14.5\r\n\r\nwhen trying to run the following command\r\ntrec = load_dataset('trec', split='train[:1000]', cache_dir='\/path\/to\/my\/dir')\r\n\r\nI keep getting error saying the command does not have permission to the default cache directory on my macbook pro machine. \r\n\r\nIt seems the cache_dir parameter cannot change the dataset saving directory from the default \r\n\r\nwhat ever explained in the https:\/\/huggingface.co\/docs\/datasets\/cache does not seem to work\n\n### Expected behavior\n\nthe dataset should be saved to the cache_dir points to \n\n### Environment info\n\ndatasets version: 2.14.5\r\nmacos X: Ventura 13.4.1 (c)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6256\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6256\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6255","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6255\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6255\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6255\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6255","id":1909842977,"node_id":"PR_kwDODunzps5bCioS","number":6255,"title":"Parallelize builder configs creation","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-09-23T11:56:20Z","updated_at":"2023-09-26T15:44:47Z","closed_at":"2023-09-26T15:44:19Z","author_association":"MEMBER","active_lock_reason":null,"body":"For datasets with lots of configs defined in YAML\r\n\r\nE.g. `load_dataset(\"uonlp\/CulturaX\", \"fr\", revision=\"refs\/pr\/6\")` from >1min to 15sec","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6255\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6255\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6255","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6255","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6255.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6255.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6254","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6254\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6254\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6254\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6254","id":1909672104,"node_id":"I_kwDODunzps5x00io","number":6254,"title":"Dataset.from_generator() cost much more time in vscode debugging mode then running mode","user":{"login":"dontnet-wuenze","id":56437469,"node_id":"MDQ6VXNlcjU2NDM3NDY5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/56437469?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dontnet-wuenze","html_url":"https:\/\/github.com\/dontnet-wuenze","followers_url":"https:\/\/api.github.com\/users\/dontnet-wuenze\/followers","following_url":"https:\/\/api.github.com\/users\/dontnet-wuenze\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dontnet-wuenze\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dontnet-wuenze\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dontnet-wuenze\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dontnet-wuenze\/orgs","repos_url":"https:\/\/api.github.com\/users\/dontnet-wuenze\/repos","events_url":"https:\/\/api.github.com\/users\/dontnet-wuenze\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dontnet-wuenze\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-09-23T02:07:26Z","updated_at":"2023-10-03T14:42:53Z","closed_at":"2023-10-03T14:42:53Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHey there,\r\nI\u2019m using Dataset.from_generator() to convert a torch_dataset to the Huggingface Dataset.\r\nHowever, when I debug my code on vscode, I find that it runs really slow on Dataset.from_generator() which may even 20 times longer then run the script on terminal.\n\n### Steps to reproduce the bug\n\nI write a simple test code :\r\n\r\n```python\r\nimport os\r\nfrom functools import partial\r\nfrom typing import Callable\r\n\r\nimport torch\r\nimport time\r\nfrom torch.utils.data import Dataset as TorchDataset\r\n\r\nfrom datasets import load_from_disk, Dataset as HFDataset\r\n  \r\nimport torch  \r\nfrom torch.utils.data import Dataset  \r\n  \r\nclass SimpleDataset(Dataset):  \r\n    def __init__(self, data):  \r\n        self.data = data  \r\n        self.keys = list(data[0].keys())\r\n      \r\n    def __len__(self):  \r\n        return len(self.data)  \r\n      \r\n    def __getitem__(self, index):  \r\n        sample = self.data[index]  \r\n        return {key: sample[key] for key in self.keys}  \r\n  \r\n\r\ndef TorchDataset2HuggingfaceDataset(torch_dataset: TorchDataset, cache_dir: str = None\r\n) -> HFDataset:\r\n    \r\n    \"\"\"\r\n        convert torch dataset to huggingface dataset\r\n    \"\"\"\r\n    generator : Callable[[], TorchDataset] = lambda: (sample for sample in torch_dataset)   \r\n\r\n    return HFDataset.from_generator(generator, cache_dir=cache_dir)\r\n\r\nif __name__ == '__main__':\r\n    data = [  \r\n        {'id': 1, 'name': 'Alice'},  \r\n        {'id': 2, 'name': 'Bob'},  \r\n        {'id': 3, 'name': 'Charlie'}  \r\n    ]\r\n    \r\n    torch_dataset = SimpleDataset(data)\r\n    start_time = time.time() \r\n    huggingface_dataset = TorchDataset2HuggingfaceDataset(torch_dataset)\r\n    end_time = time.time()\r\n    print(\"time: \", end_time - start_time)\r\n    print(huggingface_dataset)\r\n```\n\n### Expected behavior\n\nthis test on my machine report that the running time on terminal is 0.086,\r\nhowever the running time in debugging mode on vscode is 0.25, which I think is much longer than expected.\r\n\r\nI\u2019d like to know is the anything wrong in the code or just because of debugging?\r\nI have traced the code and I find is this func which I get stuck.\r\n\r\n```python\r\ndef create_config_id(\r\n        self,\r\n        config_kwargs: dict,\r\n        custom_features: Optional[Features] = None,\r\n    ) -> str:\r\n...\r\n# stuck in this line\r\nsuffix = Hasher.hash(config_kwargs_to_add_to_suffix)\r\n```\r\n\r\n\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31\r\n- Python version: 3.11.3\r\n- Huggingface_hub version: 0.17.2\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6254\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6254\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6253","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6253\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6253\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6253\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6253","id":1906618910,"node_id":"PR_kwDODunzps5a3s__","number":6253,"title":"Check builder cls default config name in inspect","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-09-21T10:15:32Z","updated_at":"2023-09-21T14:16:44Z","closed_at":"2023-09-21T14:08:00Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix https:\/\/github.com\/huggingface\/datasets-server\/issues\/1812\r\n\r\nthis was causing this issue:\r\n\r\n```ipython\r\nIn [1]: from datasets import *\r\n\r\nIn [2]: inspect.get_dataset_config_names(\"aakanksha\/udpos\")\r\nOut[2]: ['default']\r\n\r\nIn [3]: load_dataset_builder(\"aakanksha\/udpos\").config.name\r\nOut[3]: 'en'\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6253\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6253\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6253","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6253","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6253.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6253.patch","merged_at":"2023-09-21T14:08:00Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6252","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6252\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6252\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6252\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6252","id":1906375378,"node_id":"I_kwDODunzps5xoPrS","number":6252,"title":"exif_transpose not done to Image (PIL problem)","user":{"login":"rhajou","id":108274349,"node_id":"U_kgDOBnQirQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/108274349?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rhajou","html_url":"https:\/\/github.com\/rhajou","followers_url":"https:\/\/api.github.com\/users\/rhajou\/followers","following_url":"https:\/\/api.github.com\/users\/rhajou\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rhajou\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rhajou\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rhajou\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rhajou\/orgs","repos_url":"https:\/\/api.github.com\/users\/rhajou\/repos","events_url":"https:\/\/api.github.com\/users\/rhajou\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rhajou\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/milestones\/10","html_url":"https:\/\/github.com\/huggingface\/datasets\/milestone\/10","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/milestones\/10\/labels","id":9038583,"node_id":"MI_kwDODunzps4Aier3","number":10,"title":"3.0","description":"Next major release","creator":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"open_issues":4,"closed_issues":0,"state":"open","created_at":"2023-02-13T16:22:42Z","updated_at":"2023-09-22T14:07:52Z","due_on":null,"closed_at":null},"comments":2,"created_at":"2023-09-21T08:11:46Z","updated_at":"2023-09-22T14:07:52Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nI noticed that some of my images loaded using PIL have some metadata related to exif that can rotate them when loading.\r\nSince the dataset.features.Image uses PIL for loading, the loaded image may be rotated (width and height will be inverted) thus for tasks as object detection and layoutLM this can create some inconsistencies (between input bboxes and input images). \r\n\r\nFor now there is no option in datasets.features.Image to specify that. We need to do the following when preparing examples (when preparing images for training, test or inference): \r\n```\r\nfrom PIL import Image, ImageOps \r\npil = ImageOps.exif_transpose(pil)\r\n```\r\n\r\nreference: https:\/\/stackoverflow.com\/a\/63950647\/5720150 \r\n\r\nIs it possible to add this by default to the datasets.feature.Image ? or to add the option to do the ImageOps.exif_transpose?\r\n\r\nThank you\n\n### Motivation\n\nPrevent having inverted data related to exif metadata that may affect object detection tasks\n\n### Your contribution\n\nChanging in datasets.featrues.Image I can help with that. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6252\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6252\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6251","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6251\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6251\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6251\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6251","id":1904418426,"node_id":"PR_kwDODunzps5awQsy","number":6251,"title":"Support streaming datasets with pyarrow.parquet.read_table","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2023-09-20T08:07:02Z","updated_at":"2023-09-27T06:37:03Z","closed_at":"2023-09-27T06:26:24Z","author_association":"MEMBER","active_lock_reason":null,"body":"Support streaming datasets with `pyarrow.parquet.read_table`.\r\n\r\nSee: https:\/\/huggingface.co\/datasets\/uonlp\/CulturaX\/discussions\/2\r\n\r\nCC: @AndreaFrancis ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6251\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6251\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6251","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6251","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6251.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6251.patch","merged_at":"2023-09-27T06:26:24Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6247","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6247\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6247\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6247\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6247","id":1901390945,"node_id":"PR_kwDODunzps5amAQ1","number":6247,"title":"Update create_dataset.mdx","user":{"login":"EswarDivi","id":76403422,"node_id":"MDQ6VXNlcjc2NDAzNDIy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/76403422?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/EswarDivi","html_url":"https:\/\/github.com\/EswarDivi","followers_url":"https:\/\/api.github.com\/users\/EswarDivi\/followers","following_url":"https:\/\/api.github.com\/users\/EswarDivi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/EswarDivi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/EswarDivi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/EswarDivi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/EswarDivi\/orgs","repos_url":"https:\/\/api.github.com\/users\/EswarDivi\/repos","events_url":"https:\/\/api.github.com\/users\/EswarDivi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/EswarDivi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-09-18T17:06:29Z","updated_at":"2023-09-19T18:51:49Z","closed_at":"2023-09-19T18:40:10Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"modified , as AudioFolder and ImageFolder not in Dataset Library. \r\n\r\n``` from datasets import AudioFolder ```  and ```from datasets import ImageFolder```  to ```from datasets import load_dataset```\r\n\r\n``` \r\ncannot import name 'AudioFolder' from 'datasets' (\/home\/eswardivi\/miniconda3\/envs\/Hugformers\/lib\/python3.10\/site-packages\/datasets\/__init__.py)\r\n\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6247\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6247\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6247","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6247","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6247.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6247.patch","merged_at":"2023-09-19T18:40:10Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6246","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6246\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6246\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6246\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6246","id":1899848414,"node_id":"I_kwDODunzps5xPWLe","number":6246,"title":"Add new column to dataset","user":{"login":"andysingal","id":20493493,"node_id":"MDQ6VXNlcjIwNDkzNDkz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20493493?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/andysingal","html_url":"https:\/\/github.com\/andysingal","followers_url":"https:\/\/api.github.com\/users\/andysingal\/followers","following_url":"https:\/\/api.github.com\/users\/andysingal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/andysingal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/andysingal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/andysingal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/andysingal\/orgs","repos_url":"https:\/\/api.github.com\/users\/andysingal\/repos","events_url":"https:\/\/api.github.com\/users\/andysingal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/andysingal\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-09-17T16:59:48Z","updated_at":"2023-09-18T16:20:09Z","closed_at":"2023-09-18T16:20:09Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n[<ipython-input-9-bd197b36b6a0>](https:\/\/localhost:8080\/#) in <cell line: 1>()\r\n----> 1 dataset['train']['\/workspace\/data']\r\n\r\n3 frames\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py](https:\/\/localhost:8080\/#) in _check_valid_column_key(key, columns)\r\n    518 def _check_valid_column_key(key: str, columns: List[str]) -> None:\r\n    519     if key not in columns:\r\n--> 520         raise KeyError(f\"Column {key} not in the dataset. Current columns in the dataset: {columns}\")\r\n    521 \r\n    522 \r\n\r\nKeyError: \"Column train not in the dataset. Current columns in the dataset: ['image', '\/workspace\/data']\"\r\n```\n\n### Steps to reproduce the bug\n\nplease find the notebook for reference: https:\/\/colab.research.google.com\/drive\/10lZ_zLtU4itYVmIVTvIEVbjfOtCZaAZy?usp=sharing\n\n### Expected behavior\n\nadd column to the dataset\n\n### Environment info\n\ncolab pro","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6246\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6246\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6244","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6244\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6244\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6244\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6244","id":1898861422,"node_id":"PR_kwDODunzps5adtD3","number":6244,"title":"Add support for `fsspec>=2023.9.0`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":19,"created_at":"2023-09-15T17:58:25Z","updated_at":"2023-09-26T15:41:38Z","closed_at":"2023-09-26T15:32:51Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6214 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6244\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6244\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6244","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6244","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6244.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6244.patch","merged_at":"2023-09-26T15:32:51Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6243","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6243\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6243\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6243\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6243","id":1898532784,"node_id":"PR_kwDODunzps5aclIy","number":6243,"title":"Fix cast from fixed size list to variable size list","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-09-15T14:23:33Z","updated_at":"2023-09-19T18:02:21Z","closed_at":"2023-09-19T17:53:17Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6242 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6243\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6243\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6243","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6243","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6243.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6243.patch","merged_at":"2023-09-19T17:53:17Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6242","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6242\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6242\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6242\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6242","id":1896899123,"node_id":"I_kwDODunzps5xEGIz","number":6242,"title":"Data alteration when loading dataset with unspecified inner sequence length","user":{"login":"qgallouedec","id":45557362,"node_id":"MDQ6VXNlcjQ1NTU3MzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45557362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/qgallouedec","html_url":"https:\/\/github.com\/qgallouedec","followers_url":"https:\/\/api.github.com\/users\/qgallouedec\/followers","following_url":"https:\/\/api.github.com\/users\/qgallouedec\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/qgallouedec\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/qgallouedec\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/qgallouedec\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/qgallouedec\/orgs","repos_url":"https:\/\/api.github.com\/users\/qgallouedec\/repos","events_url":"https:\/\/api.github.com\/users\/qgallouedec\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/qgallouedec\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-09-14T16:12:45Z","updated_at":"2023-09-19T17:53:18Z","closed_at":"2023-09-19T17:53:18Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nWhen a dataset saved with a specified inner sequence length is loaded without specifying that length, the original data is altered and becomes inconsistent.\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Dataset, Features, Value, Sequence, load_dataset\r\n\r\n# Repository ID\r\nrepo_id = \"my_repo_id\"\r\n\r\n# Define features with a specific length of 3 for each inner sequence\r\nspecified_features = Features({\"key\": Sequence(Sequence(Value(\"float32\"), length=3))})\r\n\r\n# Create a dataset with the specified features\r\ndata = [\r\n    [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],\r\n    [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]],\r\n]\r\ndataset = Dataset.from_dict({\"key\": data}, features=specified_features)\r\n\r\n# Push the dataset to the hub\r\ndataset.push_to_hub(repo_id)\r\n\r\n# Define features without specifying the length\r\nunspecified_features = Features({\"key\": Sequence(Sequence(Value(\"float32\")))})\r\n\r\n# Load the dataset from the hub with this new feature definition\r\ndataset = load_dataset(f\"qgallouedec\/{repo_id}\", split=\"train\", features=unspecified_features)\r\n\r\n# The obtained data is altered\r\nprint(dataset.to_dict())  # {'key': [[[1.0], [2.0]], [[3.0], [4.0]]]}\r\n```\r\n\n\n### Expected behavior\n\n```python\r\nprint(dataset.to_dict())  # {'key': [[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]]}\r\n```\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6242\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6242\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6241","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6241\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6241\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6241\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6241","id":1896429694,"node_id":"PR_kwDODunzps5aVfl-","number":6241,"title":"Remove unused global variables in `audio.py`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-09-14T12:06:32Z","updated_at":"2023-09-15T15:57:10Z","closed_at":"2023-09-15T15:46:07Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6241\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6241\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6241","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6241","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6241.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6241.patch","merged_at":"2023-09-15T15:46:07Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6240","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6240\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6240\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6240\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6240","id":1895723888,"node_id":"I_kwDODunzps5w_nNw","number":6240,"title":"Dataloader stuck on multiple GPUs","user":{"login":"kuri54","id":40049003,"node_id":"MDQ6VXNlcjQwMDQ5MDAz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/40049003?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kuri54","html_url":"https:\/\/github.com\/kuri54","followers_url":"https:\/\/api.github.com\/users\/kuri54\/followers","following_url":"https:\/\/api.github.com\/users\/kuri54\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kuri54\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kuri54\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kuri54\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kuri54\/orgs","repos_url":"https:\/\/api.github.com\/users\/kuri54\/repos","events_url":"https:\/\/api.github.com\/users\/kuri54\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kuri54\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-09-14T05:30:30Z","updated_at":"2023-09-14T23:54:42Z","closed_at":"2023-09-14T23:54:42Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am trying to get CLIP to fine-tuning with my code.\r\nWhen I tried to run it on multiple GPUs using accelerate, I encountered the following phenomenon.\r\n\r\n- Validation dataloader stuck in 2nd epoch only on multi-GPU\r\nSpecifically, when the \"for inputs in valid_loader:\" process is finished, it does not proceed to the next step. train_loader process is completed. Also, both train and valid are working correctly in the first epoch.\r\nThe accelerate command at that time is as follows.\r\n`accelerate launch --multi_gpu --num_processes=2 {script_name.py} {--arg1} {--arg2} ...`\r\n\r\n- This will not happen when single GPU is used.\r\n`CUDA_VISIBLE_DEVICES=\"0\" accelerate launch {script_name.py} --arg1 --arg2 ...`\r\n- Setting num_workers=0 in dataloader did not change the result.\n\n### Steps to reproduce the bug\n\n1. The codes for fine-tuning the regular CLIP were updated for accelerate.\r\n2. Run the code with the accelerate command as `accelerate launch --multi_gpu --num_processes=2 {script_name.py} {--arg1} {--arg2} ...` and the above problem will occur.\r\n3. CUDA_VISIBLE_DEVICES=\"0\" accelerate launch {script_name.py} --arg1 --arg2 ...` , it works fine.\n\n### Expected behavior\n\nIt Should end normally as if it was run on a single GPU.\n\n### Environment info\n\nSince `datasets-cli env` did not work, the environment is described below.\r\n\r\n- OS: Ubuntu 22.04 with Docker\r\n- Docker: 24.0.5, build ced0996\r\n- Python: 3.10.12\r\n- torch==2.0.1\r\n- accelerate==0.21.0\r\n- transformers==4.33.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6240\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6240\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6239","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6239\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6239\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6239\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6239","id":1895349382,"node_id":"I_kwDODunzps5w-LyG","number":6239,"title":"Load local audio data doesn't work","user":{"login":"abodacs","id":554032,"node_id":"MDQ6VXNlcjU1NDAzMg==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/554032?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/abodacs","html_url":"https:\/\/github.com\/abodacs","followers_url":"https:\/\/api.github.com\/users\/abodacs\/followers","following_url":"https:\/\/api.github.com\/users\/abodacs\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/abodacs\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/abodacs\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/abodacs\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/abodacs\/orgs","repos_url":"https:\/\/api.github.com\/users\/abodacs\/repos","events_url":"https:\/\/api.github.com\/users\/abodacs\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/abodacs\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-09-13T22:30:01Z","updated_at":"2023-09-15T14:32:10Z","closed_at":"2023-09-15T14:32:10Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI get a RuntimeError from the following code:\r\n\r\n```python\r\n audio_dataset = Dataset.from_dict({\"audio\": [\"\/kaggle\/input\/bengaliai-speech\/train_mp3s\/000005f3362c.mp3\"]}).cast_column(\"audio\", Audio())\r\n audio_dataset[0]\r\n ```\r\n \r\n ### Traceback\r\n\r\n<details>\r\n\r\n```python\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[33], line 1\r\n----> 1 train_dataset[0]\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py:1764, in Dataset.__getitem__(self, key)\r\n   1762 def __getitem__(self, key):  # noqa: F811\r\n   1763     \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 1764     return self._getitem(\r\n   1765         key,\r\n   1766     )\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py:1749, in Dataset._getitem(self, key, decoded, **kwargs)\r\n   1747 formatter = get_formatter(format_type, features=self.features, decoded=decoded, **format_kwargs)\r\n   1748 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n-> 1749 formatted_output = format_table(\r\n   1750     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n   1751 )\r\n   1752 return formatted_output\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/formatting\/formatting.py:532, in format_table(table, key, formatter, format_columns, output_all_columns)\r\n    530 python_formatter = PythonFormatter(features=None)\r\n    531 if format_columns is None:\r\n--> 532     return formatter(pa_table, query_type=query_type)\r\n    533 elif query_type == \"column\":\r\n    534     if key in format_columns:\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/formatting\/formatting.py:281, in Formatter.__call__(self, pa_table, query_type)\r\n    279 def __call__(self, pa_table: pa.Table, query_type: str) -> Union[RowFormat, ColumnFormat, BatchFormat]:\r\n    280     if query_type == \"row\":\r\n--> 281         return self.format_row(pa_table)\r\n    282     elif query_type == \"column\":\r\n    283         return self.format_column(pa_table)\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/formatting\/formatting.py:312, in PythonFormatter.format_row(self, pa_table)\r\n    310 row = self.python_arrow_extractor().extract_row(pa_table)\r\n    311 if self.decoded:\r\n--> 312     row = self.python_features_decoder.decode_row(row)\r\n    313 return row\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/formatting\/formatting.py:221, in PythonFeaturesDecoder.decode_row(self, row)\r\n    220 def decode_row(self, row: dict) -> dict:\r\n--> 221     return self.features.decode_example(row) if self.features else row\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/features\/features.py:1386, in Features.decode_example(self, example)\r\n   1376 def decode_example(self, example: dict):\r\n   1377     \"\"\"Decode example with custom feature decoding.\r\n   1378 \r\n   1379     Args:\r\n   (...)\r\n   1383         :obj:`dict[str, Any]`\r\n   1384     \"\"\"\r\n-> 1386     return {\r\n   1387         column_name: decode_nested_example(feature, value)\r\n   1388         if self._column_requires_decoding[column_name]\r\n   1389         else value\r\n   1390         for column_name, (feature, value) in zip_dict(\r\n   1391             {key: value for key, value in self.items() if key in example}, example\r\n   1392         )\r\n   1393     }\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/features\/features.py:1387, in <dictcomp>(.0)\r\n   1376 def decode_example(self, example: dict):\r\n   1377     \"\"\"Decode example with custom feature decoding.\r\n   1378 \r\n   1379     Args:\r\n   (...)\r\n   1383         :obj:`dict[str, Any]`\r\n   1384     \"\"\"\r\n   1386     return {\r\n-> 1387         column_name: decode_nested_example(feature, value)\r\n   1388         if self._column_requires_decoding[column_name]\r\n   1389         else value\r\n   1390         for column_name, (feature, value) in zip_dict(\r\n   1391             {key: value for key, value in self.items() if key in example}, example\r\n   1392         )\r\n   1393     }\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/features\/features.py:1087, in decode_nested_example(schema, obj)\r\n   1085 # Object with special decoding:\r\n   1086 elif isinstance(schema, (Audio, Image)):\r\n-> 1087     return schema.decode_example(obj) if obj is not None else None\r\n   1088 return obj\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/features\/audio.py:103, in Audio.decode_example(self, value)\r\n    101     raise ValueError(f\"An audio sample should have one of 'path' or 'bytes' but both are None in {value}.\")\r\n    102 elif path is not None and path.endswith(\"mp3\"):\r\n--> 103     array, sampling_rate = self._decode_mp3(file if file else path)\r\n    104 elif path is not None and path.endswith(\"opus\"):\r\n    105     if file:\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/features\/audio.py:241, in Audio._decode_mp3(self, path_or_file)\r\n    238 except RuntimeError as err:\r\n    239     raise ImportError(\"To support decoding 'mp3' audio files, please install 'sox'.\") from err\r\n--> 241 array, sampling_rate = torchaudio.load(path_or_file, format=\"mp3\")\r\n    242 if self.sampling_rate and self.sampling_rate != sampling_rate:\r\n    243     if not hasattr(self, \"_resampler\") or self._resampler.orig_freq != sampling_rate:\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/torchaudio\/backend\/sox_io_backend.py:256, in load(filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n    254 if ret is not None:\r\n    255     return ret\r\n--> 256 return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n\r\nFile \/opt\/conda\/lib\/python3.10\/site-packages\/torchaudio\/backend\/sox_io_backend.py:30, in _fail_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n     22 def _fail_load(\r\n     23     filepath: str,\r\n     24     frame_offset: int = 0,\r\n   (...)\r\n     28     format: Optional[str] = None,\r\n     29 ) -> Tuple[torch.Tensor, int]:\r\n---> 30     raise RuntimeError(\"Failed to load audio from {}\".format(filepath))\r\n\r\nRuntimeError: Failed to load audio from \/kaggle\/input\/bengaliai-speech\/train_mp3s\/000005f3362c.mp3\r\n```\r\n\r\n<\/details>\r\n\n\n### Steps to reproduce the bug\n\n1. - Create a custom dataset using Local files of type mp3.\r\n3. - Try to read the first audio item.\n\n### Expected behavior\n\nExpected output\r\n```python\r\naudio_dataset[0][\"audio\"]\r\n{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\r\n         0.        ,  0.        ], dtype=float32),\r\n 'path': 'path\/to\/audio_1',\r\n 'sampling_rate': 16000}\r\n ```\n\n### Environment info\n\nN\/A","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6239\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6239\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6238","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6238\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6238\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6238\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6238","id":1895207828,"node_id":"I_kwDODunzps5w9pOU","number":6238,"title":"`dataset.filter` ALWAYS removes the first item from the dataset when using batched=True","user":{"login":"Taytay","id":1330693,"node_id":"MDQ6VXNlcjEzMzA2OTM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1330693?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Taytay","html_url":"https:\/\/github.com\/Taytay","followers_url":"https:\/\/api.github.com\/users\/Taytay\/followers","following_url":"https:\/\/api.github.com\/users\/Taytay\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Taytay\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Taytay\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Taytay\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Taytay\/orgs","repos_url":"https:\/\/api.github.com\/users\/Taytay\/repos","events_url":"https:\/\/api.github.com\/users\/Taytay\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Taytay\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-09-13T20:20:37Z","updated_at":"2023-09-17T07:05:07Z","closed_at":"2023-09-17T07:05:07Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nIf you call batched=True when calling `filter`, the first item is _always_ filtered out, regardless of the filter condition.\r\n\r\n\n\n### Steps to reproduce the bug\n\nHere's a minimal example:\r\n```python\r\ndef filter_batch_always_true(batch, indices):\r\n    print(\"First index being passed into this filter function: \", indices[0])\r\n    return indices  # Keep all indices\r\n\r\ndata = {\"value\": list(range(10))}\r\ndataset = Dataset.from_dict(data)\r\n\r\nfiltered_dataset = dataset.filter(filter_batch_always_true, with_indices=True, batched=True)\r\nprint(\"Length of original dataset: \", len(dataset))\r\nprint(\"Length of filtered_dataset: \", len(filtered_dataset))\r\nprint(\"Is equal to original? \", len(filtered_dataset) == len(dataset))\r\nprint(\"First item of filtered dataset: \", filtered_dataset[0])\r\nprint(\"Last item of filtered dataset: \", filtered_dataset[-1])\r\n```\r\nprints:\r\n```\r\nFirst index being passed into this filter function:  0\r\nLength of original dataset:  10\r\nLength of filtered_dataset:  9\r\nIs equal to original?  False\r\nFirst item of filtered dataset:  {'value': 1}\r\nLast item of filtered dataset:  {'value': 9}\r\n```\n\n### Expected behavior\n\nFilter should respect the filter condition.\n\n### Environment info\n\n\r\n- `datasets` version: 2.14.4\r\n- Platform: macOS-13.5-arm64-arm-64bit\r\n- Python version: 3.9.18\r\n- Huggingface_hub version: 0.17.1\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 2.0.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6238\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6238\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6237","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6237\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6237\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6237\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6237","id":1893822321,"node_id":"I_kwDODunzps5w4W9x","number":6237,"title":"Tokenization with multiple workers is too slow","user":{"login":"macabdul9","id":25720695,"node_id":"MDQ6VXNlcjI1NzIwNjk1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/25720695?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/macabdul9","html_url":"https:\/\/github.com\/macabdul9","followers_url":"https:\/\/api.github.com\/users\/macabdul9\/followers","following_url":"https:\/\/api.github.com\/users\/macabdul9\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/macabdul9\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/macabdul9\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/macabdul9\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/macabdul9\/orgs","repos_url":"https:\/\/api.github.com\/users\/macabdul9\/repos","events_url":"https:\/\/api.github.com\/users\/macabdul9\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/macabdul9\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-09-13T06:18:34Z","updated_at":"2023-09-19T21:54:58Z","closed_at":"2023-09-19T21:54:58Z","author_association":"NONE","active_lock_reason":null,"body":"I am trying to tokenize a few million documents with multiple workers but the tokenization process is taking forever. \r\n\r\nCode snippet:\r\n\r\n```\r\nraw_datasets.map(\r\n            encode_function,\r\n            batched=False,\r\n            num_proc=args.preprocessing_num_workers,\r\n            load_from_cache_file=not args.overwrite_cache,\r\n            remove_columns=[name for name in raw_datasets[\"train\"].column_names if name not in [\"input_ids\", \"labels\", \"attention_mask\"]],\r\n            desc=\"Tokenizing data\",\r\n        )\r\n```\r\n\r\nDetails:\r\n\r\n```\r\ntransformers==4.28.0.dev0\r\ndatasets==4.28.0.dev0\r\npreprocessing_num_workers==48\r\n```\r\ntokenizer == decapoda-research\/llama-7b-hf\r\n\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6237\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6237\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6236","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6236\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6236\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6236\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6236","id":1893648480,"node_id":"I_kwDODunzps5w3shg","number":6236,"title":"Support buffer shuffle for to_tf_dataset","user":{"login":"EthanRock","id":7635551,"node_id":"MDQ6VXNlcjc2MzU1NTE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7635551?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/EthanRock","html_url":"https:\/\/github.com\/EthanRock","followers_url":"https:\/\/api.github.com\/users\/EthanRock\/followers","following_url":"https:\/\/api.github.com\/users\/EthanRock\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/EthanRock\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/EthanRock\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/EthanRock\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/EthanRock\/orgs","repos_url":"https:\/\/api.github.com\/users\/EthanRock\/repos","events_url":"https:\/\/api.github.com\/users\/EthanRock\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/EthanRock\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-09-13T03:19:44Z","updated_at":"2023-09-18T01:11:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nI'm using to_tf_dataset to convert a large dataset to tf.data.Dataset and use Keras fit to train model. \r\nCurrently, to_tf_dataset only supports full size shuffle, which can be very slow on large dataset. \r\ntf.data.Dataset support buffer shuffle by default. \r\nshuffle(\r\n    buffer_size, seed=None, reshuffle_each_iteration=None, name=None\r\n)\n\n### Motivation\n\nI'm very frustrated to find the loading with shuffling large dataset is very slow. It seems impossible to shuffle before training Keras with big dataset. \n\n### Your contribution\n\nNA","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6236\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6236\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6235","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6235\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6235\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6235\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6235","id":1893337083,"node_id":"I_kwDODunzps5w2gf7","number":6235,"title":"Support multiprocessing for download\/extract nestedly","user":{"login":"hgt312","id":22725729,"node_id":"MDQ6VXNlcjIyNzI1NzI5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22725729?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/hgt312","html_url":"https:\/\/github.com\/hgt312","followers_url":"https:\/\/api.github.com\/users\/hgt312\/followers","following_url":"https:\/\/api.github.com\/users\/hgt312\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/hgt312\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/hgt312\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/hgt312\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/hgt312\/orgs","repos_url":"https:\/\/api.github.com\/users\/hgt312\/repos","events_url":"https:\/\/api.github.com\/users\/hgt312\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/hgt312\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-09-12T21:51:08Z","updated_at":"2023-09-12T21:51:08Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nCurrent multiprocessing for download\/extract is not done nestedly. For example, when processing SlimPajama, there is only 3 processes (for train\/test\/val), while there are many files inside these 3 folders\r\n\r\n```\r\nDownloading data files #0:   0%|          | 0\/1 [00:00<?, ?obj\/s]\r\nDownloading data files #1:   0%|          | 0\/1 [00:00<?, ?obj\/s]\r\nDownloading data files #2:   0%|          | 0\/1 [00:00<?, ?obj\/s]\r\n\r\nExtracting data files #0:   0%|          | 0\/1 [00:00<?, ?obj\/s]\r\nExtracting data files #1:   0%|          | 0\/1 [00:00<?, ?obj\/s]\u001b[A\r\nExtracting data files #2:   0%|          | 0\/1 [00:00<?, ?obj\/s]\u001b[A\u001b[A\r\n```\n\n### Motivation\n\nspeedup dataset loading\n\n### Your contribution\n\nI can help test the feature","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6235\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6235\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6233","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6233\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6233\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6233\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6233","id":1891804286,"node_id":"PR_kwDODunzps5aF3kd","number":6233,"title":"Update README.md","user":{"login":"NinoRisteski","id":95188570,"node_id":"U_kgDOBax2Wg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/95188570?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NinoRisteski","html_url":"https:\/\/github.com\/NinoRisteski","followers_url":"https:\/\/api.github.com\/users\/NinoRisteski\/followers","following_url":"https:\/\/api.github.com\/users\/NinoRisteski\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NinoRisteski\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NinoRisteski\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NinoRisteski\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NinoRisteski\/orgs","repos_url":"https:\/\/api.github.com\/users\/NinoRisteski\/repos","events_url":"https:\/\/api.github.com\/users\/NinoRisteski\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NinoRisteski\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-09-12T06:53:06Z","updated_at":"2023-09-13T18:20:50Z","closed_at":"2023-09-13T18:10:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"fixed a typo","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6233\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6233\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6233","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6233","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6233.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6233.patch","merged_at":"2023-09-13T18:10:04Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6232","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6232\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6232\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6232\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6232","id":1891109762,"node_id":"PR_kwDODunzps5aDhhK","number":6232,"title":"Improve error message for missing function parameters","user":{"login":"suavemint","id":4016832,"node_id":"MDQ6VXNlcjQwMTY4MzI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/4016832?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/suavemint","html_url":"https:\/\/github.com\/suavemint","followers_url":"https:\/\/api.github.com\/users\/suavemint\/followers","following_url":"https:\/\/api.github.com\/users\/suavemint\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/suavemint\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/suavemint\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/suavemint\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/suavemint\/orgs","repos_url":"https:\/\/api.github.com\/users\/suavemint\/repos","events_url":"https:\/\/api.github.com\/users\/suavemint\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/suavemint\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-09-11T19:11:58Z","updated_at":"2023-09-15T18:07:56Z","closed_at":"2023-09-15T17:59:02Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The error message in the fingerprint module was missing the f-string 'f' symbol, so the error message returned by fingerprint.py, line 469 was literally \"function {func} is missing parameters {fingerprint_names} in signature.\"\r\n\r\nThis has been fixed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6232\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6232\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6232","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6232","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6232.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6232.patch","merged_at":"2023-09-15T17:59:02Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6231","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6231\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6231\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6231\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6231","id":1890863249,"node_id":"PR_kwDODunzps5aCr8_","number":6231,"title":"Overwrite legacy default config name in `dataset_infos.json` in packaged datasets","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-09-11T16:27:09Z","updated_at":"2023-09-26T11:19:36Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Currently if we push data as default config with `.push_to_hub` to a repo that has a legacy `dataset_infos.json` file containing a legacy default config name like `{username}--{dataset_name}`, new key `\"default\"` is added to `dataset_infos.json` along with the legacy one. I think the legacy one should be dropped in this case.\r\n\r\nAlso, in `load.py` I suggest to check if a legacy config name is indeed a legacy config name because after this fix it might not be the case (this check was first introduced in https:\/\/github.com\/huggingface\/datasets\/pull\/6218)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6231\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6231\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6231","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6231","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6231.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6231.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6230","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6230\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6230\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6230\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6230","id":1890521006,"node_id":"PR_kwDODunzps5aBh6L","number":6230,"title":"Don't skip hidden files in `dl_manager.iter_files` when they are given as input","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-09-11T13:29:19Z","updated_at":"2023-09-13T18:21:28Z","closed_at":"2023-09-13T18:12:09Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Required for `load_dataset(<format>, data_files=[\"path\/to\/.hidden_file\"])` to work as expected","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6230\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6230\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6230","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6230","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6230.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6230.patch","merged_at":"2023-09-13T18:12:09Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6229","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6229\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6229\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6229\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6229","id":1889050954,"node_id":"I_kwDODunzps5wmKFK","number":6229,"title":"Apply inference on all images in the dataset","user":{"login":"andysingal","id":20493493,"node_id":"MDQ6VXNlcjIwNDkzNDkz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20493493?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/andysingal","html_url":"https:\/\/github.com\/andysingal","followers_url":"https:\/\/api.github.com\/users\/andysingal\/followers","following_url":"https:\/\/api.github.com\/users\/andysingal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/andysingal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/andysingal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/andysingal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/andysingal\/orgs","repos_url":"https:\/\/api.github.com\/users\/andysingal\/repos","events_url":"https:\/\/api.github.com\/users\/andysingal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/andysingal\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-09-10T08:36:12Z","updated_at":"2023-09-20T16:11:53Z","closed_at":"2023-09-20T16:11:52Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\nCell In[14], line 11\r\n      9 for idx, example in enumerate(dataset['train']):\r\n     10     image_path = example['image']\r\n---> 11     mask_image = process_image(image_path)\r\n     12     mask_image.save(f\"mask_{idx}.png\")\r\n\r\nCell In[14], line 4, in process_image(image_path)\r\n      2 def process_image(image_path):\r\n      3     print(\"Processing image:\", image_path)\r\n----> 4     result = inferencer(image_path)['predictions']\r\n      5     mask = np.where(result == 12, 255, 0).astype('uint8')\r\n      6     return Image.fromarray(mask)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/mmseg\/apis\/mmseg_inferencer.py:183, in MMSegInferencer.__call__(self, inputs, return_datasamples, batch_size, show, wait_time, out_dir, img_out_dir, pred_out_dir, **kwargs)\r\n    180     pred_out_dir = ''\r\n    181     img_out_dir = ''\r\n--> 183 return super().__call__(\r\n    184     inputs=inputs,\r\n    185     return_datasamples=return_datasamples,\r\n    186     batch_size=batch_size,\r\n    187     show=show,\r\n    188     wait_time=wait_time,\r\n    189     img_out_dir=img_out_dir,\r\n    190     pred_out_dir=pred_out_dir,\r\n    191     **kwargs)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/mmengine\/infer\/infer.py:221, in BaseInferencer.__call__(self, inputs, return_datasamples, batch_size, **kwargs)\r\n    218 inputs = self.preprocess(\r\n    219     ori_inputs, batch_size=batch_size, **preprocess_kwargs)\r\n    220 preds = []\r\n--> 221 for data in (track(inputs, description='Inference')\r\n    222              if self.show_progress else inputs):\r\n    223     preds.extend(self.forward(data, **forward_kwargs))\r\n    224 visualization = self.visualize(\r\n    225     ori_inputs, preds,\r\n    226     **visualize_kwargs)  # type: ignore  # noqa: E501\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/rich\/progress.py:168, in track(sequence, description, total, auto_refresh, console, transient, get_time, refresh_per_second, style, complete_style, finished_style, pulse_style, update_period, disable, show_speed)\r\n    157 progress = Progress(\r\n    158     *columns,\r\n    159     auto_refresh=auto_refresh,\r\n   (...)\r\n    164     disable=disable,\r\n    165 )\r\n    167 with progress:\r\n--> 168     yield from progress.track(\r\n    169         sequence, total=total, description=description, update_period=update_period\r\n    170     )\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/rich\/progress.py:1210, in Progress.track(self, sequence, total, task_id, description, update_period)\r\n   1208 if self.live.auto_refresh:\r\n   1209     with _TrackThread(self, task_id, update_period) as track_thread:\r\n-> 1210         for value in sequence:\r\n   1211             yield value\r\n   1212             track_thread.completed += 1\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/mmengine\/infer\/infer.py:291, in BaseInferencer.preprocess(self, inputs, batch_size, **kwargs)\r\n    266 \"\"\"Process the inputs into a model-feedable format.\r\n    267 \r\n    268 Customize your preprocess by overriding this method. Preprocess should\r\n   (...)\r\n    287     Any: Data processed by the ``pipeline`` and ``collate_fn``.\r\n    288 \"\"\"\r\n    289 chunked_data = self._get_chunk_data(\r\n    290     map(self.pipeline, inputs), batch_size)\r\n--> 291 yield from map(self.collate_fn, chunked_data)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/mmengine\/infer\/infer.py:588, in BaseInferencer._get_chunk_data(self, inputs, chunk_size)\r\n    586 chunk_data = []\r\n    587 for _ in range(chunk_size):\r\n--> 588     processed_data = next(inputs_iter)\r\n    589     chunk_data.append(processed_data)\r\n    590 yield chunk_data\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/mmcv\/transforms\/base.py:12, in BaseTransform.__call__(self, results)\r\n      9 def __call__(self,\r\n     10              results: Dict) -> Optional[Union[Dict, Tuple[List, List]]]:\r\n---> 12     return self.transform(results)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/mmcv\/transforms\/wrappers.py:88, in Compose.transform(self, results)\r\n     79 \"\"\"Call function to apply transforms sequentially.\r\n     80 \r\n     81 Args:\r\n   (...)\r\n     85     dict or None: Transformed results.\r\n     86 \"\"\"\r\n     87 for t in self.transforms:\r\n---> 88     results = t(results)  # type: ignore\r\n     89     if results is None:\r\n     90         return None\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/mmcv\/transforms\/base.py:12, in BaseTransform.__call__(self, results)\r\n      9 def __call__(self,\r\n     10              results: Dict) -> Optional[Union[Dict, Tuple[List, List]]]:\r\n---> 12     return self.transform(results)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/mmseg\/datasets\/transforms\/loading.py:496, in InferencerLoader.transform(self, single_input)\r\n    494     inputs = single_input\r\n    495 else:\r\n--> 496     raise NotImplementedError\r\n    498 if 'img' in inputs:\r\n    499     return self.from_ndarray(inputs)\r\n\r\nNotImplementedError: \r\n````\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('Andyrasika\/cat_kingdom')\r\ndataset\r\n\r\nfrom mmseg.apis import MMSegInferencer\r\n\r\ncheckpoint_name = 'segformer_mit-b5_8xb2-160k_ade20k-640x640'\r\n\r\ninferencer = MMSegInferencer(model=checkpoint_name)\r\n\r\n# Define a function to apply the code to each image in the dataset\r\ndef process_image(image_path):\r\n    print(\"Processing image:\", image_path)\r\n    result = inferencer(image_path)['predictions']\r\n    mask = np.where(result == 12, 255, 0).astype('uint8')\r\n    return Image.fromarray(mask)\r\n\r\n# Process and save masks for each image in the dataset\r\nfor idx, example in enumerate(dataset['train']):\r\n    image_path = example['image']\r\n    mask_image = process_image(image_path)\r\n    mask_image.save(f\"mask_{idx}.png\")\r\n```\n\n### Expected behavior\n\ncreate a separate column with masks in the dataset and further shows as a separate column in hub\n\n### Environment info\n\njupyter notebook RTX 3090","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6229\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6229\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6228","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6228\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6228\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6228\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6228","id":1887959311,"node_id":"PR_kwDODunzps5Z5HZi","number":6228,"title":"Remove RGB -> BGR image conversion in Object Detection tutorial","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-09-08T16:09:13Z","updated_at":"2023-09-08T18:02:49Z","closed_at":"2023-09-08T17:52:16Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6225 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6228\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6228\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6228","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6228","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6228.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6228.patch","merged_at":"2023-09-08T17:52:16Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6226","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6226\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6226\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6226\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6226","id":1887462591,"node_id":"PR_kwDODunzps5Z3arq","number":6226,"title":"Add push_to_hub with multiple configs docs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-09-08T11:08:55Z","updated_at":"2023-09-08T12:29:21Z","closed_at":"2023-09-08T12:20:51Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6226\/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":2,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6226\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6226","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6226","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6226.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6226.patch","merged_at":"2023-09-08T12:20:51Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6225","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6225\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6225\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6225\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6225","id":1887054320,"node_id":"I_kwDODunzps5weinw","number":6225,"title":"Conversion from RGB to BGR in Object Detection tutorial","user":{"login":"samokhinv","id":33297401,"node_id":"MDQ6VXNlcjMzMjk3NDAx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/33297401?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/samokhinv","html_url":"https:\/\/github.com\/samokhinv","followers_url":"https:\/\/api.github.com\/users\/samokhinv\/followers","following_url":"https:\/\/api.github.com\/users\/samokhinv\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/samokhinv\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/samokhinv\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/samokhinv\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/samokhinv\/orgs","repos_url":"https:\/\/api.github.com\/users\/samokhinv\/repos","events_url":"https:\/\/api.github.com\/users\/samokhinv\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/samokhinv\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-09-08T06:49:19Z","updated_at":"2023-09-08T17:52:18Z","closed_at":"2023-09-08T17:52:17Z","author_association":"NONE","active_lock_reason":null,"body":"The [tutorial](https:\/\/huggingface.co\/docs\/datasets\/main\/en\/object_detection) mentions the necessity of conversion the input image from BGR to RGB\r\n\r\n> albumentations expects the image to be in BGR format, not RGB, so you\u2019ll have to convert the image before applying the transform.\r\n\r\n[Link to tutorial](https:\/\/github.com\/huggingface\/datasets\/blob\/0a068dbf3b446417ffd89d32857608394ec699e6\/docs\/source\/object_detection.mdx#L77)\r\n\r\nHowever, relevant albumentations' tutorials [on channels conversion](https:\/\/albumentations.ai\/docs\/examples\/example\/#read-the-image-from-the-disk-and-convert-it-from-the-bgr-color-space-to-the-rgb-color-space) and  [on boxes](https:\/\/albumentations.ai\/docs\/examples\/example_bboxes\/) imply that it's not really true no more.\r\n\r\nI suggest removing this outdated conversion from the tutorial.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6225\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6225\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6224","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6224\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6224\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6224\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6224","id":1886043692,"node_id":"PR_kwDODunzps5Zym3j","number":6224,"title":"Ignore `dataset_info.json` in data files resolution","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-09-07T14:43:51Z","updated_at":"2023-09-07T15:46:10Z","closed_at":"2023-09-07T15:37:20Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"`save_to_disk` creates this file, but also [`HugginFaceDatasetSever`](https:\/\/github.com\/gradio-app\/gradio\/blob\/26fef8c7f85a006c7e25cdbed1792df19c512d02\/gradio\/flagging.py#L214), so this is needed to avoid issues such as [this one](https:\/\/discord.com\/channels\/879548962464493619\/1149295819938349107\/1149295819938349107).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6224\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6224\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6224","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6224","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6224.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6224.patch","merged_at":"2023-09-07T15:37:20Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6223","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6223\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6223\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6223\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6223","id":1885710696,"node_id":"PR_kwDODunzps5Zxd5c","number":6223,"title":"Update README.md","user":{"login":"NinoRisteski","id":95188570,"node_id":"U_kgDOBax2Wg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/95188570?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NinoRisteski","html_url":"https:\/\/github.com\/NinoRisteski","followers_url":"https:\/\/api.github.com\/users\/NinoRisteski\/followers","following_url":"https:\/\/api.github.com\/users\/NinoRisteski\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NinoRisteski\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NinoRisteski\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NinoRisteski\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NinoRisteski\/orgs","repos_url":"https:\/\/api.github.com\/users\/NinoRisteski\/repos","events_url":"https:\/\/api.github.com\/users\/NinoRisteski\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NinoRisteski\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-09-07T11:33:20Z","updated_at":"2023-09-13T22:32:31Z","closed_at":"2023-09-13T22:23:42Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"fixed a few typos","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6223\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6223\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6223","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6223","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6223.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6223.patch","merged_at":"2023-09-13T22:23:42Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6222","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6222\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6222\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6222\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6222","id":1884875510,"node_id":"PR_kwDODunzps5Zup2f","number":6222,"title":"fix typo in Audio dataset documentation","user":{"login":"prassanna-ravishankar","id":3224332,"node_id":"MDQ6VXNlcjMyMjQzMzI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3224332?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/prassanna-ravishankar","html_url":"https:\/\/github.com\/prassanna-ravishankar","followers_url":"https:\/\/api.github.com\/users\/prassanna-ravishankar\/followers","following_url":"https:\/\/api.github.com\/users\/prassanna-ravishankar\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/prassanna-ravishankar\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/prassanna-ravishankar\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/prassanna-ravishankar\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/prassanna-ravishankar\/orgs","repos_url":"https:\/\/api.github.com\/users\/prassanna-ravishankar\/repos","events_url":"https:\/\/api.github.com\/users\/prassanna-ravishankar\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/prassanna-ravishankar\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-09-06T23:17:24Z","updated_at":"2023-10-03T14:18:41Z","closed_at":"2023-09-07T15:39:09Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"There is a typo in the section of the documentation dedicated to creating an audio dataset. The Dataset is incorrectly suffixed with a `Config`\r\n\r\n\r\nhttps:\/\/huggingface.co\/datasets\/indonesian-nlp\/librivox-indonesia\/blob\/main\/librivox-indonesia.py#L59","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6222\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6222\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6222","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6222","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6222.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6222.patch","merged_at":"2023-09-07T15:39:09Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6221","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6221\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6221\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6221\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6221","id":1884324631,"node_id":"I_kwDODunzps5wUIMX","number":6221,"title":"Support saving datasets with custom formatting","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-09-06T16:03:32Z","updated_at":"2023-09-06T18:32:07Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Requested in https:\/\/discuss.huggingface.co\/t\/using-set-transform-on-a-dataset-leads-to-an-exception\/53036.\r\n\r\nI am not sure if supporting this is the best idea for the following reasons:\r\n\r\n>For this to work, we would have to pickle a custom transform, which means the transform and the objects it references need to be serializable. Also, deserializing these bytes would make `load_from_disk` unsafe, so I'm not sure this is a good idea. \r\n\r\n@lhoestq WDYT?\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6221\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6221\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6220","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6220\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6220\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6220\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6220","id":1884285980,"node_id":"PR_kwDODunzps5ZspRb","number":6220,"title":"Set dev version","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-09-06T15:40:33Z","updated_at":"2023-09-06T15:52:33Z","closed_at":"2023-09-06T15:41:13Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6220\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6220\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6220","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6220","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6220.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6220.patch","merged_at":"2023-09-06T15:41:13Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6219","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6219\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6219\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6219\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6219","id":1884244334,"node_id":"PR_kwDODunzps5ZsgPK","number":6219,"title":"Release: 2.14.5","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-09-06T15:17:10Z","updated_at":"2023-09-06T15:46:20Z","closed_at":"2023-09-06T15:18:51Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6219\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6219\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6219","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6219","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6219.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6219.patch","merged_at":"2023-09-06T15:18:51Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6218","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6218\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6218\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6218\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6218","id":1883734000,"node_id":"PR_kwDODunzps5Zqw3Y","number":6218,"title":"Rename old push_to_hub configs to \"default\" in dataset_infos","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-09-06T10:40:05Z","updated_at":"2023-09-07T08:31:29Z","closed_at":"2023-09-06T11:23:56Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\n\r\nb = load_dataset_builder(\"lambdalabs\/pokemon-blip-captions\", \"default\")\r\nprint(b.info)\r\n```\r\n\r\nwhich should return\r\n\r\n```\r\nDatasetInfo(\r\n  features={'image': Image(decode=True, id=None), 'text': Value(dtype='string', id=None)}, \r\n  dataset_name='pokemon-blip-captions',\r\n  config_name='default', \r\n  version=0.0.0,\r\n  splits={'train': SplitInfo(name='train', num_bytes=119417410.0, num_examples=833, shard_lengths=None, dataset_name='pokemon-blip-captions')},\r\n  download_checksums=None,\r\n  download_size=99672355,\r\n  dataset_size=119417410.0,\r\n  size_in_bytes=219089765.0,\r\n  ...\r\n)\r\n```\r\n\r\ninstead of and empty dataset info.\r\n\r\nThe dataset has a dataset_infos.json file with a deprecated config name \"lambdalabs--pokemon-blip-captions\". We switched those config names to \"default\" in 2.14, so the builder.info should take this into account.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6218\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6218\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6218","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6218","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6218.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6218.patch","merged_at":"2023-09-06T11:23:56Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6217","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6217\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6217\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6217\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6217","id":1883614607,"node_id":"I_kwDODunzps5wRa2P","number":6217,"title":"`Dataset.to_dict()` ignore `decode=True` with Image feature","user":{"login":"qgallouedec","id":45557362,"node_id":"MDQ6VXNlcjQ1NTU3MzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45557362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/qgallouedec","html_url":"https:\/\/github.com\/qgallouedec","followers_url":"https:\/\/api.github.com\/users\/qgallouedec\/followers","following_url":"https:\/\/api.github.com\/users\/qgallouedec\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/qgallouedec\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/qgallouedec\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/qgallouedec\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/qgallouedec\/orgs","repos_url":"https:\/\/api.github.com\/users\/qgallouedec\/repos","events_url":"https:\/\/api.github.com\/users\/qgallouedec\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/qgallouedec\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-09-06T09:26:16Z","updated_at":"2023-09-08T17:08:52Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\n`Dataset.to_dict` seems to ignore the decoding instruction passed in features.\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\nimg = np.random.randint(0, 256, (5, 5, 3), dtype=np.uint8)\r\nimg = Image.fromarray(img)\r\n\r\nfeatures = datasets.Features({\"image\": datasets.Image(decode=True)})\r\ndataset = datasets.Dataset.from_dict({\"image\": [img]}, features=features)\r\nprint({key: dataset[key] for key in dataset.column_names})\r\n# {'image': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=5x5 at 0x7EFBC80E15B0>]}\r\nprint(dataset.to_dict())\r\n# {'image': [{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x05\\x08\\x02\\x00\\x00\\x00\\x02\\r\\xb1\\xb2\\x00\\x00\\x00[IDATx\\x9c\\x01P\\x00\\xaf\\xff\\x01\\x13\\x1b<7\\xe7\\xe0\\xdc^6\\xed\\x04\\xc7M\\xd2\\x9f\\x00X\\x1b\\xb0?\\x1ba\\x15\\xc5 o\\xd0\\x80\\xbe\\x19\/\\x01\\xec\\x95\\x1f\\x9f\\xffj\\xfa1\\xa7\\xc4X\\xea\\xbe\\xa4g\\x00\\xc4\\x15\\xdeC\\xc7 \\xbbaqe\\xc8\\xb9\\xa9q\\xe7\\x00,?M\\xc0)\\xdaD`}\\xb1\\xdci\\x1e\\xafC\\xa9]%.@\\xa6\\xf0\\xb3\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82', 'path': None}]}\r\n```\r\n\r\n\r\n\n\n### Expected behavior\n\nI would expect `{key: dataset[key] for key in dataset.column_names}` and `dataset.to_dict()` to be equivalent. If the previous behavior is expected, then it should be stated [in the doc](https:\/\/huggingface.co\/docs\/datasets\/v2.14.4\/en\/package_reference\/main_classes#datasets.Dataset.to_dict).\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-6.2.0-31-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n- Pillow 9.5.0\r\n- numpy 1.25.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6217\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6217\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6216","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6216\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6216\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6216\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6216","id":1883492703,"node_id":"PR_kwDODunzps5Zp8al","number":6216,"title":"Release: 2.13.2","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-09-06T08:15:32Z","updated_at":"2023-09-06T08:52:18Z","closed_at":"2023-09-06T08:22:43Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6216\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6216\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6216","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6216","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6216.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6216.patch","merged_at":"2023-09-06T08:22:43Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6215","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6215\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6215\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6215\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6215","id":1882176970,"node_id":"PR_kwDODunzps5ZlcqC","number":6215,"title":"Fix checking patterns to infer packaged builder ","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-09-05T15:10:47Z","updated_at":"2023-09-06T10:34:00Z","closed_at":"2023-09-06T10:25:00Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Don't ignore results of pattern resolving if `self.data_files` is not None. Otherwise lines 854 and 1037 make no sense.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6215\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6215\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6215","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6215","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6215.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6215.patch","merged_at":"2023-09-06T10:25:00Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6214","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6214\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6214\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6214\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6214","id":1881736469,"node_id":"I_kwDODunzps5wKQUV","number":6214,"title":"Unpin fsspec < 2023.9.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-09-05T11:02:58Z","updated_at":"2023-09-26T15:32:52Z","closed_at":"2023-09-26T15:32:52Z","author_association":"MEMBER","active_lock_reason":null,"body":"Once root issue is fixed, remove temporary pin of fsspec < 2023.9.0 introduced by:\r\n- #6210\r\n\r\nRelated to issue:\r\n- #6209\r\n\r\nAfter investigation, I think the root issue is related to the new glob behavior with double asterisk `**` they have introduced in:\r\n- https:\/\/github.com\/fsspec\/filesystem_spec\/pull\/1329","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6214\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6214\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6213","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6213\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6213\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6213\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6213","id":1880592987,"node_id":"PR_kwDODunzps5ZgHLO","number":6213,"title":"Better list array values handling in cast\/embed storage","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-09-04T16:21:23Z","updated_at":"2023-10-05T15:25:05Z","closed_at":"2023-10-05T15:24:34Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Use [`array.flatten`](https:\/\/arrow.apache.org\/docs\/python\/generated\/pyarrow.ListArray.html#pyarrow.ListArray.flatten) that takes `.offset` into account instead of `array.values` in array cast\/embed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6213\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6213\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6213","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6213","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6213.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6213.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6212","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6212\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6212\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6212\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6212","id":1880399516,"node_id":"I_kwDODunzps5wFJ6c","number":6212,"title":"Tilde (~) is not supported for data_files","user":{"login":"exs-avianello","id":128361578,"node_id":"U_kgDOB6akag","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/128361578?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/exs-avianello","html_url":"https:\/\/github.com\/exs-avianello","followers_url":"https:\/\/api.github.com\/users\/exs-avianello\/followers","following_url":"https:\/\/api.github.com\/users\/exs-avianello\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/exs-avianello\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/exs-avianello\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/exs-avianello\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/exs-avianello\/orgs","repos_url":"https:\/\/api.github.com\/users\/exs-avianello\/repos","events_url":"https:\/\/api.github.com\/users\/exs-avianello\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/exs-avianello\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-09-04T14:23:49Z","updated_at":"2023-09-05T08:28:39Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nAttempting to `load_dataset` from a path starting with `~` (as a shorthand for the user's home directory) seems not to be fully working - at least as far as the `parquet` dataset builder is concerned.\r\n\r\n(the same file can be loaded correctly if providing its absolute path instead)\r\n\r\nI think that this is very similar to https:\/\/github.com\/huggingface\/datasets\/issues\/5757, but for `data_files` rather than `data_dir`\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\n# save a parquet file at ~\/path\/to\/data.parquet\r\n\r\ndata_files = \"~\/path\/to\/data.parquet\"\r\ndataset = datasets.load_dataset(\"parquet\", data_files=data_files)\r\n```\r\n```\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 12671.61it\/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 22671.91it\/s]\r\nGenerating train split: 0 examples [00:00, ? examples\/s]\r\nTraceback (most recent call last):\r\n  File \".venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1949, in _prepare_split_single\r\n    num_examples, num_bytes = writer.finalize()\r\n                              ^^^^^^^^^^^^^^^^^\r\n  File \".venv\/lib\/python3.11\/site-packages\/datasets\/arrow_writer.py\", line 598, in finalize\r\n    raise SchemaInferenceError(\"Please pass `features` or at least one example when writing data\")\r\ndatasets.arrow_writer.SchemaInferenceError: Please pass `features` or at least one example when writing data\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \".venv\/lib\/python3.11\/site-packages\/datasets\/load.py\", line 2133, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \".venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 954, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \".venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1049, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \".venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1813, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \".venv\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1958, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n\r\n```\n\n### Expected behavior\n\nCan use `~` shorthand in paths when loading local (parquet) datasets.\n\n### Environment info\n\n`datasets                               2.14.3`\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6212\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6212\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6211","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6211\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6211\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6211\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6211","id":1880265906,"node_id":"PR_kwDODunzps5Ze-pv","number":6211,"title":"Fix empty splitinfo json","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-09-04T13:13:53Z","updated_at":"2023-09-04T14:58:34Z","closed_at":"2023-09-04T14:47:17Z","author_association":"MEMBER","active_lock_reason":null,"body":"If a split is empty, then the JSON split info should mention num_bytes = 0 and num_examples = 0.\r\n\r\nUntil now they were omited because the JSON dumps ignore the fields that are equal to the default values.\r\n\r\nThis is needed in datasets-server since we parse this information to the viewer","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6211\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6211\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6211","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6211","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6211.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6211.patch","merged_at":"2023-09-04T14:47:17Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6210","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6210\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6210\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6210\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6210","id":1879649731,"node_id":"PR_kwDODunzps5Zc4JF","number":6210,"title":"Temporarily pin fsspec < 2023.9.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-09-04T07:07:07Z","updated_at":"2023-09-04T07:40:23Z","closed_at":"2023-09-04T07:30:00Z","author_association":"MEMBER","active_lock_reason":null,"body":"Temporarily pin fsspec < 2023.9.0 until permanent solution is found.\r\n\r\nHot fix #6209.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6210\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6210\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6210","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6210","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6210.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6210.patch","merged_at":"2023-09-04T07:30:00Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6209","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6209\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6209\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6209\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6209","id":1879622000,"node_id":"I_kwDODunzps5wCMFw","number":6209,"title":"CI is broken with AssertionError: 3 failed, 12 errors","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-09-04T06:47:05Z","updated_at":"2023-09-04T07:30:01Z","closed_at":"2023-09-04T07:30:01Z","author_association":"MEMBER","active_lock_reason":null,"body":"Our CI is broken: 3 failed, 12 errors\r\n\r\nSee: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/6069947111\/job\/16465138041\r\n```\r\n=========================== short test summary info ============================\r\nFAILED tests\/test_load.py::ModuleFactoryTest::test_LocalDatasetModuleFactoryWithoutScript_with_data_dir - AssertionError: assert ({NamedSplit('train'): ['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_LocalDatasetModuleFactory2\/data_dir2\/subdir1\/train.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_LocalDatasetModuleFactory2\/data_dir2\/subdir1\/train.txt'], NamedSplit('test'): ['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_LocalDatasetModuleFactory2\/data_dir2\/subdir1\/test.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_LocalDatasetModuleFactory2\/data_dir2\/subdir1\/test.txt']} is not None and 2 == 1)\r\n +  where 2 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_LocalDatasetModuleFactory2\/data_dir2\/subdir1\/train.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_LocalDatasetModuleFactory2\/data_dir2\/subdir1\/train.txt'])\r\nFAILED tests\/test_load.py::test_load_dataset_arrow[False] - AssertionError: assert 20 == 10\r\n +  where 20 = Dataset({\\n    features: ['col_1'],\\n    num_rows: 20\\n}).num_rows\r\nFAILED tests\/test_load.py::test_load_dataset_arrow[True] - assert 20 == 10\r\nERROR tests\/packaged_modules\/test_audiofolder.py::test_data_files_with_metadata_and_multiple_splits[jsonl-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_2\/audiofolder_data_dir_with_metadata\/train\/audio_file.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_2\/audiofolder_data_dir_with_metadata\/train\/audio_file2.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_2\/audiofolder_data_dir_with_metadata\/train\/metadata.jsonl', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_2\/audiofolder_data_dir_with_metadata\/train\/audio_file.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_2\/audiofolder_data_dir_with_metadata\/train\/audio_file2.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_2\/audiofolder_data_dir_with_metadata\/train\/metadata.jsonl'])\r\nERROR tests\/packaged_modules\/test_audiofolder.py::test_data_files_with_metadata_and_multiple_splits[jsonl-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_3\/audiofolder_data_dir_with_metadata\/train\/audio_file.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_3\/audiofolder_data_dir_with_metadata\/train\/audio_file2.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_3\/audiofolder_data_dir_with_metadata\/train\/metadata.jsonl', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_3\/audiofolder_data_dir_with_metadata\/train\/audio_file.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_3\/audiofolder_data_dir_with_metadata\/train\/audio_file2.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_3\/audiofolder_data_dir_with_metadata\/train\/metadata.jsonl'])\r\nERROR tests\/packaged_modules\/test_audiofolder.py::test_data_files_with_metadata_and_multiple_splits[csv-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_4\/audiofolder_data_dir_with_metadata\/train\/audio_file.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_4\/audiofolder_data_dir_with_metadata\/train\/audio_file2.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_4\/audiofolder_data_dir_with_metadata\/train\/metadata.csv', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_4\/audiofolder_data_dir_with_metadata\/train\/audio_file.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_4\/audiofolder_data_dir_with_metadata\/train\/audio_file2.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_4\/audiofolder_data_dir_with_metadata\/train\/metadata.csv'])\r\nERROR tests\/packaged_modules\/test_audiofolder.py::test_data_files_with_metadata_and_multiple_splits[csv-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_5\/audiofolder_data_dir_with_metadata\/train\/audio_file.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_5\/audiofolder_data_dir_with_metadata\/train\/audio_file2.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_5\/audiofolder_data_dir_with_metadata\/train\/metadata.csv', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_5\/audiofolder_data_dir_with_metadata\/train\/audio_file.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_5\/audiofolder_data_dir_with_metadata\/train\/audio_file2.wav', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_5\/audiofolder_data_dir_with_metadata\/train\/metadata.csv'])\r\nERROR tests\/packaged_modules\/test_folder_based_builder.py::test_data_files_with_metadata_and_splits[1-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_3\/autofolder_data_dir_with_metadata_two_splits\/train\/file.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_3\/autofolder_data_dir_with_metadata_two_splits\/train\/file2.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_3\/autofolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_3\/autofolder_data_dir_with_metadata_two_splits\/train\/file.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_3\/autofolder_data_dir_with_metadata_two_splits\/train\/file2.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_3\/autofolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl'])\r\nERROR tests\/packaged_modules\/test_folder_based_builder.py::test_data_files_with_metadata_and_splits[1-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_4\/autofolder_data_dir_with_metadata_two_splits\/train\/file.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_4\/autofolder_data_dir_with_metadata_two_splits\/train\/file2.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_4\/autofolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_4\/autofolder_data_dir_with_metadata_two_splits\/train\/file.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_4\/autofolder_data_dir_with_metadata_two_splits\/train\/file2.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_4\/autofolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl'])\r\nERROR tests\/packaged_modules\/test_folder_based_builder.py::test_data_files_with_metadata_and_splits[2-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_5\/autofolder_data_dir_with_metadata_two_splits\/train\/file.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_5\/autofolder_data_dir_with_metadata_two_splits\/train\/file2.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_5\/autofolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_5\/autofolder_data_dir_with_metadata_two_splits\/train\/file.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_5\/autofolder_data_dir_with_metadata_two_splits\/train\/file2.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_5\/autofolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl'])\r\nERROR tests\/packaged_modules\/test_imagefolder.py::test_data_files_with_metadata_and_multiple_splits[jsonl-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_12\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_12\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb2.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_12\/imagefolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_12\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_12\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb2.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_12\/imagefolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl'])\r\nERROR tests\/packaged_modules\/test_imagefolder.py::test_data_files_with_metadata_and_multiple_splits[jsonl-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_13\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_13\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb2.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_13\/imagefolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_13\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_13\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb2.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_13\/imagefolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl'])\r\nERROR tests\/packaged_modules\/test_folder_based_builder.py::test_data_files_with_metadata_and_splits[2-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_6\/autofolder_data_dir_with_metadata_two_splits\/train\/file.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_6\/autofolder_data_dir_with_metadata_two_splits\/train\/file2.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_6\/autofolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_6\/autofolder_data_dir_with_metadata_two_splits\/train\/file.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_6\/autofolder_data_dir_with_metadata_two_splits\/train\/file2.txt', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw0\/test_data_files_with_metadata_6\/autofolder_data_dir_with_metadata_two_splits\/train\/metadata.jsonl'])\r\nERROR tests\/packaged_modules\/test_imagefolder.py::test_data_files_with_metadata_and_multiple_splits[csv-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_14\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_14\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb2.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_14\/imagefolder_data_dir_with_metadata_two_splits\/train\/metadata.csv', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_14\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_14\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb2.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_14\/imagefolder_data_dir_with_metadata_two_splits\/train\/metadata.csv'])\r\nERROR tests\/packaged_modules\/test_imagefolder.py::test_data_files_with_metadata_and_multiple_splits[csv-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_15\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_15\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb2.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_15\/imagefolder_data_dir_with_metadata_two_splits\/train\/metadata.csv', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_15\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_15\/imagefolder_data_dir_with_metadata_two_splits\/train\/image_rgb2.jpg', '\/tmp\/pytest-of-runner\/pytest-0\/popen-gw1\/test_data_files_with_metadata_15\/imagefolder_data_dir_with_metadata_two_splits\/train\/metadata.csv'])\r\n= 3 failed, 2383 passed, 26 skipped, 9 warnings, 12 errors in 280.79s (0:04:40) =\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6209\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6209\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6208","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6208\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6208\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6208\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6208","id":1879572646,"node_id":"PR_kwDODunzps5ZcnpJ","number":6208,"title":"Do not filter out .zip extensions from no-script datasets","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-09-04T06:07:12Z","updated_at":"2023-09-04T09:22:19Z","closed_at":"2023-09-04T09:13:32Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR is a hotfix of:\r\n- #6207\r\n\r\nThat PR introduced the filtering out of `.zip` extensions. This PR reverts that.\r\n\r\nHot fix #6207.\r\n\r\nMaybe we should do patch releases: the bug was introduced in 2.13.1.\r\n\r\nCC: @lhoestq ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6208\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6208\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6208","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6208","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6208.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6208.patch","merged_at":"2023-09-04T09:13:32Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6207","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6207\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6207\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6207\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6207","id":1879555234,"node_id":"I_kwDODunzps5wB7yi","number":6207,"title":"No-script datasets with ZIP files do not load","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-09-04T05:50:27Z","updated_at":"2023-09-04T09:13:33Z","closed_at":"2023-09-04T09:13:33Z","author_association":"MEMBER","active_lock_reason":null,"body":"While investigating an issue on a Hub dataset, I have discovered the no-script datasets containing ZIP files do not load.\r\n\r\nFor example, that no-script dataset containing ZIP files, raises NonMatchingSplitsSizesError:\r\n```python\r\nIn [2]: ds = load_dataset(\"sidovic\/LearningQ-qg\")\r\n\r\nNonMatchingSplitsSizesError: [\r\n  {\r\n    'expected': SplitInfo(name='train', num_bytes=0, num_examples=188660, shard_lengths=None, dataset_name=None), \r\n    'recorded': SplitInfo(name='train', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='learning_q-qg')\r\n  }, {\r\n    'expected': SplitInfo(name='validation', num_bytes=0, num_examples=20630, shard_lengths=None, dataset_name=None), \r\n    'recorded': SplitInfo(name='validation', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='learning_q-qg')\r\n  }, {\r\n    'expected': SplitInfo(name='test', num_bytes=0, num_examples=18227, shard_lengths=None, dataset_name=None), \r\n    'recorded': SplitInfo(name='test', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='learning_q-qg')\r\n  }\r\n]\r\n```\r\n\r\nAs another example, a no-script dataset containing just a (CSV)-ZIP file, raises a DatasetGenerationError:\r\n```\r\n>               num_examples, num_bytes = writer.finalize()\r\n\r\nsrc\/datasets\/builder.py:1949: \r\n\r\n>           raise SchemaInferenceError(\"Please pass `features` or at least one example when writing data\")\r\nE           datasets.arrow_writer.SchemaInferenceError: Please pass `features` or at least one example when writing data\r\n\r\nsrc\/datasets\/arrow_writer.py:598: SchemaInferenceError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nsrc\/datasets\/load.py:2143: in load_dataset\r\n    builder_instance.download_and_prepare(\r\nsrc\/datasets\/builder.py:954: in download_and_prepare\r\n    self._download_and_prepare(\r\nsrc\/datasets\/builder.py:1049: in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\nsrc\/datasets\/builder.py:1813: in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n\r\n>           raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\nE           datasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n\r\nsrc\/datasets\/builder.py:1958: DatasetGenerationError\r\n```\r\n\r\nAfter investigating, I think this bug was introduced in this PR:\r\n- #5972\r\n\r\nRelated to:\r\n- https:\/\/huggingface.co\/datasets\/sidovic\/LearningQ-qg\/discussions\/1\r\n\r\nCC: @lhoestq ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6207\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6207\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6206","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6206\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6206\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6206\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6206","id":1879473745,"node_id":"I_kwDODunzps5wBn5R","number":6206,"title":"When calling load_dataset, raise error: pyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays ","user":{"login":"aihao2000","id":51043929,"node_id":"MDQ6VXNlcjUxMDQzOTI5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/51043929?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/aihao2000","html_url":"https:\/\/github.com\/aihao2000","followers_url":"https:\/\/api.github.com\/users\/aihao2000\/followers","following_url":"https:\/\/api.github.com\/users\/aihao2000\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/aihao2000\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/aihao2000\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/aihao2000\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/aihao2000\/orgs","repos_url":"https:\/\/api.github.com\/users\/aihao2000\/repos","events_url":"https:\/\/api.github.com\/users\/aihao2000\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/aihao2000\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-09-04T04:14:00Z","updated_at":"2023-09-04T06:05:50Z","closed_at":"2023-09-04T06:05:49Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen calling load_dataset, raise error\r\n```\r\nTraceback (most recent call last):                                                                             \r\n  File \"\/home\/aihao\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1694, in _pre\r\npare_split_single                                                                                              \r\n    writer.write(example, key)                                                                                 \r\n  File \"\/home\/aihao\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/arrow_writer.py\", line 490, in \r\nwrite                                                                                                          \r\n    self.write_examples_on_file()                                                                              \r\n  File \"\/home\/aihao\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/arrow_writer.py\", line 448, in \r\nwrite_examples_on_file                                                                                         \r\n    self.write_batch(batch_examples=batch_examples)                                                            \r\n  File \"\/home\/aihao\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/arrow_writer.py\", line 559, in \r\nwrite_batch \r\n    self.write_table(pa_table, writer_batch_size)      \r\n  File \"\/home\/aihao\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/arrow_writer.py\", line 571, in \r\nwrite_table                \r\n    pa_table = pa_table.combine_chunks()               \r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^               \r\n  File \"pyarrow\/table.pxi\", line 3439, in pyarrow.lib.Table.combine_chunks                                     \r\n  File \"pyarrow\/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status                             \r\n  File \"pyarrow\/error.pxi\", line 100, in pyarrow.lib.check_status                                              \r\npyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\r\nThe above exception was the direct cause of the following exception:                                           \r\n                                                                                                               \r\nTraceback (most recent call last):\r\ndataset = load_dataset(                            \r\n              ^^^^^^^^^^^^^                            \r\n  File \"\/home\/aihao\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/load.py\", line 2133, in load_da\r\ntaset                      \r\n    builder_instance.download_and_prepare(             \r\n  File \"\/home\/aihao\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 954, in downl\r\noad_and_prepare            \r\n    self._download_and_prepare(                        \r\n  File \"\/home\/aihao\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1717, in _dow\r\nnload_and_prepare          \r\n    super()._download_and_prepare(                     \r\n  File \"\/home\/aihao\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1049, in _dow\r\nnload_and_prepare          \r\n    self._prepare_split(split_generator, **prepare_split_kwargs)                                               \r\n  File \"\/home\/aihao\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1555, in _pre\r\npare_split                 \r\n    for job_id, done, content in self._prepare_split_single(                                                   \r\n  File \"\/home\/aihao\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/builder.py\", line 1712, in _pre\r\npare_split_single          \r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e                      \r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset                        \r\nSetting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.\r\n09\/04\/2023 12:02:04 - WARNING - datasets.builder - Setting num_proc from 8 back to 1 for the train split to dis\r\nable multiprocessing as it only contains one shard.\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nCall load_dataset with the large image as feature\r\n\r\n### Expected behavior\r\n\r\nno error\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.3\r\n- Platform: Linux-6.2.0-31-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.4\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6206\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6206\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6203","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6203\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6203\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6203\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6203","id":1877491602,"node_id":"I_kwDODunzps5v6D-S","number":6203,"title":"Support loading from a DVC remote repository","user":{"login":"bilelomrani1","id":16692099,"node_id":"MDQ6VXNlcjE2NjkyMDk5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16692099?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/bilelomrani1","html_url":"https:\/\/github.com\/bilelomrani1","followers_url":"https:\/\/api.github.com\/users\/bilelomrani1\/followers","following_url":"https:\/\/api.github.com\/users\/bilelomrani1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/bilelomrani1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/bilelomrani1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/bilelomrani1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/bilelomrani1\/orgs","repos_url":"https:\/\/api.github.com\/users\/bilelomrani1\/repos","events_url":"https:\/\/api.github.com\/users\/bilelomrani1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/bilelomrani1\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-09-01T14:04:52Z","updated_at":"2023-09-15T15:11:27Z","closed_at":"2023-09-15T15:11:27Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nAdding support for loading a file from a DVC repository, tracked remotely on a SCM.\n\n### Motivation\n\nDVC is a popular version control system to version and manage datasets. The files are stored on a remote object storage platform, but they are tracked using Git. Integration with DVC is possible through the `DVCFileSystem`.\r\n\r\nI have a Gitlab repository where multiple files are tracked using DVC and stored in a GCP bucket. I would like to be able to load these files using `datasets` directly using an URL. My goal is to write a generic code that abstracts the storage layer, such that my users will only have to pass in an `fsspec`-compliant URL and the corresponding files will be loaded.\n\n### Your contribution\n\nI managed to instantiate a `DVCFileSystem` pointing to a Gitlab repo from a `fsspec` chained URL in [this pull request](https:\/\/github.com\/iterative\/dvc\/pull\/9903) to DVC. \r\n\r\n```python\r\nfrom fsspec.core import url_to_fs\r\n\r\nfs, _ = url_to_fs(\"dvc::https:\/\/gitlab.com\/repository\/group\/my-repo\")\r\n```\r\n\r\nFrom now I'm not sure how to continue, it seems that `datasets` expects the URL to be fully qualified like so: `dvc::https:\/\/gitlab.com\/repository\/group\/my-repo\/my-folder\/my-file.json` but this fails because `DVCFileSystem` expects the URL to point to the root of an SCM repo. Is there a way to make this work with `datasets`?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6203\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6203\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6202","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6202\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6202\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6202\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6202","id":1876630351,"node_id":"I_kwDODunzps5v2xtP","number":6202,"title":"avoid downgrading jax version","user":{"login":"chrisflesher","id":1332458,"node_id":"MDQ6VXNlcjEzMzI0NTg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1332458?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/chrisflesher","html_url":"https:\/\/github.com\/chrisflesher","followers_url":"https:\/\/api.github.com\/users\/chrisflesher\/followers","following_url":"https:\/\/api.github.com\/users\/chrisflesher\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/chrisflesher\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/chrisflesher\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/chrisflesher\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/chrisflesher\/orgs","repos_url":"https:\/\/api.github.com\/users\/chrisflesher\/repos","events_url":"https:\/\/api.github.com\/users\/chrisflesher\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/chrisflesher\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-09-01T02:57:57Z","updated_at":"2023-10-12T16:28:59Z","closed_at":"2023-10-12T16:28:59Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nWhenever I `pip install datasets[jax]` it downgrades jax to version 0.3.25. I seem to be able to install this library first then upgrade jax back to version 0.4.13.\r\n\r\n### Motivation\r\n\r\nIt would be nice to not overwrite currently installed version of jax if possible.\r\n\r\n### Your contribution\r\n\r\nI would be willing to beta test. Or maybe write some code if I could get pointed in the right direction, I'm not super familiar with this codebase.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6202\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6202\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6201","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6201\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6201\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6201\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6201","id":1875256775,"node_id":"PR_kwDODunzps5ZOVbV","number":6201,"title":"Fix to_json ValueError and remove pandas pin","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-31T10:38:08Z","updated_at":"2023-09-05T11:07:07Z","closed_at":"2023-09-05T10:58:21Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR fixes the root cause of the issue:\r\n- #6197\r\n\r\nThis PR also removes the temporary pin of `pandas` introduced by:\r\n- #6200\r\n\r\n\r\nNote that for orient in ['records', 'values'], index value is ignored but\r\n- in `pandas` < 2.1.0, a ValueError is raised if not index and orient not in ['split', 'table']\r\n  - for orient = 'records', we need index = True\r\n  - default index value is True\r\n- in `pandas` = 2.1.0, a ValueError is raised if index is True and orient in ['records', 'values']\r\n  - for orient = 'records', we need index = False or None\r\n  - default index value is None\r\n\r\nThis PR fixes the issue by not passing index and thus using default index value (valid for all pandas versions), unless orient is 'split' or 'table' (where we pass index = False, as it was done before this fix).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6201\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6201\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6201","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6201","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6201.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6201.patch","merged_at":"2023-09-05T10:58:21Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6200","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6200\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6200\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6200\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6200","id":1875169551,"node_id":"PR_kwDODunzps5ZOCee","number":6200,"title":"Temporarily pin pandas < 2.1.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-31T09:45:17Z","updated_at":"2023-08-31T10:33:24Z","closed_at":"2023-08-31T10:24:38Z","author_association":"MEMBER","active_lock_reason":null,"body":"Temporarily pin `pandas` < 2.1.0 until permanent solution is found.\r\n\r\nHot fix #6197.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6200\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6200\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6200","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6200","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6200.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6200.patch","merged_at":"2023-08-31T10:24:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6199","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6199\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6199\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6199\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6199","id":1875165185,"node_id":"I_kwDODunzps5vxMAB","number":6199,"title":"Use load_dataset for local json files, but it not works","user":{"login":"Garen-in-bush","id":50519434,"node_id":"MDQ6VXNlcjUwNTE5NDM0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/50519434?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Garen-in-bush","html_url":"https:\/\/github.com\/Garen-in-bush","followers_url":"https:\/\/api.github.com\/users\/Garen-in-bush\/followers","following_url":"https:\/\/api.github.com\/users\/Garen-in-bush\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Garen-in-bush\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Garen-in-bush\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Garen-in-bush\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Garen-in-bush\/orgs","repos_url":"https:\/\/api.github.com\/users\/Garen-in-bush\/repos","events_url":"https:\/\/api.github.com\/users\/Garen-in-bush\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Garen-in-bush\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-31T09:42:34Z","updated_at":"2023-08-31T19:05:07Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nwhen I use load_dataset to load my local datasets\uff0cit always goes to Hugging Face to download the data instead of loading the local dataset.\n\n### Steps to reproduce the bug\n\n`raw_datasets = load_dataset(\r\n        \u2018json\u2019,\r\n        data_files=data_files)`\r\n\n\n### Expected behavior\n\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/50519434\/add3747f-6481-4da7-b374-8f81c5a6472c)\r\n\n\n### Environment info\n\npython version 3.8.5\r\ndatasets version 2.12\r\nos version unbuntu 18.04","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6199\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6199\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6198","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6198\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6198\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6198\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6198","id":1875092027,"node_id":"PR_kwDODunzps5ZNyBq","number":6198,"title":"Preserve split order in DataFilesDict","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-31T09:00:26Z","updated_at":"2023-08-31T13:57:31Z","closed_at":"2023-08-31T13:48:42Z","author_association":"MEMBER","active_lock_reason":null,"body":"After investigation, I have found that this copy forces the splits to be sorted alphabetically: https:\/\/github.com\/huggingface\/datasets\/blob\/029227a116c14720afca71b9b22e78eb2a1c09a6\/src\/datasets\/builder.py#L556\r\n\r\nThis PR removes the alphabetically sort of `DataFilesDict` keys.\r\n- Note that for a `dict`, the order of keys is relevant when hashing:\r\n  ```python\r\n  hash1 = Hasher.hash({'train': 'train.csv', 'test': 'test.csv'})\r\n  hash2 = Hasher.hash({'test': 'test.csv', 'train': 'train.csv'})\r\n  assert hash1 != hash2\r\n  ```\r\n- The `DataFilesDict` is a subclass of `dict`, thus the order should be relevant as well\r\n  ```python\r\n  hash1 = Hasher.hash(DataFilesDict({'train': 'train.csv', 'test': 'test.csv'}))\r\n  hash2 = Hasher.hash(DataFilesDict({'test': 'test.csv', 'train': 'train.csv'}))\r\n  assert hash1 != hash2\r\n  ```\r\n\r\nFix #6196.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6198\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6198\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6198","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6198","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6198.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6198.patch","merged_at":"2023-08-31T13:48:42Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6197","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6197\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6197\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6197\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6197","id":1875078155,"node_id":"I_kwDODunzps5vw2wL","number":6197,"title":"ValueError: 'index=True' is only valid when 'orient' is 'split', 'table', 'index', or 'columns'","user":{"login":"exs-avianello","id":128361578,"node_id":"U_kgDOB6akag","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/128361578?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/exs-avianello","html_url":"https:\/\/github.com\/exs-avianello","followers_url":"https:\/\/api.github.com\/users\/exs-avianello\/followers","following_url":"https:\/\/api.github.com\/users\/exs-avianello\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/exs-avianello\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/exs-avianello\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/exs-avianello\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/exs-avianello\/orgs","repos_url":"https:\/\/api.github.com\/users\/exs-avianello\/repos","events_url":"https:\/\/api.github.com\/users\/exs-avianello\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/exs-avianello\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2023-08-31T08:51:50Z","updated_at":"2023-09-01T10:35:10Z","closed_at":"2023-08-31T10:24:40Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nSaving a dataset `.to_json()` fails with a `ValueError` since the latest `pandas` [release](https:\/\/pandas.pydata.org\/docs\/dev\/whatsnew\/v2.1.0.html) (`2.1.0`)\r\n\r\nIn their latest release we have:\r\n\r\n> Improved error handling when using [DataFrame.to_json()](https:\/\/pandas.pydata.org\/docs\/dev\/reference\/api\/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json) with incompatible index and orient arguments ([GH 52143](https:\/\/github.com\/pandas-dev\/pandas\/issues\/52143))\r\n\r\ni.e. an error is now raised for invalid combinations of `index` and `orient`.\r\n\r\nThis means that unfortunately the custom logic at this line might sometimes lead to contradictions:\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/029227a116c14720afca71b9b22e78eb2a1c09a6\/src\/datasets\/io\/json.py#L96\r\n\r\ne.g. for the default case `orient=records` leads to `index=True`, which now raises a `ValueError`\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    dataset = datasets.Dataset.from_dict({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\r\n    dataset.to_json(\"dataset.json\")\r\n```\r\n\r\n```shell\r\n>>>\r\nValueError: 'index=True' is only valid when 'orient' is 'split', 'table', 'index', or 'columns'.\r\n```\n\n### Expected behavior\n\nThe dataset is successfully saved as `.json`\n\n### Environment info\n\n`python >= 3.9`\r\n`pandas >= 2.1.0`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6197\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6197\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6196","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6196\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6196\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6196\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6196","id":1875070972,"node_id":"I_kwDODunzps5vw0_8","number":6196,"title":"Split order is not preserved","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-08-31T08:47:16Z","updated_at":"2023-08-31T13:48:43Z","closed_at":"2023-08-31T13:48:43Z","author_association":"MEMBER","active_lock_reason":null,"body":"I have noticed that in some cases the split order is not preserved.\r\n\r\nFor example, consider a no-script dataset with configs:\r\n```yaml\r\nconfigs:\r\n- config_name: default\r\n  data_files:\r\n  - split: train\r\n    path: train.csv\r\n  - split: test\r\n    path: test.csv\r\n```\r\n- Note the defined split order is [train, test]\r\n\r\nOnce the dataset is loaded, the split order is not preserved:\r\n```python\r\nIn [16]: ds\r\nOut[16]: \r\nDatasetDict({\r\n    test: Dataset({\r\n        features: ['text', 'label'],\r\n        num_rows: 1\r\n    })\r\n    train: Dataset({\r\n        features: ['text', 'label'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n- Note the obtained split order is [test, train]","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6196\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6196\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6195","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6195\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6195\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6195\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6195","id":1874195585,"node_id":"I_kwDODunzps5vtfSB","number":6195,"title":"Force to reuse cache at given path","user":{"login":"Luosuu","id":43507393,"node_id":"MDQ6VXNlcjQzNTA3Mzkz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/43507393?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Luosuu","html_url":"https:\/\/github.com\/Luosuu","followers_url":"https:\/\/api.github.com\/users\/Luosuu\/followers","following_url":"https:\/\/api.github.com\/users\/Luosuu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Luosuu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Luosuu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Luosuu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Luosuu\/orgs","repos_url":"https:\/\/api.github.com\/users\/Luosuu\/repos","events_url":"https:\/\/api.github.com\/users\/Luosuu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Luosuu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-30T18:44:54Z","updated_at":"2023-11-03T10:14:21Z","closed_at":"2023-08-30T19:00:45Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI have run the official example of MLM like:\r\n\r\n```bash\r\n  python run_mlm.py \\\r\n      --model_name_or_path roberta-base \\\r\n      --dataset_name togethercomputer\/RedPajama-Data-1T \\\r\n      --dataset_config_name arxiv \\\r\n      --per_device_train_batch_size 10 \\\r\n      --preprocessing_num_workers 20 \\\r\n      --validation_split_percentage 0 \\\r\n      --cache_dir \/project\/huggingface_cache\/datasets \\\r\n      --line_by_line \\\r\n      --do_train \\\r\n      --pad_to_max_length \\\r\n      --output_dir \/project\/huggingface_cache\/test-mlm\r\n```\r\nit successfully runs and at my cache folder has `cache-1982fea76aa54a13_00001_of_00020.arrow`.....  `cache-1982fea76aa54a13_00020_of_00020.arrow ` as tokenization cache of `map` method. And the cache works fine every time I run the command above.\r\n\r\nHowever, when I switched to jupyter notebook (since I do not want to load datasets every time when I changed other parameters not related to the dataloading). It is not recognizing the cache files and starts to re-run the entire tokenization process. \r\n\r\nI changed my code to \r\n```python\r\ntokenized_datasets = raw_datasets[\"train\"].map(\r\n                tokenize_function,\r\n                batched=True,\r\n                num_proc=data_args.preprocessing_num_workers,\r\n                remove_columns=[text_column_name],\r\n                load_from_cache_file=True,\r\n                desc=\"Running tokenizer on dataset line_by_line\",\r\n                # cache_file_names= {\"train\": \"cache-1982fea76aa54a13.arrow\"}\r\n                cache_file_name=\"cache-1982fea76aa54a13.arrow\",\r\n                new_fingerprint=\"1982fea76aa54a13\"\r\n            )\r\n```\r\nit still does not recognize the previously cached files and trying to re-run the tokenization process.\n\n### Steps to reproduce the bug\n\nuse jupyter notebook for dataset map function.\n\n### Expected behavior\n\nthe map function accepts the given cache_file_name and new_fingerprint then load the previously cached files.\n\n### Environment info\n\n- `datasets` version: 2.14.4.dev0\r\n- Platform: Linux-3.10.0-1160.59.1.el7.x86_64-x86_64-with-glibc2.10\r\n- Python version: 3.8.8\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6195\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6195\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6194","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6194\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6194\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6194\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6194","id":1872598223,"node_id":"I_kwDODunzps5vnZTP","number":6194,"title":"Support custom fingerprinting with `Dataset.from_generator`","user":{"login":"bilelomrani1","id":16692099,"node_id":"MDQ6VXNlcjE2NjkyMDk5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16692099?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/bilelomrani1","html_url":"https:\/\/github.com\/bilelomrani1","followers_url":"https:\/\/api.github.com\/users\/bilelomrani1\/followers","following_url":"https:\/\/api.github.com\/users\/bilelomrani1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/bilelomrani1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/bilelomrani1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/bilelomrani1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/bilelomrani1\/orgs","repos_url":"https:\/\/api.github.com\/users\/bilelomrani1\/repos","events_url":"https:\/\/api.github.com\/users\/bilelomrani1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/bilelomrani1\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-29T22:43:13Z","updated_at":"2023-09-30T16:56:51Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nWhen using `Dataset.from_generator`, the generator is hashed when building the fingerprint. Similar to `.map`, it would be interesting to let the user bypass this hashing by accepting a `fingerprint` argument to `.from_generator`.\r\n\r\n### Motivation\r\n\r\nUsing the `.from_generator` constructor with a non-picklable generator fails. By accepting a `fingerprint` argument to `.from_generator`, the user would have the opportunity to manually fingerprint the dataset and thus bypass the crash.\r\n\r\n### Your contribution\r\n\r\nIf validated, I can try to submit a PR for this.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6194\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6194\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6193","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6193\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6193\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6193\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6193","id":1872285153,"node_id":"I_kwDODunzps5vmM3h","number":6193,"title":"Dataset loading script method does not work with .pyc file","user":{"login":"riteshkumarumassedu","id":43389071,"node_id":"MDQ6VXNlcjQzMzg5MDcx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/43389071?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/riteshkumarumassedu","html_url":"https:\/\/github.com\/riteshkumarumassedu","followers_url":"https:\/\/api.github.com\/users\/riteshkumarumassedu\/followers","following_url":"https:\/\/api.github.com\/users\/riteshkumarumassedu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/riteshkumarumassedu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/riteshkumarumassedu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/riteshkumarumassedu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/riteshkumarumassedu\/orgs","repos_url":"https:\/\/api.github.com\/users\/riteshkumarumassedu\/repos","events_url":"https:\/\/api.github.com\/users\/riteshkumarumassedu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/riteshkumarumassedu\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-29T19:35:06Z","updated_at":"2023-08-31T19:47:29Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThe huggingface dataset library specifically looks for \u2018.py\u2019 file while loading the dataset using loading script approach and it does not work with \u2018.pyc\u2019 file.\r\nWhile deploying in production, it becomes an issue when we are restricted to use only .pyc files. Is there any work around for this ?\n\n### Steps to reproduce the bug\n\n1. Create a dataset  loading script to read the custom data.\r\n2. compile the code to make sure that .pyc file is created \r\n3. Delete the loading script and re-run the code. Usually, python should make use of complied .pyc files. However, in this case, the dataset library errors out with the message that it's unable to find the data loader loading script.\n\n### Expected behavior\n\nThe code should make use of .pyc file and run without any error.\n\n### Environment info\n\nNA","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6193\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6193\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6192","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6192\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6192\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6192\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6192","id":1871911640,"node_id":"PR_kwDODunzps5ZDGnI","number":6192,"title":"Set minimal fsspec version requirement to 2023.1.0","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-08-29T15:23:41Z","updated_at":"2023-08-30T14:01:56Z","closed_at":"2023-08-30T13:51:32Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix https:\/\/github.com\/huggingface\/datasets\/issues\/6141\r\n\r\nColab installs 2023.6.0, so we should be good \ud83d\ude42\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6192\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6192\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6192","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6192","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6192.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6192.patch","merged_at":"2023-08-30T13:51:32Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6191","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6191\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6191\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6191\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6191","id":1871634840,"node_id":"PR_kwDODunzps5ZCKmv","number":6191,"title":"Add missing `revision` argument","user":{"login":"qgallouedec","id":45557362,"node_id":"MDQ6VXNlcjQ1NTU3MzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45557362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/qgallouedec","html_url":"https:\/\/github.com\/qgallouedec","followers_url":"https:\/\/api.github.com\/users\/qgallouedec\/followers","following_url":"https:\/\/api.github.com\/users\/qgallouedec\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/qgallouedec\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/qgallouedec\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/qgallouedec\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/qgallouedec\/orgs","repos_url":"https:\/\/api.github.com\/users\/qgallouedec\/repos","events_url":"https:\/\/api.github.com\/users\/qgallouedec\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/qgallouedec\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-29T13:05:04Z","updated_at":"2023-09-04T06:38:17Z","closed_at":"2023-08-31T13:50:00Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I've noticed that when you're not working on the main branch, there are sometimes errors in the files returned. After some investigation, I realized that the revision was not properly passed  everywhere. This PR proposes a fix.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6191\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6191\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6191","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6191","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6191.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6191.patch","merged_at":"2023-08-31T13:50:00Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6190","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6190\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6190\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6190\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6190","id":1871582175,"node_id":"I_kwDODunzps5vjhPf","number":6190,"title":"`Invalid user token` even when correct user token is passed!","user":{"login":"Vaibhavs10","id":18682411,"node_id":"MDQ6VXNlcjE4NjgyNDEx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/18682411?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Vaibhavs10","html_url":"https:\/\/github.com\/Vaibhavs10","followers_url":"https:\/\/api.github.com\/users\/Vaibhavs10\/followers","following_url":"https:\/\/api.github.com\/users\/Vaibhavs10\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Vaibhavs10\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Vaibhavs10\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Vaibhavs10\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Vaibhavs10\/orgs","repos_url":"https:\/\/api.github.com\/users\/Vaibhavs10\/repos","events_url":"https:\/\/api.github.com\/users\/Vaibhavs10\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Vaibhavs10\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-29T12:37:03Z","updated_at":"2023-08-29T13:01:10Z","closed_at":"2023-08-29T13:01:09Z","author_association":"MEMBER","active_lock_reason":null,"body":"### Describe the bug\n\nI'm working on a dataset which comprises other datasets on the hub.\r\nURL: https:\/\/huggingface.co\/datasets\/open-asr-leaderboard\/datasets-test-only\r\n\r\nNote: Some of the sub-datasets in this metadataset require explicit access.\r\n\r\nAll the other datasets work fine, except, `common_voice`.\n\n### Steps to reproduce the bug\n\nhttps:\/\/github.com\/Vaibhavs10\/scratchpad\/blob\/main\/cv_datasets_bug_repro.ipynb\n\n### Expected behavior\n\nIt should work if the provided access token is valid (as it does for all the other datasets)\n\n### Environment info\n\ndatasets version -> 2.14.4","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6190\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6190\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6189","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6189\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6189\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6189\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6189","id":1871569855,"node_id":"PR_kwDODunzps5ZB8Z9","number":6189,"title":"Don't alter input in Features.from_dict","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-29T12:29:47Z","updated_at":"2023-08-29T13:04:59Z","closed_at":"2023-08-29T12:52:48Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6189\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6189\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6189","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6189","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6189.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6189.patch","merged_at":"2023-08-29T12:52:48Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6188","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6188\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6188\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6188\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6188","id":1870987640,"node_id":"I_kwDODunzps5vhQF4","number":6188,"title":"[Feature Request] Check the length of batch before writing so that empty batch is allowed","user":{"login":"namespace-Pt","id":61188463,"node_id":"MDQ6VXNlcjYxMTg4NDYz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/61188463?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/namespace-Pt","html_url":"https:\/\/github.com\/namespace-Pt","followers_url":"https:\/\/api.github.com\/users\/namespace-Pt\/followers","following_url":"https:\/\/api.github.com\/users\/namespace-Pt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/namespace-Pt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/namespace-Pt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/namespace-Pt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/namespace-Pt\/orgs","repos_url":"https:\/\/api.github.com\/users\/namespace-Pt\/repos","events_url":"https:\/\/api.github.com\/users\/namespace-Pt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/namespace-Pt\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-29T06:37:34Z","updated_at":"2023-09-19T21:55:38Z","closed_at":"2023-09-19T21:55:37Z","author_association":"NONE","active_lock_reason":null,"body":"### Use Case\r\nI use `dataset.map(process_fn, batched=True)` to process the dataset, with data **augmentations or filtering**. However, when all examples within a batch is filtered out, i.e. **an empty batch is returned**, the following error will be thrown:\r\n  ```\r\n  ValueError: Schema and number of arrays unequal\r\n  ```\r\nThis is because the empty batch does not comply with the schema of other batches. I think an empty batch should be allowed to facilitate coding (one does not need to assign an empty list manually for all keys.)\r\n\r\nA simple fix is to check the length of `batch` before writing:\r\n```\r\nif len(batch):\r\n  writer.write_batch(batch)\r\n```\r\ninstead of \r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/74d60213dcbd7c99484c62ce1d3dfd90a1df0770\/src\/datasets\/arrow_dataset.py#L3493\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6188\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6188\/timeline","performed_via_github_app":null,"state_reason":"not_planned","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6187","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6187\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6187\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6187\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6187","id":1870936143,"node_id":"I_kwDODunzps5vhDhP","number":6187,"title":"Couldn't find a dataset script at \/content\/tsv\/tsv.py or any data file in the same directory","user":{"login":"andysingal","id":20493493,"node_id":"MDQ6VXNlcjIwNDkzNDkz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20493493?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/andysingal","html_url":"https:\/\/github.com\/andysingal","followers_url":"https:\/\/api.github.com\/users\/andysingal\/followers","following_url":"https:\/\/api.github.com\/users\/andysingal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/andysingal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/andysingal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/andysingal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/andysingal\/orgs","repos_url":"https:\/\/api.github.com\/users\/andysingal\/repos","events_url":"https:\/\/api.github.com\/users\/andysingal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/andysingal\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-29T05:49:56Z","updated_at":"2023-08-29T16:21:45Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n[<ipython-input-48-6a7b3e847019>](https:\/\/localhost:8080\/#) in <cell line: 7>()\r\n      5 }\r\n      6 \r\n----> 7 csv_datasets_reloaded = load_dataset(\"tsv\", data_files=data_files)\r\n      8 csv_datasets_reloaded\r\n\r\n2 frames\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py](https:\/\/localhost:8080\/#) in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1489                     raise e1 from None\r\n   1490                 if isinstance(e1, FileNotFoundError):\r\n-> 1491                     raise FileNotFoundError(\r\n   1492                         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1493                         f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n\r\nFileNotFoundError: Couldn't find a dataset script at \/content\/tsv\/tsv.py or any data file in the same directory. Couldn't find 'tsv' on the Hugging Face Hub either: FileNotFoundError: Dataset 'tsv' doesn't exist on the Hub\r\n```\n\n### Steps to reproduce the bug\n\n```\r\ndata_files = {\r\n    \"train\": \"\/content\/PUBHEALTH\/train.tsv\",\r\n    \"validation\": \"\/content\/PUBHEALTH\/dev.tsv\",\r\n    \"test\": \"\/content\/PUBHEALTH\/test.tsv\",\r\n}\r\n\r\ntsv_datasets_reloaded = load_dataset(\"tsv\", data_files=data_files)\r\ntsv_datasets_reloaded\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-48-6a7b3e847019> in <cell line: 7>()\r\n      5 }\r\n      6 \r\n----> 7 csv_datasets_reloaded = load_dataset(\"tsv\", data_files=data_files)\r\n      8 csv_datasets_reloaded\r\n\r\n2 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1489                     raise e1 from None\r\n   1490                 if isinstance(e1, FileNotFoundError):\r\n-> 1491                     raise FileNotFoundError(\r\n   1492                         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1493                         f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n\r\nFileNotFoundError: Couldn't find a dataset script at \/content\/tsv\/tsv.py or any data file in the same directory. Couldn't find 'tsv' on the Hugging Face Hub either: FileNotFoundError: Dataset 'tsv' doesn't exist on the Hub\r\n```\r\n\n\n### Expected behavior\n\nload the data, push to hub\n\n### Environment info\n\njupyter notebook RTX 3090","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6187\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6187\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6186","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6186\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6186\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6186\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6186","id":1869431457,"node_id":"I_kwDODunzps5vbUKh","number":6186,"title":"Feature request: add code example of multi-GPU processing","user":{"login":"NielsRogge","id":48327001,"node_id":"MDQ6VXNlcjQ4MzI3MDAx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/48327001?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NielsRogge","html_url":"https:\/\/github.com\/NielsRogge","followers_url":"https:\/\/api.github.com\/users\/NielsRogge\/followers","following_url":"https:\/\/api.github.com\/users\/NielsRogge\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NielsRogge\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NielsRogge\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NielsRogge\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NielsRogge\/orgs","repos_url":"https:\/\/api.github.com\/users\/NielsRogge\/repos","events_url":"https:\/\/api.github.com\/users\/NielsRogge\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NielsRogge\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"},{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-28T10:00:59Z","updated_at":"2023-11-22T15:42:20Z","closed_at":"2023-11-22T15:42:20Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\r\n\r\nWould be great to add a code example of how to do multi-GPU processing with \ud83e\udd17 Datasets in the documentation. cc @stevhliu\r\n\r\nCurrently the docs has a small [section](https:\/\/huggingface.co\/docs\/datasets\/v2.3.2\/en\/process#map) on this saying \"your big GPU call goes here\", however it didn't work for me out-of-the-box.\r\n\r\nLet's say you have a PyTorch model that can do translation, and you have multiple GPUs. In that case, you'd like to duplicate the model on each GPU, each processing (translating) a chunk of the data in parallel.\r\n\r\nHere's how I tried to do that:\r\n\r\n```\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\r\nfrom multiprocess import set_start_method\r\nimport torch\r\nimport os\r\n\r\ndataset = load_dataset(\"mlfoundations\/datacomp_small\")\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"facebook\/nllb-200-distilled-600M\")\r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook\/nllb-200-distilled-600M\")\r\n\r\n# put model on each available GPU\r\n# also, should I do it like this or use nn.DataParallel?\r\nmodel.to(\"cuda:0\")\r\nmodel.to(\"cuda:1\")\r\n\r\nset_start_method(\"spawn\")\r\n\r\ndef translate_captions(batch, rank):\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(rank % torch.cuda.device_count())\r\n    \r\n    texts = batch[\"text\"]\r\n    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\r\n\r\n    translated_tokens = model.generate(\r\n        **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"], max_length=30\r\n    )\r\n    translated_texts = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\r\n\r\n    batch[\"translated_text\"] = translated_texts\r\n    \r\n    return batch\r\n\r\nupdated_dataset = dataset.map(translate_captions, with_rank=True, num_proc=2, batched=True, batch_size=256)\r\n```\r\n\r\nI've personally tried running this script on a machine with 2 A100 GPUs.\r\n\r\n## Error 1\r\n\r\nRunning the code snippet above from the terminal (python script.py) resulted in the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/spawn.py\", line 125, in _main\r\n    prepare(preparation_data)\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/spawn.py\", line 236, in prepare\r\n    _fixup_main_from_path(data['init_main_from_path'])\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/spawn.py\", line 287, in _fixup_main_from_path\r\n    main_content = runpy.run_path(main_path,\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/runpy.py\", line 289, in run_path\r\n    return _run_module_code(code, init_globals, run_name,\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/runpy.py\", line 96, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/niels\/python_projects\/datacomp\/datasets_multi_gpu.py\", line 16, in <module>\r\n    set_start_method(\"spawn\")\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/context.py\", line 247, in set_start_method\r\n    raise RuntimeError('context has already been set')\r\nRuntimeError: context has already been set\r\n```\r\n\r\n## Error 2\r\nThen, based on [this Stackoverflow answer](https:\/\/stackoverflow.com\/a\/71616344\/7762882), I put the `set_start_method(\"spawn\")` section in a try: catch block. This resulted in the following error:\r\n```\r\nFile \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/datasets\/dataset_dict.py\", line 817, in <dictcomp>\r\n    k: dataset.map(\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2926, in map\r\n    with Pool(nb_of_missing_shards, initargs=initargs, initializer=initializer) as pool:\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/context.py\", line 119, in Pool\r\n    return Pool(processes, initializer, initargs, maxtasksperchild,\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/pool.py\", line 215, in __init__\r\n    self._repopulate_pool()\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/pool.py\", line 306, in _repopulate_pool\r\n    return self._repopulate_pool_static(self._ctx, self.Process,\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/pool.py\", line 329, in _repopulate_pool_static\r\n    w.start()\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/context.py\", line 288, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/popen_spawn_posix.py\", line 42, in _launch\r\n    prep_data = spawn.get_preparation_data(process_obj._name)\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/spawn.py\", line 154, in get_preparation_data\r\n    _check_not_importing_main()\r\n  File \"\/home\/niels\/anaconda3\/envs\/datacomp\/lib\/python3.10\/site-packages\/multiprocess\/spawn.py\", line 134, in _check_not_importing_main\r\n    raise RuntimeError('''\r\nRuntimeError: \r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == '__main__':\r\n                freeze_support()\r\n                ...\r\n\r\n        The \"freeze_support()\" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.\r\n```\r\n\r\nSo then I put the last line under a `if __name__ == '__main__':` block. Then the code snippet seemed to work, but it seemed that it's only leveraging a single GPU (based on monitoring `nvidia-smi`):\r\n\r\n```\r\nMon Aug 28 12:19:24 2023       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |\r\n| N\/A   55C    P0    76W \/ 275W |   8747MiB \/ 81920MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\r\n| N\/A   67C    P0   274W \/ 275W |  59835MiB \/ 81920MiB |    100%      Default |\r\n|                               |                      |             Disabled |\r\n```\r\n\r\nBoth GPUs should have equal GPU usage, but I've always noticed that the last GPU has way more usage than the other ones. This made me think that `os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(rank % torch.cuda.device_count())` might not work inside a Python script, especially if done after importing PyTorch?\r\n\r\n### Motivation\r\n\r\nWould be great to clarify how to do multi-GPU data processing.\r\n\r\n### Your contribution\r\n\r\nIf my code snippet can be fixed, I can contribute it to the docs :) ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6186\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6186\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6185","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6185\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6185\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6185\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6185","id":1868077748,"node_id":"I_kwDODunzps5vWJq0","number":6185,"title":"Error in saving the PIL image into *.arrow files using datasets.arrow_writer","user":{"login":"HaozheZhao","id":14247682,"node_id":"MDQ6VXNlcjE0MjQ3Njgy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14247682?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/HaozheZhao","html_url":"https:\/\/github.com\/HaozheZhao","followers_url":"https:\/\/api.github.com\/users\/HaozheZhao\/followers","following_url":"https:\/\/api.github.com\/users\/HaozheZhao\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/HaozheZhao\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/HaozheZhao\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/HaozheZhao\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/HaozheZhao\/orgs","repos_url":"https:\/\/api.github.com\/users\/HaozheZhao\/repos","events_url":"https:\/\/api.github.com\/users\/HaozheZhao\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/HaozheZhao\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-26T12:15:57Z","updated_at":"2023-08-29T14:49:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am using the ArrowWriter from datasets.arrow_writer to save a json-style file as arrow files. Within the dictionary, it contains a feature called \"image\" which is a list of PIL.Image objects.\r\nI am saving the json using the following script:\r\n```\r\ndef save_to_arrow(path,temp):\r\n    with ArrowWriter(path=path,writer_batch_size=20) as writer: \r\n        writer.write_batch(temp) \r\n        writer.finalize() \r\n```\r\nHowever, when I attempt to restore the dataset and use the ```Dataset.from_file(path)``` function to load the arrow file, there seems to be an issue with the PIL.Image object in the dataset. The list of PIL.Images appears as follows rather than a normal PIL.Image object:\r\n![1693051705440](https:\/\/github.com\/huggingface\/datasets\/assets\/14247682\/03b204c2-d0fa-4d19-beff-6f4d7b83c848)\r\n\n\n### Steps to reproduce the bug\n\n1. Storing the data json into arrow files:\r\n```\r\ndef save_to_arrow(path,temp):\r\n    with ArrowWriter(path=path,writer_batch_size=20) as writer: \r\n        writer.write_batch(temp) \r\n        writer.finalize() \r\nsave_to_arrow( path, json_file )\r\n```\r\n2.  try to load the arrow file into the Dataset object using the ```Dataset.from_file(path)```\r\n\n\n### Expected behavior\n\nExcept to saving the contained \"image\" feature as a list PIL.Image objects as the arrow file. And I can restore the dataset from the file.\n\n### Environment info\n\n\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.4.0-150-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.17\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.4.4","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6185\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6185\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6184","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6184\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6184\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6184\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6184","id":1867766143,"node_id":"I_kwDODunzps5vU9l_","number":6184,"title":"Map cache does not detect function changes in another module","user":{"login":"jonathanasdf","id":511073,"node_id":"MDQ6VXNlcjUxMTA3Mw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/511073?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jonathanasdf","html_url":"https:\/\/github.com\/jonathanasdf","followers_url":"https:\/\/api.github.com\/users\/jonathanasdf\/followers","following_url":"https:\/\/api.github.com\/users\/jonathanasdf\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jonathanasdf\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jonathanasdf\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jonathanasdf\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jonathanasdf\/orgs","repos_url":"https:\/\/api.github.com\/users\/jonathanasdf\/repos","events_url":"https:\/\/api.github.com\/users\/jonathanasdf\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jonathanasdf\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892865,"node_id":"MDU6TGFiZWwxOTM1ODkyODY1","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/duplicate","name":"duplicate","color":"cfd3d7","default":true,"description":"This issue or pull request already exists"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-25T22:59:14Z","updated_at":"2023-08-29T20:57:07Z","closed_at":"2023-08-29T20:56:49Z","author_association":"NONE","active_lock_reason":null,"body":"```python\r\n# dataset.py\r\nimport os\r\nimport datasets\r\n\r\nif not os.path.exists('\/tmp\/test.json'):\r\n  with open('\/tmp\/test.json', 'w') as file:\r\n    file.write('[{\"text\": \"hello\"}]')\r\n\r\ndef transform(example):\r\n  text = example['text']\r\n  # text += ' world'\r\n  return {'text': text}\r\n\r\ndata = datasets.load_dataset('json', data_files=['\/tmp\/test.json'], split='train')\r\ndata = data.map(transform)\r\n```\r\n\r\n```python\r\n# test.py\r\nimport dataset\r\nprint(next(iter(dataset.data)))\r\n```\r\n\r\nInitialize cache\r\n```\r\npython3 test.py\r\n# {'text': 'hello'}\r\n```\r\n\r\nEdit dataset.py and uncomment the commented line, run again\r\n```\r\npython3 test.py\r\n# {'text': 'hello'}\r\n# expected: {'text': 'hello world'}\r\n```\r\n\r\nClear cache and run again\r\n```\r\nrm -rf ~\/.cache\/huggingface\/datasets\/*\r\npython3 test.py\r\n# {'text': 'hello world'}\r\n```\r\n\r\n\r\nIf instead the two files are combined, then changes to the function are detected correctly. But it's expected when working on any realistic codebase that things will be modularized into separate files.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6184\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6184\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6183","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6183\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6183\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6183\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6183","id":1867743276,"node_id":"I_kwDODunzps5vU4As","number":6183,"title":"Load dataset with  non-existent file","user":{"login":"freQuensy23-coder","id":64750224,"node_id":"MDQ6VXNlcjY0NzUwMjI0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/64750224?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/freQuensy23-coder","html_url":"https:\/\/github.com\/freQuensy23-coder","followers_url":"https:\/\/api.github.com\/users\/freQuensy23-coder\/followers","following_url":"https:\/\/api.github.com\/users\/freQuensy23-coder\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/freQuensy23-coder\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/freQuensy23-coder\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/freQuensy23-coder\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/freQuensy23-coder\/orgs","repos_url":"https:\/\/api.github.com\/users\/freQuensy23-coder\/repos","events_url":"https:\/\/api.github.com\/users\/freQuensy23-coder\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/freQuensy23-coder\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-25T22:21:22Z","updated_at":"2023-08-29T13:26:22Z","closed_at":"2023-08-29T13:26:22Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen load a dataset from datasets and pass a wrong path to json with the data, error message does not contain something abount \"wrong path\" or \"file do not exist\"  - \r\n```SchemaInferenceError: Please pass `features` or at least one example when writing data``` \r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset('json', data_files='\/home\/alexey\/unreal_file.json')\r\n```\r\n\r\n### Expected behavior\r\n\r\nRaise os FileNotFound error or custom error with informative message\r\n\r\n### Environment info\r\n```\r\n# packages in environment at \/home\/alexey\/.conda\/envs\/alex_LoRA:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main  \r\n_openmp_mutex             5.1                       1_gnu  \r\naccelerate                0.21.0                   pypi_0    pypi\r\naiohttp                   3.8.5                    pypi_0    pypi\r\naiosignal                 1.3.1                    pypi_0    pypi\r\nantlr4-python3-runtime    4.9.3                    pypi_0    pypi\r\nappdirs                   1.4.4                    pypi_0    pypi\r\nasttokens                 2.0.5              pyhd3eb1b0_0  \r\nasync-timeout             4.0.3                    pypi_0    pypi\r\nattrs                     23.1.0                   pypi_0    pypi\r\nbackcall                  0.2.0              pyhd3eb1b0_0  \r\nbitsandbytes              0.41.1                   pypi_0    pypi\r\nbzip2                     1.0.8                h7b6447c_0  \r\nca-certificates           2023.05.30           h06a4308_0  \r\ncertifi                   2023.7.22                pypi_0    pypi\r\ncharset-normalizer        3.2.0                    pypi_0    pypi\r\nclick                     8.1.6                    pypi_0    pypi\r\ncmake                     3.27.2                   pypi_0    pypi\r\ncomm                      0.1.2           py310h06a4308_0  \r\ncontourpy                 1.1.0                    pypi_0    pypi\r\ncycler                    0.11.0                   pypi_0    pypi\r\ndatasets                  2.14.4                   pypi_0    pypi\r\ndebugpy                   1.6.7           py310h6a678d5_0  \r\ndecorator                 5.1.1              pyhd3eb1b0_0  \r\ndill                      0.3.7                    pypi_0    pypi\r\ndocker-pycreds            0.4.0                    pypi_0    pypi\r\nexecuting                 0.8.3              pyhd3eb1b0_0  \r\nfilelock                  3.12.2                   pypi_0    pypi\r\nfire                      0.5.0                    pypi_0    pypi\r\nfonttools                 4.42.0                   pypi_0    pypi\r\nfrozenlist                1.4.0                    pypi_0    pypi\r\nfsspec                    2023.6.0                 pypi_0    pypi\r\ngitdb                     4.0.10                   pypi_0    pypi\r\ngitpython                 3.1.32                   pypi_0    pypi\r\nhuggingface-hub           0.16.4                   pypi_0    pypi\r\nidna                      3.4                      pypi_0    pypi\r\nipykernel                 6.25.0          py310h2f386ee_0  \r\nipython                   8.12.2          py310h06a4308_0  \r\nipython-genutils          0.2.0                    pypi_0    pypi\r\nipywidgets                8.0.4           py310h06a4308_0  \r\njedi                      0.18.1          py310h06a4308_1  \r\njinja2                    3.1.2                    pypi_0    pypi\r\njsonschema                4.19.0                   pypi_0    pypi\r\njsonschema-specifications 2023.7.1                 pypi_0    pypi\r\njupyter_client            8.1.0           py310h06a4308_0  \r\njupyter_core              5.3.0           py310h06a4308_0  \r\njupyterlab_widgets        3.0.5           py310h06a4308_0  \r\nkiwisolver                1.4.4                    pypi_0    pypi\r\nld_impl_linux-64          2.38                 h1181459_1  \r\nlibffi                    3.3                  he6710b0_2  \r\nlibgcc-ng                 11.2.0               h1234567_1  \r\nlibgomp                   11.2.0               h1234567_1  \r\nlibsodium                 1.0.18               h7b6447c_0  \r\nlibstdcxx-ng              11.2.0               h1234567_1  \r\nlibuuid                   1.41.5               h5eee18b_0  \r\nlightning-utilities       0.9.0                    pypi_0    pypi\r\nlit                       16.0.6                   pypi_0    pypi\r\nmarkupsafe                2.1.3                    pypi_0    pypi\r\nmatplotlib                3.7.2                    pypi_0    pypi\r\nmatplotlib-inline         0.1.6           py310h06a4308_0  \r\nmpmath                    1.3.0                    pypi_0    pypi\r\nmultidict                 6.0.4                    pypi_0    pypi\r\nmultiprocess              0.70.15                  pypi_0    pypi\r\nnbformat                  4.2.0                    pypi_0    pypi\r\nncurses                   6.4                  h6a678d5_0  \r\nnest-asyncio              1.5.6           py310h06a4308_0  \r\nnetworkx                  3.1                      pypi_0    pypi\r\nnumpy                     1.25.2                   pypi_0    pypi\r\nnvidia-cublas-cu11        11.10.3.66               pypi_0    pypi\r\nnvidia-cuda-cupti-cu11    11.7.101                 pypi_0    pypi\r\nnvidia-cuda-nvrtc-cu11    11.7.99                  pypi_0    pypi\r\nnvidia-cuda-runtime-cu11  11.7.99                  pypi_0    pypi\r\nnvidia-cudnn-cu11         8.5.0.96                 pypi_0    pypi\r\nnvidia-cufft-cu11         10.9.0.58                pypi_0    pypi\r\nnvidia-curand-cu11        10.2.10.91               pypi_0    pypi\r\nnvidia-cusolver-cu11      11.4.0.1                 pypi_0    pypi\r\nnvidia-cusparse-cu11      11.7.4.91                pypi_0    pypi\r\nnvidia-nccl-cu11          2.14.3                   pypi_0    pypi\r\nnvidia-nvtx-cu11          11.7.91                  pypi_0    pypi\r\nomegaconf                 2.3.0                    pypi_0    pypi\r\nopenssl                   1.1.1v               h7f8727e_0  \r\npackaging                 23.0            py310h06a4308_0  \r\npandas                    2.0.3                    pypi_0    pypi\r\nparso                     0.8.3              pyhd3eb1b0_0  \r\npathtools                 0.1.2                    pypi_0    pypi\r\npeft                      0.4.0                    pypi_0    pypi\r\npexpect                   4.8.0              pyhd3eb1b0_3  \r\npickleshare               0.7.5           pyhd3eb1b0_1003  \r\npillow                    10.0.0                   pypi_0    pypi\r\npip                       23.2.1          py310h06a4308_0  \r\nplatformdirs              2.5.2           py310h06a4308_0  \r\nplotly                    5.16.1                   pypi_0    pypi\r\nprompt-toolkit            3.0.36          py310h06a4308_0  \r\nprotobuf                  4.24.0                   pypi_0    pypi\r\npsutil                    5.9.0           py310h5eee18b_0  \r\nptyprocess                0.7.0              pyhd3eb1b0_2  \r\npure_eval                 0.2.2              pyhd3eb1b0_0  \r\npyarrow                   12.0.1                   pypi_0    pypi\r\npygments                  2.15.1          py310h06a4308_1  \r\npyparsing                 3.0.9                    pypi_0    pypi\r\npython                    3.10.0               h12debd9_5  \r\npython-dateutil           2.8.2              pyhd3eb1b0_0  \r\npytorch-lightning         2.0.6                    pypi_0    pypi\r\npytz                      2023.3                   pypi_0    pypi\r\npyyaml                    6.0.1                    pypi_0    pypi\r\npyzmq                     25.1.0          py310h6a678d5_0  \r\nreadline                  8.2                  h5eee18b_0  \r\nreferencing               0.30.2                   pypi_0    pypi\r\nregex                     2023.8.8                 pypi_0    pypi\r\nrequests                  2.31.0                   pypi_0    pypi\r\nrpds-py                   0.9.2                    pypi_0    pypi\r\nsafetensors               0.3.2                    pypi_0    pypi\r\nscipy                     1.11.1                   pypi_0    pypi\r\nsentencepiece             0.1.99                   pypi_0    pypi\r\nsentry-sdk                1.29.2                   pypi_0    pypi\r\nsetproctitle              1.3.2                    pypi_0    pypi\r\nsetuptools                68.0.0          py310h06a4308_0  \r\nsix                       1.16.0             pyhd3eb1b0_1  \r\nsmmap                     5.0.0                    pypi_0    pypi\r\nsqlite                    3.41.2               h5eee18b_0  \r\nstack_data                0.2.0              pyhd3eb1b0_0  \r\nsympy                     1.12                     pypi_0    pypi\r\ntenacity                  8.2.3                    pypi_0    pypi\r\ntermcolor                 2.3.0                    pypi_0    pypi\r\ntk                        8.6.12               h1ccaba5_0  \r\ntokenizers                0.13.3                   pypi_0    pypi\r\ntorch                     2.0.1                    pypi_0    pypi\r\ntorchmetrics              1.0.3                    pypi_0    pypi\r\ntornado                   6.3.2           py310h5eee18b_0  \r\ntqdm                      4.66.1                   pypi_0    pypi\r\ntraitlets                 5.7.1           py310h06a4308_0  \r\ntransformers              4.31.0                   pypi_0    pypi\r\ntriton                    2.0.0                    pypi_0    pypi\r\ntyping-extensions         4.7.1                    pypi_0    pypi\r\ntzdata                    2023.3                   pypi_0    pypi\r\nurllib3                   2.0.4                    pypi_0    pypi\r\nwandb                     0.15.8                   pypi_0    pypi\r\nwcwidth                   0.2.5              pyhd3eb1b0_0  \r\nwheel                     0.38.4          py310h06a4308_0  \r\nwidgetsnbextension        4.0.5           py310h06a4308_0  \r\nxxhash                    3.3.0                    pypi_0    pypi\r\nxz                        5.4.2                h5eee18b_0  \r\nyarl                      1.9.2                    pypi_0    pypi\r\nzeromq                    4.3.4                h2531618_0  \r\nzlib                      1.2.13               h5eee18b_0  \r\n    active environment : None\r\n       user config file : \/home\/alexey\/.condarc\r\n populated config files : \r\n          conda version : 23.1.0\r\n    conda-build version : 3.22.0\r\n         python version : 3.9.13.final.0\r\n       virtual packages : __archspec=1=x86_64\r\n                          __cuda=12.0=0\r\n                          __glibc=2.35=0\r\n                          __linux=5.19.0=0\r\n                          __unix=0=0\r\n       base environment : \/opt\/anaconda\/anaconda3  (read only)\r\n      conda av data dir : \/opt\/anaconda\/anaconda3\/etc\/conda\r\n  conda av metadata url : None\r\n           channel URLs : https:\/\/repo.anaconda.com\/pkgs\/main\/linux-64\r\n                          https:\/\/repo.anaconda.com\/pkgs\/main\/noarch\r\n                          https:\/\/repo.anaconda.com\/pkgs\/r\/linux-64\r\n                          https:\/\/repo.anaconda.com\/pkgs\/r\/noarch\r\n          package cache : \/opt\/anaconda\/anaconda3\/pkgs\r\n                          \/home\/alexey\/.conda\/pkgs\r\n       envs directories : \/home\/alexey\/.conda\/envs\r\n                          \/opt\/anaconda\/anaconda3\/envs\r\n               platform : linux-64\r\n             user-agent : conda\/23.1.0 requests\/2.31.0 CPython\/3.9.13 Linux\/5.19.0-46-generic ubuntu\/22.04.2 glibc\/2.35\r\n                UID:GID : 1009:1009\r\n             netrc file : \/home\/alexey\/.netrc\r\n           offline mode : False\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6183\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":1,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6183\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6182","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6182\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6182\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6182\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6182","id":1867203131,"node_id":"I_kwDODunzps5vS0I7","number":6182,"title":"Loading Meteor metric in HF evaluate module crashes due to datasets import issue ","user":{"login":"dsashulya","id":42322648,"node_id":"MDQ6VXNlcjQyMzIyNjQ4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42322648?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dsashulya","html_url":"https:\/\/github.com\/dsashulya","followers_url":"https:\/\/api.github.com\/users\/dsashulya\/followers","following_url":"https:\/\/api.github.com\/users\/dsashulya\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dsashulya\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dsashulya\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dsashulya\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dsashulya\/orgs","repos_url":"https:\/\/api.github.com\/users\/dsashulya\/repos","events_url":"https:\/\/api.github.com\/users\/dsashulya\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dsashulya\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-25T14:54:06Z","updated_at":"2023-09-04T16:41:11Z","closed_at":"2023-08-31T14:38:23Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen using python3.9 and ```evaluate``` module loading Meteor metric crashes at a non-existent import from ```datasets.config``` in ```datasets v2.14```\n\n### Steps to reproduce the bug\n\n```\r\nfrom evaluate import load\r\nmeteor = load(\"meteor\")\r\n```\r\n\r\nproduces the following error:\r\n  ```\r\n    from datasets.config import importlib_metadata, version\r\nImportError: cannot import name 'importlib_metadata' from 'datasets.config' (<path_to_project>\/venv\/lib\/python3.9\/site-packages\/datasets\/config.py)\r\n```\n\n### Expected behavior\n\n```datasets``` of v2.10 has the following workaround in ```config.py```:\r\n\r\n```\r\nif PY_VERSION < version.parse(\"3.8\"):\r\n    import importlib_metadata\r\nelse:\r\n    import importlib.metadata as importlib_metadata\r\n```\r\n\r\nHowever, it's absent in v2.14 which might be the cause of the issue.\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: macOS-13.5-arm64-arm-64bit\r\n- Python version: 3.9.6\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n- Evaluate version: 0.4.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6182\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6182\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6181","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6181\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6181\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6181\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6181","id":1867035522,"node_id":"PR_kwDODunzps5Yy2VO","number":6181,"title":"Fix import in `image_load` doc","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-25T13:12:19Z","updated_at":"2023-08-25T16:12:46Z","closed_at":"2023-08-25T16:02:24Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Reported on [Discord](https:\/\/discord.com\/channels\/879548962464493619\/1144295822209581168\/1144295822209581168)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6181\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6181\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6181","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6181","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6181.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6181.patch","merged_at":"2023-08-25T16:02:24Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6180","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6180\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6180\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6180\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6180","id":1867032578,"node_id":"PR_kwDODunzps5Yy1r-","number":6180,"title":"Use `hf-internal-testing` repos for hosting test dataset repos","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-25T13:10:26Z","updated_at":"2023-08-25T16:58:02Z","closed_at":"2023-08-25T16:46:22Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Use `hf-internal-testing` for hosting instead of the maintainers' dataset repos.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6180\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6180\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6180","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6180","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6180.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6180.patch","merged_at":"2023-08-25T16:46:22Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6179","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6179\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6179\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6179\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6179","id":1867009016,"node_id":"I_kwDODunzps5vSEv4","number":6179,"title":"Map cache with tokenizer","user":{"login":"jonathanasdf","id":511073,"node_id":"MDQ6VXNlcjUxMTA3Mw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/511073?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jonathanasdf","html_url":"https:\/\/github.com\/jonathanasdf","followers_url":"https:\/\/api.github.com\/users\/jonathanasdf\/followers","following_url":"https:\/\/api.github.com\/users\/jonathanasdf\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jonathanasdf\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jonathanasdf\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jonathanasdf\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jonathanasdf\/orgs","repos_url":"https:\/\/api.github.com\/users\/jonathanasdf\/repos","events_url":"https:\/\/api.github.com\/users\/jonathanasdf\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jonathanasdf\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-25T12:55:18Z","updated_at":"2023-08-31T15:17:24Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Similar issue to https:\/\/github.com\/huggingface\/datasets\/issues\/5985, but across different sessions rather than two calls in the same session.\r\n\r\nUnlike that issue, explicitly calling tokenizer(my_args) before the map() doesn't help, because the tokenizer was created with a different hash to begin with...\r\n\r\nsetup\r\n```\r\nfrom transformers import AutoTokenizer\r\nAutoTokenizer.from_pretrained('bert-base-uncased').save_pretrained(\"tok\")\r\n```\r\n\r\nthis prints different value each time\r\n```\r\nfrom transformers import AutoTokenizer\r\nfrom datasets.utils.py_utils import dumps # Huggingface datasets\r\nprint(hash(dumps(AutoTokenizer.from_pretrained(\"tok\"))))\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6179\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6179\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6178","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6178\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6178\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6178\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6178","id":1866610102,"node_id":"I_kwDODunzps5vQjW2","number":6178,"title":"'import datasets' throws \"invalid syntax error\"","user":{"login":"elia-ashraf","id":128580829,"node_id":"U_kgDOB6n83Q","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/128580829?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/elia-ashraf","html_url":"https:\/\/github.com\/elia-ashraf","followers_url":"https:\/\/api.github.com\/users\/elia-ashraf\/followers","following_url":"https:\/\/api.github.com\/users\/elia-ashraf\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/elia-ashraf\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/elia-ashraf\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/elia-ashraf\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/elia-ashraf\/orgs","repos_url":"https:\/\/api.github.com\/users\/elia-ashraf\/repos","events_url":"https:\/\/api.github.com\/users\/elia-ashraf\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/elia-ashraf\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-25T08:35:14Z","updated_at":"2023-09-27T17:33:39Z","closed_at":"2023-09-27T17:33:39Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nHi,\r\n\r\nI have been trying to import the datasets library but I keep gtting this error.\r\n\r\n`Traceback (most recent call last):\r\n\r\n  File \/opt\/local\/jupyterhub\/lib64\/python3.9\/site-packages\/IPython\/core\/interactiveshell.py:3508 in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n\r\n  Cell In[2], line 1\r\n    import datasets\r\n\r\n  File \/opt\/local\/jupyterhub\/lib64\/python3.9\/site-packages\/datasets\/__init__.py:22\r\n    from .arrow_dataset import Dataset\r\n\r\n  File \/opt\/local\/jupyterhub\/lib64\/python3.9\/site-packages\/datasets\/arrow_dataset.py:67\r\n    from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n\r\n  File \/opt\/local\/jupyterhub\/lib64\/python3.9\/site-packages\/datasets\/arrow_writer.py:27\r\n    from .features import Features, Image, Value\r\n\r\n  File \/opt\/local\/jupyterhub\/lib64\/python3.9\/site-packages\/datasets\/features\/__init__.py:17\r\n    from .audio import Audio\r\n\r\n  File \/opt\/local\/jupyterhub\/lib64\/python3.9\/site-packages\/datasets\/features\/audio.py:11\r\n    from ..download.streaming_download_manager import xopen, xsplitext\r\n\r\n  File \/opt\/local\/jupyterhub\/lib64\/python3.9\/site-packages\/datasets\/download\/__init__.py:10\r\n    from .streaming_download_manager import StreamingDownloadManager\r\n\r\n  File \/opt\/local\/jupyterhub\/lib64\/python3.9\/site-packages\/datasets\/download\/streaming_download_manager.py:18\r\n    from aiohttp.client_exceptions import ClientError\r\n\r\n  File \/opt\/local\/jupyterhub\/lib64\/python3.9\/site-packages\/aiohttp\/__init__.py:7\r\n    from .connector import *  # noqa\r\n\r\n  File \/opt\/local\/jupyterhub\/lib64\/python3.9\/site-packages\/aiohttp\/connector.py:12\r\n    from .client import ClientRequest\r\n\r\n  File \/opt\/local\/jupyterhub\/lib64\/python3.9\/site-packages\/aiohttp\/client.py:144\r\n    yield from asyncio.async(resp.release(), loop=loop)\r\n                       ^\r\nSyntaxError: invalid syntax`\r\n\r\nI have simply used these commands:\r\n`import datasets`\r\nand\r\n`from datasets import load_dataset`\r\n\r\n### Environment info\r\n\r\nThe library has been installed a virtual machine on JupyterHub. Although I have used this library multiple times (on the same VM) before, to train\/test an ASR or other ML models, I had never encountered this error. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6178\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6178\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6177","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6177\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6177\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6177\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6177","id":1865490962,"node_id":"PR_kwDODunzps5Ytky-","number":6177,"title":"Use object detection images from `huggingface\/documentation-images`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-24T16:16:09Z","updated_at":"2023-08-25T16:30:00Z","closed_at":"2023-08-25T16:21:17Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6177\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6177\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6177","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6177","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6177.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6177.patch","merged_at":"2023-08-25T16:21:17Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6176","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6176\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6176\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6176\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6176","id":1864436408,"node_id":"I_kwDODunzps5vIQq4","number":6176,"title":"how to limit the size of memory mapped file?","user":{"login":"williamium3000","id":47763855,"node_id":"MDQ6VXNlcjQ3NzYzODU1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47763855?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/williamium3000","html_url":"https:\/\/github.com\/williamium3000","followers_url":"https:\/\/api.github.com\/users\/williamium3000\/followers","following_url":"https:\/\/api.github.com\/users\/williamium3000\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/williamium3000\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/williamium3000\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/williamium3000\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/williamium3000\/orgs","repos_url":"https:\/\/api.github.com\/users\/williamium3000\/repos","events_url":"https:\/\/api.github.com\/users\/williamium3000\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/williamium3000\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-08-24T05:33:45Z","updated_at":"2023-10-11T06:00:10Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHuggingface datasets use memory-mapped file to map large datasets in memory for fast access.\r\nHowever, it seems like huggingface will occupy all the memory for memory-mapped files, which makes a troublesome situation since we cluster will distribute a small portion of memory to me (once it's over the limit, memory cannot be allocated), however, when the dataset checks the total memory, all of the memory will be taken into account which makes huggingface dataset try to allocate more memory than allowed. \r\nSo is there a way to explicitly limit the size of memory mapped file?\n\n### Steps to reproduce the bug\n\npython\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"c4\", \"en\", streaming=True)\n\n### Expected behavior\n\nIn a normal environment, this will not have any problem.\r\nHowever, when the system allocates a portion of the memory to the program and when the dataset checks the total memory, all of the memory will be taken into account which makes huggingface dataset try to allocate more memory than allowed. \n\n### Environment info\n\nlinux cluster with SGE\uff08Sun Grid Engine\uff09","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6176\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6176\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6175","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6175\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6175\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6175\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6175","id":1863592678,"node_id":"PR_kwDODunzps5YnKlx","number":6175,"title":"PyArrow 13 CI fixes","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-23T15:45:53Z","updated_at":"2023-08-25T13:15:59Z","closed_at":"2023-08-25T13:06:52Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fixes:\r\n* bumps the PyArrow version check in the `cast_array_to_feature` to avoid the offset bug (still not fixed)\r\n* aligns the Pandas formatting tests with the Numpy ones (the current test fails due to https:\/\/github.com\/apache\/arrow\/pull\/35656, which requires `.to_pandas(coerce_temporal_nanoseconds=True)` to always return `datetime [ns]` objects)\r\n\r\nFix #6173 \r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6175\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6175\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6175","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6175","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6175.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6175.patch","merged_at":"2023-08-25T13:06:52Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6173","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6173\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6173\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6173\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6173","id":1863422065,"node_id":"I_kwDODunzps5vEZBx","number":6173,"title":"Fix CI for pyarrow 13.0.0","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-08-23T14:11:20Z","updated_at":"2023-08-25T13:06:53Z","closed_at":"2023-08-25T13:06:53Z","author_association":"MEMBER","active_lock_reason":null,"body":"pyarrow 13.0.0 just came out\r\n\r\n```\r\nFAILED tests\/test_formatting.py::ArrowExtractorTest::test_pandas_extractor - AssertionError: Attributes of Series are different\r\n\r\nAttribute \"dtype\" are different\r\n[left]:  datetime64[us, UTC]\r\n[right]: datetime64[ns, UTC]\r\n```\r\n\r\n```\r\nFAILED tests\/test_table.py::test_cast_sliced_fixed_size_array_to_features - TypeError: Couldn't cast array of type\r\nfixed_size_list<item: int32>[3]\r\nto\r\nSequence(feature=Value(dtype='int64', id=None), length=3, id=None)\r\n```\r\n\r\ne.g. in https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/5952253963\/job\/16143847230\r\n\r\nfirst error may be related to https:\/\/github.com\/apache\/arrow\/issues\/33321\r\n\r\nsecond one maybe because `feature.length * len(array) == len(array_values)` is not satisfied anymore somehow ?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6173\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6173\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6172","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6172\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6172\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6172\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6172","id":1863318027,"node_id":"I_kwDODunzps5vD_oL","number":6172,"title":"Make Dataset streaming queries retryable","user":{"login":"rojagtap","id":42299342,"node_id":"MDQ6VXNlcjQyMjk5MzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42299342?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rojagtap","html_url":"https:\/\/github.com\/rojagtap","followers_url":"https:\/\/api.github.com\/users\/rojagtap\/followers","following_url":"https:\/\/api.github.com\/users\/rojagtap\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rojagtap\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rojagtap\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rojagtap\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rojagtap\/orgs","repos_url":"https:\/\/api.github.com\/users\/rojagtap\/repos","events_url":"https:\/\/api.github.com\/users\/rojagtap\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rojagtap\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-23T13:15:38Z","updated_at":"2023-11-06T13:54:16Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nStreaming datasets, as intended, do not load the entire dataset in memory or disk. However, while querying the next data chunk from the remote, sometimes it is possible that the service is down or there might be other issues that may cause the query to fail. In such a scenario, it would be nice to make these queries retryable (perhaps with a backoff strategy).\n\n### Motivation\n\nI was working on a model and the model checkpoints after every 1000 steps. At step 1800 I got a 504 HTTP status code error from Huggingface hub for my pytorch `dataloader`. Given the size of my model and data, it took around 2 hours to reach 1800 steps and now it will take about an hour to recover the lost 800. It would be better to get a retryable querying strategy.\n\n### Your contribution\n\nIt would be better if someone having experience in this area takes this up as this would require some testing.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6172\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6172\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6171","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6171\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6171\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6171\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6171","id":1862922767,"node_id":"PR_kwDODunzps5Yk4AS","number":6171,"title":"Fix typo in about_mapstyle_vs_iterable.mdx","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-23T09:21:11Z","updated_at":"2023-08-23T09:32:59Z","closed_at":"2023-08-23T09:21:19Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6171\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6171\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6171","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6171","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6171.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6171.patch","merged_at":"2023-08-23T09:21:19Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6170","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6170\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6170\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6170\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6170","id":1862705731,"node_id":"PR_kwDODunzps5YkJOV","number":6170,"title":"feat: Return the name of the currently loaded file","user":{"login":"Amitesh-Patel","id":124021133,"node_id":"U_kgDOB2RpjQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/124021133?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Amitesh-Patel","html_url":"https:\/\/github.com\/Amitesh-Patel","followers_url":"https:\/\/api.github.com\/users\/Amitesh-Patel\/followers","following_url":"https:\/\/api.github.com\/users\/Amitesh-Patel\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Amitesh-Patel\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Amitesh-Patel\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Amitesh-Patel\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Amitesh-Patel\/orgs","repos_url":"https:\/\/api.github.com\/users\/Amitesh-Patel\/repos","events_url":"https:\/\/api.github.com\/users\/Amitesh-Patel\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Amitesh-Patel\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-23T07:08:17Z","updated_at":"2023-08-29T12:41:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Added an optional parameter return_file_name in the load_dataset function. When it is set to True, the function will include the name of the file corresponding to the current line as a feature in the returned output.\r\n\r\nI added this here https:\/\/github.com\/huggingface\/datasets\/blob\/main\/src\/datasets\/packaged_modules\/json\/json.py#L92.\r\n\r\nfixes #5806","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6170\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6170\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6170","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6170","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6170.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6170.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6169","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6169\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6169\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6169\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6169","id":1862360199,"node_id":"I_kwDODunzps5vAVyH","number":6169,"title":"Configurations in yaml not working","user":{"login":"tsor13","id":45085098,"node_id":"MDQ6VXNlcjQ1MDg1MDk4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45085098?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/tsor13","html_url":"https:\/\/github.com\/tsor13","followers_url":"https:\/\/api.github.com\/users\/tsor13\/followers","following_url":"https:\/\/api.github.com\/users\/tsor13\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/tsor13\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/tsor13\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/tsor13\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/tsor13\/orgs","repos_url":"https:\/\/api.github.com\/users\/tsor13\/repos","events_url":"https:\/\/api.github.com\/users\/tsor13\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/tsor13\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-23T00:13:22Z","updated_at":"2023-08-23T15:35:31Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Dataset configurations cannot be created in YAML\/README\r\n\r\nHello! I'm trying to follow the docs here in order to create structure in my dataset as added from here (#5331): https:\/\/github.com\/huggingface\/datasets\/blob\/8b8e6ee067eb74e7965ca2a6768f15f9398cb7c8\/docs\/source\/repository_structure.mdx#L110-L118\r\n\r\nI have the exact example in my config file for [my data repo](https:\/\/huggingface.co\/datasets\/tsor13\/test):\r\n```\r\nconfigs:\r\n- config_name: main_data\r\n  data_files: \"main_data.csv\"\r\n- config_name: additional_data\r\n  data_files: \"additional_data.csv\"\r\n```\r\n\r\nYet, I'm unable to load different configurations:\r\n```\r\nfrom datasets import get_dataset_config_names\r\nget_dataset_config_names('tsor13\/test', use_auth_token=True)\r\n```\r\nreturns a single split, `['tsor13--test']`\r\n\r\nDoes anyone have any insights?\r\n\r\n@polinaeterna thank you for adding this feature, it is super useful. Do you happen to have any ideas?\r\n\r\n### Steps to reproduce the bug\r\n\r\nfrom datasets import get_dataset_config_names\r\nget_dataset_config_names('tsor13\/test')\r\n\r\n### Expected behavior\r\n\r\nI would expect there to be two splits, `main_data` and `additional_data`. However, only `['tsor13--test']` test is returned.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.4\r\n- Platform: macOS-13.4-arm64-arm-64bit\r\n- Python version: 3.11.4\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6169\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6169\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6168","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6168\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6168\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6168\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6168","id":1861867274,"node_id":"PR_kwDODunzps5YhT7Y","number":6168,"title":"Fix ArrayXD YAML conversion","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-08-22T17:02:54Z","updated_at":"2023-12-12T15:06:59Z","closed_at":"2023-12-12T15:00:43Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Replace the `shape` tuple with a list in the `ArrayXD` YAML conversion. \r\n\r\nFix #6112 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6168\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6168\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6168","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6168","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6168.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6168.patch","merged_at":"2023-12-12T15:00:43Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6167","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6167\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6167\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6167\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6167","id":1861474327,"node_id":"PR_kwDODunzps5Yf9-t","number":6167,"title":"Allow hyphen in split name","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-08-22T13:30:59Z","updated_at":"2023-08-22T15:39:24Z","closed_at":"2023-08-22T15:38:53Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"To fix https:\/\/discuss.huggingface.co\/t\/error-when-setting-up-the-dataset-viewer-streamingrowserror\/51276.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6167\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6167\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6167","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6167","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6167.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6167.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6166","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6166\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6166\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6166\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6166","id":1861259055,"node_id":"PR_kwDODunzps5YfOt0","number":6166,"title":"Document BUILDER_CONFIG_CLASS","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-22T11:27:41Z","updated_at":"2023-08-23T14:01:25Z","closed_at":"2023-08-23T13:52:36Z","author_association":"MEMBER","active_lock_reason":null,"body":"Related to https:\/\/github.com\/huggingface\/datasets\/issues\/6130","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6166\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6166\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6166","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6166","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6166.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6166.patch","merged_at":"2023-08-23T13:52:36Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6165","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6165\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6165\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6165\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6165","id":1861124284,"node_id":"PR_kwDODunzps5YexBL","number":6165,"title":"Fix multiprocessing with spawn in iterable datasets","user":{"login":"Hubert-Bonisseur","id":48770768,"node_id":"MDQ6VXNlcjQ4NzcwNzY4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/48770768?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur","html_url":"https:\/\/github.com\/Hubert-Bonisseur","followers_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/followers","following_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/orgs","repos_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/repos","events_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-08-22T10:07:23Z","updated_at":"2023-08-29T13:27:14Z","closed_at":"2023-08-29T13:18:11Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The \"Spawn\" method is preferred when multiprocessing on macOS or Windows systems, instead of the \"Fork\" method on linux systems.\r\n\r\nThis causes some methods of Iterable Datasets to break when using a dataloader with more than 0 workers.\r\n\r\nI fixed the issue by replacing lambda and local methods which are not pickle-able.\r\n\r\nSee the example below:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom torch.utils.data import DataLoader\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    dataset = load_dataset(\"lhoestq\/demo1\", split=\"train\")\r\n    dataset = dataset.to_iterable_dataset(num_shards=3)\r\n\r\n    dataset = dataset.remove_columns([\"package_name\"])\r\n    dataset = dataset.rename_columns({\r\n        \"review\": \"review1\"\r\n    })\r\n    dataset = dataset.rename_column(\"date\", \"date1\")\r\n    for sample in DataLoader(dataset, batch_size=None, num_workers=3):\r\n        print(sample)\r\n```\r\n\r\nTo notice the fix on a linux system, adding these lines should do the trick:\r\n\r\n```python\r\nimport multiprocessing\r\nmultiprocessing.set_start_method('spawn')\r\n```\r\n\r\nI also removed what looks like code duplication between rename_colums and rename_column\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6165\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6165\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6165","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6165","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6165.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6165.patch","merged_at":"2023-08-29T13:18:11Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6164","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6164\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6164\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6164\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6164","id":1859560007,"node_id":"PR_kwDODunzps5YZZAJ","number":6164,"title":"Fix: Missing a MetadataConfigs init when the repo has a `datasets_info.json` but no README","user":{"login":"clefourrier","id":22726840,"node_id":"MDQ6VXNlcjIyNzI2ODQw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22726840?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/clefourrier","html_url":"https:\/\/github.com\/clefourrier","followers_url":"https:\/\/api.github.com\/users\/clefourrier\/followers","following_url":"https:\/\/api.github.com\/users\/clefourrier\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/clefourrier\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/clefourrier\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/clefourrier\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/clefourrier\/orgs","repos_url":"https:\/\/api.github.com\/users\/clefourrier\/repos","events_url":"https:\/\/api.github.com\/users\/clefourrier\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/clefourrier\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-21T14:57:54Z","updated_at":"2023-08-21T16:27:05Z","closed_at":"2023-08-21T16:18:26Z","author_association":"MEMBER","active_lock_reason":null,"body":"When I try to push to an arrow repo (can provide the link on Slack), it uploads the files but fails to update the metadata, with\r\n```\r\n  File \"app.py\", line 123, in add_new_eval\r\n    eval_results[level].push_to_hub(my_repo, token=TOKEN, split=SPLIT)\r\n  File \"blabla_my_env_path\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 5501, in push_to_hub\r\n    if not metadata_configs:\r\nUnboundLocalError: local variable 'metadata_configs' referenced before assignment\r\n```\r\n\r\nThis fixes it.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6164\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6164\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6164","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6164","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6164.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6164.patch","merged_at":"2023-08-21T16:18:26Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6163","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6163\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6163\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6163\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6163","id":1857682241,"node_id":"I_kwDODunzps5uuftB","number":6163,"title":"Error type: ArrowInvalid Details: Failed to parse string: '[254,254]' as a scalar of type int32","user":{"login":"shishirCTC","id":90616801,"node_id":"MDQ6VXNlcjkwNjE2ODAx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/90616801?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/shishirCTC","html_url":"https:\/\/github.com\/shishirCTC","followers_url":"https:\/\/api.github.com\/users\/shishirCTC\/followers","following_url":"https:\/\/api.github.com\/users\/shishirCTC\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/shishirCTC\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/shishirCTC\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/shishirCTC\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/shishirCTC\/orgs","repos_url":"https:\/\/api.github.com\/users\/shishirCTC\/repos","events_url":"https:\/\/api.github.com\/users\/shishirCTC\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/shishirCTC\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-19T11:34:40Z","updated_at":"2023-08-21T13:28:16Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am getting the following error while I am trying to upload the CSV sheet to train a model. My CSV sheet content is exactly same as shown in the example CSV file in the Auto Train page. Attaching screenshot of error for reference. I have also tried converting the index of the answer that are integer into string by placing inverted commas and also without inverted commas. \r\nCan anyone please help me out? \r\nFYI : I am using Chrome browser.\r\n\r\nError type: ArrowInvalid\r\nDetails: Failed to parse string: '[254,254]' as a scalar of type int32\r\n\r\n![Screenshot 2023-08-19 165827](https:\/\/github.com\/huggingface\/datasets\/assets\/90616801\/95fad96e-7dce-4bb5-9f83-9f1659a32891)\r\n\n\n### Steps to reproduce the bug\n\nKindly let me know how to fix this?\n\n### Expected behavior\n\nKindly let me know how to fix this?\n\n### Environment info\n\nKindly let me know how to fix this?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6163\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6163\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6162","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6162\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6162\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6162\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6162","id":1856198342,"node_id":"I_kwDODunzps5uo1bG","number":6162,"title":"load_dataset('json',...) from togethercomputer\/RedPajama-Data-1T errors when jsonl rows contains different data fields","user":{"login":"rbrugaro","id":82971690,"node_id":"MDQ6VXNlcjgyOTcxNjkw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/82971690?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rbrugaro","html_url":"https:\/\/github.com\/rbrugaro","followers_url":"https:\/\/api.github.com\/users\/rbrugaro\/followers","following_url":"https:\/\/api.github.com\/users\/rbrugaro\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rbrugaro\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rbrugaro\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rbrugaro\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rbrugaro\/orgs","repos_url":"https:\/\/api.github.com\/users\/rbrugaro\/repos","events_url":"https:\/\/api.github.com\/users\/rbrugaro\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rbrugaro\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-18T07:19:39Z","updated_at":"2023-08-18T17:00:35Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen loading some jsonl from redpajama-data-1T github source [togethercomputer\/RedPajama-Data-1T](https:\/\/huggingface.co\/datasets\/togethercomputer\/RedPajama-Data-1T) fails due to one row of the file containing an extra field called **symlink_target: string>**. \r\nWhen deleting that line the loading is successful. \r\n\r\nWe also tried loading this file with the discrepancy using this function and it is successful\r\n```python\r\nos.environ[\"RED_PAJAMA_DATA_DIR\"] =\"\/path_to_local_copy_of_RedPajama-Data-1T\"\r\nds = load_dataset('togethercomputer\/RedPajama-Data-1T', 'github',cache_dir=\"\/path_to_folder_with_jsonl\",streaming=True)['train']\r\n```\n\n### Steps to reproduce the bug\n\nSteps to reproduce the behavior:\r\n1. Load one jsonl from the redpajama-data-1T\r\n```bash \r\nwget https:\/\/data.together.xyz\/redpajama-data-1T\/v1.0.0\/github\/filtered_27f05c041a1c401783f90b9415e40e4b.sampled.jsonl\r\n```\r\n\r\n2.Load dataset will give error:\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset('json', data_files='\/path_to\/filtered_27f05c041a1c401783f90b9415e40e4b.sampled.jsonl')\r\n```\r\n\r\n_TypeError: Couldn't cast array of type\r\nStruct\r\n<content_hash: string, \r\ntimestamp: string, \r\nsource: string, \r\nline_count: int64, \r\nmax_line_length: int64,\r\navg_line_length: double, \r\nalnum_prop: double, \r\nrepo_name: string, \r\nid: string, \r\nsize: string, \r\nbinary: bool, \r\ncopies: string, \r\nref: string, \r\npath: string, \r\nmode: string, \r\nlicense: string, \r\nlanguage: list<item: struct<name: string, bytes: string>>, **symlink_target: string>**\r\nto\r\n{'content_hash': Value(dtype='string', id=None), \r\n'timestamp': Value(dtype='string', id=None), \r\n'source': Value(dtype='string', id=None), \r\n'line_count': Value(dtype='int64', id=None), \r\n'max_line_length': Value(dtype='int64', id=None), \r\n'avg_line_length': Value(dtype='float64', id=None), \r\n'alnum_prop': Value(dtype='float64', id=None), \r\n'repo_name': Value(dtype='string', id=None), \r\n'id': Value(dtype='string', id=None), \r\n'size': Value(dtype='string', id=None), \r\n'binary': Value(dtype='bool', id=None), \r\n'copies': Value(dtype='string', id=None), \r\n'ref': Value(dtype='string', id=None), \r\n'path': Value(dtype='string', id=None), \r\n'mode': Value(dtype='string', id=None), \r\n'license': Value(dtype='string', id=None), \r\n'language': [{'name': Value(dtype='string', id=None), 'bytes': Value(dtype='string', id=None)}]}_\r\n\r\n3. To remove the line causing the problem that includes the **symlink_target: string>** do:\r\n```bash \r\nsed -i '112252d' filtered_27f05c041a1c401783f90b9415e40e4b.sampled.jsonl\r\n```\r\n4. Rerun the loading function now is succesful:\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset('json', data_files='\/path_to\/filtered_27f05c041a1c401783f90b9415e40e4b.sampled.jsonl')\r\n```\n\n### Expected behavior\n\nHave a clean dataset without discrepancies on the jsonl fields or have the load_dataset('json',...) method not error out. \r\n\n\n### Environment info\n\n- `datasets` version: 2.14.1\r\n- Platform: Linux-4.18.0-425.13.1.el8_7.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.9.17\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6162\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6162\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6161","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6161\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6161\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6161\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6161","id":1855794354,"node_id":"PR_kwDODunzps5YM0g7","number":6161,"title":"Fix protocol prefix for Beam","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-17T22:40:37Z","updated_at":"2023-08-18T13:47:59Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6147","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6161\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6161\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6161","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6161","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6161.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6161.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6160","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6160\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6160\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6160\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6160","id":1855760543,"node_id":"PR_kwDODunzps5YMtLQ","number":6160,"title":"Fix Parquet loading with `columns`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-17T21:58:24Z","updated_at":"2023-08-17T22:44:59Z","closed_at":"2023-08-17T22:36:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6149 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6160\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6160\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6160","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6160","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6160.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6160.patch","merged_at":"2023-08-17T22:36:04Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6159","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6159\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6159\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6159\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6159","id":1855691512,"node_id":"I_kwDODunzps5um5r4","number":6159,"title":"Add `BoundingBox` feature","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-08-17T20:49:51Z","updated_at":"2023-08-17T20:49:51Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"... to make working with object detection datasets easier. Currently, `Sequence(int_or_float, length=4)` can be used to represent this feature optimally (in the storage backend), so I only see this feature being useful if we make it work with the viewer. Also, bounding boxes usually come in 4 different formats (explained [here](https:\/\/albumentations.ai\/docs\/getting_started\/bounding_boxes_augmentation\/)), so we need to decide which one to support (or maybe all of them).\r\n\r\ncc @NielsRogge @severo","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6159\/reactions","total_count":2,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6159\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6158","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6158\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6158\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6158\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6158","id":1855374220,"node_id":"PR_kwDODunzps5YLZBf","number":6158,"title":"[docs] Complete `to_iterable_dataset`","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-17T17:02:11Z","updated_at":"2023-08-17T19:24:20Z","closed_at":"2023-08-17T19:13:15Z","author_association":"MEMBER","active_lock_reason":null,"body":"Finishes the `to_iterable_dataset` documentation by adding it to the relevant sections in the tutorial and guide.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6158\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6158\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6158","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6158","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6158.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6158.patch","merged_at":"2023-08-17T19:13:15Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6157","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6157\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6157\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6157\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6157","id":1855265663,"node_id":"I_kwDODunzps5ulRt_","number":6157,"title":"DatasetInfo.__init__() got an unexpected keyword argument '_column_requires_decoding'","user":{"login":"aihao2000","id":51043929,"node_id":"MDQ6VXNlcjUxMDQzOTI5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/51043929?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/aihao2000","html_url":"https:\/\/github.com\/aihao2000","followers_url":"https:\/\/api.github.com\/users\/aihao2000\/followers","following_url":"https:\/\/api.github.com\/users\/aihao2000\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/aihao2000\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/aihao2000\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/aihao2000\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/aihao2000\/orgs","repos_url":"https:\/\/api.github.com\/users\/aihao2000\/repos","events_url":"https:\/\/api.github.com\/users\/aihao2000\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/aihao2000\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2023-08-17T15:48:11Z","updated_at":"2023-09-27T17:36:14Z","closed_at":"2023-09-27T17:36:14Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen I was in load_dataset, it said \"DatasetInfo.__init__() got an unexpected keyword argument '_column_requires_decoding'\". The second time I ran it, there was no error and the dataset object worked\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[3], line 1\r\n----> 1 dataset = load_dataset(\r\n      2     \"\/home\/aihao\/workspace\/DeepLearningContent\/datasets\/manga\",\r\n      3     data_dir=\"\/home\/aihao\/workspace\/DeepLearningContent\/datasets\/manga\",\r\n      4     split=\"train\",\r\n      5 )\r\n\r\nFile [~\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/load.py:2146](https:\/\/vscode-remote+ssh-002dremote-002bhome.vscode-resource.vscode-cdn.net\/home\/aihao\/workspace\/DeepLearningContent\/datasets\/~\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/load.py:2146), in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2142 # Build dataset for splits\r\n   2143 keep_in_memory = (\r\n   2144     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   2145 )\r\n-> 2146 ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n   2147 # Rename and cast features to match task schema\r\n   2148 if task is not None:\r\n   2149     # To avoid issuing the same warning twice\r\n\r\nFile [~\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/builder.py:1190](https:\/\/vscode-remote+ssh-002dremote-002bhome.vscode-resource.vscode-cdn.net\/home\/aihao\/workspace\/DeepLearningContent\/datasets\/~\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/builder.py:1190), in DatasetBuilder.as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\r\n   1187 verification_mode = VerificationMode(verification_mode or VerificationMode.BASIC_CHECKS)\r\n   1189 # Create a dataset for each of the given splits\r\n-> 1190 datasets = map_nested(\r\n   1191     partial(\r\n   1192         self._build_single_dataset,\r\n...\r\nFile [~\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/info.py:379](https:\/\/vscode-remote+ssh-002dremote-002bhome.vscode-resource.vscode-cdn.net\/home\/aihao\/workspace\/DeepLearningContent\/datasets\/~\/miniconda3\/envs\/torch\/lib\/python3.11\/site-packages\/datasets\/info.py:379), in DatasetInfo.copy(self)\r\n    378 def copy(self) -> \"DatasetInfo\":\r\n--> 379     return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})\r\n\r\nTypeError: DatasetInfo.__init__() got an unexpected keyword argument '_column_requires_decoding'\r\n```\r\n### Steps to reproduce the bug\r\n\/home\/aihao\/workspace\/DeepLearningContent\/datasets\/images\/images.py\r\n```python\r\nfrom logging import config\r\nimport datasets\r\nimport os\r\nfrom PIL import Image\r\nimport csv\r\nimport json\r\n\r\n\r\nclass ImagesConfig(datasets.BuilderConfig):\r\n    def __init__(self, **kwargs):\r\n        super(ImagesConfig, self).__init__(**kwargs)\r\n\r\n\r\nclass Images(datasets.GeneratorBasedBuilder):\r\n    def _split_generators(self, dl_manager: datasets.DownloadManager):\r\n        return [\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TRAIN,\r\n                gen_kwargs={\"split\": datasets.Split.TRAIN},\r\n            )\r\n        ]\r\n\r\n    BUILDER_CONFIGS = [\r\n        ImagesConfig(\r\n            name=\"similar_pairs\",\r\n            description=\"simliar pair dataset,item is a pair of similar images\",\r\n        ),\r\n        ImagesConfig(\r\n            name=\"image_prompt_pairs\",\r\n            description=\"image prompt pairs\",\r\n        ),\r\n    ]\r\n\r\n    def _info(self):\r\n        if self.config.name == \"similar_pairs\":\r\n            return datasets.Features(\r\n                {\r\n                    \"image1\": datasets.features.Image(),\r\n                    \"image2\": datasets.features.Image(),\r\n                    \"similarity\": datasets.Value(\"float32\"),\r\n                }\r\n            )\r\n        elif self.config.name == \"image_prompt_pairs\":\r\n            return datasets.Features(\r\n                {\"image\": datasets.features.Image(), \"prompt\": datasets.Value(\"string\")}\r\n            )\r\n\r\n    def _generate_examples(self, split):\r\n        data_path = os.path.join(self.config.data_dir, \"data\")\r\n        if self.config.name == \"similar_pairs\":\r\n            prompts = {}\r\n            with open(os.path.join(data_path ,\"prompts.json\"), \"r\") as f:\r\n                prompts = json.load(f)\r\n            with open(os.path.join(data_path, \"similar_pairs.csv\"), \"r\") as f:\r\n                reader = csv.reader(f)\r\n                for row in reader:\r\n                    image1_path, image2_path, similarity = row\r\n                    yield image1_path + \":\" + image2_path + \":\", {\r\n                        \"image1\": Image.open(image1_path),\r\n                        \"prompt1\": prompts[image1_path],\r\n                        \"image2\": Image.open(image2_path),\r\n                        \"prompt2\": prompts[image2_path],\r\n                        \"similarity\": float(similarity),\r\n                    }\r\n```\r\n\r\nCode that indicates an error:\r\n```python\r\nfrom datasets import load_dataset\r\nimport json\r\nimport csv\r\nimport ast\r\nimport torch\r\ndata_dir = \"\/home\/aihao\/workspace\/DeepLearningContent\/datasets\/images\"\r\ndataset = load_dataset(data_dir, data_dir=data_dir, name=\"similar_pairs\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe first execution gives an error, but it works fine\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.3\r\n- Platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.4\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6157\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6157\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6156","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6156\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6156\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6156\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6156","id":1854768618,"node_id":"I_kwDODunzps5ujYXq","number":6156,"title":"Why not use self._epoch as seed to shuffle in distributed training with IterableDataset","user":{"login":"npuichigo","id":11533479,"node_id":"MDQ6VXNlcjExNTMzNDc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11533479?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/npuichigo","html_url":"https:\/\/github.com\/npuichigo","followers_url":"https:\/\/api.github.com\/users\/npuichigo\/followers","following_url":"https:\/\/api.github.com\/users\/npuichigo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/npuichigo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/npuichigo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/npuichigo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/npuichigo\/orgs","repos_url":"https:\/\/api.github.com\/users\/npuichigo\/repos","events_url":"https:\/\/api.github.com\/users\/npuichigo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/npuichigo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-17T10:58:20Z","updated_at":"2023-08-17T14:33:15Z","closed_at":"2023-08-17T14:33:14Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nCurrently, distributed training with `IterableDataset` needs to pass fixed seed to shuffle to keep each node use the same seed to avoid overlapping.\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/a7f8d9019e7cb104eac4106bdc6ec0292f0dc61a\/src\/datasets\/iterable_dataset.py#L1174-L1177\r\n\r\nMy question is why not directly use `self._epoch` which is set by `set_epoch` as seed? It's almost the same across nodes.\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/a7f8d9019e7cb104eac4106bdc6ec0292f0dc61a\/src\/datasets\/iterable_dataset.py#L1790-L1801\r\n\r\nIf not using `self._epoch` as shuffling seed, what does this method do to prepare an epoch seeded generator?\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/a7f8d9019e7cb104eac4106bdc6ec0292f0dc61a\/src\/datasets\/iterable_dataset.py#L1206\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nAs mentioned above.\r\n\r\n### Expected behavior\r\n\r\nAs mentioned above.\r\n\r\n### Environment info\r\n\r\nNot related","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6156\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6156\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6155","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6155\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6155\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6155\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6155","id":1854661682,"node_id":"PR_kwDODunzps5YI8Pc","number":6155,"title":"Raise FileNotFoundError when passing data_files that don't exist","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-08-17T09:49:48Z","updated_at":"2023-08-18T13:45:58Z","closed_at":"2023-08-18T13:35:13Z","author_association":"MEMBER","active_lock_reason":null,"body":"e.g. when running `load_dataset(\"parquet\", data_files=\"doesnt_exist.parquet\")`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6155\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6155\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6155","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6155","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6155.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6155.patch","merged_at":"2023-08-18T13:35:13Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6154","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6154\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6154\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6154\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6154","id":1854595943,"node_id":"PR_kwDODunzps5YItlH","number":6154,"title":"Use yaml instead of get data patterns when possible","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-08-17T09:17:05Z","updated_at":"2023-08-17T20:46:25Z","closed_at":"2023-08-17T20:37:19Z","author_association":"MEMBER","active_lock_reason":null,"body":"This would make the data files resolution faster: no need to list all the data files to infer the dataset builder to use.\r\n\r\nfix https:\/\/github.com\/huggingface\/datasets\/issues\/6140","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6154\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6154\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6154","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6154","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6154.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6154.patch","merged_at":"2023-08-17T20:37:19Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6152","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6152\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6152\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6152\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6152","id":1852494646,"node_id":"I_kwDODunzps5uatM2","number":6152,"title":"FolderBase Dataset automatically resolves under current directory when data_dir is not specified","user":{"login":"npuichigo","id":11533479,"node_id":"MDQ6VXNlcjExNTMzNDc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11533479?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/npuichigo","html_url":"https:\/\/github.com\/npuichigo","followers_url":"https:\/\/api.github.com\/users\/npuichigo\/followers","following_url":"https:\/\/api.github.com\/users\/npuichigo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/npuichigo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/npuichigo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/npuichigo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/npuichigo\/orgs","repos_url":"https:\/\/api.github.com\/users\/npuichigo\/repos","events_url":"https:\/\/api.github.com\/users\/npuichigo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/npuichigo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892877,"node_id":"MDU6TGFiZWwxOTM1ODkyODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/good%20first%20issue","name":"good first issue","color":"7057ff","default":true,"description":"Good for newcomers"}],"state":"open","locked":false,"assignee":{"login":"debrupf2946","id":126772439,"node_id":"U_kgDOB45k1w","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/126772439?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/debrupf2946","html_url":"https:\/\/github.com\/debrupf2946","followers_url":"https:\/\/api.github.com\/users\/debrupf2946\/followers","following_url":"https:\/\/api.github.com\/users\/debrupf2946\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/debrupf2946\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/debrupf2946\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/debrupf2946\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/debrupf2946\/orgs","repos_url":"https:\/\/api.github.com\/users\/debrupf2946\/repos","events_url":"https:\/\/api.github.com\/users\/debrupf2946\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/debrupf2946\/received_events","type":"User","site_admin":false},"assignees":[{"login":"debrupf2946","id":126772439,"node_id":"U_kgDOB45k1w","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/126772439?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/debrupf2946","html_url":"https:\/\/github.com\/debrupf2946","followers_url":"https:\/\/api.github.com\/users\/debrupf2946\/followers","following_url":"https:\/\/api.github.com\/users\/debrupf2946\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/debrupf2946\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/debrupf2946\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/debrupf2946\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/debrupf2946\/orgs","repos_url":"https:\/\/api.github.com\/users\/debrupf2946\/repos","events_url":"https:\/\/api.github.com\/users\/debrupf2946\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/debrupf2946\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":8,"created_at":"2023-08-16T04:38:09Z","updated_at":"2023-10-10T16:30:19Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nFolderBase Dataset automatically resolves under current directory when data_dir is not specified.\r\n\r\nFor example:\r\n```\r\nload_dataset(\"audiofolder\")\r\n```\r\ntakes long time to resolve and collect data_files from current directory. But I think it should reach out to this line for error handling https:\/\/github.com\/huggingface\/datasets\/blob\/cb8c5de5145c7e7eee65391cb7f4d92f0d565d62\/src\/datasets\/packaged_modules\/folder_based_builder\/folder_based_builder.py#L58-L59\n\n### Steps to reproduce the bug\n\n```\r\nload_dataset(\"audiofolder\")\r\n```\n\n### Expected behavior\n\nError report\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.15.0-78-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.15\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6152\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6152\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6151","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6151\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6151\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6151\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6151","id":1851497818,"node_id":"I_kwDODunzps5uW51a","number":6151,"title":"Faster sorting for single key items","user":{"login":"jackapbutler","id":47942453,"node_id":"MDQ6VXNlcjQ3OTQyNDUz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47942453?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jackapbutler","html_url":"https:\/\/github.com\/jackapbutler","followers_url":"https:\/\/api.github.com\/users\/jackapbutler\/followers","following_url":"https:\/\/api.github.com\/users\/jackapbutler\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jackapbutler\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jackapbutler\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jackapbutler\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jackapbutler\/orgs","repos_url":"https:\/\/api.github.com\/users\/jackapbutler\/repos","events_url":"https:\/\/api.github.com\/users\/jackapbutler\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jackapbutler\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-15T14:02:31Z","updated_at":"2023-08-21T14:38:26Z","closed_at":"2023-08-21T14:38:25Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nA faster way to sort a dataset which contains a large number of rows.\r\n\r\n### Motivation\r\n\r\nThe current sorting implementations took significantly longer than expected when I was running on a dataset trying to sort by timestamps. \r\n\r\n**Code snippet:**\r\n```python\r\nds = datasets.load_dataset( \"json\", **{\"data_files\": {\"train\": \"path-to-jsonlines\"}, \"split\": \"train\"}, num_proc=os.cpu_count(), keep_in_memory=True) \r\nsorted_ds = ds.sort(\"pubDate\", keep_in_memory=True)\r\n```\r\n\r\nHowever, once I switched to a different method which\r\n1. unpacked to a list of tuples\r\n2. sorted tuples by key\r\n3. run `.select` with the sorted list of indices\r\nIt was significantly faster (orders of magnitude, especially with M's of rows)\r\n\r\n### Your contribution\r\n\r\nI'd be happy to implement a crude single key sorting algorithm so that other users can benefit from this trick. Broadly, this would take a `Dataset` and perform;\r\n\r\n```python\r\n# ds is a Dataset object\r\n# key_name is the sorting key\r\n\r\nclass Dataset:\r\n   ...\r\n   def _sort(key_name: str) -> Dataset:\r\n      index_keys = [(i,x) for i,x in enumerate(self[key_name])]\r\n      sorted_rows = sorted(row_pubdate, key=lambda x: x[1])\r\n      sorted_indicies = [x[0] for x in sorted_rows]\r\n      return self.select(sorted_indicies)\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6151\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6151\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6150","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6150\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6150\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6150\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6150","id":1850740456,"node_id":"I_kwDODunzps5uUA7o","number":6150,"title":"Allow dataset implement .take","user":{"login":"brando90","id":1855278,"node_id":"MDQ6VXNlcjE4NTUyNzg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1855278?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/brando90","html_url":"https:\/\/github.com\/brando90","followers_url":"https:\/\/api.github.com\/users\/brando90\/followers","following_url":"https:\/\/api.github.com\/users\/brando90\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/brando90\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/brando90\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/brando90\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/brando90\/orgs","repos_url":"https:\/\/api.github.com\/users\/brando90\/repos","events_url":"https:\/\/api.github.com\/users\/brando90\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/brando90\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-15T00:17:51Z","updated_at":"2023-08-17T13:49:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nI want to do:\r\n```\r\ndataset.take(512)\r\n```\r\n\r\nbut it only works with streaming = True\n\n### Motivation\n\nuniform interface to data sets. Really surprising the above only works with streaming = True. \n\n### Your contribution\n\nShould be trivial to copy paste the IterableDataset .take to use the local path in the data (when streaming = False)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6150\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6150\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6149","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6149\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6149\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6149\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6149","id":1850700624,"node_id":"I_kwDODunzps5uT3NQ","number":6149,"title":"Dataset.from_parquet cannot load subset of columns","user":{"login":"dwyatte","id":2512762,"node_id":"MDQ6VXNlcjI1MTI3NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2512762?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dwyatte","html_url":"https:\/\/github.com\/dwyatte","followers_url":"https:\/\/api.github.com\/users\/dwyatte\/followers","following_url":"https:\/\/api.github.com\/users\/dwyatte\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dwyatte\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dwyatte\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dwyatte\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dwyatte\/orgs","repos_url":"https:\/\/api.github.com\/users\/dwyatte\/repos","events_url":"https:\/\/api.github.com\/users\/dwyatte\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dwyatte\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-08-14T23:28:22Z","updated_at":"2023-08-17T22:36:05Z","closed_at":"2023-08-17T22:36:05Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen using `Dataset.from_parquet(path_or_paths, columns=[...])` and a subset of columns, loading fails with a variant of the following\r\n\r\n```\r\nValueError: Couldn't cast\r\na: int64\r\n-- schema metadata --\r\npandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 273\r\nto\r\n{'a': Value(dtype='int64', id=None), 'b': Value(dtype='int64', id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n```\r\n\r\nLooks to be triggered by https:\/\/github.com\/huggingface\/datasets\/blob\/c02a44715c036b5261686669727394b1308a3a4b\/src\/datasets\/table.py#L2285-L2286\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nimport pandas as pd\r\nfrom datasets import Dataset\r\n\r\n\r\npd.DataFrame([{\"a\": 1, \"b\": 2}]).to_parquet(\"test.pq\")\r\nDataset.from_parquet(\"test.pq\", columns=[\"a\"])\r\n```\r\n\r\n### Expected behavior\r\n\r\nA subset of columns should be loaded without error\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.10.0-23-cloud-amd64-x86_64-with-glibc2.2.5\r\n- Python version: 3.8.16\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6149\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6149\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6148","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6148\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6148\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6148\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6148","id":1849524683,"node_id":"PR_kwDODunzps5X3oqv","number":6148,"title":"Ignore parallel warning in map_nested","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-14T10:43:41Z","updated_at":"2023-08-17T08:54:06Z","closed_at":"2023-08-17T08:43:58Z","author_association":"MEMBER","active_lock_reason":null,"body":"This warning message was shown every time you pass num_proc to `load_dataset` because of `map_nested`\r\n\r\n```\r\nparallel_map is experimental and might be subject to breaking changes in the future\r\n```\r\n\r\nThis PR removes it for `map_nested`. If someone uses another parallel backend they're already warned when `parallel_backend` is called anyway","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6148\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6148\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6148","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6148","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6148.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6148.patch","merged_at":"2023-08-17T08:43:58Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6147","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6147\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6147\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6147\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6147","id":1848914830,"node_id":"I_kwDODunzps5uNDOO","number":6147,"title":"ValueError when running BeamBasedBuilder with GCS path in cache_dir","user":{"login":"ktrk115","id":13844767,"node_id":"MDQ6VXNlcjEzODQ0NzY3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/13844767?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ktrk115","html_url":"https:\/\/github.com\/ktrk115","followers_url":"https:\/\/api.github.com\/users\/ktrk115\/followers","following_url":"https:\/\/api.github.com\/users\/ktrk115\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ktrk115\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ktrk115\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ktrk115\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ktrk115\/orgs","repos_url":"https:\/\/api.github.com\/users\/ktrk115\/repos","events_url":"https:\/\/api.github.com\/users\/ktrk115\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ktrk115\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-14T03:11:34Z","updated_at":"2023-08-14T03:19:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen running the BeamBasedBuilder with a GCS path specified in the cache_dir, the following ValueError occurs:\r\n\r\n```\r\nValueError: Unable to get filesystem from specified path, please use the correct path or ensure the required dependency is installed, e.g., pip install apache-beam[gcp]. Path specified: gcs:\/\/my-bucket\/huggingface_datasets\/my_beam_dataset\/default\/0.0.0\/my_beam_dataset-train [while running 'train\/Save to parquet\/Write\/WriteImpl\/InitializeWrite']\r\n```\r\n\r\nSame error occurs after running `pip install apache-beam[gcp]` as instructed.\n\n### Steps to reproduce the bug\n\nPut `my_beam_dataset.py`:\r\n\r\n```python\r\nimport datasets\r\n\r\n\r\nclass MyBeamDataset(datasets.BeamBasedBuilder):\r\n    def _info(self):\r\n        features = datasets.Features({\"value\": datasets.Value(\"int64\")})\r\n        return datasets.DatasetInfo(features=features)\r\n\r\n    def _split_generators(self, dl_manager, pipeline):\r\n        return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={})]\r\n\r\n    def _build_pcollection(self, pipeline):\r\n        import apache_beam as beam\r\n\r\n        return pipeline | beam.Create([{\"value\": i} for i in range(10)])\r\n```\r\n\r\nRun:\r\n```bash\r\ndatasets-cli run_beam my_beam_dataset.py --cache_dir=gs:\/\/my-bucket\/huggingface_datasets\/ --beam_pipeline_options=\"runner=DirectRunner\"\r\n```\n\n### Expected behavior\n\nRunning the BeamBasedBuilder with a GCS cache path without any errors.\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: macOS-13.4-arm64-arm-64bit\r\n- Python version: 3.9.17\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6147\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6147\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6146","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6146\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6146\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6146\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6146","id":1848417366,"node_id":"I_kwDODunzps5uLJxW","number":6146,"title":"DatasetGenerationError when load glue benchmark datasets from `load_dataset` ","user":{"login":"yusx-swapp","id":78742415,"node_id":"MDQ6VXNlcjc4NzQyNDE1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/78742415?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yusx-swapp","html_url":"https:\/\/github.com\/yusx-swapp","followers_url":"https:\/\/api.github.com\/users\/yusx-swapp\/followers","following_url":"https:\/\/api.github.com\/users\/yusx-swapp\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yusx-swapp\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yusx-swapp\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yusx-swapp\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yusx-swapp\/orgs","repos_url":"https:\/\/api.github.com\/users\/yusx-swapp\/repos","events_url":"https:\/\/api.github.com\/users\/yusx-swapp\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yusx-swapp\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-13T05:17:56Z","updated_at":"2023-08-26T22:09:09Z","closed_at":"2023-08-26T22:09:09Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n\r\nPackage version: datasets-2.14.4\r\n\r\nWhen I run the codes:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"glue\", \"ax\")\r\n```\r\n\r\n\r\nI got the following errors:\r\n\r\n\r\n---------------------------------------------------------------------------\r\nSchemaInferenceError                      Traceback (most recent call last)\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/builder.py:1949, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1948 num_shards = shard_id + 1\r\n-> 1949 num_examples, num_bytes = writer.finalize()\r\n   1950 writer.close()\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py:598, in ArrowWriter.finalize(self, close_stream)\r\n    597         self.stream.close()\r\n--> 598     raise SchemaInferenceError(\"Please pass `features` or at least one example when writing data\")\r\n    599 logger.debug(\r\n    600     f\"Done writing {self._num_examples} {self.unit} in {self._num_bytes} bytes {self._path if self._path else ''}.\"\r\n    601 )\r\n\r\nSchemaInferenceError: Please pass `features` or at least one example when writing data\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\nCell In[5], line 3\r\n      1 from datasets import load_dataset\r\n----> 3 dataset = load_dataset(\"glue\", \"ax\")\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/load.py:2136, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2133 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\r\n   2135 # Download and prepare data\r\n-> 2136 builder_instance.download_and_prepare(\r\n   2137     download_config=download_config,\r\n   2138     download_mode=download_mode,\r\n   2139     verification_mode=verification_mode,\r\n   2140     try_from_hf_gcs=try_from_hf_gcs,\r\n   2141     num_proc=num_proc,\r\n   2142     storage_options=storage_options,\r\n   2143 )\r\n   2145 # Build dataset for splits\r\n   2146 keep_in_memory = (\r\n   2147     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   2148 )\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/builder.py:954, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    952     if num_proc is not None:\r\n    953         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 954     self._download_and_prepare(\r\n    955         dl_manager=dl_manager,\r\n    956         verification_mode=verification_mode,\r\n    957         **prepare_split_kwargs,\r\n    958         **download_and_prepare_kwargs,\r\n    959     )\r\n    960 # Sync info\r\n    961 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/builder.py:1049, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   1045 split_dict.add(split_generator.split_info)\r\n   1047 try:\r\n   1048     # Prepare split will record examples associated to the split\r\n-> 1049     self._prepare_split(split_generator, **prepare_split_kwargs)\r\n   1050 except OSError as e:\r\n   1051     raise OSError(\r\n   1052         \"Cannot find data file. \"\r\n   1053         + (self.manual_download_instructions or \"\")\r\n   1054         + \"\\nOriginal error:\\n\"\r\n   1055         + str(e)\r\n   1056     ) from None\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/builder.py:1813, in ArrowBasedBuilder._prepare_split(self, split_generator, file_format, num_proc, max_shard_size)\r\n   1811 job_id = 0\r\n   1812 with pbar:\r\n-> 1813     for job_id, done, content in self._prepare_split_single(\r\n   1814         gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n   1815     ):\r\n   1816         if done:\r\n   1817             result = content\r\n\r\nFile ~\/anaconda3\/envs\/python3\/lib\/python3.10\/site-packages\/datasets\/builder.py:1958, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1956     if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\r\n   1957         e = e.__context__\r\n-> 1958     raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1960 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"glue\", \"ax\")\n\n### Expected behavior\n\nWhen generating the train split:\r\nGenerating train split:\r\n0\/0 [00:00<?, ? examples\/s]\r\n\r\nIt raise the error:\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n\n\n### Environment info\n\ndatasets-2.14.4.\r\nPython 3.10","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6146\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6146\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6153","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6153\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6153\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6153\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6153","id":1852630074,"node_id":"I_kwDODunzps5ubOQ6","number":6153,"title":"custom load dataset to hub","user":{"login":"andysingal","id":20493493,"node_id":"MDQ6VXNlcjIwNDkzNDkz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20493493?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/andysingal","html_url":"https:\/\/github.com\/andysingal","followers_url":"https:\/\/api.github.com\/users\/andysingal\/followers","following_url":"https:\/\/api.github.com\/users\/andysingal\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/andysingal\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/andysingal\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/andysingal\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/andysingal\/orgs","repos_url":"https:\/\/api.github.com\/users\/andysingal\/repos","events_url":"https:\/\/api.github.com\/users\/andysingal\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/andysingal\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-08-13T04:42:22Z","updated_at":"2023-11-21T11:50:28Z","closed_at":"2023-10-08T17:04:16Z","author_association":"NONE","active_lock_reason":null,"body":"### System Info\n\nkaggle notebook\r\n\r\ni transformed dataset:\r\n```\r\ndataset = load_dataset(\"Dahoas\/first-instruct-human-assistant-prompt\")\r\n```\r\nto \r\nformatted_dataset: \r\n```\r\nDataset({\r\n    features: ['message_tree_id', 'message_tree_text'],\r\n    num_rows: 33143\r\n})\r\n```\r\nbut would like to know how to upload to hub\n\n### Who can help?\n\n@ArthurZucker @younesbelkada\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nshared above\n\n### Expected behavior\n\nload dataset to hub","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6153\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6153\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6145","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6145\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6145\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6145\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6145","id":1847811310,"node_id":"PR_kwDODunzps5Xx5If","number":6145,"title":"Export to_iterable_dataset to document","user":{"login":"npuichigo","id":11533479,"node_id":"MDQ6VXNlcjExNTMzNDc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11533479?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/npuichigo","html_url":"https:\/\/github.com\/npuichigo","followers_url":"https:\/\/api.github.com\/users\/npuichigo\/followers","following_url":"https:\/\/api.github.com\/users\/npuichigo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/npuichigo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/npuichigo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/npuichigo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/npuichigo\/orgs","repos_url":"https:\/\/api.github.com\/users\/npuichigo\/repos","events_url":"https:\/\/api.github.com\/users\/npuichigo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/npuichigo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-12T07:00:14Z","updated_at":"2023-08-15T17:04:01Z","closed_at":"2023-08-15T16:55:24Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix the export of a missing method of `Dataset`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6145\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6145\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6145","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6145","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6145.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6145.patch","merged_at":"2023-08-15T16:55:24Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6144","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6144\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6144\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6144\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6144","id":1847296711,"node_id":"I_kwDODunzps5uG4LH","number":6144,"title":"NIH exporter file not found","user":{"login":"brando90","id":1855278,"node_id":"MDQ6VXNlcjE4NTUyNzg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1855278?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/brando90","html_url":"https:\/\/github.com\/brando90","followers_url":"https:\/\/api.github.com\/users\/brando90\/followers","following_url":"https:\/\/api.github.com\/users\/brando90\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/brando90\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/brando90\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/brando90\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/brando90\/orgs","repos_url":"https:\/\/api.github.com\/users\/brando90\/repos","events_url":"https:\/\/api.github.com\/users\/brando90\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/brando90\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-08-11T19:05:25Z","updated_at":"2023-08-14T23:28:38Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\ncan't use or download the nih exporter pile data. \r\n```\r\n15     experiment_compute_diveristy_coeff_single_dataset_then_combined_datasets_with_domain_weights()\r\n16   File \"\/lfs\/ampere1\/0\/brando9\/beyond-scale-language-data-diversity\/src\/diversity\/div_coeff.py\", line 474, in experiment_compute_diveristy_coeff_single_dataset_then_combined_datasets_with_domain_weights\r\n17     column_names = next(iter(dataset)).keys()\r\n18   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/datasets\/iterable_dataset.py\", line 1353, in __iter__\r\n19     for key, example in ex_iterable:\r\n20   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/datasets\/iterable_dataset.py\", line 207, in __iter__\r\n21     yield from self.generate_examples_fn(**self.kwargs)\r\n22   File \"\/lfs\/ampere1\/0\/brando9\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/EleutherAI--pile\/ebea56d358e91cf4d37b0fde361d563bed1472fbd8221a21b38fc8bb4ba554fb\/pile.py\", line 236, in _generate_examples\r\n23     with zstd.open(open(files[subset], \"rb\"), \"rt\", encoding=\"utf-8\") as f:\r\n24   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/datasets\/streaming.py\", line 74, in wrapper\r\n25     return function(*args, download_config=download_config, **kwargs)\r\n26   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/datasets\/download\/streaming_download_manager.py\", line 496, in xopen\r\n27     file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n28   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/fsspec\/core.py\", line 134, in open\r\n29     return self.__enter__()\r\n30   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/fsspec\/core.py\", line 102, in __enter__\r\n31     f = self.fs.open(self.path, mode=mode)\r\n32   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/fsspec\/spec.py\", line 1241, in open\r\n33     f = self._open(\r\n34   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/fsspec\/implementations\/http.py\", line 356, in _open\r\n35     size = size or self.info(path, **kwargs)[\"size\"]\r\n36   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 121, in wrapper\r\n37     return sync(self.loop, func, *args, **kwargs)\r\n38   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 106, in sync\r\n39     raise return_result\r\n40   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 61, in _runner\r\n41     result[0] = await coro\r\n42   File \"\/lfs\/ampere1\/0\/brando9\/miniconda\/envs\/beyond_scale\/lib\/python3.10\/site-packages\/fsspec\/implementations\/http.py\", line 430, in _info\r\n43     raise FileNotFoundError(url) from exc\r\n44 FileNotFoundError: https:\/\/the-eye.eu\/public\/AI\/pile_preliminary_components\/NIH_ExPORTER_awarded_grant_text.jsonl.zst\r\n```\n\n### Steps to reproduce the bug\n\nrun this:\r\n```\r\nfrom datasets import load_dataset\r\npath, name = 'EleutherAI\/pile', 'nih_exporter'\r\n\r\n# -- Get data set\r\ndataset = load_dataset(path, name, streaming=True, split=\"train\").with_format(\"torch\")\r\nbatch = dataset.take(512)\r\nprint(f'{batch=}')\r\n```\n\n### Expected behavior\n\nprint the batch\n\n### Environment info\n\n```\r\n(beyond_scale) brando9@ampere1:~\/beyond-scale-language-data-diversity$ datasets-cli env\r\n\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.4.0-122-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6144\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6144\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6142","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6142\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6142\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6142\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6142","id":1846205216,"node_id":"I_kwDODunzps5uCtsg","number":6142,"title":"the-stack-dedup fails to generate","user":{"login":"michaelroyzen","id":45830328,"node_id":"MDQ6VXNlcjQ1ODMwMzI4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45830328?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/michaelroyzen","html_url":"https:\/\/github.com\/michaelroyzen","followers_url":"https:\/\/api.github.com\/users\/michaelroyzen\/followers","following_url":"https:\/\/api.github.com\/users\/michaelroyzen\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/michaelroyzen\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/michaelroyzen\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/michaelroyzen\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/michaelroyzen\/orgs","repos_url":"https:\/\/api.github.com\/users\/michaelroyzen\/repos","events_url":"https:\/\/api.github.com\/users\/michaelroyzen\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/michaelroyzen\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2023-08-11T05:10:49Z","updated_at":"2023-08-17T09:26:13Z","closed_at":"2023-08-17T09:26:13Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm getting an error generating the-stack-dedup with datasets 2.13.1, and with 2.14.4 nothing happens.\n\n### Steps to reproduce the bug\n\nMy code: \r\n\r\n```\r\nimport os\r\nimport datasets as ds\r\n\r\nMY_CACHE_DIR = \"\/home\/ubuntu\/the-stack-dedup-local\"\r\nMY_TOKEN=\"my-token\"\r\n\r\nthe_stack_ds = ds.load_dataset(\"bigcode\/the-stack-dedup\", split=\"train\", download_mode=\"reuse_cache_if_exists\", cache_dir=MY_CACHE_DIR, use_auth_token=MY_TOKEN, num_proc=64)\r\n```\r\n\r\nThe exception:\r\n\r\n```\r\nGenerating train split: 233248251 examples [54:31, 57280.00 examples\/s]\r\nmultiprocess.pool.RemoteTraceback:                                     \r\n\"\"\"                                                                    \r\nTraceback (most recent call last):                                     \r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/build\r\ner.py\", line 1879, in _prepare_split_single                            \r\n    for _, table in generator:                                         \r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/packa\r\nged_modules\/parquet\/parquet.py\", line 82, in _generate_tables          \r\n    yield f\"{file_idx}_{batch_idx}\", self._cast_table(pa_table)        \r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/packa\r\nged_modules\/parquet\/parquet.py\", line 61, in _cast_table               \r\n    pa_table = table_cast(pa_table, self.info.features.arrow_schema)   \r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/table\r\n.py\", line 2324, in table_cast                                         \r\n    return cast_table_to_schema(table, schema)                         \r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/table\r\n.py\", line 2282, in cast_table_to_schema                               \r\n    raise ValueError(f\"Couldn't cast\\n{table.schema}\\nto\\n{features}\\nb\r\necause column names don't match\")                                      \r\nValueError: Couldn't cast                                              \r\nhexsha: string                                                         \r\nsize: int64                                                            \r\next: string                                                            \r\nlang: string                                                           \r\nmax_stars_repo_path: string                                            \r\nmax_stars_repo_name: string                                            \r\nmax_stars_repo_head_hexsha: string                                     \r\nmax_stars_repo_licenses: list<item: string>                            \r\n  child 0, item: string                                                \r\nmax_stars_count: int64                                                 \r\nmax_stars_repo_stars_event_min_datetime: string                        \r\nmax_stars_repo_stars_event_max_datetime: string                        \r\nmax_issues_repo_path: string                                           \r\nmax_issues_repo_name: string                                           \r\nmax_issues_repo_head_hexsha: string                                    \r\nmax_issues_repo_licenses: list<item: string>                           \r\n  child 0, item: string                                                \r\nmax_issues_count: int64                                                \r\nmax_issues_repo_issues_event_min_datetime: string                      \r\nmax_issues_repo_issues_event_max_datetime: string                      \r\nmax_forks_repo_path: string                                            \r\nmax_forks_repo_name: string                                            \r\nmax_forks_repo_head_hexsha: string                                     \r\nmax_forks_repo_licenses: list<item: string>                            \r\n  child 0, item: string                                                \r\nmax_forks_count: int64\r\nmax_forks_repo_forks_event_min_datetime: string\r\nmax_forks_repo_forks_event_max_datetime: string\r\ncontent: string\r\navg_line_length: double\r\nmax_line_length: int64\r\nalphanum_fraction: double\r\n__id__: int64\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"hexsha\": {\"dtype\": \"string\", \"_type' + 1979\r\nto\r\n{'hexsha': Value(dtype='string', id=None), 'size': Value(dtype='int64', id=None), 'ext': Value(dtype='string', id=None), 'lang': Value(dtype='string', id=None), 'max_stars_repo_path': Value(dtype='string', id=None), 'max_stars_repo_name': Value(dtype='string', id=None), 'max_stars_repo_head_hexsha': Value(dtype='string', id=None), 'max_stars_repo_licenses': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'max_stars_count': Value(dtype='int64', id=None), 'max_stars_repo_stars_event_min_datetime': Value(dtype='string', id=None), 'max_stars_repo_stars_event_max_datetime': Value(dtype='string', id=None), 'max_issues_repo_path': Value(dtype='string', id=None), 'max_issues_repo_name': Value(dtype='string', id=None), 'max_issues_repo_head_hexsha': Value(dtype='string', id=None), 'max_issues_repo_licenses': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'max_issues_count': Value(dtype='int64', id=None), 'max_issues_repo_issues_event_min_datetime': Value(dtype='string', id=None), 'max_issues_repo_issues_event_max_datetime': Value(dtype='string', id=None), 'max_forks_repo_path': Value(dtype='string', id=None), 'max_forks_repo_name': Value(dtype='string', id=None), 'max_forks_repo_head_hexsha': Value(dtype='string', id=None), 'max_forks_repo_licenses': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'max_forks_count': Value(dtype='int64', id=None), 'max_forks_repo_forks_event_min_datetime': Value(dtype='string', id=None), 'max_forks_repo_forks_event_max_datetime': Value(dtype='string', id=None), 'content': Value(dtype='string', id=None), 'avg_line_length': Value(dtype='float64', id=None), 'max_line_length': Value(dtype='int64', id=None), 'alphanum_fraction': Value(dtype='float64', id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/multiprocess\/p\r\nool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/utils\r\n\/py_utils.py\", line 1328, in _write_generator_to_queue\r\n    for i, result in enumerate(func(**kwargs)):\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/build\r\ner.py\", line 1912, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating th\r\ne dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while genera\r\nting the dataset\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/ubuntu\/download_the_stack.py\", line 7, in <module>\r\n    the_stack_ds = ds.load_dataset(\"bigcode\/the-stack-dedup\", split=\"tr\r\nain\", download_mode=\"reuse_cache_if_exists\", cache_dir=MY_CACHE_DIR, us\r\ne_auth_token=MY_TOKEN, num_proc=64) \r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/load.\r\npy\", line 1809, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/build\r\ner.py\", line 909, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/build\r\ner.py\", line 1004, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/build\r\ner.py\", line 1796, in _prepare_split\r\n    for job_id, done, content in iflatmap_unordered(\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/utils\r\n\/py_utils.py\", line 1354, in iflatmap_unordered\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/datasets\/utils\r\n\/py_utils.py\", line 1354, in <listcomp>\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"\/home\/ubuntu\/.local\/lib\/python3.10\/site-packages\/multiprocess\/p\r\nool.py\", line 774, in get\r\n    raise self._value\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\n\n### Expected behavior\n\nThe dataset downloads properly. @lhoestq @loub\n\n### Environment info\n\nDatasets 2.13.1, large VM with 2TB RAM, Ubuntu 20.04","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6142\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6142\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6141","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6141\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6141\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6141\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6141","id":1846117729,"node_id":"I_kwDODunzps5uCYVh","number":6141,"title":"TypeError: ClientSession._request() got an unexpected keyword argument 'https'","user":{"login":"q935970314","id":35994018,"node_id":"MDQ6VXNlcjM1OTk0MDE4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35994018?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/q935970314","html_url":"https:\/\/github.com\/q935970314","followers_url":"https:\/\/api.github.com\/users\/q935970314\/followers","following_url":"https:\/\/api.github.com\/users\/q935970314\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/q935970314\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/q935970314\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/q935970314\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/q935970314\/orgs","repos_url":"https:\/\/api.github.com\/users\/q935970314\/repos","events_url":"https:\/\/api.github.com\/users\/q935970314\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/q935970314\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-11T02:40:32Z","updated_at":"2023-08-30T13:51:33Z","closed_at":"2023-08-30T13:51:33Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nHello, when I ran the [code snippet](https:\/\/huggingface.co\/docs\/datasets\/v2.14.4\/en\/loading#json) on the document, I encountered the following problem:\r\n\r\n```\r\nPython 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from datasets import load_dataset\r\n>>> base_url = \"https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/\"\r\n>>> dataset = load_dataset(\"json\", data_files={\"train\": base_url + \"train-v1.1.json\", \"validation\": base_url + \"dev-v1.1.json\"}, field=\"data\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 2112, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1798, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1413, in dataset_module_factory\r\n    ).get_module()\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 949, in get_module\r\n    data_files = DataFilesDict.from_patterns(\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/datasets\/data_files.py\", line 672, in from_patterns\r\n    DataFilesList.from_patterns(\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/datasets\/data_files.py\", line 578, in from_patterns\r\n    resolve_pattern(\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/datasets\/data_files.py\", line 340, in resolve_pattern\r\n    for filepath, info in fs.glob(pattern, detail=True).items()\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 113, in wrapper\r\n    return sync(self.loop, func, *args, **kwargs)\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 98, in sync\r\n    raise return_result\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 53, in _runner\r\n    result[0] = await coro\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/fsspec\/implementations\/http.py\", line 449, in _glob\r\n    elif await self._exists(path):\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/fsspec\/implementations\/http.py\", line 306, in _exists\r\n    r = await session.get(self.encode_url(path), **kw)\r\n  File \"\/home\/liushuai\/anaconda3\/lib\/python3.10\/site-packages\/aiohttp\/client.py\", line 922, in get\r\n    self._request(hdrs.METH_GET, url, allow_redirects=allow_redirects, **kwargs)\r\nTypeError: ClientSession._request() got an unexpected keyword argument 'https'\r\n```\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset  \r\nbase_url = \"https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/\"  \r\ndataset = load_dataset(\"json\", data_files={\"train\": base_url + \"train-v1.1.json\", \"validation\": base_url + \"dev-v1.1.json\"}, field=\"data\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nable to load normally\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.4.54-2-x86_64-with-glibc2.27\r\n- Python version: 3.10.9\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6141\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6141\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6140","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6140\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6140\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6140\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6140","id":1845384712,"node_id":"I_kwDODunzps5t_lYI","number":6140,"title":"Misalignment between file format specified in configs metadata YAML and the inferred builder","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-08-10T15:07:34Z","updated_at":"2023-08-17T20:37:20Z","closed_at":"2023-08-17T20:37:20Z","author_association":"MEMBER","active_lock_reason":null,"body":"There is a misalignment between the format of the `data_files` specified in the configs metadata YAML (CSV):\r\n```yaml\r\nconfigs:\r\n  - config_name: default\r\n    data_files:\r\n      - split: train\r\n        path: data.csv\r\n``` \r\nand the inferred builder (JSON). Note there are multiple JSON files in the repo, but they do not appear in the configs metadata YAML.\r\n\r\nSee: https:\/\/huggingface.co\/datasets\/freddyaboulton\/chatinterface_with_image_csv\/discussions\/1\r\n\r\nCC: @freddyaboulton @polinaeterna ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6140\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6140\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6139","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6139\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6139\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6139\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6139","id":1844991583,"node_id":"I_kwDODunzps5t-FZf","number":6139,"title":"Offline dataset viewer","user":{"login":"yuvalkirstain","id":57996478,"node_id":"MDQ6VXNlcjU3OTk2NDc4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57996478?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yuvalkirstain","html_url":"https:\/\/github.com\/yuvalkirstain","followers_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/followers","following_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/orgs","repos_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/repos","events_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yuvalkirstain\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-08-10T11:30:00Z","updated_at":"2023-09-29T13:10:23Z","closed_at":"2023-09-29T13:10:22Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nThe dataset viewer feature is very nice. It enables to the user to easily view the dataset. However, when working for private companies we cannot always upload the dataset to the hub. Is there a way to create dataset viewer offline? I.e. to run a code that will open some kind of html or something that makes it easy to view the dataset.\n\n### Motivation\n\nI want to easily view my dataset even when it is hosted locally.\n\n### Your contribution\n\nN.A.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6139\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6139\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6138","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6138\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6138\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6138\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6138","id":1844952496,"node_id":"PR_kwDODunzps5XoH2V","number":6138,"title":"Ignore CI lint rule violation in Pickler.memoize","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-10T11:03:15Z","updated_at":"2023-08-10T11:31:45Z","closed_at":"2023-08-10T11:22:56Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR ignores the violation of the lint rule E721 in `Pickler.memoize`.\r\n\r\nThe lint rule violation was introduced in this PR:\r\n- #3182\r\n\r\n@lhoestq is there a reason you did not use `isinstance` instead?\r\n\r\nAs a hotfix, we just ignore the violation of the lint rule.\r\n\r\nFix #6136.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6138\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6138\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6138","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6138","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6138.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6138.patch","merged_at":"2023-08-10T11:22:56Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6137","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6137\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6137\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6137\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6137","id":1844952312,"node_id":"I_kwDODunzps5t97z4","number":6137,"title":"(`from_spark()`) Unable to connect HDFS in pyspark YARN setting ","user":{"login":"kyoungrok0517","id":1051900,"node_id":"MDQ6VXNlcjEwNTE5MDA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1051900?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kyoungrok0517","html_url":"https:\/\/github.com\/kyoungrok0517","followers_url":"https:\/\/api.github.com\/users\/kyoungrok0517\/followers","following_url":"https:\/\/api.github.com\/users\/kyoungrok0517\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kyoungrok0517\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kyoungrok0517\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kyoungrok0517\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kyoungrok0517\/orgs","repos_url":"https:\/\/api.github.com\/users\/kyoungrok0517\/repos","events_url":"https:\/\/api.github.com\/users\/kyoungrok0517\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kyoungrok0517\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-08-10T11:03:08Z","updated_at":"2023-08-10T11:03:08Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nrelated issue: https:\/\/github.com\/apache\/arrow\/issues\/37057#issue-1841013613\r\n\r\n---\r\n\r\n\r\nHello. I'm trying to interact with HDFS storage from a driver and workers of pyspark YARN cluster. Precisely I'm using **huggingface's `datasets`** ([link](https:\/\/github.com\/huggingface\/datasets)) library that relies on pyarrow to communicate with HDFS. The `from_spark()` ([link](https:\/\/huggingface.co\/docs\/datasets\/use_with_spark#load-from-spark)) is what I'm invoking in my script.\r\n\r\nBelow is the error I'm encountering. Note that I've masked sensitive paths. My code is sent to worker containers (docker) from driver container then executed. I confirmed that in both driver and worker images I can connect to HDFS using pyarrow since the envs and required jars are properly set, but strangely that becomes impossible when the same image runs as remote worker process. \r\n\r\nThese are some peculiarities in my environment that might caused this issue. \r\n* **Cluster requires kerberos authentication**\r\n  * But I think the error message implies that's not the problem in this case\r\n* **The user that runs the worker process is different from that built the docker image**\r\n  * To avoid permission-related issues I made all directories that are accessed from the script accessible to everyone\r\n* **Pyspark-part of my code has no problem interacting with HDFS.** \r\n  * Even pyarrow doesn't experience problem when I run the code in interactive session of the same docker images (driver, worker)\r\n  * The problem occurs only when it runs as cluster's worker runtime\r\n\r\nHope I could get some help. Thanks.\r\n\r\n```bash\r\n2023-08-08 18:51:19,638 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n2023-08-08 18:51:20,280 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\r\n23\/08\/08 18:51:22 WARN TaskSetManager: Lost task 0.0 in stage 142.0 (TID 9732) (ac3bax2062.bdp.bdata.ai executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"<MASKED>\/application_1682476586273_25865777\/container_e143_1682476586273_25865777_01_000003\/pyspark.zip\/pyspark\/worker.py\", line 830, in main\r\n    process()\r\n  File \"<MASKED>\/application_1682476586273_25865777\/container_e143_1682476586273_25865777_01_000003\/pyspark.zip\/pyspark\/worker.py\", line 820, in process\r\n    out_iter = func(split_index, iterator)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/root\/spark\/python\/pyspark\/rdd.py\", line 5405, in pipeline_func\r\n  File \"\/root\/spark\/python\/pyspark\/rdd.py\", line 828, in func\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/datasets\/packaged_modules\/spark\/spark.py\", line 130, in create_cache_and_write_probe\r\n    open(probe_file, \"a\")\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/datasets\/streaming.py\", line 74, in wrapper\r\n    return function(*args, download_config=download_config, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/datasets\/download\/streaming_download_manager.py\", line 496, in xopen\r\n    file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/core.py\", line 439, in open\r\n    out = open_files(\r\n          ^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/core.py\", line 282, in open_files\r\n    fs, fs_token, paths = get_fs_token_paths(\r\n                          ^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/core.py\", line 609, in get_fs_token_paths\r\n    fs = filesystem(protocol, **inkwargs)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/registry.py\", line 267, in filesystem\r\n    return cls(**storage_options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/spec.py\", line 79, in __call__\r\n    obj = super().__call__(*args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/implementations\/arrow.py\", line 278, in __init__\r\n    fs = HadoopFileSystem(\r\n         ^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow\/_hdfs.pyx\", line 96, in pyarrow._hdfs.HadoopFileSystem.__init__\r\n  File \"pyarrow\/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 115, in pyarrow.lib.check_status\r\nOSError: HDFS connection failed\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n23\/08\/08 18:51:24 WARN TaskSetManager: Lost task 0.1 in stage 142.0 (TID 9733) (ac3iax2079.bdp.bdata.ai executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"<MASKED>\/application_1682476586273_25865777\/container_e143_1682476586273_25865777_01_000005\/pyspark.zip\/pyspark\/worker.py\", line 830, in main\r\n    process()\r\n  File \"<MASKED>\/application_1682476586273_25865777\/container_e143_1682476586273_25865777_01_000005\/pyspark.zip\/pyspark\/worker.py\", line 820, in process\r\n    out_iter = func(split_index, iterator)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/root\/spark\/python\/pyspark\/rdd.py\", line 5405, in pipeline_func\r\n  File \"\/root\/spark\/python\/pyspark\/rdd.py\", line 828, in func\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/datasets\/packaged_modules\/spark\/spark.py\", line 130, in create_cache_and_write_probe\r\n    open(probe_file, \"a\")\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/datasets\/streaming.py\", line 74, in wrapper\r\n    return function(*args, download_config=download_config, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/datasets\/download\/streaming_download_manager.py\", line 496, in xopen\r\n    file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/core.py\", line 439, in open\r\n    out = open_files(\r\n          ^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/core.py\", line 282, in open_files\r\n    fs, fs_token, paths = get_fs_token_paths(\r\n                          ^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/core.py\", line 609, in get_fs_token_paths\r\n    fs = filesystem(protocol, **inkwargs)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/registry.py\", line 267, in filesystem\r\n    return cls(**storage_options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/spec.py\", line 79, in __call__\r\n    obj = super().__call__(*args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/implementations\/arrow.py\", line 278, in __init__\r\n    fs = HadoopFileSystem(\r\n         ^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow\/_hdfs.pyx\", line 96, in pyarrow._hdfs.HadoopFileSystem.__init__\r\n  File \"pyarrow\/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 115, in pyarrow.lib.check_status\r\nOSError: HDFS connection failed\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n23\/08\/08 18:51:38 WARN TaskSetManager: Lost task 0.2 in stage 142.0 (TID 9734) (<MASKED> executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"<MASKED>\/application_1682476586273_25865777\/container_e143_1682476586273_25865777_01_000008\/pyspark.zip\/pyspark\/worker.py\", line 830, in main\r\n    process()\r\n  File \"<MASKED>\/application_1682476586273_25865777\/container_e143_1682476586273_25865777_01_000008\/pyspark.zip\/pyspark\/worker.py\", line 820, in process\r\n    out_iter = func(split_index, iterator)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/root\/spark\/python\/pyspark\/rdd.py\", line 5405, in pipeline_func\r\n  File \"\/root\/spark\/python\/pyspark\/rdd.py\", line 828, in func\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/datasets\/packaged_modules\/spark\/spark.py\", line 130, in create_cache_and_write_probe\r\n    open(probe_file, \"a\")\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/datasets\/streaming.py\", line 74, in wrapper\r\n    return function(*args, download_config=download_config, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/datasets\/download\/streaming_download_manager.py\", line 496, in xopen\r\n    file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/core.py\", line 439, in open\r\n    out = open_files(\r\n          ^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/core.py\", line 282, in open_files\r\n    fs, fs_token, paths = get_fs_token_paths(\r\n                          ^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/core.py\", line 609, in get_fs_token_paths\r\n    fs = filesystem(protocol, **inkwargs)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/registry.py\", line 267, in filesystem\r\n    return cls(**storage_options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/spec.py\", line 79, in __call__\r\n    obj = super().__call__(*args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/opt\/conda\/lib\/python3.11\/site-packages\/fsspec\/implementations\/arrow.py\", line 278, in __init__\r\n    fs = HadoopFileSystem(\r\n         ^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow\/_hdfs.pyx\", line 96, in pyarrow._hdfs.HadoopFileSystem.__init__\r\n  File \"pyarrow\/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 115, in pyarrow.lib.check_status\r\nOSError: HDFS connection failed\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\n\n\n### Steps to reproduce the bug\n\nUse `from_spark()` function in pyspark YARN setting. I set `cache_dir` to HDFS path.\n\n### Expected behavior\n\nWork as described in document\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.11.4\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6137\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6137\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6136","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6136\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6136\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6136\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6136","id":1844887866,"node_id":"I_kwDODunzps5t9sE6","number":6136,"title":"CI check_code_quality error: E721 Do not compare types, use `isinstance()`","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":4296013012,"node_id":"LA_kwDODunzps8AAAABAA_01A","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/maintenance","name":"maintenance","color":"d4c5f9","default":false,"description":"Maintenance tasks"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-08-10T10:19:50Z","updated_at":"2023-08-10T11:22:58Z","closed_at":"2023-08-10T11:22:58Z","author_association":"MEMBER","active_lock_reason":null,"body":"After latest release of `ruff` (https:\/\/pypi.org\/project\/ruff\/0.0.284\/), we get the following CI error:\r\n```\r\nsrc\/datasets\/utils\/py_utils.py:689:12: E721 Do not compare types, use `isinstance()`\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6136\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6136\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6135","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6135\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6135\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6135\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6135","id":1844870943,"node_id":"PR_kwDODunzps5Xn2AT","number":6135,"title":"Remove unused allowed_extensions param","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-08-10T10:09:54Z","updated_at":"2023-08-10T12:08:38Z","closed_at":"2023-08-10T12:00:02Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR removes unused `allowed_extensions` parameter from `create_builder_configs_from_metadata_configs`.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6135\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6135\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6135","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6135","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6135.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6135.patch","merged_at":"2023-08-10T12:00:01Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6134","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6134\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6134\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6134\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6134","id":1844535142,"node_id":"I_kwDODunzps5t8V9m","number":6134,"title":"`datasets` cannot be installed alongside `apache-beam`","user":{"login":"boyleconnor","id":6520892,"node_id":"MDQ6VXNlcjY1MjA4OTI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6520892?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/boyleconnor","html_url":"https:\/\/github.com\/boyleconnor","followers_url":"https:\/\/api.github.com\/users\/boyleconnor\/followers","following_url":"https:\/\/api.github.com\/users\/boyleconnor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/boyleconnor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/boyleconnor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/boyleconnor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/boyleconnor\/orgs","repos_url":"https:\/\/api.github.com\/users\/boyleconnor\/repos","events_url":"https:\/\/api.github.com\/users\/boyleconnor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/boyleconnor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-10T06:54:32Z","updated_at":"2023-09-01T03:19:49Z","closed_at":"2023-08-10T15:22:10Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nIf one installs `apache-beam` alongside `datasets` (which is required for the [wikipedia](https:\/\/huggingface.co\/datasets\/wikipedia#dataset-summary) dataset) in certain environments (such as a Google Colab notebook), they appear to install successfully, however, actually trying to do something such as importing the `load_dataset` method from `datasets` results in a crashing error.\r\n\r\n\r\nI think the problem is that `apache-beam` version 2.49.0 requires `dill>=0.3.1.1,<0.3.2`, but the latest version of `multiprocess` (0.70.15) (on which `datasets` depends) requires  `dill>=0.3.7,`, so this is causing the dependency resolver to use an older version of `multiprocess` which leads to the `datasets` crashing since it doesn't actually appear to be compatible with older versions.\r\n\r\n### Steps to reproduce the bug\r\n\r\nSee this [Google Colab notebook](https:\/\/colab.research.google.com\/drive\/1PTeGlshamFcJZix_GiS3vMXX_YzAhGv0?usp=sharing) to easily reproduce the bug.\r\n\r\nIn some environments, I have been able to reproduce the bug by running the following in Bash:\r\n\r\n```bash\r\n$ pip install datasets apache-beam\r\n```\r\n\r\nthen the following in a Python shell:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n```\r\n\r\nHere is my stacktrace from running on Google Colab:\r\n\r\n<details>\r\n<summary>stacktrace<\/summary>\r\n\r\n```\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/__init__.py](https:\/\/localhost:8080\/#) in <module>\r\n     20 __version__ = \"2.14.4\"\r\n     21 \r\n---> 22 from .arrow_dataset import Dataset\r\n     23 from .arrow_reader import ReadInstruction\r\n     24 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py](https:\/\/localhost:8080\/#) in <module>\r\n     64 \r\n     65 from . import config\r\n---> 66 from .arrow_reader import ArrowReader\r\n     67 from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n     68 from .data_files import sanitize_patterns\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_reader.py](https:\/\/localhost:8080\/#) in <module>\r\n     28 import pyarrow.parquet as pq\r\n     29 \r\n---> 30 from .download.download_config import DownloadConfig\r\n     31 from .naming import _split_re, filenames_for_dataset_split\r\n     32 from .table import InMemoryTable, MemoryMappedTable, Table, concat_tables\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/download\/__init__.py](https:\/\/localhost:8080\/#) in <module>\r\n      7 \r\n      8 from .download_config import DownloadConfig\r\n----> 9 from .download_manager import DownloadManager, DownloadMode\r\n     10 from .streaming_download_manager import StreamingDownloadManager\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/download\/download_manager.py](https:\/\/localhost:8080\/#) in <module>\r\n     33 from ..utils.info_utils import get_size_checksum_dict\r\n     34 from ..utils.logging import get_logger, is_progress_bar_enabled, tqdm\r\n---> 35 from ..utils.py_utils import NestedDataStructure, map_nested, size_str\r\n     36 from .download_config import DownloadConfig\r\n     37 \r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/utils\/py_utils.py](https:\/\/localhost:8080\/#) in <module>\r\n     38 import dill\r\n     39 import multiprocess\r\n---> 40 import multiprocess.pool\r\n     41 import numpy as np\r\n     42 from packaging import version\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/pool.py](https:\/\/localhost:8080\/#) in <module>\r\n    607 #\r\n    608 \r\n--> 609 class ThreadPool(Pool):\r\n    610 \r\n    611     from .dummy import Process\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/pool.py](https:\/\/localhost:8080\/#) in ThreadPool()\r\n    609 class ThreadPool(Pool):\r\n    610 \r\n--> 611     from .dummy import Process\r\n    612 \r\n    613     def __init__(self, processes=None, initializer=None, initargs=()):\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/dummy\/__init__.py](https:\/\/localhost:8080\/#) in <module>\r\n     85 #\r\n     86 \r\n---> 87 class Condition(threading._Condition):\r\n     88     # XXX\r\n     89     if sys.version_info < (3, 0):\r\n\r\nAttributeError: module 'threading' has no attribute '_Condition'\r\n```\r\n\r\n<\/details>\r\n\r\nI've also found that attempting to install these `datasets` and `apache-beam` in certain environments (e.g. via pip inside a conda env) simply causes pip to hang indefinitely.\r\n\r\n### Expected behavior\r\n\r\nI would expect to be able to import methods from `datasets` without crashing. I have tested that this is possible as long as I do not attempt to install `apache-beam`.\r\n\r\n### Environment info\r\n\r\nGoogle Colab","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6134\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6134\/timeline","performed_via_github_app":null,"state_reason":"not_planned","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6133","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6133\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6133\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6133\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6133","id":1844511519,"node_id":"I_kwDODunzps5t8QMf","number":6133,"title":"Dataset is slower after calling `to_iterable_dataset`","user":{"login":"npuichigo","id":11533479,"node_id":"MDQ6VXNlcjExNTMzNDc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11533479?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/npuichigo","html_url":"https:\/\/github.com\/npuichigo","followers_url":"https:\/\/api.github.com\/users\/npuichigo\/followers","following_url":"https:\/\/api.github.com\/users\/npuichigo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/npuichigo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/npuichigo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/npuichigo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/npuichigo\/orgs","repos_url":"https:\/\/api.github.com\/users\/npuichigo\/repos","events_url":"https:\/\/api.github.com\/users\/npuichigo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/npuichigo\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-10T06:36:23Z","updated_at":"2023-08-16T09:18:54Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nCan anyone explain why looping over a dataset becomes slower after calling `to_iterable_dataset` to convert to `IterableDataset`\n\n### Steps to reproduce the bug\n\nAny dataset after converting to `IterableDataset`\n\n### Expected behavior\n\nMaybe it should be faster on big dataset? I only test on small dataset\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.15\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6133\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6133\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6132","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6132\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6132\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6132\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6132","id":1843491020,"node_id":"I_kwDODunzps5t4XDM","number":6132,"title":"to_iterable_dataset is missing in document","user":{"login":"npuichigo","id":11533479,"node_id":"MDQ6VXNlcjExNTMzNDc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11533479?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/npuichigo","html_url":"https:\/\/github.com\/npuichigo","followers_url":"https:\/\/api.github.com\/users\/npuichigo\/followers","following_url":"https:\/\/api.github.com\/users\/npuichigo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/npuichigo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/npuichigo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/npuichigo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/npuichigo\/orgs","repos_url":"https:\/\/api.github.com\/users\/npuichigo\/repos","events_url":"https:\/\/api.github.com\/users\/npuichigo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/npuichigo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-09T15:15:03Z","updated_at":"2023-08-16T04:43:36Z","closed_at":"2023-08-16T04:43:29Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nto_iterable_dataset is missing in document\n\n### Steps to reproduce the bug\n\nto_iterable_dataset is missing in document\n\n### Expected behavior\n\ndocument enhancement\n\n### Environment info\n\nunrelated","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6132\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6132\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6130","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6130\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6130\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6130\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6130","id":1843158846,"node_id":"I_kwDODunzps5t3F8-","number":6130,"title":"default config name doesn't work when config kwargs are specified.","user":{"login":"npuichigo","id":11533479,"node_id":"MDQ6VXNlcjExNTMzNDc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11533479?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/npuichigo","html_url":"https:\/\/github.com\/npuichigo","followers_url":"https:\/\/api.github.com\/users\/npuichigo\/followers","following_url":"https:\/\/api.github.com\/users\/npuichigo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/npuichigo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/npuichigo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/npuichigo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/npuichigo\/orgs","repos_url":"https:\/\/api.github.com\/users\/npuichigo\/repos","events_url":"https:\/\/api.github.com\/users\/npuichigo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/npuichigo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":15,"created_at":"2023-08-09T12:43:15Z","updated_at":"2023-11-22T11:50:49Z","closed_at":"2023-11-22T11:50:48Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/12cfc1196e62847e2e8239fbd727a02cbc86ddec\/src\/datasets\/builder.py#L518-L522\r\n\r\nIf `config_name` is `None`, `DEFAULT_CONFIG_NAME` should be select. But once users pass `config_kwargs` to their customized `BuilderConfig`, the logic is ignored, and dataset cannot select the default config from multiple configs.\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset('\/dataset\/with\/multiple\/config'')  # Ok\r\ndatasets.load_dataset('\/dataset\/with\/multiple\/config', some_field_in_config='some')  # Err\r\n```\n\n### Expected behavior\n\nDefault config behavior should be consistent.\n\n### Environment info\n\n- `datasets` version: 2.14.3\r\n- Platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.15\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6130\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6130\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6129","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6129\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6129\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6129\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6129","id":1841563517,"node_id":"PR_kwDODunzps5Xcmuw","number":6129,"title":"Release 2.14.4","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-08-08T15:43:56Z","updated_at":"2023-08-08T16:08:22Z","closed_at":"2023-08-08T15:49:06Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6129\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6129\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6129","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6129","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6129.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6129.patch","merged_at":"2023-08-08T15:49:06Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6128","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6128\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6128\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6128\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6128","id":1841545493,"node_id":"I_kwDODunzps5tw8EV","number":6128,"title":"IndexError: Invalid key: 88 is out of bounds for size 0","user":{"login":"TomasAndersonFang","id":38727343,"node_id":"MDQ6VXNlcjM4NzI3MzQz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/38727343?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/TomasAndersonFang","html_url":"https:\/\/github.com\/TomasAndersonFang","followers_url":"https:\/\/api.github.com\/users\/TomasAndersonFang\/followers","following_url":"https:\/\/api.github.com\/users\/TomasAndersonFang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/TomasAndersonFang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/TomasAndersonFang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/TomasAndersonFang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/TomasAndersonFang\/orgs","repos_url":"https:\/\/api.github.com\/users\/TomasAndersonFang\/repos","events_url":"https:\/\/api.github.com\/users\/TomasAndersonFang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/TomasAndersonFang\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-08-08T15:32:08Z","updated_at":"2023-12-26T07:51:57Z","closed_at":"2023-08-11T13:35:09Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThis bug generates when I use torch.compile(model) in my code, which seems to raise an error in datasets lib.\n\n### Steps to reproduce the bug\n\nI use the following code to fine-tune Falcon on my private dataset.\r\n\r\n```python\r\nimport transformers\r\nfrom transformers import (\r\n    AutoModelForCausalLM, \r\n    AutoTokenizer,\r\n    AutoConfig,\r\n    DataCollatorForSeq2Seq,\r\n    Trainer,\r\n    Seq2SeqTrainer,\r\n    HfArgumentParser,\r\n    Seq2SeqTrainingArguments,\r\n    BitsAndBytesConfig,\r\n)\r\n\r\nfrom peft import (\r\n    LoraConfig,\r\n    get_peft_model,\r\n    get_peft_model_state_dict,\r\n    prepare_model_for_int8_training,\r\n    set_peft_model_state_dict,\r\n)\r\n\r\nimport torch\r\nimport os\r\nimport evaluate\r\nimport functools\r\nfrom datasets import load_dataset\r\nimport bitsandbytes as bnb\r\nimport logging\r\nimport json\r\nimport copy\r\nfrom typing import Dict, Optional, Sequence\r\nfrom dataclasses import dataclass, field\r\n\r\n\r\n# Lora settings\r\nLORA_R = 8\r\nLORA_ALPHA = 16\r\nLORA_DROPOUT= 0.05\r\nLORA_TARGET_MODULES = [\"query_key_value\"]\r\n\r\n\r\n@dataclass\r\nclass ModelArguments:\r\n    model_name_or_path: Optional[str] = field(default=\"Salesforce\/codegen2-7B\")\r\n\r\n\r\n@dataclass\r\nclass DataArguments:\r\n    data_path: str = field(default=None, metadata={\"help\": \"Path to the training data.\"})\r\n    train_file: str = field(default=None, metadata={\"help\": \"Path to the evaluation data.\"})\r\n    eval_file: str = field(default=None, metadata={\"help\": \"Path to the evaluation data.\"})\r\n    cache_path: str = field(default=None, metadata={\"help\": \"Path to the cache directory.\"})\r\n    num_proc: int = field(default=4, metadata={\"help\": \"Number of processes to use for data preprocessing.\"})\r\n\r\n\r\n@dataclass\r\nclass TrainingArguments(transformers.TrainingArguments):\r\n    # cache_dir: Optional[str] = field(default=None)\r\n    optim: str = field(default=\"adamw_torch\")\r\n    model_max_length: int = field(\r\n        default=512,\r\n        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\r\n    )\r\n    is_lora: bool = field(default=True, metadata={\"help\": \"Whether to use LORA.\"})\r\n\r\n\r\ndef tokenize(text, tokenizer, max_seq_len=512, add_eos_token=True):\r\n    result = tokenizer(\r\n        text,\r\n        truncation=True,\r\n        max_length=max_seq_len,\r\n        padding=False,\r\n        return_tensors=None,\r\n    )\r\n    if (\r\n        result[\"input_ids\"][-1] != tokenizer.eos_token_id\r\n        and len(result[\"input_ids\"]) < max_seq_len\r\n        and add_eos_token\r\n    ):\r\n        result[\"input_ids\"].append(tokenizer.eos_token_id)\r\n        result[\"attention_mask\"].append(1)\r\n\r\n    if add_eos_token and len(result[\"input_ids\"]) >= max_seq_len:\r\n        result[\"input_ids\"][max_seq_len - 1] = tokenizer.eos_token_id\r\n        result[\"attention_mask\"][max_seq_len - 1] = 1\r\n\r\n    result[\"labels\"] = result[\"input_ids\"].copy()\r\n    return result\r\n\r\n\r\ndef main():\r\n    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\r\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\r\n\r\n    config = AutoConfig.from_pretrained(\r\n        model_args.model_name_or_path, \r\n        cache_dir=data_args.cache_path,\r\n        trust_remote_code=True,\r\n    )\r\n\r\n    if training_args.is_lora:\r\n        model = AutoModelForCausalLM.from_pretrained(\r\n            model_args.model_name_or_path,\r\n            cache_dir=data_args.cache_path,\r\n            torch_dtype=torch.float16,\r\n            trust_remote_code=True,\r\n            load_in_8bit=True,\r\n            quantization_config=BitsAndBytesConfig(\r\n                load_in_8bit=True,\r\n                llm_int8_threshold=6.0\r\n            ),\r\n        )\r\n        model = prepare_model_for_int8_training(model)\r\n\r\n        config = LoraConfig(\r\n            r=LORA_R,\r\n            lora_alpha=LORA_ALPHA,\r\n            target_modules=LORA_TARGET_MODULES,\r\n            lora_dropout=LORA_DROPOUT,\r\n            bias=\"none\",\r\n            task_type=\"CAUSAL_LM\",\r\n        )\r\n        model = get_peft_model(model, config)\r\n    else:\r\n        model = AutoModelForCausalLM.from_pretrained(\r\n            model_args.model_name_or_path,\r\n            torch_dtype=torch.float16,\r\n            cache_dir=data_args.cache_path,\r\n            trust_remote_code=True,\r\n        )\r\n    model.config.use_cache = False\r\n    \r\n    def print_trainable_parameters(model):\r\n        \"\"\"\r\n        Prints the number of trainable parameters in the model.\r\n        \"\"\"\r\n        trainable_params = 0\r\n        all_param = 0\r\n        for _, param in model.named_parameters():\r\n            all_param += param.numel()\r\n            if param.requires_grad:\r\n                trainable_params += param.numel()\r\n        print(\r\n            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params \/ all_param}\"\r\n        )\r\n    print_trainable_parameters(model)\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\r\n        model_args.model_name_or_path,\r\n        cache_dir=data_args.cache_path,\r\n        model_max_length=training_args.model_max_length,\r\n        padding_side=\"left\",\r\n        use_fast=True,\r\n        trust_remote_code=True,\r\n    )\r\n    tokenizer.pad_token = tokenizer.eos_token\r\n    \r\n    # Load dataset\r\n\r\n    def generate_and_tokenize_prompt(sample):\r\n        input_text = sample[\"input\"]\r\n        target_text = sample[\"output\"] + tokenizer.eos_token\r\n        full_text = input_text + target_text\r\n        tokenized_full_text = tokenize(full_text, tokenizer, max_seq_len=512)\r\n        tokenized_input_text = tokenize(input_text, tokenizer, max_seq_len=512)\r\n        input_len = len(tokenized_input_text[\"input_ids\"]) - 1 # -1 for eos token\r\n        tokenized_full_text[\"labels\"] = [-100] * input_len + tokenized_full_text[\"labels\"][input_len:]\r\n        return tokenized_full_text\r\n\r\n    data_files = {}\r\n    if data_args.train_file is not None:\r\n        data_files[\"train\"] = data_args.train_file\r\n    if data_args.eval_file is not None:\r\n        data_files[\"eval\"] = data_args.eval_file\r\n    \r\n    dataset = load_dataset(data_args.data_path, data_files=data_files)\r\n    train_dataset = dataset[\"train\"]\r\n    eval_dataset = dataset[\"eval\"]\r\n    train_dataset = train_dataset.map(generate_and_tokenize_prompt, num_proc=data_args.num_proc)\r\n    eval_dataset = eval_dataset.map(generate_and_tokenize_prompt, num_proc=data_args.num_proc)\r\n    data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True)\r\n\r\n    # Evaluation metrics\r\n    def compute_metrics(eval_preds, tokenizer):\r\n        metric = evaluate.load('exact_match')\r\n        preds, labels = eval_preds\r\n        # In case the model returns more than the prediction logits\r\n        if isinstance(preds, tuple):\r\n            preds = preds[0]\r\n\r\n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=False)\r\n\r\n        # Replace -100s in the labels as we can't decode them\r\n        labels[labels == -100] = tokenizer.pad_token_id\r\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=False)\r\n\r\n        # Some simple post-processing\r\n        decoded_preds = [pred.strip() for pred in decoded_preds]\r\n        decoded_labels = [label.strip() for label in decoded_labels]\r\n\r\n        result = metric.compute(predictions=decoded_preds, references=decoded_labels)\r\n        return {'exact_match': result['exact_match']} \r\n    \r\n    compute_metrics_fn = functools.partial(compute_metrics, tokenizer=tokenizer)\r\n\r\n    model = torch.compile(model)\r\n\r\n    # Training\r\n    trainer = Trainer(\r\n        model=model, \r\n        train_dataset=train_dataset,\r\n        eval_dataset=eval_dataset,  \r\n        args=training_args,\r\n        data_collator=data_collator,\r\n        compute_metrics=compute_metrics_fn,\r\n    )\r\n    trainer.train()\r\n    trainer.save_state()\r\n    trainer.save_model(output_dir=training_args.output_dir)\r\n    tokenizer.save_pretrained(save_directory=training_args.output_dir)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nWhen I didn't use `torch.cpmpile(model)`, my code worked well. But when I added this line to my code, It produced the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"falcon_sft.py\", line 230, in <module>\r\n    main()\r\n  File \"falcon_sft.py\", line 223, in main\r\n    trainer.train()\r\n  File \"python3.10\/site-packages\/transformers\/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"python3.10\/site-packages\/transformers\/trainer.py\", line 1787, in _inner_training_loop\r\n    for step, inputs in enumerate(epoch_iterator):\r\n  File \"python3.10\/site-packages\/accelerate\/data_loader.py\", line 384, in __iter__\r\n    current_batch = next(dataloader_iter)\r\n  File \"python3.10\/site-packages\/torch\/utils\/data\/dataloader.py\", line 633, in __next__\r\n    data = self._next_data()\r\n  File \"python3.10\/site-packages\/torch\/utils\/data\/dataloader.py\", line 677, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"python3.10\/site-packages\/torch\/utils\/data\/_utils\/fetch.py\", line 49, in fetch\r\n    data = self.dataset.__getitems__(possibly_batched_index)\r\n  File \"python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2807, in __getitems__\r\n    batch = self.__getitem__(keys)\r\n  File \"python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2803, in __getitem__\r\n    return self._getitem(key)\r\n  File \"python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2787, in _getitem\r\n    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n  File \"python3.10\/site-packages\/datasets\/formatting\/formatting.py\", line 583, in query_table\r\n    _check_valid_index_key(key, size)\r\n  File \"python3.10\/site-packages\/datasets\/formatting\/formatting.py\", line 536, in _check_valid_index_key\r\n    _check_valid_index_key(int(max(key)), size=size)\r\n  File \"python3.10\/site-packages\/datasets\/formatting\/formatting.py\", line 526, in _check_valid_index_key\r\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\r\nIndexError: Invalid key: 88 is out of bounds for size 0\r\n```\r\n\r\nSo I'm confused about why this error was generated, and how to fix it. Is this error produced by datasets or `torch.compile`?\n\n### Expected behavior\n\nI want to use `torch.compile` in my code.\n\n### Environment info\n\n- `datasets` version: 2.14.3\r\n- Platform: Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.10.8\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6128\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6128\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6127","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6127\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6127\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6127\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6127","id":1839746721,"node_id":"PR_kwDODunzps5XWdP5","number":6127,"title":"Fix authentication issues","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-08-07T15:41:25Z","updated_at":"2023-08-08T15:24:59Z","closed_at":"2023-08-08T15:16:22Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR fixes 3 authentication issues:\r\n- Fix authentication when passing `token`.\r\n- Fix authentication in `Audio.decode_example` and `Image.decode_example`.\r\n- Fix authentication to resolve `data_files` in repositories without script.\r\n\r\nThis PR also fixes our CI so that we properly test when passing `token` and we do not use the token stored in `HfFolder`.\r\n\r\nFix #6126.\r\n\r\n## Details\r\n\r\n\r\n### Fix authentication when passing `token`\r\n\r\nSee c0a77dc943de68a17f23f141517028c734c78623\r\n\r\nThe root issue was caused when the `token` was set in an already instantiated `DownloadConfig` and thus not propagated to `self._storage_options`:\r\n\r\n  ```python\r\n  download_config.token = token\r\n  ```\r\n\r\nAs this usage pattern is very common, the fix consists in overriding `DownloadConfig.__setattr__`.\r\n\r\nThis fixes authentication issues in the following functions:\r\n- `load_dataset` and `load_dataset_builder`\r\n- `Dataset.push_to_hub` and `Dataset.push_to_hub`\r\n- `inspect.get_dataset_config_info`, `inspect.get_dataset_infos` and `inspect.get_dataset_split_names`\r\n\r\n### Fix authentication in `Audio.decode_example` and `Image.decode_example`. \r\n\r\nSee: 58e62af004b6b8b84dcfd897a4bc71637cfa6c3f\r\n\r\nThe `token` was not set because the `repo_id` was wrongly tried to be parsed from an HTTP URL (`\"http:\/\/...\"`), instead of an HFFileSystem URL (`\"hf:\/\/\"`)\r\n\r\n### Fix authentication to resolve `data_files` in repositories without script\r\n\r\nSee: e4684fc1032321abf0d494b0c130ea7c82ebda80\r\n\r\nThis is fixed by passing `download_config` to the function `create_builder_configs_from_metadata_configs`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6127\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6127\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6127","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6127","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6127.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6127.patch","merged_at":"2023-08-08T15:16:22Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6126","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6126\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6126\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6126\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6126","id":1839675320,"node_id":"I_kwDODunzps5tpze4","number":6126,"title":"Private datasets do not load when passing token","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2023-08-07T15:06:47Z","updated_at":"2023-08-08T15:16:23Z","closed_at":"2023-08-08T15:16:23Z","author_association":"MEMBER","active_lock_reason":null,"body":"### Describe the bug\n\nSince the release of `datasets` 2.14, private\/gated datasets do not load when passing `token`: they raise  `EmptyDatasetError`.\r\n\r\nThis is a non-planned  backward incompatible breaking change.\r\n\r\nNote that private datasets do load if instead `download_config` is passed:\r\n```python\r\nfrom datasets import DownloadConfig, load_dataset\r\n\r\nds = load_dataset(\"albertvillanova\/tmp-private\", split=\"train\", download_config=DownloadConfig(token=\"<MY-TOKEN>\"))\r\nds\r\n```\r\ngives\r\n```\r\nDataset({\r\n    features: ['text'],\r\n    num_rows: 4\r\n})\r\n```\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"albertvillanova\/tmp-private\", split=\"train\", token=\"<MY-TOKEN>\")\r\n```\r\ngives\r\n```\r\n---------------------------------------------------------------------------\r\nEmptyDatasetError                         Traceback (most recent call last)\r\n[<ipython-input-2-25b48732107a>](https:\/\/localhost:8080\/#) in <cell line: 3>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 ds = load_dataset(\"albertvillanova\/tmp-private\", split=\"train\", token=\"<MY-TOKEN>\")\r\n\r\n5 frames\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py](https:\/\/localhost:8080\/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2107 \r\n   2108     # Create a dataset builder\r\n-> 2109     builder_instance = load_dataset_builder(\r\n   2110         path=path,\r\n   2111         name=name,\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py](https:\/\/localhost:8080\/#) in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\r\n   1793         download_config = download_config.copy() if download_config else DownloadConfig()\r\n   1794         download_config.storage_options.update(storage_options)\r\n-> 1795     dataset_module = dataset_module_factory(\r\n   1796         path,\r\n   1797         revision=revision,\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py](https:\/\/localhost:8080\/#) in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1484                     raise ConnectionError(f\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\") from None\r\n   1485                 if isinstance(e1, EmptyDatasetError):\r\n-> 1486                     raise e1 from None\r\n   1487                 if isinstance(e1, FileNotFoundError):\r\n   1488                     raise FileNotFoundError(\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py](https:\/\/localhost:8080\/#) in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1474                     download_config=download_config,\r\n   1475                     download_mode=download_mode,\r\n-> 1476                 ).get_module()\r\n   1477         except (\r\n   1478             Exception\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py](https:\/\/localhost:8080\/#) in get_module(self)\r\n   1030             sanitize_patterns(self.data_files)\r\n   1031             if self.data_files is not None\r\n-> 1032             else get_data_patterns(base_path, download_config=self.download_config)\r\n   1033         )\r\n   1034         data_files = DataFilesDict.from_patterns(\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/data_files.py](https:\/\/localhost:8080\/#) in get_data_patterns(base_path, download_config)\r\n    457         return _get_data_files_patterns(resolver)\r\n    458     except FileNotFoundError:\r\n--> 459         raise EmptyDatasetError(f\"The directory at {base_path} doesn't contain any data files\") from None\r\n    460 \r\n    461 \r\n\r\nEmptyDatasetError: The directory at hf:\/\/datasets\/albertvillanova\/tmp-private@79b9e4fe79670a9a050d6ebc385464891915a71d doesn't contain any data files\r\n```\n\n### Expected behavior\n\nThe dataset should load.\n\n### Environment info\n\n- `datasets` version: 2.14.3\r\n- Platform: Linux-5.15.109+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6126\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6126\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6125","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6125\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6125\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6125\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6125","id":1837980986,"node_id":"I_kwDODunzps5tjV06","number":6125,"title":"Reinforcement Learning and Robotics are not task categories in HF datasets metadata","user":{"login":"StoneT2000","id":35373228,"node_id":"MDQ6VXNlcjM1MzczMjI4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35373228?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/StoneT2000","html_url":"https:\/\/github.com\/StoneT2000","followers_url":"https:\/\/api.github.com\/users\/StoneT2000\/followers","following_url":"https:\/\/api.github.com\/users\/StoneT2000\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/StoneT2000\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/StoneT2000\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/StoneT2000\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/StoneT2000\/orgs","repos_url":"https:\/\/api.github.com\/users\/StoneT2000\/repos","events_url":"https:\/\/api.github.com\/users\/StoneT2000\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/StoneT2000\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-08-05T23:59:42Z","updated_at":"2023-08-18T12:28:42Z","closed_at":"2023-08-18T12:28:42Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nIn https:\/\/huggingface.co\/models there are task categories for RL and robotics but none in https:\/\/huggingface.co\/datasets\r\n\r\nOur lab is currently moving our datasets over to hugging face and would like to be able to add those 2 tags\r\n\r\nMoreover we see some older datasets that do have that tag, but we can't seem to add it ourselves.\n\n### Steps to reproduce the bug\n\n1. Create a new dataset on Hugging face\r\n2. Try to type reinforcemement-learning or robotics into the tasks categories, it does not allow you to commit\n\n### Expected behavior\n\nExpected to be able to add RL and robotics as task categories as some previous datasets have these tags\n\n### Environment info\n\nN\/A","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6125\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6125\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6124","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6124\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6124\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6124\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6124","id":1837868112,"node_id":"I_kwDODunzps5ti6RQ","number":6124,"title":"Datasets crashing runs due to KeyError","user":{"login":"conceptofmind","id":25208228,"node_id":"MDQ6VXNlcjI1MjA4MjI4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/25208228?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/conceptofmind","html_url":"https:\/\/github.com\/conceptofmind","followers_url":"https:\/\/api.github.com\/users\/conceptofmind\/followers","following_url":"https:\/\/api.github.com\/users\/conceptofmind\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/conceptofmind\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/conceptofmind\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/conceptofmind\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/conceptofmind\/orgs","repos_url":"https:\/\/api.github.com\/users\/conceptofmind\/repos","events_url":"https:\/\/api.github.com\/users\/conceptofmind\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/conceptofmind\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-08-05T17:48:56Z","updated_at":"2023-11-30T16:28:57Z","closed_at":"2023-11-30T16:28:57Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi all,\r\n\r\nI have been running into a pretty persistent issue recently when trying to load datasets. \r\n```python\r\n    train_dataset = load_dataset(\r\n        'llama-2-7b-tokenized', \r\n        split = 'train'\r\n    )\r\n```\r\nI receive a KeyError which crashes the runs.\r\n```\r\nTraceback (most recent call last):\r\n    main()\r\n\r\n    train_dataset = load_dataset(\r\n                    ^^^^^^^^^^^^^\r\n    builder_instance = load_dataset_builder(\r\n                       ^^^^^^^^^^^^^^^^^^^^^\r\n    dataset_module = dataset_module_factory(\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^\r\n    raise e1 from None\r\n\r\n    ).get_module()\r\n      ^^^^^^^^^^^^\r\n    else get_data_patterns(base_path, download_config=self.download_config)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    return _get_data_files_patterns(resolver)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    data_files = pattern_resolver(pattern)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    fs, _, _ = get_fs_token_paths(pattern, storage_options=storage_options)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    paths = [f for f in sorted(fs.glob(paths)) if not fs.isdir(f)]\r\n                               ^^^^^^^^^^^^^^\r\n\r\n    allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    for _, dirs, files in self.walk(path, maxdepth, detail=True, **kwargs):\r\n\r\n    listing = self.ls(path, detail=True, **kwargs)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    \"last_modified\": parse_datetime(tree_item[\"lastCommit\"][\"date\"]),\r\n                                    ~~~~~~~~~^^^^^^^^^^^^^^\r\nKeyError: 'lastCommit'\r\n```\r\nAny help would be greatly appreciated.\r\n\r\nThank you,\r\n\r\nEnrico\n\n### Steps to reproduce the bug\n\nLoad the dataset from the Huggingface hub.\r\n```python\r\n    train_dataset = load_dataset(\r\n        'llama-2-7b-tokenized', \r\n        split = 'train'\r\n    )\r\n```\n\n### Expected behavior\n\nLoads the dataset.\n\n### Environment info\n\ndatasets-2.14.3\r\nCUDA 11.8\r\nPython 3.11","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6124\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6124\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6123","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6123\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6123\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6123\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6123","id":1837789294,"node_id":"I_kwDODunzps5tinBu","number":6123,"title":"Inaccurate Bounding Boxes in \"wildreceipt\" Dataset","user":{"login":"HamzaGbada","id":50714796,"node_id":"MDQ6VXNlcjUwNzE0Nzk2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/50714796?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/HamzaGbada","html_url":"https:\/\/github.com\/HamzaGbada","followers_url":"https:\/\/api.github.com\/users\/HamzaGbada\/followers","following_url":"https:\/\/api.github.com\/users\/HamzaGbada\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/HamzaGbada\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/HamzaGbada\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/HamzaGbada\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/HamzaGbada\/orgs","repos_url":"https:\/\/api.github.com\/users\/HamzaGbada\/repos","events_url":"https:\/\/api.github.com\/users\/HamzaGbada\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/HamzaGbada\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-05T14:34:13Z","updated_at":"2023-08-17T14:25:27Z","closed_at":"2023-08-17T14:25:26Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI would like to bring to your attention an issue related to the accuracy of bounding boxes within the \"wildreceipt\" dataset, which is made available through the Hugging Face API. Specifically, I have identified a discrepancy between the bounding boxes generated by the dataset loading commands, namely `load_dataset(\"Theivaprakasham\/wildreceipt\")` and `load_dataset(\"jinhybr\/WildReceipt\")`, and the actual labels and corresponding bounding boxes present in the dataset.\r\n\r\nTo illustrate this divergence, I've provided two examples in the form of screenshots. These screenshots highlight the contrasting outcomes between my personal implementation of the dataloader and the implementation offered by Hugging Face:\r\n\r\n**Example 1:**\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/50714796\/7a6604d2-899d-4102-a008-1a28c90698f1)\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/50714796\/eba458c7-d3af-4868-a520-8b683aa96f66)\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/50714796\/9f394891-5f5b-46f7-8e52-071b724aedab)\r\n\r\n**Example 2:**\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/50714796\/a2b2a8d3-124e-4990-b64a-5133cf4be2fe)\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/50714796\/6ee25642-35aa-40ad-ac1e-899d33be90df)\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/50714796\/5e42ff91-9fc4-4520-8803-0e225656f96c)\r\n\r\nIt's important to note that my dataloader implementation is based on the same dataset files as utilized in the Hugging Face implementation. For your reference, you can access the dataset files through this link: [wildreceipt dataset files](https:\/\/download.openmmlab.com\/mmocr\/data\/wildreceipt.tar).\r\n\r\nThis inconsistency in bounding box accuracy warrants investigation and rectification for maintaining the integrity of the \"wildreceipt\" dataset. Your attention and assistance in addressing this matter would be greatly appreciated.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nfrom datasets import load_dataset\r\n\r\n\r\n# Define functions to convert bounding box formats\r\ndef convert_format1(box):\r\n    x, y, w, h = box\r\n    x2, y2 = x + w, y + h\r\n    return [x, y, x2, y2]\r\n\r\n\r\ndef convert_format2(box):\r\n    x1, y1, x2, y2 = box\r\n    return [x1, y1, x2, y2]\r\n\r\n\r\ndef plot_cropped_image(image, box, title):\r\n    cropped_image = image.crop(box)\r\n    plt.imshow(cropped_image)\r\n    plt.title(title)\r\n    plt.axis('off')\r\n    plt.savefig(title+'.png')\r\n    plt.show()\r\n\r\n\r\ndoc_index = 1\r\nword_index = 3\r\ndataset = load_dataset(\"Theivaprakasham\/wildreceipt\")['train']\r\n\r\nbbox_hugging_face = dataset[doc_index]['bboxes'][word_index]\r\ntext_unit_face = dataset[doc_index]['words'][word_index]\r\n\r\ncommon_box_hugface_1 = convert_format1(bbox_hugging_face)\r\ncommon_box_hugface_2 = convert_format2(bbox_hugging_face)\r\n\r\nplot_cropped_image(image_hugging, common_box_hugface_1,\r\n                   f'Hugging Face Bouding boxes (x,y,w,h format) \\n its associated text unit: {text_unit_face}')\r\nplot_cropped_image(image_hugging, common_box_hugface_2,\r\n                   f'Hugging Face Bouding boxes (x1,y1,x2, y2 format) \\n its associated text unit: {text_unit_face}')\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nThe bounding boxes generated by the \"wildreceipt\" dataset in HuggingFace implementation loading commands should accurately match the actual labels and bounding boxes of the dataset.\r\n\r\n\r\n### Environment info\r\n\r\n- Python version: 3.8\r\n- Hugging Face datasets version: 2.14.2\r\n- Dataset file taken from this link: https:\/\/download.openmmlab.com\/mmocr\/data\/wildreceipt.tar","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6123\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6123\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6122","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6122\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6122\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6122\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6122","id":1837335721,"node_id":"I_kwDODunzps5tg4Sp","number":6122,"title":"Upload README via `push_to_hub`","user":{"login":"liyucheng09","id":27999909,"node_id":"MDQ6VXNlcjI3OTk5OTA5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27999909?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/liyucheng09","html_url":"https:\/\/github.com\/liyucheng09","followers_url":"https:\/\/api.github.com\/users\/liyucheng09\/followers","following_url":"https:\/\/api.github.com\/users\/liyucheng09\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/liyucheng09\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/liyucheng09\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/liyucheng09\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/liyucheng09\/orgs","repos_url":"https:\/\/api.github.com\/users\/liyucheng09\/repos","events_url":"https:\/\/api.github.com\/users\/liyucheng09\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/liyucheng09\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-04T21:00:27Z","updated_at":"2023-08-21T18:18:54Z","closed_at":"2023-08-21T18:18:54Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\n`push_to_hub` now allows users to upload datasets programmatically. However, based on the latest doc, we still need to open the dataset page to add readme file manually.\r\n\r\nHowever, I do discover snippets to intialize a README for every `push_to_hub`:\r\n```\r\ndataset_card = (\r\n    DatasetCard(\r\n        \"---\\n\"\r\n        + str(dataset_card_data)\r\n        + \"\\n---\\n\"\r\n        + f'# Dataset Card for \"{repo_id.split(\"\/\")[-1]}\"\\n\\n[More Information needed](https:\/\/github.com\/huggingface\/datasets\/blob\/main\/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)'\r\n    )\r\n    if dataset_card is None\r\n    else dataset_card\r\n)\r\n\r\nHfApi(endpoint=config.HF_ENDPOINT).upload_file(\r\n    path_or_fileobj=str(dataset_card).encode(),\r\n    path_in_repo=\"README.md\",\r\n    repo_id=repo_id,\r\n    token=token,\r\n    repo_type=\"dataset\",\r\n    revision=branch,\r\n)\r\n```\r\n\r\nSo, if we can enable `push_to_hub` to upload a readme file by ourselves instead of using the auto generated ones, it can save ton of time, and will definitely alleviate the current \"lack-of-dataset-card\" situation.\r\n\r\n### Motivation\r\n\r\nas elabrated above.\r\n\r\n### Your contribution\r\n\r\nI might be able to make a pr.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6122\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6122\/timeline","performed_via_github_app":null,"state_reason":"not_planned","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6121","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6121\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6121\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6121\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6121","id":1836761712,"node_id":"PR_kwDODunzps5XMsWd","number":6121,"title":"Small typo in the code example of create imagefolder dataset","user":{"login":"WangXin93","id":19688994,"node_id":"MDQ6VXNlcjE5Njg4OTk0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/19688994?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/WangXin93","html_url":"https:\/\/github.com\/WangXin93","followers_url":"https:\/\/api.github.com\/users\/WangXin93\/followers","following_url":"https:\/\/api.github.com\/users\/WangXin93\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/WangXin93\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/WangXin93\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/WangXin93\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/WangXin93\/orgs","repos_url":"https:\/\/api.github.com\/users\/WangXin93\/repos","events_url":"https:\/\/api.github.com\/users\/WangXin93\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/WangXin93\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-04T13:36:59Z","updated_at":"2023-08-04T13:45:32Z","closed_at":"2023-08-04T13:41:43Z","author_association":"NONE","active_lock_reason":null,"body":"Fix type of code example of load imagefolder dataset","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6121\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6121\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6121","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6121","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6121.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6121.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6120","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6120\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6120\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6120\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6120","id":1836026938,"node_id":"I_kwDODunzps5tb4w6","number":6120,"title":"Lookahead streaming support?","user":{"login":"PicoCreator","id":17175484,"node_id":"MDQ6VXNlcjE3MTc1NDg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17175484?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/PicoCreator","html_url":"https:\/\/github.com\/PicoCreator","followers_url":"https:\/\/api.github.com\/users\/PicoCreator\/followers","following_url":"https:\/\/api.github.com\/users\/PicoCreator\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/PicoCreator\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/PicoCreator\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/PicoCreator\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/PicoCreator\/orgs","repos_url":"https:\/\/api.github.com\/users\/PicoCreator\/repos","events_url":"https:\/\/api.github.com\/users\/PicoCreator\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/PicoCreator\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-04T04:01:52Z","updated_at":"2023-08-17T17:48:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nFrom what I understand, streaming dataset currently pulls the data, and process the data as it is requested.\r\nThis can introduce significant latency delays when data is loaded into the training process, needing to wait for each segment.\r\n\r\nWhile the delays might be dataset specific (or even mapping instruction\/tokenizer specific)\r\n\r\nIs it possible to introduce a `streaming_lookahead` parameter, which is used for predictable workloads (even shuffled dataset with fixed seed). As we can predict in advance what the next few datasamples will be. And fetch them while the current set is being trained.\r\n\r\nWith enough CPU & bandwidth to keep up with the training process, and a sufficiently large lookahead, this will reduce the various latency involved while waiting for the dataset to be ready between batches.\r\n\r\n### Motivation\r\n\r\nFaster streaming performance, while training over extra large TB sized datasets\r\n\r\n### Your contribution\r\n\r\nI currently use HF dataset, with pytorch lightning trainer for RWKV project, and would be able to help test this feature if supported.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6120\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6120\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6119","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6119\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6119\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6119\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6119","id":1835996350,"node_id":"PR_kwDODunzps5XKI19","number":6119,"title":"[Docs] Add description of `select_columns` to guide","user":{"login":"unifyh","id":18213435,"node_id":"MDQ6VXNlcjE4MjEzNDM1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/18213435?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/unifyh","html_url":"https:\/\/github.com\/unifyh","followers_url":"https:\/\/api.github.com\/users\/unifyh\/followers","following_url":"https:\/\/api.github.com\/users\/unifyh\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/unifyh\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/unifyh\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/unifyh\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/unifyh\/orgs","repos_url":"https:\/\/api.github.com\/users\/unifyh\/repos","events_url":"https:\/\/api.github.com\/users\/unifyh\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/unifyh\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-04T03:13:30Z","updated_at":"2023-08-16T10:13:02Z","closed_at":"2023-08-16T10:02:52Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Closes #6116 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6119\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6119\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6119","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6119","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6119.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6119.patch","merged_at":"2023-08-16T10:02:52Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6118","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6118\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6118\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6118\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6118","id":1835940417,"node_id":"I_kwDODunzps5tbjpB","number":6118,"title":"IterableDataset.from_generator() fails with pickle error when provided a generator or iterator","user":{"login":"finkga","id":1281051,"node_id":"MDQ6VXNlcjEyODEwNTE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1281051?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/finkga","html_url":"https:\/\/github.com\/finkga","followers_url":"https:\/\/api.github.com\/users\/finkga\/followers","following_url":"https:\/\/api.github.com\/users\/finkga\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/finkga\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/finkga\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/finkga\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/finkga\/orgs","repos_url":"https:\/\/api.github.com\/users\/finkga\/repos","events_url":"https:\/\/api.github.com\/users\/finkga\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/finkga\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-04T01:45:04Z","updated_at":"2023-12-04T09:28:50Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n**Description**\r\nProviding a generator in an instantiation of IterableDataset.from_generator() fails with `TypeError: cannot pickle 'generator' object` when the generator argument is supplied with a generator. \r\n\r\n**Code example**\r\n```\r\ndef line_generator(files: List[Path]):\r\n\r\n    if isinstance(files, str):\r\n        files = [Path(files)]\r\n\r\n    for file in files:\r\n        if isinstance(file, str):\r\n            file = Path(file)\r\n        yield from open(file,'r').readlines()\r\n\r\n...\r\nmodel_training_files = ['file1.txt', 'file2.txt', 'file3.txt']\r\ntrain_dataset = IterableDataset.from_generator(generator=line_generator(model_training_files))\r\n```\r\n\r\n**Traceback**\r\nTraceback (most recent call last):\r\n  File \"\/Library\/Developer\/CommandLineTools\/Library\/Frameworks\/Python3.framework\/Versions\/3.9\/lib\/python3.9\/contextlib.py\", line 135, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"\/Users\/d3p692\/code\/clem_bert\/venv\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in _no_cache_fields\r\n    yield\r\n  File \"\/Users\/d3p692\/code\/clem_bert\/venv\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 701, in dumps\r\n    dump(obj, file)\r\n  File \"\/Users\/d3p692\/code\/clem_bert\/venv\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 676, in dump\r\n    Pickler(file, recurse=True).dump(obj)\r\n  File \"\/Users\/d3p692\/code\/clem_bert\/venv\/lib\/python3.9\/site-packages\/dill\/_dill.py\", line 394, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \"\/Library\/Developer\/CommandLineTools\/Library\/Frameworks\/Python3.framework\/Versions\/3.9\/lib\/python3.9\/pickle.py\", line 487, in dump\r\n    self.save(obj)\r\n  File \"\/Users\/d3p692\/code\/clem_bert\/venv\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 666, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/Users\/d3p692\/code\/clem_bert\/venv\/lib\/python3.9\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/Library\/Developer\/CommandLineTools\/Library\/Frameworks\/Python3.framework\/Versions\/3.9\/lib\/python3.9\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/Users\/d3p692\/code\/clem_bert\/venv\/lib\/python3.9\/site-packages\/dill\/_dill.py\", line 1186, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"\/Library\/Developer\/CommandLineTools\/Library\/Frameworks\/Python3.framework\/Versions\/3.9\/lib\/python3.9\/pickle.py\", line 971, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/Library\/Developer\/CommandLineTools\/Library\/Frameworks\/Python3.framework\/Versions\/3.9\/lib\/python3.9\/pickle.py\", line 997, in _batch_setitems\r\n    save(v)\r\n  File \"\/Users\/d3p692\/code\/clem_bert\/venv\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 666, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/Users\/d3p692\/code\/clem_bert\/venv\/lib\/python3.9\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/Library\/Developer\/CommandLineTools\/Library\/Frameworks\/Python3.framework\/Versions\/3.9\/lib\/python3.9\/pickle.py\", line 578, in save\r\n    rv = reduce(self.proto)\r\nTypeError: cannot pickle 'generator' object\r\n\n\n### Steps to reproduce the bug\n\n1. Create a set of text files to iterate over. \r\n2. Create a generator that returns the lines in each file until all files are exhausted.\r\n3. Instantiate the dataset over the generator by instantiating an IterableDataset.from_generator().\r\n4. Wait for the explosion.\n\n### Expected behavior\n\nI would expect that since the function claims to accept a generator that there would be no crash. Instead, I would expect the dataset to return all the lines in the files as queued up in the `line_generator()` function.\n\n### Environment info\n\ndatasets.__version__ == '2.13.1'\r\nPython 3.9.6\r\nPlatform: Darwin WE35261 22.5.0 Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:22 PDT 2023; root:xnu-8796.121.3~7\/RELEASE_X86_64 x86_64\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6118\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6118\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6117","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6117\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6117\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6117\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6117","id":1835213848,"node_id":"PR_kwDODunzps5XHktw","number":6117,"title":"Set dev version","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-03T14:46:04Z","updated_at":"2023-08-03T14:56:59Z","closed_at":"2023-08-03T14:46:18Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6117\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6117\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6117","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6117","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6117.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6117.patch","merged_at":"2023-08-03T14:46:18Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6116","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6116\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6116\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6116\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6116","id":1835098484,"node_id":"I_kwDODunzps5tYWF0","number":6116,"title":"[Docs] The \"Process\" how-to guide lacks description of `select_columns` function","user":{"login":"unifyh","id":18213435,"node_id":"MDQ6VXNlcjE4MjEzNDM1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/18213435?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/unifyh","html_url":"https:\/\/github.com\/unifyh","followers_url":"https:\/\/api.github.com\/users\/unifyh\/followers","following_url":"https:\/\/api.github.com\/users\/unifyh\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/unifyh\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/unifyh\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/unifyh\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/unifyh\/orgs","repos_url":"https:\/\/api.github.com\/users\/unifyh\/repos","events_url":"https:\/\/api.github.com\/users\/unifyh\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/unifyh\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-03T13:45:10Z","updated_at":"2023-08-16T10:02:53Z","closed_at":"2023-08-16T10:02:53Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nThe [how to process dataset guide](https:\/\/huggingface.co\/docs\/datasets\/main\/en\/process) currently does not mention the [`select_columns`](https:\/\/huggingface.co\/docs\/datasets\/main\/en\/package_reference\/main_classes#datasets.Dataset.select_columns) function. It would be nice to include it in the guide.\n\n### Motivation\n\nThis function is a commonly requested feature (see this [forum thread](https:\/\/discuss.huggingface.co\/t\/how-to-create-a-new-dataset-from-another-dataset-and-select-specific-columns-and-the-data-along-with-the-column\/15120) and #5468 #5474). However, it has not been included in the guide since its implementation by PR #5480.\r\n\r\nMentioning it in the guide would help future users discover this added feature.\n\n### Your contribution\n\nI could submit a PR to add a brief description of the function to said guide.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6116\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6116\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6115","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6115\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6115\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6115\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6115","id":1834765485,"node_id":"PR_kwDODunzps5XGChP","number":6115,"title":"Release: 2.14.3","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-08-03T10:18:32Z","updated_at":"2023-08-03T15:08:02Z","closed_at":"2023-08-03T10:24:57Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6115\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6115\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6115","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6115","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6115.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6115.patch","merged_at":"2023-08-03T10:24:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6114","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6114\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6114\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6114\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6114","id":1834015584,"node_id":"I_kwDODunzps5tUNtg","number":6114,"title":"Cache not being used when loading commonvoice 8.0.0","user":{"login":"clabornd","id":31082141,"node_id":"MDQ6VXNlcjMxMDgyMTQx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/31082141?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/clabornd","html_url":"https:\/\/github.com\/clabornd","followers_url":"https:\/\/api.github.com\/users\/clabornd\/followers","following_url":"https:\/\/api.github.com\/users\/clabornd\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/clabornd\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/clabornd\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/clabornd\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/clabornd\/orgs","repos_url":"https:\/\/api.github.com\/users\/clabornd\/repos","events_url":"https:\/\/api.github.com\/users\/clabornd\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/clabornd\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-08-02T23:18:11Z","updated_at":"2023-08-18T23:59:00Z","closed_at":"2023-08-18T23:59:00Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI have commonvoice 8.0.0 downloaded in `~\/.cache\/huggingface\/datasets\/mozilla-foundation___common_voice_8_0\/en\/8.0.0\/b2f8b72f8f30b2e98c41ccf855954d9e35a5fa498c43332df198534ff9797a4a`.  The folder contains all the arrow files etc, and was used as the cached version last time I touched the ec2 instance I'm working on.  Now, with the same command that downloaded it initially:\r\n\r\n```\r\ndataset = load_dataset(\"mozilla-foundation\/common_voice_8_0\", \"en\", use_auth_token=\"<mytoken>\")\r\n```\r\n\r\nit tries to redownload the dataset to `~\/.cache\/huggingface\/datasets\/mozilla-foundation___common_voice_8_0\/en\/8.0.0\/05bdc7940b0a336ceeaeef13470c89522c29a8e4494cbeece64fb472a87acb32`\r\n\r\n### Steps to reproduce the bug\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.  ```dataset = load_dataset(\"mozilla-foundation\/common_voice_8_0\", \"en\", use_auth_token=\"<mytoken>\")```\r\n2. dataset is updated by maintainers\r\n3. ```dataset = load_dataset(\"mozilla-foundation\/common_voice_8_0\", \"en\", use_auth_token=\"<mytoken>\")```\r\n\r\n### Expected behavior\r\n\r\nI expect that it uses the already downloaded data in `~\/.cache\/huggingface\/datasets\/mozilla-foundation___common_voice_8_0\/en\/8.0.0\/b2f8b72f8f30b2e98c41ccf855954d9e35a5fa498c43332df198534ff9797a4a`.\r\n\r\nNot sure what's happening in 2. but if, say it's an issue with the dataset referenced by \"mozilla-foundation\/common_voice_8_0\" being modified by the maintainers, how would I force datasets to point to the original version I downloaded?\r\n\r\nEDIT:  It was indeed that the maintainers had updated the dataset (v 8.0.0).  However I still cant load the dataset from disk instead of redownloading, with for example:\r\n\r\n```\r\nload_dataset(\".cache\/huggingface\/datasets\/downloads\/extracted\/<hash>\/cv-corpus-8.0-2022-01-19\/en\/\", \"en\")\r\n\r\n> ...\r\n> File [~\/miniconda3\/envs\/aa_torch2\/lib\/python3.10\/site-packages\/datasets\/table.py:1938](...\/ python3.10\/site-packages\/datasets\/table.py:1938), in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1937 elif not isinstance(feature, (Sequence, dict, list, tuple)):\r\n-> 1938     return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n...\r\n   1794         e = e.__context__\r\n-> 1795     raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1797 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n\r\n### Environment info\r\n\r\ndatasets==2.7.0\r\npython==3.10.8\r\nOS:  AWS Linux","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6114\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6114\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6113","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6113\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6113\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6113\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6113","id":1833854030,"node_id":"I_kwDODunzps5tTmRO","number":6113,"title":"load_dataset() fails with streamlit caching inside docker","user":{"login":"fierval","id":987574,"node_id":"MDQ6VXNlcjk4NzU3NA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/987574?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/fierval","html_url":"https:\/\/github.com\/fierval","followers_url":"https:\/\/api.github.com\/users\/fierval\/followers","following_url":"https:\/\/api.github.com\/users\/fierval\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/fierval\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/fierval\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/fierval\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/fierval\/orgs","repos_url":"https:\/\/api.github.com\/users\/fierval\/repos","events_url":"https:\/\/api.github.com\/users\/fierval\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/fierval\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-02T20:20:26Z","updated_at":"2023-08-21T18:18:27Z","closed_at":"2023-08-21T18:18:27Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen calling `load_dataset` in a streamlit application running within a docker container, get a failure with the error message:\r\n\r\nEmptyDatasetError: The directory at hf:\/\/datasets\/fetch-rewards\/inc-rings-2000@bea27cf60842b3641eae418f38864a2ec4cde684 doesn't contain any data files\r\nTraceback:\r\nFile \"\/opt\/conda\/lib\/python3.10\/site-packages\/streamlit\/runtime\/scriptrunner\/script_runner.py\", line 552, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"\/home\/user\/app\/app.py\", line 62, in <module>\r\n    dashboard()\r\nFile \"\/home\/user\/app\/app.py\", line 47, in dashboard\r\n    feat_dict, path_gml = load_data(hf_repo, model_gml_dict[selected_model], hf_token)\r\nFile \"\/opt\/conda\/lib\/python3.10\/site-packages\/streamlit\/runtime\/caching\/cache_utils.py\", line 211, in wrapper\r\n    return cached_func(*args, **kwargs)\r\nFile \"\/opt\/conda\/lib\/python3.10\/site-packages\/streamlit\/runtime\/caching\/cache_utils.py\", line 240, in __call__\r\n    return self._get_or_create_cached_value(args, kwargs)\r\nFile \"\/opt\/conda\/lib\/python3.10\/site-packages\/streamlit\/runtime\/caching\/cache_utils.py\", line 266, in _get_or_create_cached_value\r\n    return self._handle_cache_miss(cache, value_key, func_args, func_kwargs)\r\nFile \"\/opt\/conda\/lib\/python3.10\/site-packages\/streamlit\/runtime\/caching\/cache_utils.py\", line 320, in _handle_cache_miss\r\n    computed_value = self._info.func(*func_args, **func_kwargs)\r\nFile \"\/home\/user\/app\/hf_interface.py\", line 16, in load_data\r\n    hf_dataset = load_dataset(repo_id, use_auth_token=hf_token)\r\nFile \"\/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 2109, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\nFile \"\/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1795, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\nFile \"\/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1486, in dataset_module_factory\r\n    raise e1 from None\r\nFile \"\/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1476, in dataset_module_factory\r\n    ).get_module()\r\nFile \"\/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1032, in get_module\r\n    else get_data_patterns(base_path, download_config=self.download_config)\r\nFile \"\/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/data_files.py\", line 458, in get_data_patterns\r\n    raise EmptyDatasetError(f\"The directory at {base_path} doesn't contain any data files\") from None\n\n### Steps to reproduce the bug\n\n```python\r\n@st.cache_resource\r\ndef load_data(repo_id: str, hf_token=None):\r\n    \"\"\"Load data from HuggingFace Hub\r\n    \"\"\"\r\n    \r\n    hf_dataset = load_dataset(repo_id, use_auth_token=hf_token)\r\n    hf_dataset = hf_dataset.map(lambda x: json.loads(x[\"ground_truth\"]), remove_columns=[\"ground_truth\"])\r\n\r\n    return hf_dataset\r\n```\n\n### Expected behavior\n\nExpect to load. \r\nNote: works fine with datasets==2.13.1\n\n### Environment info\n\ndatasets==2.14.2, \r\nUbuntu bionic-based Docker container.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6113\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6113\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6112","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6112\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6112\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6112\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6112","id":1833693299,"node_id":"I_kwDODunzps5tS_Bz","number":6112,"title":"yaml error using push_to_hub with generated README.md","user":{"login":"kevintee","id":1643887,"node_id":"MDQ6VXNlcjE2NDM4ODc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1643887?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kevintee","html_url":"https:\/\/github.com\/kevintee","followers_url":"https:\/\/api.github.com\/users\/kevintee\/followers","following_url":"https:\/\/api.github.com\/users\/kevintee\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kevintee\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kevintee\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kevintee\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kevintee\/orgs","repos_url":"https:\/\/api.github.com\/users\/kevintee\/repos","events_url":"https:\/\/api.github.com\/users\/kevintee\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kevintee\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-08-02T18:21:21Z","updated_at":"2023-12-12T15:00:44Z","closed_at":"2023-12-12T15:00:44Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I construct a dataset with the following features:\r\n```\r\nfeatures = Features(\r\n    {\r\n        \"pixel_values\": Array3D(dtype=\"float64\", shape=(3, 224, 224)),\r\n        \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\r\n        \"attention_mask\": Sequence(Value(dtype=\"int64\")),\r\n        \"tokens\": Sequence(Value(dtype=\"string\")),\r\n        \"bbox\": Array2D(dtype=\"int64\", shape=(512, 4)),\r\n    }\r\n)\r\n```\r\nand run `push_to_hub`, the individual `*.parquet` files are pushed, but when trying to upload the auto-generated README, I run into the following error: \r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/kevintee\/.pyenv\/versions\/dev2\/lib\/python3.10\/site-packages\/huggingface_hub\/utils\/_errors.py\", line 261, in hf_raise_for_status\r\n    response.raise_for_status()\r\n  File \"\/Users\/kevintee\/.pyenv\/versions\/dev2\/lib\/python3.10\/site-packages\/requests\/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/huggingface.co\/api\/datasets\/looppayments\/multitask_document_classification_dataset\/commit\/main\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/kevintee\/loop-payments\/ml\/src\/ml\/data_scripts\/build_document_classification_training_data.py\", line 297, in <module>\r\n    build_dataset()\r\n  File \"\/Users\/kevintee\/loop-payments\/ml\/src\/ml\/data_scripts\/build_document_classification_training_data.py\", line 290, in build_dataset\r\n    push_to_hub(dataset, \"multitask_document_classification_dataset\")\r\n  File \"\/Users\/kevintee\/loop-payments\/ml\/src\/ml\/data_scripts\/build_document_classification_training_data.py\", line 135, in push_to_hub\r\n    dataset.push_to_hub(f\"looppayments\/{dataset_name}\", private=True)\r\n  File \"\/Users\/kevintee\/.pyenv\/versions\/dev2\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 5577, in push_to_hub\r\n    HfApi(endpoint=config.HF_ENDPOINT).upload_file(\r\n  File \"\/Users\/kevintee\/.pyenv\/versions\/dev2\/lib\/python3.10\/site-packages\/huggingface_hub\/utils\/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/Users\/kevintee\/.pyenv\/versions\/dev2\/lib\/python3.10\/site-packages\/huggingface_hub\/hf_api.py\", line 828, in _inner\r\n    return fn(self, *args, **kwargs)\r\n  File \"\/Users\/kevintee\/.pyenv\/versions\/dev2\/lib\/python3.10\/site-packages\/huggingface_hub\/hf_api.py\", line 3221, in upload_file\r\n    commit_info = self.create_commit(\r\n  File \"\/Users\/kevintee\/.pyenv\/versions\/dev2\/lib\/python3.10\/site-packages\/huggingface_hub\/utils\/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/Users\/kevintee\/.pyenv\/versions\/dev2\/lib\/python3.10\/site-packages\/huggingface_hub\/hf_api.py\", line 828, in _inner\r\n    return fn(self, *args, **kwargs)\r\n  File \"\/Users\/kevintee\/.pyenv\/versions\/dev2\/lib\/python3.10\/site-packages\/huggingface_hub\/hf_api.py\", line 2728, in create_commit\r\n    hf_raise_for_status(commit_resp, endpoint_name=\"commit\")\r\n  File \"\/Users\/kevintee\/.pyenv\/versions\/dev2\/lib\/python3.10\/site-packages\/huggingface_hub\/utils\/_errors.py\", line 299, in hf_raise_for_status\r\n    raise BadRequestError(message, response=response) from e\r\nhuggingface_hub.utils._errors.BadRequestError:  (Request ID: Root=1-64ca9c3d-2d2bbef354e102482a9a168e;bc00371c-8549-4859-9f41-43ff140ad36e)\r\n\r\nBad request for commit endpoint:\r\nInvalid YAML in README.md: unknown tag !<tag:yaml.org,2002:python\/tuple> (10:9)\r\n\r\n  7 |         - 3\r\n  8 |         - 224\r\n  9 |         - 224\r\n 10 |         dtype: float64\r\n--------------^\r\n 11 |   - name: input_ids\r\n 12 |     sequence: int64\r\n```\r\n\r\nMy guess is that the auto-generated yaml is unable to be parsed for some reason.\n\n### Steps to reproduce the bug\n\nThe description contains most of what's needed to reproduce the issue, but I've added a shortened code snippet:\r\n\r\n```\r\nfrom datasets import Array2D, Array3D, ClassLabel, Dataset, Features, Sequence, Value\r\nfrom PIL import Image\r\nfrom transformers import AutoProcessor\r\n\r\nfeatures = Features(\r\n    {\r\n        \"pixel_values\": Array3D(dtype=\"float64\", shape=(3, 224, 224)),\r\n        \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\r\n        \"attention_mask\": Sequence(Value(dtype=\"int64\")),\r\n        \"tokens\": Sequence(Value(dtype=\"string\")),\r\n        \"bbox\": Array2D(dtype=\"int64\", shape=(512, 4)),\r\n    }\r\n)\r\n\r\nprocessor = AutoProcessor.from_pretrained(\"microsoft\/layoutlmv3-base\", apply_ocr=False)\r\n\r\ndef preprocess_dataset(rows):\r\n    # Get images\r\n    images = [\r\n        Image.open(png_filename).convert(\"RGB\") for png_filename in rows[\"png_filename\"]\r\n    ]\r\n\r\n    encoding = processor(\r\n        images,\r\n        rows[\"tokens\"],\r\n        boxes=rows[\"bbox\"],\r\n        truncation=True,\r\n        padding=\"max_length\",\r\n    )\r\n    encoding[\"tokens\"] = rows[\"tokens\"]\r\n    return encoding\r\n\r\ndataset = dataset.map(\r\n    preprocess_dataset,\r\n    batched=True,\r\n    batch_size=5,\r\n    features=features,\r\n)\r\n```\n\n### Expected behavior\n\nUsing datasets==2.11.0, I'm able to succesfully push_to_hub, no issues, but with datasets==2.14.2, I run into the above error.\n\n### Environment info\n\n- `datasets` version: 2.14.2\r\n- Platform: macOS-12.5-arm64-arm-64bit\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6112\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6112\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6111","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6111\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6111\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6111\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6111","id":1832781654,"node_id":"I_kwDODunzps5tPgdW","number":6111,"title":"raise FileNotFoundError(\"Directory {dataset_path} is neither a `Dataset` directory nor a `DatasetDict` directory.\" )","user":{"login":"2catycm","id":41530341,"node_id":"MDQ6VXNlcjQxNTMwMzQx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/41530341?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/2catycm","html_url":"https:\/\/github.com\/2catycm","followers_url":"https:\/\/api.github.com\/users\/2catycm\/followers","following_url":"https:\/\/api.github.com\/users\/2catycm\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/2catycm\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/2catycm\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/2catycm\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/2catycm\/orgs","repos_url":"https:\/\/api.github.com\/users\/2catycm\/repos","events_url":"https:\/\/api.github.com\/users\/2catycm\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/2catycm\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-08-02T09:17:29Z","updated_at":"2023-08-29T02:00:28Z","closed_at":"2023-08-29T02:00:28Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nFor researchers in some countries or regions, it is usually the case that the download ability of `load_dataset` is disabled due to the complex network environment. People in these regions often prefer to use git clone or other programming tricks to manually download the files to the disk (for example, [How to elegantly download hf models, zhihu zhuanlan](https:\/\/zhuanlan.zhihu.com\/p\/475260268) proposed a crawlder based solution,  and [Is there any mirror for hf_hub, zhihu answer](https:\/\/www.zhihu.com\/question\/371644077) provided some cloud based solutions, and [How to avoid pitfalls on Hugging face downloading, zhihu zhuanlan] gave some useful suggestions), and then use `load_from_disk` to get the dataset object. \r\nHowever, when one finally has the local files on the disk, it is still buggy when trying to load the files into objects. \n\n### Steps to reproduce the bug\n\nSteps to reproduce the bug:\r\n1. Found CIFAR dataset in hugging face: https:\/\/huggingface.co\/datasets\/cifar100\/tree\/main\r\n2. Click \":\" button to show \"Clone repository\" option, and then follow the prompts on the box:\r\n ```bash\r\n  cd my_directory_absolute\r\n  git lfs install\r\n  git clone https:\/\/huggingface.co\/datasets\/cifar100\r\n  ls  my_directory_absolute\/cifar100 # confirm that the directory exists and it is OK. \r\n  ```\r\n3. Write A python file to try to load the dataset\r\n```python\r\nfrom datasets import load_dataset, load_from_disk\r\ndataset = load_from_disk(\"my_directory_absolute\/cifar100\")\r\n```\r\nNotice that according to issue #3700 , it is wrong to use load_dataset(\"my_directory_absolute\/cifar100\"), so we must use load_from_disk instead. \r\n\r\n4. Then you will see the error reported:\r\n```log\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\nCell In[5], line 9\r\n      1 from datasets import load_dataset, load_from_disk\r\n----> 9 dataset = load_from_disk(\"my_directory_absolute\/cifar100\")\r\n\r\nFile [~\/miniconda3\/envs\/ai\/lib\/python3.10\/site-packages\/datasets\/load.py:2232), in load_from_disk(dataset_path, fs, keep_in_memory, storage_options)\r\n   2230     return DatasetDict.load_from_disk(dataset_path, keep_in_memory=keep_in_memory, storage_options=storage_options)\r\n   2231 else:\r\n-> 2232     raise FileNotFoundError(\r\n   2233         f\"Directory {dataset_path} is neither a `Dataset` directory nor a `DatasetDict` directory.\"\r\n   2234     )\r\n\r\nFileNotFoundError: Directory my_directory_absolute\/cifar100 is neither a `Dataset` directory nor a `DatasetDict` directory.\r\n```\n\n### Expected behavior\n\nThe dataset should be load successfully. \n\n### Environment info\n\n```bash\r\ndatasets-cli env\r\n```\r\n-> results:\r\n```txt\r\n\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.14.2\r\n- Platform: Linux-4.18.0-372.32.1.el8_6.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6111\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6111\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6110","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6110\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6110\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6110\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6110","id":1831110633,"node_id":"I_kwDODunzps5tJIfp","number":6110,"title":"[BUG] Dataset initialized from in-memory data does not create cache.","user":{"login":"MattYoon","id":57797966,"node_id":"MDQ6VXNlcjU3Nzk3OTY2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57797966?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/MattYoon","html_url":"https:\/\/github.com\/MattYoon","followers_url":"https:\/\/api.github.com\/users\/MattYoon\/followers","following_url":"https:\/\/api.github.com\/users\/MattYoon\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/MattYoon\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/MattYoon\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/MattYoon\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/MattYoon\/orgs","repos_url":"https:\/\/api.github.com\/users\/MattYoon\/repos","events_url":"https:\/\/api.github.com\/users\/MattYoon\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/MattYoon\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-08-01T11:58:58Z","updated_at":"2023-08-17T14:03:01Z","closed_at":"2023-08-17T14:03:00Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\n`Dataset` initialized from in-memory data (dictionary in my case, haven't tested with other types) does not create cache when processed with the `map` method, unlike `Dataset` initialized by other methods such as `load_dataset`.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\n# below code was run the second time so the map function can be loaded from cache if exists\r\n\r\nfrom datasets import load_dataset, Dataset\r\n\r\ndataset = load_dataset(\"tatsu-lab\/alpaca\")['train']\r\n\r\ndataset = dataset.map(lambda x: {'input': x['input'] + 'hi'}) # some random map\r\nprint(len(dataset.cache_files))\r\n# 1\r\n\r\n# copy the exact same data but initialize from a dictionary\r\nmemory_dataset = Dataset.from_dict({\r\n    'instruction': dataset['instruction'],\r\n    'input': dataset['input'],\r\n    'output': dataset['output'],\r\n    'text': dataset['text']})\r\n\r\nmemory_dataset = memory_dataset.map(lambda x: {'input': x['input'] + 'hi'}) # exact same map\r\nprint(len(memory_dataset.cache_files))\r\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 52002[\/52002]\r\n# 0\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe `map` function should create cache regardless of the method the `Dataset` was created.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.2\r\n- Platform: Linux-5.15.0-41-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6110\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6110\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6109","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6109\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6109\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6109\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6109","id":1830753793,"node_id":"I_kwDODunzps5tHxYB","number":6109,"title":"Problems in downloading Amazon reviews from HF","user":{"login":"610v4nn1","id":52964960,"node_id":"MDQ6VXNlcjUyOTY0OTYw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/52964960?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/610v4nn1","html_url":"https:\/\/github.com\/610v4nn1","followers_url":"https:\/\/api.github.com\/users\/610v4nn1\/followers","following_url":"https:\/\/api.github.com\/users\/610v4nn1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/610v4nn1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/610v4nn1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/610v4nn1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/610v4nn1\/orgs","repos_url":"https:\/\/api.github.com\/users\/610v4nn1\/repos","events_url":"https:\/\/api.github.com\/users\/610v4nn1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/610v4nn1\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-08-01T08:38:29Z","updated_at":"2023-08-02T07:12:07Z","closed_at":"2023-08-02T07:12:07Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI have a script downloading `amazon_reviews_multi`.\r\n\r\nWhen the download starts, I get\r\n\r\n```\r\nDownloading data files:   0%|          | 0\/1 [00:00<?, ?it\/s]\r\nDownloading data: 243B [00:00, 1.43MB\/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:01<00:00,  1.54s\/it]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 842.40it\/s]\r\nDownloading data files:   0%|          | 0\/1 [00:00<?, ?it\/s]\r\nDownloading data: 243B [00:00, 928kB\/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:01<00:00,  1.42s\/it]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 832.70it\/s]\r\nDownloading data files:   0%|          | 0\/1 [00:00<?, ?it\/s]\r\nDownloading data: 243B [00:00, 1.81MB\/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:01<00:00,  1.40s\/it]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 1294.14it\/s]\r\nGenerating train split:   0%|          | 0\/200000 [00:00<?, ? examples\/s]\r\n```\r\n\r\nthe file is clearly too small to contain the requested dataset, in fact it contains en error message:\r\n\r\n```\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<Error><Code>AccessDenied<\/Code><Message>Access Denied<\/Message><RequestId>AGJWSY3ZADT2QVWE<\/RequestId><HostId>Gx1O2KXnxtQFqvzDLxyVSTq3+TTJuTnuVFnJL3SP89Yp8UzvYLPTVwd1PpniE4EvQzT3tCaqEJw=<\/HostId><\/Error>\r\n```\r\n\r\nobviously the script fails:\r\n\r\n```\r\n>           raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\nE           datasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n\n\n### Steps to reproduce the bug\n\n1. load_dataset(\"amazon_reviews_multi\", name=\"en\", split=\"train\", cache_dir=\"ADDYOURPATHHERE\")\n\n### Expected behavior\n\nI would expect the dataset to be downloaded and processed\n\n### Environment info\n\n* The problem is present with both datasets 2.12.0 and 2.14.2\r\n* python version 3.10.12","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6109\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6109\/timeline","performed_via_github_app":null,"state_reason":"not_planned","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6108","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6108\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6108\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6108\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6108","id":1830347187,"node_id":"I_kwDODunzps5tGOGz","number":6108,"title":"Loading local datasets got strangely stuck","user":{"login":"LoveCatc","id":48412571,"node_id":"MDQ6VXNlcjQ4NDEyNTcx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/48412571?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/LoveCatc","html_url":"https:\/\/github.com\/LoveCatc","followers_url":"https:\/\/api.github.com\/users\/LoveCatc\/followers","following_url":"https:\/\/api.github.com\/users\/LoveCatc\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/LoveCatc\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/LoveCatc\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/LoveCatc\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/LoveCatc\/orgs","repos_url":"https:\/\/api.github.com\/users\/LoveCatc\/repos","events_url":"https:\/\/api.github.com\/users\/LoveCatc\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/LoveCatc\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-08-01T02:28:06Z","updated_at":"2023-09-25T06:30:52Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI try to use `load_dataset()` to load several local `.jsonl` files as a dataset. Every line of these files is a json structure only containing one key `text` (yeah it is a dataset for NLP model). The code snippet is as:\r\n```python\r\nds = load_dataset(\"json\", data_files=LIST_OF_FILE_PATHS, num_proc=16)['train']\r\n```\r\nHowever, I found that the loading process can get stuck -- the progress bar `Generating train split` no more proceed. When I was trying to find the cause and solution, I found a really strange behavior. If I load the dataset in this way:\r\n```python\r\ndlist = list()\r\nfor _ in LIST_OF_FILE_PATHS:\r\n    dlist.append(load_dataset(\"json\", data_files=_)['train'])\r\nds = concatenate_datasets(dlist)\r\n```\r\nI can actually successfully load all the files despite its slow speed. But if I load them in batch like above, things go wrong. I did try to use Control-C to trace the stuck point but the program cannot be terminated in this way when `num_proc` is set to `None`. The only thing I can do is use Control-Z to hang it up then kill it. If I use more than 2 cpus, a Control-C would simply cause the following error:\r\n```bash\r\n^C\r\nProcess ForkPoolWorker-1:\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/pool.py\", line 114, in worker\r\n    task = get()\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/queues.py\", line 368, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/connection.py\", line 224, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/connection.py\", line 422, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/connection.py\", line 387, in _recv\r\n    chunk = read(handle, remaining)\r\nKeyboardInterrupt\r\nGenerating train split: 92431 examples [01:23, 1104.25 examples\/s]  \r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/utils\/py_utils.py\", line 1373, in iflatmap_unordered\r\n    yield queue.get(timeout=0.05)\r\n  File \"<string>\", line 2, in get\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/managers.py\", line 818, in _callmethod\r\n    kind, result = conn.recv()\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/connection.py\", line 258, in recv\r\n    buf = self._recv_bytes()\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/connection.py\", line 422, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/connection.py\", line 387, in _recv\r\n    chunk = read(handle, remaining)\r\nKeyboardInterrupt\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/mnt\/data\/liyongyuan\/source\/batch_load.py\", line 11, in <module>\r\n    a = load_dataset(\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/load.py\", line 2133, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/builder.py\", line 954, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/builder.py\", line 1049, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/builder.py\", line 1842, in _prepare_split\r\n    for job_id, done, content in iflatmap_unordered(\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/utils\/py_utils.py\", line 1387, in iflatmap_unordered\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/utils\/py_utils.py\", line 1387, in <listcomp>\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/multiprocess\/pool.py\", line 770, in get\r\n    raise TimeoutError\r\nmultiprocess.context.TimeoutError\r\n```\r\nI have validated the basic correctness of these `.jsonl` files. They are correctly formatted (or they cannot be loaded singly by `load_dataset`) though some of the json may contain too long text (more than 1e7 characters). I do not know if this could be the problem. And there should not be any bottleneck in system's resource. The whole dataset is ~300GB, and I am using a cloud server with plenty of storage and 1TB ram. \r\nThanks for your efforts and patience! Any suggestion or help would be appreciated.\n\n### Steps to reproduce the bug\n\n1. use load_dataset() with `data_files = LIST_OF_FILES`\n\n### Expected behavior\n\nAll the files should be smoothly loaded. \n\n### Environment info\n\n- Datasets: A private dataset. ~2500 `.jsonl` files. ~300GB in total. Each json structure only contains one key: `text`. Format checked.\r\n- `datasets` version: 2.14.2\r\n- Platform: Linux-4.19.91-014.kangaroo.alios7.x86_64-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 10.0.1.dev0+ga6eabc2b.d20230609\r\n- Pandas version: 1.5.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6108\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6108\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6107","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6107\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6107\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6107\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6107","id":1829625320,"node_id":"PR_kwDODunzps5W0rLR","number":6107,"title":"Fix deprecation of use_auth_token in file_utils","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-31T16:32:01Z","updated_at":"2023-08-03T10:13:32Z","closed_at":"2023-08-03T10:04:18Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix issues with the deprecation of `use_auth_token`  introduced by:\r\n- #5996\r\n\r\nin functions:\r\n- `get_authentication_headers_for_url`\r\n- `request_etag`\r\n- `get_from_cache`\r\n\r\nCurrently, `TypeError` is raised: https:\/\/github.com\/huggingface\/datasets-server\/actions\/runs\/5711650666\/job\/15484685570?pr=1588\r\n```\r\nFAILED tests\/job_runners\/config\/test_parquet_and_info.py::test__is_too_big_external_files[None-None-False] - TypeError: get_authentication_headers_for_url() got an unexpected keyword argument 'use_auth_token'\r\n\r\nFAILED tests\/job_runners\/config\/test_parquet_and_info.py::test_fill_builder_info[None-False] - libcommon.exceptions.FileSystemError: Could not read the parquet files: get_authentication_headers_for_url() got an unexpected keyword argument 'use_auth_token'\r\n```\r\n\r\nRelated to:\r\n- #6094","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6107\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6107\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6107","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6107","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6107.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6107.patch","merged_at":"2023-08-03T10:04:18Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6106","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6106\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6106\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6106\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6106","id":1829131223,"node_id":"I_kwDODunzps5tBlPX","number":6106,"title":"load local json_file as dataset","user":{"login":"CiaoHe","id":39040787,"node_id":"MDQ6VXNlcjM5MDQwNzg3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/39040787?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/CiaoHe","html_url":"https:\/\/github.com\/CiaoHe","followers_url":"https:\/\/api.github.com\/users\/CiaoHe\/followers","following_url":"https:\/\/api.github.com\/users\/CiaoHe\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/CiaoHe\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/CiaoHe\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/CiaoHe\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/CiaoHe\/orgs","repos_url":"https:\/\/api.github.com\/users\/CiaoHe\/repos","events_url":"https:\/\/api.github.com\/users\/CiaoHe\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/CiaoHe\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-31T12:53:49Z","updated_at":"2023-08-18T01:46:35Z","closed_at":"2023-08-18T01:46:35Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI tried to load local json file as dataset but failed to parsing json file because some columns are 'float' type.\n\n### Steps to reproduce the bug\n\n1. load json file with certain columns are 'float' type. For example `data = load_data(\"json\", data_files=JSON_PATH)`\r\n2. Then, the error will be triggered like `ArrowInvalid: Could not convert '-0.2253' with type str: tried to convert to double\n\n### Expected behavior\n\nShould allow some columns are 'float' type, at least it should convert those columns to str type.\r\nI tried to avoid the error by naively convert the float item to str:\r\n```python\r\n# if col type is not str, we need to convert it to str\r\nmapping = {}\r\nfor col in keys:\r\n    if isinstance(dataset[0][col], str):\r\n        mapping[col] = [row.get(col) for row in dataset]\r\n    else:\r\n        mapping[col] = [str(row.get(col)) for row in dataset]\r\n```\n\n### Environment info\n\n- `datasets` version: 2.14.2\r\n- Platform: Linux-5.4.0-52-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6106\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6106\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6105","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6105\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6105\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6105\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6105","id":1829008430,"node_id":"PR_kwDODunzps5WyiJD","number":6105,"title":"Fix error when loading from GCP bucket","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-07-31T11:44:46Z","updated_at":"2023-08-01T10:48:52Z","closed_at":"2023-08-01T10:38:54Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix `resolve_pattern` for filesystems with tuple protocol.\r\n\r\nFix #6100.\r\n\r\nThe bug code lines were introduced by:\r\n- #6028","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6105\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6105\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6105","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6105","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6105.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6105.patch","merged_at":"2023-08-01T10:38:54Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6104","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6104\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6104\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6104\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6104","id":1828959107,"node_id":"I_kwDODunzps5tA7OD","number":6104,"title":"HF Datasets data access is extremely slow even when in memory","user":{"login":"NightMachinery","id":36224762,"node_id":"MDQ6VXNlcjM2MjI0NzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36224762?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NightMachinery","html_url":"https:\/\/github.com\/NightMachinery","followers_url":"https:\/\/api.github.com\/users\/NightMachinery\/followers","following_url":"https:\/\/api.github.com\/users\/NightMachinery\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NightMachinery\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NightMachinery\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NightMachinery\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NightMachinery\/orgs","repos_url":"https:\/\/api.github.com\/users\/NightMachinery\/repos","events_url":"https:\/\/api.github.com\/users\/NightMachinery\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NightMachinery\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-31T11:12:19Z","updated_at":"2023-08-01T11:22:43Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nDoing a simple `some_dataset[:10]` can take more than a minute.\r\n\r\nProfiling it:\r\n<img width=\"1280\" alt=\"image\" src=\"https:\/\/github.com\/huggingface\/datasets\/assets\/36224762\/e641fb95-ff02-4072-9016-5416a65f75ab\">\r\n\r\n`some_dataset` is completely in memory with no disk cache.\r\n\r\nThis is proving fatal to my usage of HF Datasets. Is there a way I can forgo the arrow format and store the dataset as PyTorch tensors so that `_tensorize` is not needed? And is `_consolidate` supposed to take this long?\r\n\r\nIt's faster to produce the dataset from scratch than to access it from HF Datasets!\r\n\r\n### Steps to reproduce the bug\r\n\r\nI have uploaded the dataset that causes this problem [here](https:\/\/huggingface.co\/datasets\/NightMachinery\/hf_datasets_bug1).\r\n\r\n```python\r\n#!\/usr\/bin\/env python3\r\nimport sys\r\nimport time\r\nimport torch\r\nfrom datasets import load_dataset\r\n\r\n\r\ndef main(dataset_name):\r\n    # Start the timer\r\n    start_time = time.time()\r\n\r\n    # Load the dataset from Hugging Face Hub\r\n    dataset = load_dataset(dataset_name)\r\n\r\n    # Set the dataset format as torch\r\n    dataset.set_format(type=\"torch\")\r\n\r\n    # Perform an identity map\r\n    dataset = dataset.map(lambda example: example, batched=True, batch_size=20)\r\n\r\n    # End the timer\r\n    end_time = time.time()\r\n\r\n    # Print the time taken\r\n    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    dataset_name = \"NightMachinery\/hf_datasets_bug1\"\r\n    print(f\"dataset_name: {dataset_name}\")\r\n    main(dataset_name)\r\n```\r\n\r\n### Expected behavior\r\n\r\n_\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.13.1\r\n- Platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6104\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6104\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6103","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6103\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6103\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6103\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6103","id":1828515165,"node_id":"PR_kwDODunzps5Ww2gV","number":6103,"title":"Set dev version","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-31T06:44:05Z","updated_at":"2023-07-31T06:55:58Z","closed_at":"2023-07-31T06:45:41Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6103\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6103\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6103","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6103","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6103.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6103.patch","merged_at":"2023-07-31T06:45:41Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6102","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6102\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6102\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6102\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6102","id":1828494896,"node_id":"PR_kwDODunzps5WwyGy","number":6102,"title":"Release 2.14.2","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-07-31T06:27:47Z","updated_at":"2023-07-31T06:48:09Z","closed_at":"2023-07-31T06:32:58Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6102\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6102\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6102","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6102","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6102.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6102.patch","merged_at":"2023-07-31T06:32:58Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6101","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6101\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6101\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6101\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6101","id":1828469648,"node_id":"PR_kwDODunzps5WwspW","number":6101,"title":"Release 2.14.2","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-31T06:05:36Z","updated_at":"2023-07-31T06:33:00Z","closed_at":"2023-07-31T06:18:17Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6101\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6101\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6101","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6101","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6101.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6101.patch","merged_at":"2023-07-31T06:18:17Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6100","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6100\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6100\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6100\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6100","id":1828118930,"node_id":"I_kwDODunzps5s9uGS","number":6100,"title":"TypeError when loading from GCP bucket","user":{"login":"bilelomrani1","id":16692099,"node_id":"MDQ6VXNlcjE2NjkyMDk5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16692099?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/bilelomrani1","html_url":"https:\/\/github.com\/bilelomrani1","followers_url":"https:\/\/api.github.com\/users\/bilelomrani1\/followers","following_url":"https:\/\/api.github.com\/users\/bilelomrani1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/bilelomrani1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/bilelomrani1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/bilelomrani1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/bilelomrani1\/orgs","repos_url":"https:\/\/api.github.com\/users\/bilelomrani1\/repos","events_url":"https:\/\/api.github.com\/users\/bilelomrani1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/bilelomrani1\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-07-30T23:03:00Z","updated_at":"2023-08-03T10:00:48Z","closed_at":"2023-08-01T10:38:55Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nLoading a dataset from a GCP bucket raises a type error. This bug was introduced recently (either in 2.14 or 2.14.1), and appeared during a migration from 2.13.1.\n\n### Steps to reproduce the bug\n\nLoad any file from a GCP bucket:\r\n\r\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset(\"json\", data_files=[\"gs:\/\/...\"])\r\n```\r\n\r\nThe following exception is raised:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n...\r\npackages\/datasets\/data_files.py\", line 335, in resolve_pattern\r\n    protocol_prefix = fs.protocol + \":\/\/\" if fs.protocol != \"file\" else \"\"\r\nTypeError: can only concatenate tuple (not \"str\") to tuple\r\n```\r\n\r\nWith a `GoogleFileSystem`, the attribute `fs.protocol` is a tuple `('gs', 'gcs')` and hence cannot be concatenated with a string.\r\n\n\n### Expected behavior\n\nThe file should be loaded without exception.\n\n### Environment info\n\n- `datasets` version: 2.14.1\r\n- Platform: macOS-13.2.1-x86_64-i386-64bit\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6100\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6100\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6099","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6099\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6099\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6099\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6099","id":1827893576,"node_id":"I_kwDODunzps5s83FI","number":6099,"title":"How do i get \"amazon_us_reviews","user":{"login":"IqraBaluch","id":57810189,"node_id":"MDQ6VXNlcjU3ODEwMTg5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57810189?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/IqraBaluch","html_url":"https:\/\/github.com\/IqraBaluch","followers_url":"https:\/\/api.github.com\/users\/IqraBaluch\/followers","following_url":"https:\/\/api.github.com\/users\/IqraBaluch\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/IqraBaluch\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/IqraBaluch\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/IqraBaluch\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/IqraBaluch\/orgs","repos_url":"https:\/\/api.github.com\/users\/IqraBaluch\/repos","events_url":"https:\/\/api.github.com\/users\/IqraBaluch\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/IqraBaluch\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2023-07-30T11:02:17Z","updated_at":"2023-08-21T05:08:08Z","closed_at":"2023-08-10T05:02:35Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nI have been trying to load 'amazon_us_dataset\" but unable to do so. \r\n\r\n`amazon_us_reviews = load_dataset('amazon_us_reviews')`\r\n`print(amazon_us_reviews)`\r\n\r\n\r\n> [ValueError: Config name is missing.\r\n\r\nPlease pick one among the available configs: ['Wireless_v1_00', 'Watches_v1_00', 'Video_Games_v1_00', 'Video_DVD_v1_00', 'Video_v1_00', 'Toys_v1_00', 'Tools_v1_00', 'Sports_v1_00', 'Software_v1_00', 'Shoes_v1_00', 'Pet_Products_v1_00', 'Personal_Care_Appliances_v1_00', 'PC_v1_00', 'Outdoors_v1_00', 'Office_Products_v1_00', 'Musical_Instruments_v1_00', 'Music_v1_00', 'Mobile_Electronics_v1_00', 'Mobile_Apps_v1_00', 'Major_Appliances_v1_00', 'Luggage_v1_00', 'Lawn_and_Garden_v1_00', 'Kitchen_v1_00', 'Jewelry_v1_00', 'Home_Improvement_v1_00', 'Home_Entertainment_v1_00', 'Home_v1_00', 'Health_Personal_Care_v1_00', 'Grocery_v1_00', 'Gift_Card_v1_00', 'Furniture_v1_00', 'Electronics_v1_00', 'Digital_Video_Games_v1_00', 'Digital_Video_Download_v1_00', 'Digital_Software_v1_00', 'Digital_Music_Purchase_v1_00', 'Digital_Ebook_Purchase_v1_00', 'Camera_v1_00', 'Books_v1_00', 'Beauty_v1_00', 'Baby_v1_00', 'Automotive_v1_00', 'Apparel_v1_00', 'Digital_Ebook_Purchase_v1_01', 'Books_v1_01', 'Books_v1_02']\r\nExample of usage:\r\n\t`load_dataset('amazon_us_reviews', 'Wireless_v1_00')`]\r\n\r\n__________________________________________________________________________\r\n`amazon_us_reviews = load_dataset('amazon_us_reviews', 'Watches_v1_00')\r\nprint(amazon_us_reviews)`\r\n\r\n**ERROR**\r\n`Generating` train split: 0%\r\n0\/960872 [00:00<?, ? examples\/s]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/builder.py in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\r\n   1692                         )\r\n-> 1693                     example = self.info.features.encode_example(record) if self.info.features is not None else record\r\n   1694                     writer.write(example, key)\r\n\r\n11 frames\r\nKeyError: 'marketplace'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/builder.py in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\r\n   1710             if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\r\n   1711                 e = e.__context__\r\n-> 1712             raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1713 \r\n   1714         yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\n\n### Motivation\n\nThe dataset I'm using\r\nhttps:\/\/huggingface.co\/datasets\/amazon_us_reviews\n\n### Your contribution\n\nWhat is the best way to load this data","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6099\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6099\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6098","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6098\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6098\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6098\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6098","id":1827655071,"node_id":"PR_kwDODunzps5WuCn1","number":6098,"title":"Expanduser in save_to_disk()","user":{"login":"Unknown3141592","id":51715864,"node_id":"MDQ6VXNlcjUxNzE1ODY0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/51715864?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Unknown3141592","html_url":"https:\/\/github.com\/Unknown3141592","followers_url":"https:\/\/api.github.com\/users\/Unknown3141592\/followers","following_url":"https:\/\/api.github.com\/users\/Unknown3141592\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Unknown3141592\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Unknown3141592\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Unknown3141592\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Unknown3141592\/orgs","repos_url":"https:\/\/api.github.com\/users\/Unknown3141592\/repos","events_url":"https:\/\/api.github.com\/users\/Unknown3141592\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Unknown3141592\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-29T20:50:45Z","updated_at":"2023-10-27T14:14:11Z","closed_at":"2023-10-27T14:04:36Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fixes #5651. The same problem occurs when loading from disk so I fixed it there too.\r\n\r\nI am not sure why the case distinction between local and remote filesystems is even necessary for `DatasetDict` when saving to disk. Imo this could be removed (leaving only `fs.makedirs(dataset_dict_path, exist_ok=True)`).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6098\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6098\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6098","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6098","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6098.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6098.patch","merged_at":"2023-10-27T14:04:36Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6097","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6097\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6097\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6097\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6097","id":1827054143,"node_id":"I_kwDODunzps5s5qI_","number":6097,"title":"Dataset.get_nearest_examples does not return all feature values for the k most similar datapoints - side effect of Dataset.set_format","user":{"login":"aschoenauer-sebag","id":2538048,"node_id":"MDQ6VXNlcjI1MzgwNDg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2538048?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/aschoenauer-sebag","html_url":"https:\/\/github.com\/aschoenauer-sebag","followers_url":"https:\/\/api.github.com\/users\/aschoenauer-sebag\/followers","following_url":"https:\/\/api.github.com\/users\/aschoenauer-sebag\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/aschoenauer-sebag\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/aschoenauer-sebag\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/aschoenauer-sebag\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/aschoenauer-sebag\/orgs","repos_url":"https:\/\/api.github.com\/users\/aschoenauer-sebag\/repos","events_url":"https:\/\/api.github.com\/users\/aschoenauer-sebag\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/aschoenauer-sebag\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-28T20:31:59Z","updated_at":"2023-07-28T20:49:58Z","closed_at":"2023-07-28T20:49:58Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi team!\r\n\r\nI observe that there seems to be a side effect of `Dataset.set_format`: after setting a format and creating a FAISS index, the method `get_nearest_examples` from the `Dataset` class, fails to retrieve anything else but the embeddings themselves - not super useful. This is not the case if not using the `set_format` method: you can also retrieve any other feature value, such as an index\/id\/etc.\r\n\r\nAre you able to reproduce what I observe?\n\n### Steps to reproduce the bug\n\n```python\r\n\r\nfrom datasets import Dataset\r\nimport numpy as np\r\n\r\nfoo = {'vectors': np.random.random((100,1024)), 'ids': [str(u) for u in range(100)]}\r\nfoo = Dataset.from_dict(foo)\r\nfoo.set_format('numpy', ['vectors'])\r\nfoo.add_faiss_index('vectors')\r\nnew_vector = np.random.random(1024)\r\nscores, res = foo.get_nearest_examples('vectors', new_vector, k=3)\r\n```\r\nThis will return, for the resulting most similar vectors to `new_vector` - in particular it will not return the `ids` feature:\r\n```\r\n{'vectors': array([[random values ...]])}\r\n```\n\n### Expected behavior\n\nThe expected behavior happens when the `set_format` method is not called:\r\n```python\r\n\r\nfrom datasets import Dataset\r\nimport numpy as np\r\n\r\nfoo = {'vectors': np.random.random((100,1024)), 'ids': [str(u) for u in range(100)]}\r\nfoo = Dataset.from_dict(foo)\r\n# foo.set_format('numpy', ['vectors'])\r\nfoo.add_faiss_index('vectors')\r\nnew_vector = np.random.random(1024)\r\nscores, res = foo.get_nearest_examples('vectors', new_vector, k=3)\r\n```\r\nThis *will*  return the `ids` of the similar vectors - with unfortunately a list of lists in lieu of the array I think for caching reasons - read it elsewhere\r\n```\r\n{'vectors': [[random values on multiple lines...]], 'ids': ['x', 'y', 'z']}\r\n```\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.4.0-155-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6097\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6097\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6096","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6096\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6096\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6096\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6096","id":1826731091,"node_id":"PR_kwDODunzps5Wq9Hb","number":6096,"title":"Add `fsspec` support for `to_json`, `to_csv`, and `to_parquet`","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-28T16:36:59Z","updated_at":"2023-09-06T13:58:09Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Hi to whoever is reading this! \ud83e\udd17 (Most likely @mariosasko)\r\n\r\n## What's in this PR?\r\n\r\nThis PR replaces the `open` from Python with `fsspec.open` and adds the argument `storage_options` for the methods `to_json`, `to_csv`, and `to_parquet`, to allow users to export any \ud83e\udd17`Dataset` into a file in a file-system as requested at #6086.\r\n\r\n## What's missing in this PR?\r\n\r\nAs per `to_json`, `to_csv`, and `to_parquet` docstrings for the recently included `storage_options` arg, I've scoped it to 2.15.0, so we should check that before merging in case we want to scope that for 2.14.2 instead.\r\n\r\nAdditionally, should we also add `fsspec` support for the `from_csv`, `from_json`, and `from_parquet` methods? If you want me to do so @mariosasko just let me know and I'll create another PR to support that too!","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6096\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6096\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6096","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6096","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6096.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6096.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6095","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6095\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6095\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6095\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6095","id":1826496967,"node_id":"PR_kwDODunzps5WqJtr","number":6095,"title":"Fix deprecation of errors in TextConfig","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-28T14:08:37Z","updated_at":"2023-07-31T05:26:32Z","closed_at":"2023-07-31T05:17:38Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR fixes an issue with the deprecation of `errors` in `TextConfig` introduced by:\r\n- #5974\r\n\r\n```python\r\nIn [1]: ds = load_dataset(\"text\", data_files=\"test.txt\", errors=\"strict\")\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-13-701c27131a5d> in <module>\r\n----> 1 ds = load_dataset(\"text\", data_files=\"test.txt\", errors=\"strict\")\r\n\r\n~\/huggingface\/datasets\/src\/datasets\/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2107 \r\n   2108     # Create a dataset builder\r\n-> 2109     builder_instance = load_dataset_builder(\r\n   2110         path=path,\r\n   2111         name=name,\r\n\r\n~\/huggingface\/datasets\/src\/datasets\/load.py in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\r\n   1830     builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\r\n   1831     # Instantiate the dataset builder\r\n-> 1832     builder_instance: DatasetBuilder = builder_cls(\r\n   1833         cache_dir=cache_dir,\r\n   1834         dataset_name=dataset_name,\r\n\r\n~\/huggingface\/datasets\/src\/datasets\/builder.py in __init__(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\r\n    371         if data_dir is not None:\r\n    372             config_kwargs[\"data_dir\"] = data_dir\r\n--> 373         self.config, self.config_id = self._create_builder_config(\r\n    374             config_name=config_name,\r\n    375             custom_features=features,\r\n\r\n~\/huggingface\/datasets\/src\/datasets\/builder.py in _create_builder_config(self, config_name, custom_features, **config_kwargs)\r\n    550             if \"version\" not in config_kwargs and hasattr(self, \"VERSION\") and self.VERSION:\r\n    551                 config_kwargs[\"version\"] = self.VERSION\r\n--> 552             builder_config = self.BUILDER_CONFIG_CLASS(**config_kwargs)\r\n    553 \r\n    554         # otherwise use the config_kwargs to overwrite the attributes\r\n\r\nTypeError: __init__() got an unexpected keyword argument 'errors'\r\n```\r\n\r\nSimilar to:\r\n- #6094","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6095\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6095\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6095","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6095","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6095.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6095.patch","merged_at":"2023-07-31T05:17:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6094","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6094\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6094\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6094\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6094","id":1826293414,"node_id":"PR_kwDODunzps5WpdpA","number":6094,"title":"Fix deprecation of use_auth_token in DownloadConfig","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-28T11:52:21Z","updated_at":"2023-07-31T05:08:41Z","closed_at":"2023-07-31T04:59:50Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR fixes an issue with the deprecation of `use_auth_token` in `DownloadConfig` introduced by:\r\n- #5996\r\n\r\n```python\r\nIn [1]: from datasets import DownloadConfig\r\n\r\nIn [2]: DownloadConfig(use_auth_token=False)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-41927b449e72> in <module>\r\n----> 1 DownloadConfig(use_auth_token=False)\r\n\r\nTypeError: __init__() got an unexpected keyword argument 'use_auth_token'\r\n```\r\n\r\n```python\r\nIn [1]: from datasets import get_dataset_config_names\r\nIn [2]: get_dataset_config_names(\"squad\", use_auth_token=False)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-22-4671992ead50> in <module>\r\n----> 1 get_dataset_config_names(\"squad\", use_auth_token=False)\r\n\r\n~\/huggingface\/datasets\/src\/datasets\/inspect.py in get_dataset_config_names(path, revision, download_config, download_mode, dynamic_modules_path, data_files, **download_kwargs)\r\n    349     ```\r\n    350     \"\"\"\r\n--> 351     dataset_module = dataset_module_factory(\r\n    352         path,\r\n    353         revision=revision,\r\n\r\n~\/huggingface\/datasets\/src\/datasets\/load.py in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1374     \"\"\"\r\n   1375     if download_config is None:\r\n-> 1376         download_config = DownloadConfig(**download_kwargs)\r\n   1377     download_mode = DownloadMode(download_mode or DownloadMode.REUSE_DATASET_IF_EXISTS)\r\n   1378     download_config.extract_compressed_file = True\r\n\r\nTypeError: __init__() got an unexpected keyword argument 'use_auth_token'\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6094\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6094\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6094","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6094","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6094.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6094.patch","merged_at":"2023-07-31T04:59:50Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6093","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6093\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6093\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6093\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6093","id":1826210490,"node_id":"PR_kwDODunzps5WpLfh","number":6093,"title":"Deprecate `download_custom`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-07-28T10:49:06Z","updated_at":"2023-08-21T17:51:34Z","closed_at":"2023-07-28T11:30:02Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Deprecate `DownloadManager.download_custom`. Users should use `fsspec` URLs (cacheable) or make direct requests with `fsspec`\/`requests` (not cacheable) instead.\r\n\r\nWe should deprecate this method as it's not compatible with streaming, and implementing the streaming version of it is hard\/impossible. There have been requests to implement the streaming version of this method on the forum, but the reason for this seems to be a tip in the docs that \"promotes\" this method (this PR removes it). \r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6093\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6093\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6093","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6093","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6093.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6093.patch","merged_at":"2023-07-28T11:30:02Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6092","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6092\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6092\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6092\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6092","id":1826111806,"node_id":"PR_kwDODunzps5Wo1mh","number":6092,"title":"Minor fix in `iter_files` for hidden files","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-28T09:50:12Z","updated_at":"2023-07-28T10:59:28Z","closed_at":"2023-07-28T10:50:10Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6090","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6092\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6092\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6092","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6092","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6092.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6092.patch","merged_at":"2023-07-28T10:50:09Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6091","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6091\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6091\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6091\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6091","id":1826086487,"node_id":"PR_kwDODunzps5Wov9Q","number":6091,"title":"Bump fsspec from 2021.11.1 to 2022.3.0","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-28T09:37:15Z","updated_at":"2023-07-28T10:16:11Z","closed_at":"2023-07-28T10:07:02Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix https:\/\/github.com\/huggingface\/datasets\/issues\/6087\r\n\r\n(Colab installs 2023.6.0, so we should be good) ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6091\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6091\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6091","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6091","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6091.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6091.patch","merged_at":"2023-07-28T10:07:02Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6090","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6090\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6090\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6090\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6090","id":1825865043,"node_id":"I_kwDODunzps5s1H1T","number":6090,"title":"FilesIterable skips all the files after a hidden file","user":{"login":"dkrivosic","id":10785413,"node_id":"MDQ6VXNlcjEwNzg1NDEz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10785413?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dkrivosic","html_url":"https:\/\/github.com\/dkrivosic","followers_url":"https:\/\/api.github.com\/users\/dkrivosic\/followers","following_url":"https:\/\/api.github.com\/users\/dkrivosic\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dkrivosic\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dkrivosic\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dkrivosic\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dkrivosic\/orgs","repos_url":"https:\/\/api.github.com\/users\/dkrivosic\/repos","events_url":"https:\/\/api.github.com\/users\/dkrivosic\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dkrivosic\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-28T07:25:57Z","updated_at":"2023-07-28T10:51:14Z","closed_at":"2023-07-28T10:50:11Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen initializing `FilesIterable` with a list of file paths using `FilesIterable.from_paths`, it will discard all the files after a hidden file.\r\nThe problem is in [this line](https:\/\/github.com\/huggingface\/datasets\/blob\/88896a7b28610ace95e444b94f9a4bc332cc1ee3\/src\/datasets\/download\/download_manager.py#L233C26-L233C26) where `return` should be replaced by `continue`. \n\n### Steps to reproduce the bug\n\nhttps:\/\/colab.research.google.com\/drive\/1SQlxs4y_LSo1Q89KnFoYDSyyKEISun_J#scrollTo=93K4_blkW-8-\n\n### Expected behavior\n\nThe script should print all the files except the hidden one.\n\n### Environment info\n\n- `datasets` version: 2.14.1\r\n- Platform: Linux-5.15.109+-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6090\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6090\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6089","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6089\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6089\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6089\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6089","id":1825761476,"node_id":"I_kwDODunzps5s0ujE","number":6089,"title":"AssertionError: daemonic processes are not allowed to have children","user":{"login":"codingl2k1","id":138426806,"node_id":"U_kgDOCEA5tg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/138426806?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/codingl2k1","html_url":"https:\/\/github.com\/codingl2k1","followers_url":"https:\/\/api.github.com\/users\/codingl2k1\/followers","following_url":"https:\/\/api.github.com\/users\/codingl2k1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/codingl2k1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/codingl2k1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/codingl2k1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/codingl2k1\/orgs","repos_url":"https:\/\/api.github.com\/users\/codingl2k1\/repos","events_url":"https:\/\/api.github.com\/users\/codingl2k1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/codingl2k1\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-28T06:04:00Z","updated_at":"2023-07-31T02:34:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I load_dataset with num_proc > 0 in a deamon process, I got an error:\r\n\r\n```python\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/download\/download_manager.py\", line 564, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/download\/download_manager.py\", line 427, in download\r\n    downloaded_path_or_paths = map_nested(\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/utils\/py_utils.py\", line 468, in map_nested\r\n    mapped = parallel_map(function, iterable, num_proc, types, disable_tqdm, desc, _single_map_nested)\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/utils\/experimental.py\", line 40, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/parallel\/parallel.py\", line 34, in parallel_map\r\n    return _map_with_multiprocessing_pool(\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/parallel\/parallel.py\", line 64, in _map_with_multiprocessing_pool\r\n    with Pool(num_proc, initargs=initargs, initializer=initializer) as pool:\r\n      ^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/multiprocessing\/context.py\", line 119, in Pool\r\n    return Pool(processes, initializer, initargs, maxtasksperchild,\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/multiprocessing\/pool.py\", line 215, in __init__\r\n    self._repopulate_pool()\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/multiprocessing\/pool.py\", line 306, in _repopulate_pool\r\n    return self._repopulate_pool_static(self._ctx, self.Process,\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/multiprocessing\/pool.py\", line 329, in _repopulate_pool_static\r\n    w.start()\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/multiprocessing\/process.py\", line 118, in start\r\n    assert not _current_process._config.get('daemon'),     ^^^^^^^^^^^^^^^^^\r\nAssertionError: daemonic processes are not allowed to have children\r\n```\r\n\r\nThe download is io-intensive computing, may be datasets can replece the multi processing pool by a multi threading pool if in a deamon process.\n\n### Steps to reproduce the bug\n\n1. start a deamon process\r\n2. run load_dataset with num_proc > 0\n\n### Expected behavior\n\nNo error.\n\n### Environment info\n\nPython 3.11.4\r\ndatasets latest master","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6089\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6089\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6088","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6088\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6088\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6088\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6088","id":1825665235,"node_id":"I_kwDODunzps5s0XDT","number":6088,"title":"Loading local data files initiates web requests","user":{"login":"lytning98","id":23375707,"node_id":"MDQ6VXNlcjIzMzc1NzA3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23375707?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lytning98","html_url":"https:\/\/github.com\/lytning98","followers_url":"https:\/\/api.github.com\/users\/lytning98\/followers","following_url":"https:\/\/api.github.com\/users\/lytning98\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lytning98\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lytning98\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lytning98\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lytning98\/orgs","repos_url":"https:\/\/api.github.com\/users\/lytning98\/repos","events_url":"https:\/\/api.github.com\/users\/lytning98\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lytning98\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-07-28T04:06:26Z","updated_at":"2023-07-28T05:02:22Z","closed_at":"2023-07-28T05:02:22Z","author_association":"NONE","active_lock_reason":null,"body":"As documented in the [official docs](https:\/\/huggingface.co\/docs\/datasets\/v2.14.0\/en\/package_reference\/loading_methods#datasets.load_dataset.example-2), I tried to load datasets from local files by\r\n```python\r\n# Load a JSON file\r\nfrom datasets import load_dataset\r\nds = load_dataset('json', data_files='path\/to\/local\/my_dataset.json')\r\n```\r\nBut this failed on a web request because I'm executing the script on a machine without Internet access. Stacktrace shows\r\n```\r\nin PackagedDatasetModuleFactory.__init__(self, name, data_dir, data_files, download_config, download_mode)\r\n    940 self.download_config = download_config\r\n    941 self.download_mode = download_mode\r\n--> 942 increase_load_count(name, resource_type=\"dataset\")\r\n```\r\nI've read from the source code that this can be fixed by setting environment variable to run in offline mode. I'm just wondering that is this an expected behaviour that even loading a LOCAL JSON file requires Internet access by default? And what's the point of requesting to `increase_load_count` on some server when loading just LOCAL data files?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6088\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6088\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6087","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6087\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6087\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6087\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6087","id":1825133741,"node_id":"I_kwDODunzps5syVSt","number":6087,"title":"fsspec dependency is set too low","user":{"login":"iXce","id":1085885,"node_id":"MDQ6VXNlcjEwODU4ODU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1085885?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/iXce","html_url":"https:\/\/github.com\/iXce","followers_url":"https:\/\/api.github.com\/users\/iXce\/followers","following_url":"https:\/\/api.github.com\/users\/iXce\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/iXce\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/iXce\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/iXce\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/iXce\/orgs","repos_url":"https:\/\/api.github.com\/users\/iXce\/repos","events_url":"https:\/\/api.github.com\/users\/iXce\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/iXce\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-27T20:08:22Z","updated_at":"2023-07-28T10:07:56Z","closed_at":"2023-07-28T10:07:03Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nfsspec.callbacks.TqdmCallback (used in https:\/\/github.com\/huggingface\/datasets\/blob\/73bed12ecda17d1573fd3bf73ed5db24d3622f86\/src\/datasets\/utils\/file_utils.py#L338) was first released in fsspec [2022.3.0](https:\/\/github.com\/fsspec\/filesystem_spec\/releases\/tag\/2022.3.0, commit where it was added: https:\/\/github.com\/fsspec\/filesystem_spec\/commit\/9577c8a482eb0a69092913b81580942a68d66a76#diff-906155c7e926a9ff58b9f23369bb513b09b445f5b0f41fa2a84015d0b471c68cR180), however the dependency is set to 2021.11.1 https:\/\/github.com\/huggingface\/datasets\/blob\/main\/setup.py#L129\r\n\r\n\n\n### Steps to reproduce the bug\n\n1. Install fsspec==2021.11.1\r\n2. Install latest datasets==2.14.1\r\n3. Import datasets, import fails due to lack of `fsspec.callbacks.TqdmCallback`\n\n### Expected behavior\n\nNo import issue\n\n### Environment info\n\nN\/A","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6087\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6087\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6086","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6086\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6086\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6086\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6086","id":1825009268,"node_id":"I_kwDODunzps5sx250","number":6086,"title":"Support `fsspec` in `Dataset.to_<format>` methods","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"assignees":[{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2023-07-27T19:08:37Z","updated_at":"2023-07-28T15:28:26Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Supporting this should be fairly easy.\r\n\r\nRequested on the forum [here](https:\/\/discuss.huggingface.co\/t\/how-can-i-convert-a-loaded-dataset-in-to-a-parquet-file-and-save-it-to-the-s3\/48353).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6086\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6086\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6085","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6085\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6085\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6085\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6085","id":1824985188,"node_id":"PR_kwDODunzps5WlAyA","number":6085,"title":"Fix `fsspec` download","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-27T18:54:47Z","updated_at":"2023-07-27T19:06:13Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Testing `ds = load_dataset(\"audiofolder\", data_files=\"s3:\/\/datasets.huggingface.co\/SpeechCommands\/v0.01\/v0.01_test.tar.gz\", storage_options={\"anon\": True})` and trying to fix the issues raised by `fsspec` ...\r\n\r\nTODO: fix\r\n```\r\n    self.session = aiobotocore.session.AioSession(**self.kwargs)\r\nTypeError: __init__() got an unexpected keyword argument 'hf'\r\n```\r\nby \"preparing `storage_options`\" for the `fsspec` head\/get","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6085\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6085\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6085","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6085","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6085.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6085.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6084","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6084\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6084\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6084\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6084","id":1824896761,"node_id":"I_kwDODunzps5sxbb5","number":6084,"title":"Changing pixel values of images in the Winoground dataset","user":{"login":"ZitengWangNYU","id":90359895,"node_id":"MDQ6VXNlcjkwMzU5ODk1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/90359895?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ZitengWangNYU","html_url":"https:\/\/github.com\/ZitengWangNYU","followers_url":"https:\/\/api.github.com\/users\/ZitengWangNYU\/followers","following_url":"https:\/\/api.github.com\/users\/ZitengWangNYU\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ZitengWangNYU\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ZitengWangNYU\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ZitengWangNYU\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ZitengWangNYU\/orgs","repos_url":"https:\/\/api.github.com\/users\/ZitengWangNYU\/repos","events_url":"https:\/\/api.github.com\/users\/ZitengWangNYU\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ZitengWangNYU\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-07-27T17:55:35Z","updated_at":"2023-07-27T17:55:35Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi, as I followed the instructions, with lasted \"datasets\" version:\r\n\"\r\nfrom datasets import load_dataset\r\nexamples = load_dataset('facebook\/winoground', use_auth_token=<YOUR USER ACCESS TOKEN>)\r\n\"\r\nI got slightly different datasets in colab and in my hpc environment. Specifically, the pixel values of images are slightly different. \r\nI thought it was due to the package version difference, but today's morning I found out that my winoground dataset in colab became the same with the one in my hpc environment. The dataset in colab can produce the correct result but now it is gone as well. \r\n\r\nCan you help me with this? What causes the datasets to have the wrong pixel values?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6084\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6084\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6083","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6083\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6083\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6083\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6083","id":1824832348,"node_id":"PR_kwDODunzps5WkgAI","number":6083,"title":"set dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-27T17:10:41Z","updated_at":"2023-07-27T17:22:05Z","closed_at":"2023-07-27T17:11:01Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6083\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6083\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6083","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6083","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6083.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6083.patch","merged_at":"2023-07-27T17:11:01Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6082","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6082\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6082\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6082\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6082","id":1824819672,"node_id":"PR_kwDODunzps5WkdIn","number":6082,"title":"Release: 2.14.1","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-07-27T17:05:54Z","updated_at":"2023-07-31T06:32:16Z","closed_at":"2023-07-27T17:08:38Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6082\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6082\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6082","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6082","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6082.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6082.patch","merged_at":"2023-07-27T17:08:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6081","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6081\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6081\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6081\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6081","id":1824486278,"node_id":"PR_kwDODunzps5WjU0k","number":6081,"title":"Deprecate `Dataset.export`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-27T14:22:18Z","updated_at":"2023-07-28T11:09:54Z","closed_at":"2023-07-28T11:01:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Deprecate `Dataset.export` that generates a TFRecord file from a dataset as this method is undocumented, and the usage seems low. Users should use [TFRecordWriter](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/io\/TFRecordWriter#write) or the official [TFRecord](https:\/\/www.tensorflow.org\/tutorials\/load_data\/tfrecord) tutorial (on which this method is based) to write TFRecord files instead.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6081\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6081\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6081","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6081","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6081.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6081.patch","merged_at":"2023-07-28T11:01:04Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6080","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6080\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6080\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6080\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6080","id":1822667554,"node_id":"PR_kwDODunzps5WdL4K","number":6080,"title":"Remove README link to deprecated Colab notebook","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-26T15:27:49Z","updated_at":"2023-07-26T16:24:43Z","closed_at":"2023-07-26T16:14:34Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6080\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6080\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6080","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6080","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6080.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6080.patch","merged_at":"2023-07-26T16:14:34Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6079","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6079\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6079\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6079\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6079","id":1822597471,"node_id":"I_kwDODunzps5soqFf","number":6079,"title":"Iterating over DataLoader based on HF datasets is stuck forever","user":{"login":"arindamsarkar93","id":5454868,"node_id":"MDQ6VXNlcjU0NTQ4Njg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5454868?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/arindamsarkar93","html_url":"https:\/\/github.com\/arindamsarkar93","followers_url":"https:\/\/api.github.com\/users\/arindamsarkar93\/followers","following_url":"https:\/\/api.github.com\/users\/arindamsarkar93\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/arindamsarkar93\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/arindamsarkar93\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/arindamsarkar93\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/arindamsarkar93\/orgs","repos_url":"https:\/\/api.github.com\/users\/arindamsarkar93\/repos","events_url":"https:\/\/api.github.com\/users\/arindamsarkar93\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/arindamsarkar93\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":14,"created_at":"2023-07-26T14:52:37Z","updated_at":"2023-10-05T02:58:43Z","closed_at":"2023-07-30T14:09:06Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI am using Amazon Sagemaker notebook (Amazon Linux 2) with python 3.10 based Conda environment.\r\nI have a dataset in parquet format locally. When I try to iterate over it, the loader is stuck forever. Note that the same code is working for python 3.6 based conda environment seamlessly. What should be my next steps here?\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\ntrain_dataset = load_dataset(\r\n    \"parquet\", data_files = {'train': tr_data_path + '*.parquet'}, \r\n    split = 'train', \r\n    collate_fn = streaming_data_collate_fn,\r\n    streaming = True\r\n).with_format('torch')\r\n\r\ntrain_dataloader = DataLoader(train_dataset, batch_size = 2, num_workers = 0)\r\n\r\nt = time.time()\r\niter_ = 0\r\nfor batch in train_dataloader:\r\n    iter_ += 1\r\n    \r\n    if iter_ == 1000:\r\n        break\r\n        \r\nprint (time.time() - t) \r\n```\r\n\r\n### Expected behavior\r\n\r\nThe snippet should work normally and load the next batch of data.\r\n\r\n### Environment info\r\n\r\ndatasets: '2.14.0'\r\npyarrow: '12.0.0'\r\ntorch: '2.0.0'\r\nPython: 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]\r\n\r\n!uname -r\r\n5.10.178-162.673.amzn2.x86_64","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6079\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6079\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6078","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6078\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6078\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6078\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6078","id":1822501472,"node_id":"I_kwDODunzps5soSpg","number":6078,"title":"resume_download with streaming=True","user":{"login":"NicolasMICAUX","id":72763959,"node_id":"MDQ6VXNlcjcyNzYzOTU5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/72763959?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NicolasMICAUX","html_url":"https:\/\/github.com\/NicolasMICAUX","followers_url":"https:\/\/api.github.com\/users\/NicolasMICAUX\/followers","following_url":"https:\/\/api.github.com\/users\/NicolasMICAUX\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NicolasMICAUX\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NicolasMICAUX\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NicolasMICAUX\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NicolasMICAUX\/orgs","repos_url":"https:\/\/api.github.com\/users\/NicolasMICAUX\/repos","events_url":"https:\/\/api.github.com\/users\/NicolasMICAUX\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NicolasMICAUX\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-26T14:08:22Z","updated_at":"2023-07-28T11:05:03Z","closed_at":"2023-07-28T11:05:03Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI used:\r\n```\r\ndataset = load_dataset(\r\n    \"oscar-corpus\/OSCAR-2201\",\r\n    token=True,\r\n    language=\"fr\",\r\n    streaming=True,\r\n    split=\"train\"\r\n)\r\n```\r\nUnfortunately, the server had a problem during the training process. I saved the step my training stopped at.\r\nBut how can I resume download from step 1_000_\u00b4000 without re-streaming all the first 1 million docs of the dataset?\r\n\r\n`download_config=DownloadConfig(resume_download=True)` seems to not work with streaming=True.\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset, DownloadConfig\r\ndataset = load_dataset(\r\n    \"oscar-corpus\/OSCAR-2201\",\r\n    token=True,\r\n    language=\"fr\",\r\n    streaming=True,  # optional\r\n    split=\"train\",\r\n    download_config=DownloadConfig(resume_download=True)\r\n)\r\n# interupt the run and try to relaunch it => this restart from scratch\r\n```\n\n### Expected behavior\n\nI would expect a parameter to start streaming from a given index in the dataset.\n\n### Environment info\n\n- `datasets` version: 2.14.0\r\n- Platform: Linux-5.19.0-45-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6078\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6078\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6077","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6077\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6077\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6077\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6077","id":1822486810,"node_id":"I_kwDODunzps5soPEa","number":6077,"title":"Mapping gets stuck at 99%","user":{"login":"Laurent2916","id":21087104,"node_id":"MDQ6VXNlcjIxMDg3MTA0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/21087104?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Laurent2916","html_url":"https:\/\/github.com\/Laurent2916","followers_url":"https:\/\/api.github.com\/users\/Laurent2916\/followers","following_url":"https:\/\/api.github.com\/users\/Laurent2916\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Laurent2916\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Laurent2916\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Laurent2916\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Laurent2916\/orgs","repos_url":"https:\/\/api.github.com\/users\/Laurent2916\/repos","events_url":"https:\/\/api.github.com\/users\/Laurent2916\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Laurent2916\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-07-26T14:00:40Z","updated_at":"2023-07-28T09:21:07Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nHi !\r\n\r\nI'm currently working with a large (~150GB) unnormalized dataset at work.\r\nThe dataset is available on a read-only filesystem internally, and I use a [loading script](https:\/\/huggingface.co\/docs\/datasets\/dataset_script) to retreive it.\r\n\r\nI want to normalize the features of the dataset, meaning I need to compute the mean and standard deviation metric for each feature of the entire dataset. I cannot load the entire dataset to RAM as it is too big, so following [this discussion on the huggingface discourse](https:\/\/discuss.huggingface.co\/t\/copy-columns-in-a-dataset-and-compute-statistics-for-a-column\/22157) I am using a [map operation](https:\/\/huggingface.co\/docs\/datasets\/v2.14.0\/en\/package_reference\/main_classes#datasets.Dataset.map) to first compute the metrics and a second map operation to apply them on the dataset.\r\n\r\nThe problem lies in the second mapping, as it gets stuck at ~99%. By checking what the process does (using `htop` and `strace`) it seems to be doing a lot of I\/O operations, and I'm not sure why.\r\n\r\nObviously, I could always normalize the dataset externally and then load it using a loading script. However, since the internal dataset is updated fairly frequently, using the library to perform normalization automatically would make it much easier for me.\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nI'm able to reproduce the problem using the following scripts:\r\n\r\n```python\r\n# random_data.py\r\n\r\nimport datasets\r\nimport torch\r\n\r\n_VERSION = \"1.0.0\"\r\n\r\n\r\nclass RandomDataset(datasets.GeneratorBasedBuilder):\r\n    def _info(self):\r\n        return datasets.DatasetInfo(\r\n            version=_VERSION,\r\n            supervised_keys=None,\r\n            features=datasets.Features(\r\n                {\r\n                    \"positions\": datasets.Array2D(\r\n                        shape=(30000, 3),\r\n                        dtype=\"float32\",\r\n                    ),\r\n                    \"normals\": datasets.Array2D(\r\n                        shape=(30000, 3),\r\n                        dtype=\"float32\",\r\n                    ),\r\n                    \"features\": datasets.Array2D(\r\n                        shape=(30000, 6),\r\n                        dtype=\"float32\",\r\n                    ),\r\n                    \"scalars\": datasets.Sequence(\r\n                        feature=datasets.Value(\"float32\"),\r\n                        length=20,\r\n                    ),\r\n                },\r\n            ),\r\n        )\r\n\r\n    def _split_generators(self, dl_manager):\r\n        return [\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TRAIN,  # type: ignore\r\n                gen_kwargs={\"nb_samples\": 1000},\r\n            ),\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TEST,  # type: ignore\r\n                gen_kwargs={\"nb_samples\": 100},\r\n            ),\r\n        ]\r\n\r\n    def _generate_examples(self, nb_samples: int):\r\n        for idx in range(nb_samples):\r\n            yield idx, {\r\n                \"positions\": torch.randn(30000, 3),\r\n                \"normals\": torch.randn(30000, 3),\r\n                \"features\": torch.randn(30000, 6),\r\n                \"scalars\": torch.randn(20),\r\n            }\r\n```\r\n\r\n```python\r\n# main.py\r\n\r\nimport datasets\r\nimport torch\r\n\r\n\r\ndef apply_mean_std(\r\n    dataset: datasets.Dataset,\r\n    means: dict[str, torch.Tensor],\r\n    stds: dict[str, torch.Tensor],\r\n) -> dict[str, torch.Tensor]:\r\n    \"\"\"Normalize the dataset using the mean and standard deviation of each feature.\r\n\r\n    Args:\r\n        dataset (`Dataset`): A huggingface dataset.\r\n        mean (`dict[str, Tensor]`): A dictionary containing the mean of each feature.\r\n        std (`dict[str, Tensor]`): A dictionary containing the standard deviation of each feature.\r\n\r\n    Returns:\r\n        dict: A dictionary containing the normalized dataset.\r\n    \"\"\"\r\n    result = {}\r\n\r\n    for key in means.keys():\r\n        # extract data from dataset\r\n        data: torch.Tensor = dataset[key]  # type: ignore\r\n\r\n        # extract mean and std from dict\r\n        mean = means[key]  # type: ignore\r\n        std = stds[key]  # type: ignore\r\n\r\n        # normalize data\r\n        normalized_data = (data - mean) \/ std\r\n\r\n        result[key] = normalized_data\r\n\r\n    return result\r\n\r\n\r\n# get dataset\r\nds = datasets.load_dataset(\r\n    path=\"random_data.py\",\r\n    split=\"train\",\r\n).with_format(\"torch\")\r\n\r\n# compute mean (along last axis)\r\nmeans = {key: torch.zeros(ds[key][0].shape[-1]) for key in ds.column_names}\r\nmeans_sq = {key: torch.zeros(ds[key][0].shape[-1]) for key in ds.column_names}\r\n\r\nfor batch in ds.iter(batch_size=8):\r\n    for key in ds.column_names:\r\n        data = batch[key]\r\n        batch_size = data.shape[0]\r\n        data = data.reshape(-1, data.shape[-1])\r\n        means[key] += data.mean(dim=0) \/ len(ds) * batch_size\r\n        means_sq[key] += (data**2).mean(dim=0) \/ len(ds) * batch_size\r\n\r\n# compute std (along last axis)\r\nstds = {key: torch.sqrt(means_sq[key] - means[key] ** 2) for key in ds.column_names}\r\n\r\n# normalize each feature of the dataset\r\nds_normalized = ds.map(\r\n    desc=\"Applying mean\/std\",  # type: ignore\r\n    function=apply_mean_std,\r\n    batched=False,\r\n    fn_kwargs={\r\n        \"means\": means,\r\n        \"stds\": stds,\r\n    },\r\n)\r\n```\r\n\r\n### Expected behavior\r\n\r\nUsing the previous scripts, the `ds_normalized` mapping completes in ~5 minutes, but any subsequent use of `ds_normalized` is really really slow, for example reapplying `apply_mean_std` to `ds_normalized` takes forever. This is very strange, I'm sure I must be missing something, but I would still expect this to be faster.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.13.1\r\n- Platform: Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6077\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6077\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6076","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6076\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6076\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6076\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6076","id":1822345597,"node_id":"PR_kwDODunzps5WcGVR","number":6076,"title":"No gzip encoding from github","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-26T12:46:07Z","updated_at":"2023-07-27T16:15:11Z","closed_at":"2023-07-27T16:14:40Z","author_association":"MEMBER","active_lock_reason":null,"body":"Don't accept gzip encoding from github, otherwise some files are not streamable + seekable.\r\n\r\nfix https:\/\/huggingface.co\/datasets\/code_x_glue_cc_code_to_code_trans\/discussions\/2#64c0e0c1a04a514ba6303e84\r\n\r\nand making sure https:\/\/github.com\/huggingface\/datasets\/issues\/2918 works as well","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6076\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6076\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6076","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6076","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6076.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6076.patch","merged_at":"2023-07-27T16:14:40Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6075","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6075\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6075\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6075\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6075","id":1822341398,"node_id":"I_kwDODunzps5snrkW","number":6075,"title":"Error loading music files using `load_dataset`","user":{"login":"susnato","id":56069179,"node_id":"MDQ6VXNlcjU2MDY5MTc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/56069179?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/susnato","html_url":"https:\/\/github.com\/susnato","followers_url":"https:\/\/api.github.com\/users\/susnato\/followers","following_url":"https:\/\/api.github.com\/users\/susnato\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/susnato\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/susnato\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/susnato\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/susnato\/orgs","repos_url":"https:\/\/api.github.com\/users\/susnato\/repos","events_url":"https:\/\/api.github.com\/users\/susnato\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/susnato\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-26T12:44:05Z","updated_at":"2023-07-26T13:08:08Z","closed_at":"2023-07-26T13:08:08Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI tried to load a music file using `datasets.load_dataset()` from the repository - https:\/\/huggingface.co\/datasets\/susnato\/pop2piano_real_music_test\r\n\r\nI got the following error - \r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 2803, in __getitem__\r\n    return self._getitem(key)\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 2788, in _getitem\r\n    formatted_output = format_table(\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/datasets\/formatting\/formatting.py\", line 629, in format_table\r\n    return formatter(pa_table, query_type=query_type)\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/datasets\/formatting\/formatting.py\", line 398, in __call__\r\n    return self.format_column(pa_table)\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/datasets\/formatting\/formatting.py\", line 442, in format_column\r\n    column = self.python_features_decoder.decode_column(column, pa_table.column_names[0])\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/datasets\/formatting\/formatting.py\", line 218, in decode_column\r\n    return self.features.decode_column(column, column_name) if self.features else column\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 1924, in decode_column\r\n    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 1924, in <listcomp>\r\n    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/datasets\/features\/features.py\", line 1325, in decode_nested_example\r\n    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/datasets\/features\/audio.py\", line 184, in decode_example\r\n    array, sampling_rate = sf.read(f)\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/soundfile.py\", line 372, in read\r\n    with SoundFile(file, 'r', samplerate, channels,\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/soundfile.py\", line 740, in __init__\r\n    self._file = self._open(file, mode_int, closefd)\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/soundfile.py\", line 1264, in _open\r\n    _error_check(_snd.sf_error(file_ptr),\r\n  File \"\/home\/susnato\/anaconda3\/envs\/p2p\/lib\/python3.9\/site-packages\/soundfile.py\", line 1455, in _error_check\r\n    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))\r\nRuntimeError: Error opening <_io.BufferedReader name='\/home\/susnato\/.cache\/huggingface\/datasets\/downloads\/d2b09cb974b967b13f91553297c40c0f02f3c0d4c8356350743598ff48d6f29e'>: Format not recognised.\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nCode to reproduce the error - \r\n\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"susnato\/pop2piano_real_music_test\", split=\"test\")\r\nprint(ds[0])\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nI should be able to read the music file without any error.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.0\r\n- Platform: Linux-5.19.0-50-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6075\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6075\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6074","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6074\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6074\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6074\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6074","id":1822299128,"node_id":"PR_kwDODunzps5Wb8O_","number":6074,"title":"Misc doc improvements","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-26T12:20:54Z","updated_at":"2023-07-27T16:16:28Z","closed_at":"2023-07-27T16:16:02Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Removes the warning about requiring to write a dataset loading script to define multiple configurations, as the README YAML can be used instead (for simple cases). Also, deletes the section about using the `BatchSampler` in `torch<=1.12.1` to speed up loading, as `torch 1.12.1` is over a year old (and `torch 2.0` has been out for a while).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6074\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6074\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6074","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6074","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6074.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6074.patch","merged_at":"2023-07-27T16:16:02Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6073","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6073\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6073\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6073\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6073","id":1822167804,"node_id":"I_kwDODunzps5snBL8","number":6073,"title":"version2.3.2   load_dataset()data_files can't include .xxxx in path","user":{"login":"BUAAChuanWang","id":45893496,"node_id":"MDQ6VXNlcjQ1ODkzNDk2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45893496?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/BUAAChuanWang","html_url":"https:\/\/github.com\/BUAAChuanWang","followers_url":"https:\/\/api.github.com\/users\/BUAAChuanWang\/followers","following_url":"https:\/\/api.github.com\/users\/BUAAChuanWang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/BUAAChuanWang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/BUAAChuanWang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/BUAAChuanWang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/BUAAChuanWang\/orgs","repos_url":"https:\/\/api.github.com\/users\/BUAAChuanWang\/repos","events_url":"https:\/\/api.github.com\/users\/BUAAChuanWang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/BUAAChuanWang\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-26T11:09:31Z","updated_at":"2023-08-29T15:53:59Z","closed_at":"2023-08-29T15:53:59Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nFirst, I cd workdir.\r\nThen, I just use load_dataset(\"json\", data_file={\"train\":\"\/a\/b\/c\/.d\/train\/train.json\", \"test\":\"\/a\/b\/c\/.d\/train\/test.json\"})\r\nthat couldn't work and \r\n<FileNotFoundError: Unable to find \r\n'\/a\/b\/c\/.d\/train\/train.jsonl' at \r\n\/a\/b\/c\/.d\/>\r\n\r\nAnd I debug, it is fine in version2.1.2\r\n\r\nSo there maybe a bug in path join.\r\n\r\nHere is the whole bug report:\r\n\/x\/datasets\/loa \u2502\r\n\u2502 d.py:1656 in load_dataset                                                    \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   1653 \u2502   ignore_verifications = ignore_verifications or save_infos         \u2502\r\n\u2502   1654 \u2502                                                                     \u2502\r\n\u2502   1655 \u2502   # Create a dataset builder                                        \u2502\r\n\u2502 \u2771 1656 \u2502   builder_instance = load_dataset_builder(                          \u2502\r\n\u2502   1657 \u2502   \u2502   path=path,                                                    \u2502\r\n\u2502   1658 \u2502   \u2502   name=name,                                                    \u2502\r\n\u2502   1659 \u2502   \u2502   data_dir=data_dir,                                            \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 x\/datasets\/loa \u2502\r\n\u2502 d.py:1439 in load_dataset_builder                                            \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   1436 \u2502   if use_auth_token is not None:                                    \u2502\r\n\u2502   1437 \u2502   \u2502   download_config = download_config.copy() if download_config e \u2502\r\n\u2502   1438 \u2502   \u2502   download_config.use_auth_token = use_auth_token               \u2502\r\n\u2502 \u2771 1439 \u2502   dataset_module = dataset_module_factory(                          \u2502\r\n\u2502   1440 \u2502   \u2502   path,                                                         \u2502\r\n\u2502   1441 \u2502   \u2502   revision=revision,                                            \u2502\r\n\u2502   1442 \u2502   \u2502   download_config=download_config,                              \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 x\/datasets\/loa \u2502\r\n\u2502 d.py:1097 in dataset_module_factory                                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   1094 \u2502                                                                     \u2502\r\n\u2502   1095 \u2502   # Try packaged                                                    \u2502\r\n\u2502   1096 \u2502   if path in _PACKAGED_DATASETS_MODULES:                            \u2502\r\n\u2502 \u2771 1097 \u2502   \u2502   return PackagedDatasetModuleFactory(                          \u2502\r\n\u2502   1098 \u2502   \u2502   \u2502   path,                                                     \u2502\r\n\u2502   1099 \u2502   \u2502   \u2502   data_dir=data_dir,                                        \u2502\r\n\u2502   1100 \u2502   \u2502   \u2502   data_files=data_files,                                    \u2502\r\n\u2502                                                                              \u2502\r\n\u2502x\/datasets\/loa \u2502\r\n\u2502 d.py:743 in get_module                                                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502    740 \u2502   \u2502   \u2502   if self.data_dir is not None                              \u2502\r\n\u2502    741 \u2502   \u2502   \u2502   else get_patterns_locally(str(Path().resolve()))          \u2502\r\n\u2502    742 \u2502   \u2502   )                                                             \u2502\r\n\u2502 \u2771  743 \u2502   \u2502   data_files = DataFilesDict.from_local_or_remote(              \u2502\r\n\u2502    744 \u2502   \u2502   \u2502   patterns,                                                 \u2502\r\n\u2502    745 \u2502   \u2502   \u2502   use_auth_token=self.download_config.use_auth_token,       \u2502\r\n\u2502    746 \u2502   \u2502   \u2502   base_path=str(Path(self.data_dir).resolve()) if self.data \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 x\/datasets\/dat \u2502\r\n\u2502 a_files.py:590 in from_local_or_remote                                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   587 \u2502   \u2502   out = cls()                                                    \u2502\r\n\u2502   588 \u2502   \u2502   for key, patterns_for_key in patterns.items():                 \u2502\r\n\u2502   589 \u2502   \u2502   \u2502   out[key] = (                                               \u2502\r\n\u2502 \u2771 590 \u2502   \u2502   \u2502   \u2502   DataFilesList.from_local_or_remote(                    \u2502\r\n\u2502   591 \u2502   \u2502   \u2502   \u2502   \u2502   patterns_for_key,                                  \u2502\r\n\u2502   592 \u2502   \u2502   \u2502   \u2502   \u2502   base_path=base_path,                               \u2502\r\n\u2502   593 \u2502   \u2502   \u2502   \u2502   \u2502   allowed_extensions=allowed_extensions,             \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/x\/datasets\/dat \u2502\r\n\u2502 a_files.py:558 in from_local_or_remote                                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   555 \u2502   \u2502   use_auth_token: Optional[Union[bool, str]] = None,             \u2502\r\n\u2502   556 \u2502   ) -> \"DataFilesList\":                                              \u2502\r\n\u2502   557 \u2502   \u2502   base_path = base_path if base_path is not None else str(Path() \u2502\r\n\u2502 \u2771 558 \u2502   \u2502   data_files = resolve_patterns_locally_or_by_urls(base_path, pa \u2502\r\n\u2502   559 \u2502   \u2502   origin_metadata = _get_origin_metadata_locally_or_by_urls(data \u2502\r\n\u2502   560 \u2502   \u2502   return cls(data_files, origin_metadata)                        \u2502\r\n\u2502   561                                                                        \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/x\/datasets\/dat \u2502\r\n\u2502 a_files.py:195 in resolve_patterns_locally_or_by_urls                        \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   192 \u2502   \u2502   if is_remote_url(pattern):                                     \u2502\r\n\u2502   193 \u2502   \u2502   \u2502   data_files.append(Url(pattern))                            \u2502\r\n\u2502   194 \u2502   \u2502   else:                                                          \u2502\r\n\u2502 \u2771 195 \u2502   \u2502   \u2502   for path in _resolve_single_pattern_locally(base_path, pat \u2502\r\n\u2502   196 \u2502   \u2502   \u2502   \u2502   data_files.append(path)                                \u2502\r\n\u2502   197 \u2502                                                                      \u2502\r\n\u2502   198 \u2502   if not data_files:                                                 \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 \/x\/datasets\/dat \u2502\r\n\u2502 a_files.py:145 in _resolve_single_pattern_locally                            \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   142 \u2502   \u2502   error_msg = f\"Unable to find '{pattern}' at {Path(base_path).r \u2502\r\n\u2502   143 \u2502   \u2502   if allowed_extensions is not None:                             \u2502\r\n\u2502   144 \u2502   \u2502   \u2502   error_msg += f\" with any supported extension {list(allowed \u2502\r\n\u2502 \u2771 145 \u2502   \u2502   raise FileNotFoundError(error_msg)                             \u2502\r\n\u2502   146 \u2502   return sorted(out)                                                 \u2502\r\n\u2502   147                         \n\n### Steps to reproduce the bug\n\n1. Version=2.3.2\r\n2. In shell, cd workdir.(cd \/a\/b\/c\/.d\/)\r\n3. load_dataset(\"json\", data_file={\"train\":\"\/a\/b\/c\/.d\/train\/train.json\", \"test\":\"\/a\/b\/c\/.d\/train\/test.json\"})\n\n### Expected behavior\n\nfix it please~\n\n### Environment info\n\n2.3.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6073\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6073\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6072","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6072\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6072\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6072\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6072","id":1822123560,"node_id":"PR_kwDODunzps5WbWFN","number":6072,"title":"Fix fsspec storage_options from load_dataset","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-07-26T10:44:23Z","updated_at":"2023-07-27T12:51:51Z","closed_at":"2023-07-27T12:42:57Z","author_association":"MEMBER","active_lock_reason":null,"body":"close https:\/\/github.com\/huggingface\/datasets\/issues\/6071","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6072\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6072\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6072","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6072","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6072.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6072.patch","merged_at":"2023-07-27T12:42:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6071","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6071\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6071\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6071\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6071","id":1821990749,"node_id":"I_kwDODunzps5smV9d","number":6071,"title":"storage_options provided to load_dataset not fully piping through since datasets 2.14.0","user":{"login":"exs-avianello","id":128361578,"node_id":"U_kgDOB6akag","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/128361578?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/exs-avianello","html_url":"https:\/\/github.com\/exs-avianello","followers_url":"https:\/\/api.github.com\/users\/exs-avianello\/followers","following_url":"https:\/\/api.github.com\/users\/exs-avianello\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/exs-avianello\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/exs-avianello\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/exs-avianello\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/exs-avianello\/orgs","repos_url":"https:\/\/api.github.com\/users\/exs-avianello\/repos","events_url":"https:\/\/api.github.com\/users\/exs-avianello\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/exs-avianello\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-26T09:37:20Z","updated_at":"2023-07-27T12:42:58Z","closed_at":"2023-07-27T12:42:58Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nSince the latest release of `datasets` (`2.14.0`), custom filesystem `storage_options` passed to `load_dataset()` do not seem to propagate through all the way - leading to problems if loading data files that need those options to be set.\r\n\r\nI think this is because of the new `_prepare_path_and_storage_options()` (https:\/\/github.com\/huggingface\/datasets\/pull\/6028), which returns the right `storage_options` to use given a path and a `DownloadConfig` - but which might not be taking into account the extra `storage_options` explicitly provided e.g. through `load_dataset()`\n\n### Steps to reproduce the bug\n\n```python\r\nimport fsspec\r\n\r\nimport pandas as pd\r\nimport datasets\r\n\r\n# Generate mock parquet file\r\ndata_files = \"demo.parquet\"\r\npd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]}).to_parquet(data_files)\r\n\r\n_storage_options = {\"x\": 1, \"y\": 2}\r\nfs = fsspec.filesystem(\"file\", **_storage_options)\r\n\r\ndataset = datasets.load_dataset(\r\n    \"parquet\",\r\n    data_files=data_files,\r\n    storage_options=fs.storage_options\r\n)\r\n```\r\n\r\nLooking at the `storage_options` resolved here:\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/b0177910b32712f28d147879395e511207e39958\/src\/datasets\/data_files.py#L331\r\n\r\n they end up being `{}`, instead of propagating through the `storage_options` that were provided to `load_dataset` (`fs.storage_options`). As these then get used for the filesystem operation a few lines below\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/b0177910b32712f28d147879395e511207e39958\/src\/datasets\/data_files.py#L339\r\n\r\nthe call will fail if the user-provided `storage_options` were needed.\r\n\r\n---\r\n\r\nA temporary workaround that seemed to work locally to bypass the problem was to bundle a duplicate of the `storage_options` into the `download_config`, so that they make their way all the way to `_prepare_path_and_storage_options()` and get extracted correctly:\r\n\r\n```python\r\ndataset = datasets.load_dataset(\r\n        \"parquet\",\r\n        data_files=data_files,\r\n        storage_options=fs.storage_options,\r\n        download_config=datasets.DownloadConfig(storage_options={fs.protocol: fs.storage_options}),\r\n    )\r\n```\n\n### Expected behavior\n\n`storage_options` provided to `load_dataset` take effect in all backend filesystem operations.\n\n### Environment info\n\ndatasets==2.14.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6071\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6071\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6070","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6070\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6070\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6070\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6070","id":1820836330,"node_id":"PR_kwDODunzps5WXDLc","number":6070,"title":"Fix Quickstart notebook link","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-25T17:48:37Z","updated_at":"2023-07-25T18:19:01Z","closed_at":"2023-07-25T18:10:16Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Reported in https:\/\/github.com\/huggingface\/datasets\/pull\/5902#issuecomment-1649885621 (cc @alvarobartt)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6070\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6070\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6070","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6070","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6070.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6070.patch","merged_at":"2023-07-25T18:10:16Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6069","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6069\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6069\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6069\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6069","id":1820831535,"node_id":"I_kwDODunzps5sh68v","number":6069,"title":"KeyError: dataset has no key \"image\"","user":{"login":"etetteh","id":28512232,"node_id":"MDQ6VXNlcjI4NTEyMjMy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/28512232?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/etetteh","html_url":"https:\/\/github.com\/etetteh","followers_url":"https:\/\/api.github.com\/users\/etetteh\/followers","following_url":"https:\/\/api.github.com\/users\/etetteh\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/etetteh\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/etetteh\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/etetteh\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/etetteh\/orgs","repos_url":"https:\/\/api.github.com\/users\/etetteh\/repos","events_url":"https:\/\/api.github.com\/users\/etetteh\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/etetteh\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-07-25T17:45:50Z","updated_at":"2023-07-27T12:42:17Z","closed_at":"2023-07-27T12:42:17Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI've loaded a local image dataset with:\r\n`ds = laod_dataset(\"imagefolder\", data_dir=path-to-data)`\r\n\r\nAnd defined a transform to process the data, following the Datasets docs.\r\n\r\nHowever, I get a keyError error, indicating there's no \"image\" key in my dataset. When I printed out the example_batch sent to the transformation function, it shows only the labels are being sent to the function.\r\nFor some reason, the images are not in the example batches.\n\n### Steps to reproduce the bug\n\nI'm using the latest stable version of datasets \n\n### Expected behavior\n\nI expect the example_batches to contain both images and labels\n\n### Environment info\n\nI'm using the latest stable version of datasets ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6069\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6069\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6068","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6068\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6068\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6068\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6068","id":1820106952,"node_id":"PR_kwDODunzps5WUkZi","number":6068,"title":"fix tqdm lock deletion","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-07-25T11:17:25Z","updated_at":"2023-07-25T15:29:39Z","closed_at":"2023-07-25T15:17:50Z","author_association":"MEMBER","active_lock_reason":null,"body":"related to https:\/\/github.com\/huggingface\/datasets\/issues\/6066","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6068\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6068\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6068","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6068","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6068.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6068.patch","merged_at":"2023-07-25T15:17:50Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6067","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6067\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6067\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6067\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6067","id":1819919025,"node_id":"PR_kwDODunzps5WT7EQ","number":6067,"title":"fix tqdm lock","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-25T09:32:16Z","updated_at":"2023-07-25T10:02:43Z","closed_at":"2023-07-25T09:54:12Z","author_association":"MEMBER","active_lock_reason":null,"body":"close https:\/\/github.com\/huggingface\/datasets\/issues\/6066","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6067\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6067\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6067","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6067","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6067.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6067.patch","merged_at":"2023-07-25T09:54:12Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6066","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6066\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6066\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6066\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6066","id":1819717542,"node_id":"I_kwDODunzps5sdq-m","number":6066,"title":"AttributeError: '_tqdm_cls' object has no attribute '_lock'","user":{"login":"codingl2k1","id":138426806,"node_id":"U_kgDOCEA5tg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/138426806?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/codingl2k1","html_url":"https:\/\/github.com\/codingl2k1","followers_url":"https:\/\/api.github.com\/users\/codingl2k1\/followers","following_url":"https:\/\/api.github.com\/users\/codingl2k1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/codingl2k1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/codingl2k1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/codingl2k1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/codingl2k1\/orgs","repos_url":"https:\/\/api.github.com\/users\/codingl2k1\/repos","events_url":"https:\/\/api.github.com\/users\/codingl2k1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/codingl2k1\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-07-25T07:24:36Z","updated_at":"2023-07-26T10:56:25Z","closed_at":"2023-07-26T10:56:24Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n```python\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/site-packages\/datasets\/load.py\", line 1034, in get_module\r\n    data_files = DataFilesDict.from_patterns(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/site-packages\/datasets\/data_files.py\", line 671, in from_patterns\r\n    DataFilesList.from_patterns(\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/site-packages\/datasets\/data_files.py\", line 586, in from_patterns\r\n    origin_metadata = _get_origin_metadata(data_files, download_config=download_config)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/site-packages\/datasets\/data_files.py\", line 502, in _get_origin_metadata\r\n    return thread_map(\r\n           ^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/site-packages\/tqdm\/contrib\/concurrent.py\", line 70, in thread_map\r\n    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/site-packages\/tqdm\/contrib\/concurrent.py\", line 48, in _executor_map\r\n    with ensure_lock(tqdm_class, lock_name=lock_name) as lk:\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/contextlib.py\", line 144, in __exit__\r\n    next(self.gen)\r\n  File \"\/Users\/codingl2k1\/.pyenv\/versions\/3.11.4\/lib\/python3.11\/site-packages\/tqdm\/contrib\/concurrent.py\", line 25, in ensure_lock\r\n    del tqdm_class._lock\r\n        ^^^^^^^^^^^^^^^^\r\nAttributeError: '_tqdm_cls' object has no attribute '_lock'\r\n```\n\n### Steps to reproduce the bug\n\nHappens ocasionally.\n\n### Expected behavior\n\nI added a print in tqdm `ensure_lock()`, got a `ensure_lock <datasets.utils.logging._tqdm_cls object at 0x16dddead0> ` print.\r\n\r\nAccording to the code in https:\/\/github.com\/tqdm\/tqdm\/blob\/master\/tqdm\/contrib\/concurrent.py#L24\r\n\r\n```python\r\n@contextmanager\r\ndef ensure_lock(tqdm_class, lock_name=\"\"):\r\n    \"\"\"get (create if necessary) and then restore `tqdm_class`'s lock\"\"\"\r\n    print(\"ensure_lock\", tqdm_class, lock_name)\r\n    old_lock = getattr(tqdm_class, '_lock', None)  # don't create a new lock\r\n    lock = old_lock or tqdm_class.get_lock()  # maybe create a new lock\r\n    lock = getattr(lock, lock_name, lock)  # maybe subtype\r\n    tqdm_class.set_lock(lock)\r\n    yield lock\r\n    if old_lock is None:\r\n        del tqdm_class._lock  # <-- It tries to del the `_lock` attribute from tqdm_class.\r\n    else:\r\n        tqdm_class.set_lock(old_lock)\r\n```\r\n\r\nBut, huggingface datasets `datasets.utils.logging._tqdm_cls` does not have the field `_lock`: https:\/\/github.com\/huggingface\/datasets\/blob\/main\/src\/datasets\/utils\/logging.py#L205\r\n\r\n```python\r\nclass _tqdm_cls:\r\n    def __call__(self, *args, disable=False, **kwargs):\r\n        if _tqdm_active and not disable:\r\n            return tqdm_lib.tqdm(*args, **kwargs)\r\n        else:\r\n            return EmptyTqdm(*args, **kwargs)\r\n\r\n    def set_lock(self, *args, **kwargs):\r\n        self._lock = None\r\n        if _tqdm_active:\r\n            return tqdm_lib.tqdm.set_lock(*args, **kwargs)\r\n\r\n    def get_lock(self):\r\n        if _tqdm_active:\r\n            return tqdm_lib.tqdm.get_lock()\r\n```\r\n\r\n\n\n### Environment info\n\nPython 3.11.4\r\ntqdm '4.65.0'\r\ndatasets master","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6066\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6066\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6065","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6065\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6065\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6065\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6065","id":1819334932,"node_id":"PR_kwDODunzps5WR8jI","number":6065,"title":"Add column type guessing from map return function","user":{"login":"piercefreeman","id":1712066,"node_id":"MDQ6VXNlcjE3MTIwNjY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1712066?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/piercefreeman","html_url":"https:\/\/github.com\/piercefreeman","followers_url":"https:\/\/api.github.com\/users\/piercefreeman\/followers","following_url":"https:\/\/api.github.com\/users\/piercefreeman\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/piercefreeman\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/piercefreeman\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/piercefreeman\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/piercefreeman\/orgs","repos_url":"https:\/\/api.github.com\/users\/piercefreeman\/repos","events_url":"https:\/\/api.github.com\/users\/piercefreeman\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/piercefreeman\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-07-25T00:34:17Z","updated_at":"2023-07-26T15:13:45Z","closed_at":"2023-07-26T15:13:44Z","author_association":"NONE","active_lock_reason":null,"body":"As discussed [here](https:\/\/github.com\/huggingface\/datasets\/issues\/5965), there are some cases where datasets is unable to automatically promote columns during mapping. The fix is to explicitly provide a `features` definition so pyarrow can configure itself with the right column types from the outset.\r\n\r\nThis PR provides an alternative approach, which is functionally equivalent to specifying features but a bit cleaner within a larger mapping pipeline. It allows clients to typehint the return variable coming from the mapper function - if we find one of these type annotations specified, and no explicit features have been passed in, we'll try to convert it into a Features map. If the map function runs and casting is unable to succeed, it will raise a DatasetTransformationNotAllowedError that indicates the typehint may be to blame. It works for batched and non-batched mapping functions.\r\n\r\nCurrently supported column types:\r\n- builtins primitives: string, int, float, bool\r\n- dictionaries, lists (nested and one-deep)\r\n- Optional types and None-Unions (synonymous with optional types)\r\n\r\nIt's used like:\r\n\r\n```python\r\nclass DatasetTyped(TypedDict):\r\n  texts: list[str]\r\n\r\ndef dataset_typed_map(batch) -> DatasetTyped:\r\n  return {\"texts\": [text.split() for text in batch[\"raw_text\"]]}\r\n\r\ndataset = {\"raw_text\": [\"\", \"This is a test\", \"This is another test\"]}\r\n\r\nwith Dataset.from_dict(dataset) as dset:\r\n  new_dataset = dset.map(\r\n    dataset_typed_map,\r\n    batched=True,\r\n    batch_size=1,\r\n    num_proc=1,\r\n)\r\n```\r\n\r\nOpen questions:\r\n- Should logging indicate we have automatically guessed these types? Or proceed quietly until we hit an error (as is the current implementation).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6065\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6065\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6065","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6065","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6065.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6065.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6064","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6064\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6064\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6064\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6064","id":1818703725,"node_id":"PR_kwDODunzps5WPzAv","number":6064,"title":"set dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-24T15:56:00Z","updated_at":"2023-07-24T16:05:19Z","closed_at":"2023-07-24T15:56:10Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6064\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6064\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6064","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6064","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6064.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6064.patch","merged_at":"2023-07-24T15:56:10Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6063","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6063\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6063\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6063\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6063","id":1818679485,"node_id":"PR_kwDODunzps5WPtxi","number":6063,"title":"Release: 2.14.0","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-07-24T15:41:19Z","updated_at":"2023-07-24T16:05:16Z","closed_at":"2023-07-24T15:47:51Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6063\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6063\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6063","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6063","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6063.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6063.patch","merged_at":"2023-07-24T15:47:51Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6062","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6062\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6062\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6062\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6062","id":1818341584,"node_id":"PR_kwDODunzps5WOj62","number":6062,"title":"Improve `Dataset.from_list` docstring","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-07-24T12:36:38Z","updated_at":"2023-07-24T14:43:48Z","closed_at":"2023-07-24T14:34:43Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6062\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6062\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6062","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6062","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6062.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6062.patch","merged_at":"2023-07-24T14:34:43Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6061","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6061\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6061\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6061\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6061","id":1818337136,"node_id":"PR_kwDODunzps5WOi79","number":6061,"title":"Dill 3.7 support","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-07-24T12:33:58Z","updated_at":"2023-07-24T14:13:20Z","closed_at":"2023-07-24T14:04:36Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Adds support for dill 3.7.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6061\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6061\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6061","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6061","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6061.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6061.patch","merged_at":"2023-07-24T14:04:36Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6060","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6060\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6060\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6060\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6060","id":1816614120,"node_id":"I_kwDODunzps5sR1To","number":6060,"title":"Dataset.map() execute twice when in PyTorch DDP mode","user":{"login":"wanghaoyucn","id":39429965,"node_id":"MDQ6VXNlcjM5NDI5OTY1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/39429965?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wanghaoyucn","html_url":"https:\/\/github.com\/wanghaoyucn","followers_url":"https:\/\/api.github.com\/users\/wanghaoyucn\/followers","following_url":"https:\/\/api.github.com\/users\/wanghaoyucn\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wanghaoyucn\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wanghaoyucn\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wanghaoyucn\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wanghaoyucn\/orgs","repos_url":"https:\/\/api.github.com\/users\/wanghaoyucn\/repos","events_url":"https:\/\/api.github.com\/users\/wanghaoyucn\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wanghaoyucn\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-22T05:06:43Z","updated_at":"2023-07-24T19:29:55Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI use `torchrun --standalone --nproc_per_node=2 train.py` to start training. And write the code following the [docs](https:\/\/huggingface.co\/docs\/datasets\/process#distributed-usage). The trick about using `torch.distributed.barrier()` to only execute map at the main process doesn't always work. When I am training model, it will map twice. When I am running a test for dataset and dataloader (just print the batches), it can work. Their code about loading dataset are same.\r\n\r\nAnd on another server with 30 CPU cores, I use 2 GPUs and it can't work neither. \r\n\r\nI have tried to use `rank` and `local_rank` to check, they all didn't make sense.\r\n\r\n### Steps to reproduce the bug\r\n\r\nuse `torchrun --standalone --nproc_per_node=2 train.py` or `torchrun --standalone train.py` to run\r\n\r\nThis is my code:\r\n\r\n```python\r\n            if args.distributed and world_size > 1:\r\n                if args.local_rank > 0:\r\n                    print(f\"Rank {args.rank}: Gpu {args.gpu} waiting for main process to perform the mapping\", force=True)\r\n                    torch.distributed.barrier()\r\n                print(\"Mapping dataset\")\r\n                dataset = dataset.map(lambda x: cut_reorder_keys(x, num_stations_list=args.num_stations_list, is_pad=True, is_train=True), num_proc=8, desc=\"cut_reorder_keys\")\r\n                dataset = dataset.map(lambda x: random_shift(x, shift_range=(-160, 0), feature_scale=16), num_proc=8, desc=\"random_shift\")\r\n                dataset_test = dataset_test.map(lambda x: cut_reorder_keys(x, num_stations_list=args.num_stations_list, is_pad=True, is_train=False), num_proc=8, desc=\"cut_reorder_keys\")\r\n                if args.local_rank == 0:\r\n                    print(\"Mapping finished, loading results from main process\")\r\n                    torch.distributed.barrier()\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nOnly the main process will execute `map`, while the sub process will load cache from disk.\r\n\r\n### Environment info\r\n\r\nserver with 64 CPU cores (AMD Ryzen Threadripper PRO 5995WX 64-Cores) and 2 RTX 4090\r\n\r\n- `python==3.9.16`\r\n- `datasets==2.13.1`\r\n- `torch==2.0.1+cu117`\r\n- `22.04.1-Ubuntu`\r\n\r\nserver with 30 CPU cores (Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz) and 2 RTX 4090\r\n\r\n- `python==3.9.0`\r\n- `datasets==2.13.1`\r\n- `torch==2.0.1+cu117`\r\n- `Ubuntu 20.04`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6060\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6060\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6059","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6059\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6059\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6059\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6059","id":1816537176,"node_id":"I_kwDODunzps5sRihY","number":6059,"title":"Provide ability to load label mappings from file","user":{"login":"david-waterworth","id":5028974,"node_id":"MDQ6VXNlcjUwMjg5NzQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5028974?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/david-waterworth","html_url":"https:\/\/github.com\/david-waterworth","followers_url":"https:\/\/api.github.com\/users\/david-waterworth\/followers","following_url":"https:\/\/api.github.com\/users\/david-waterworth\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/david-waterworth\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/david-waterworth\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/david-waterworth\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/david-waterworth\/orgs","repos_url":"https:\/\/api.github.com\/users\/david-waterworth\/repos","events_url":"https:\/\/api.github.com\/users\/david-waterworth\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/david-waterworth\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-07-22T02:04:19Z","updated_at":"2023-07-22T02:04:19Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\n\r\nMy task is classification of a dataset containing a large label set that includes a hierarchy. Even ignoring the hierarchy I'm not able to find an example using `datasets` where the label names aren't hard-coded. This works find for classification of a handful of labels but ideally there would be a way of loading the name\/id mappings required for `datasets.features.ClassLabel` from a file.\r\n\r\nIt is possible to pass a file to ClassLabel but I cannot see an easy way of using this with `GeneratorBasedBuilder` since `self._info` is called before the `dl_manager` is constructed so even if my dataset contains say `label_mappings.json` there's no way of loading it in order to construct the `datasets.DatasetInfo`\r\n\r\nI can see other uses to accessing the `download_manager` from `self._info` - i.e. if the files contain a schema (i.e. `arrow` or `parquet` files) the `datasets.DatasetInfo` could be inferred.\r\n\r\nThe workaround that was suggested in the forum is to generate a `.py` file from the `label_mappings.json` and import it.\r\n\r\n```\r\nclass TestDatasetBuilder(datasets.GeneratorBasedBuilder):\r\n    VERSION = datasets.Version(\"1.0.0\")\r\n\r\n    def _info(self):\r\n        return datasets.DatasetInfo(\r\n            description=_DESCRIPTION,\r\n            features=datasets.Features(\r\n                {\r\n                    \"text\": datasets.Value(\"string\"),\r\n                    \"label\": datasets.features.ClassLabel(names=[\"label_1\", \"label_2\"]),\r\n                }\r\n            ),\r\n            task_templates=[TextClassification(text_column=\"text\", label_column=\"label\")],\r\n        )\r\n\r\n    def _split_generators(self, dl_manager):\r\n        train_path = dl_manager.download_and_extract(_TRAIN_DOWNLOAD_URL)\r\n        test_path = dl_manager.download_and_extract(_TEST_DOWNLOAD_URL)\r\n        return [\r\n            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": train_path}),\r\n            datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs={\"filepath\": test_path}),\r\n        ]\r\n\r\n    def _generate_examples(self, filepath):\r\n        \"\"\"Generate AG News examples.\"\"\"\r\n        with open(filepath, encoding=\"utf-8\") as csv_file:\r\n            csv_reader = csv.DictReader(csv_file)\r\n            for id_, row in enumerate(csv_reader):\r\n                yield id_, row\r\n```\n\n### Motivation\n\nAllow `datasets.DatasetInfo` to be generated based on the contents of the dataset. \n\n### Your contribution\n\nI'm willing to work on a PR with guidence.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6059\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6059\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6058","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6058\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6058\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6058\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6058","id":1815131397,"node_id":"I_kwDODunzps5sMLUF","number":6058,"title":"laion-coco download error","user":{"login":"yangyijune","id":54424110,"node_id":"MDQ6VXNlcjU0NDI0MTEw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54424110?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yangyijune","html_url":"https:\/\/github.com\/yangyijune","followers_url":"https:\/\/api.github.com\/users\/yangyijune\/followers","following_url":"https:\/\/api.github.com\/users\/yangyijune\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yangyijune\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yangyijune\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yangyijune\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yangyijune\/orgs","repos_url":"https:\/\/api.github.com\/users\/yangyijune\/repos","events_url":"https:\/\/api.github.com\/users\/yangyijune\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yangyijune\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-21T04:24:15Z","updated_at":"2023-07-22T01:42:06Z","closed_at":"2023-07-22T01:42:06Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThe full trace:\r\n```\r\n\/home\/bian\/anaconda3\/envs\/sd\/lib\/python3.10\/site-packages\/datasets\/load.py:1744: FutureWarning: 'ignore_verifications' was de\r\nprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\r\nYou can remove this warning by passing 'verification_mode=no_checks' instead.\r\n  warnings.warn(               \r\nDownloading and preparing dataset parquet\/laion--laion-coco to \/home\/bian\/.cache\/huggingface\/datasets\/laion___parquet\/laion--\r\nlaion-coco-cb4205d7f1863066\/0.0.0\/bcacc8bdaa0614a5d73d0344c813275e590940c6ea8bc569da462847103a1afd...\r\nDownloading data: 100%|\u2588| 1.89G\/1.89G [04:57<00:00,           \r\nDownloading data files: 100%|\u2588| 1\/1 [04:59<00:00, 2           \r\nExtracting data files: 100%|\u2588| 1\/1 [00:00<00:00, 13           \r\nGenerating train split: 0 examples [00:00, ? examples\/s]<_io.BufferedReader \r\nname='\/home\/bian\/.cache\/huggingface\/datasets\/downlo           \r\nads\/26d7a016d25bbd9443115cfa3092136e8eb2f1f5bcd4154           \r\n0cb9234572927f04c'>                                           \r\n                                                   Traceback (most recent call last):\r\n  File \"\/home\/bian\/data\/ZOC\/download_laion_coco.py\", line 4, in <module>\r\n    dataset = load_dataset(\"laion\/laion-coco\", ignore_verifications=True)\r\n  File \"\/home\/bian\/anaconda3\/envs\/sd\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1791, in load_dataset\r\n    builder_instance.download_and_prepare(                    \r\n  File \"\/home\/bian\/anaconda3\/envs\/sd\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 891, in download_and_prepare\r\n    self._download_and_prepare(                               \r\n  File \"\/home\/bian\/anaconda3\/envs\/sd\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 986, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/home\/bian\/anaconda3\/envs\/sd\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1748, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"\/home\/bian\/anaconda3\/envs\/sd\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1842, in _prepare_split_single\r\n    generator = self._generate_tables(**gen_kwargs)           \r\n  File \"\/home\/bian\/anaconda3\/envs\/sd\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/parquet\/parquet.py\", line 67, in \r\n_generate_tables                                              \r\n    parquet_file = pq.ParquetFile(f)                          \r\n  File \"\/home\/bian\/anaconda3\/envs\/sd\/lib\/python3.10\/site-packages\/pyarrow\/parquet\/core.py\", line 323, in __init__\r\n    self.reader.open(          \r\n  File \"pyarrow\/_parquet.pyx\", line 1227, in pyarrow._parquet.ParquetReader.open\r\n  File \"pyarrow\/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file\r\n.\r\n```\r\n\r\n\r\nI have carefully followed the instructions in  #5264  but still get the same error.\r\n\r\nOther helpful information: \r\n\r\n\r\n```\r\nds = load_dataset(\"parquet\", data_files=\r\n    ...: \"https:\/\/huggingface.co\/datasets\/laion\/l\r\n    ...: aion-coco\/resolve\/d22869de3ccd39dfec1507\r\n    ...: f7ded32e4a518dad24\/part-00000-2256f782-1\r\n    ...: 26f-4dc6-b9c6-e6757637749d-c000.snappy.p\r\n    ...: arquet\")\r\nFound cached dataset parquet (\/home\/bian\/.cache\/huggingface\/datasets\/parquet\/default-a02eea00aeb08b0e\/0.0.0\/bb8ccf89d9ee38581ff5e51506d721a9b37f14df8090dc9b2d8fb4a40957833f)\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00,  4.55it\/s]\r\n```\n\n### Steps to reproduce the bug\n\n``` \r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"laion\/laion-coco\", ignore_verifications=True\/False)\r\n```\n\n### Expected behavior\n\nProperly load Laion-coco dataset\n\n### Environment info\n\ndatasets==2.11.0 torch==1.12.1 python 3.10","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6058\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6058\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6057","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6057\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6057\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6057\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6057","id":1815100151,"node_id":"I_kwDODunzps5sMDr3","number":6057,"title":"Why is the speed difference of gen example so big?","user":{"login":"pixeli99","id":46072190,"node_id":"MDQ6VXNlcjQ2MDcyMTkw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/46072190?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/pixeli99","html_url":"https:\/\/github.com\/pixeli99","followers_url":"https:\/\/api.github.com\/users\/pixeli99\/followers","following_url":"https:\/\/api.github.com\/users\/pixeli99\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/pixeli99\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/pixeli99\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/pixeli99\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/pixeli99\/orgs","repos_url":"https:\/\/api.github.com\/users\/pixeli99\/repos","events_url":"https:\/\/api.github.com\/users\/pixeli99\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/pixeli99\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-21T03:34:49Z","updated_at":"2023-10-04T18:06:16Z","closed_at":"2023-10-04T18:06:15Z","author_association":"NONE","active_lock_reason":null,"body":"```python\r\ndef _generate_examples(self, metadata_path, images_dir, conditioning_images_dir):\r\n        with open(metadata_path, 'r') as file:\r\n            metadata = json.load(file)\r\n\r\n        for idx, item in enumerate(metadata):\r\n            image_path = item.get('image_path')\r\n            text_content = item.get('text_content')\r\n            image_data = open(image_path, \"rb\").read()\r\n            yield idx, {\r\n                \"text\": text_content,\r\n                \"image\": {\r\n                    \"path\": image_path,\r\n                    \"bytes\": image_data,\r\n                },\r\n                \"conditioning_image\": {\r\n                    \"path\": image_path,\r\n                    \"bytes\": image_data,\r\n                },\r\n            }\r\n```\r\nHello, \r\n\r\nI use the above function to deal with my local data set, but I am very surprised that the speed at which I generate example is very different. When I start a training task, **sometimes 1000examples\/s, sometimes only 10examples\/s.**\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/46072190\/cdc17661-8267-4fd8-b30c-b74d505efd9b)\r\n\r\nI'm not saying that speed is changing all the time. I mean, the reading speed is different in different training, which will cause me to start training over and over again until the speed of this generation of examples is normal.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6057\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6057\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6056","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6056\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6056\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6056\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6056","id":1815086963,"node_id":"PR_kwDODunzps5WD4RY","number":6056,"title":"Implement proper checkpointing for dataset uploading with resume function that does not require remapping shards that have already been uploaded","user":{"login":"AntreasAntoniou","id":10792502,"node_id":"MDQ6VXNlcjEwNzkyNTAy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10792502?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/AntreasAntoniou","html_url":"https:\/\/github.com\/AntreasAntoniou","followers_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/followers","following_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/orgs","repos_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/repos","events_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-07-21T03:13:21Z","updated_at":"2023-08-17T08:26:53Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Context: issue #5990\r\n\r\nIn order to implement the checkpointing, I introduce a metadata folder that keeps one yaml file for each set that one is uploading. This yaml keeps track of what shards have already been uploaded, and which one the idx of the latest one was. Using this information I am then able to easily get the push_to_hub function to retrieve on demand past history of uploads and continue mapping and uploading from where it was left off. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6056\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6056\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6056","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6056","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6056.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6056.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6055","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6055\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6055\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6055\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6055","id":1813524145,"node_id":"I_kwDODunzps5sGC6x","number":6055,"title":"Fix host URL in The Pile datasets ","user":{"login":"nickovchinnikov","id":7540752,"node_id":"MDQ6VXNlcjc1NDA3NTI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7540752?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nickovchinnikov","html_url":"https:\/\/github.com\/nickovchinnikov","followers_url":"https:\/\/api.github.com\/users\/nickovchinnikov\/followers","following_url":"https:\/\/api.github.com\/users\/nickovchinnikov\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nickovchinnikov\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nickovchinnikov\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nickovchinnikov\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nickovchinnikov\/orgs","repos_url":"https:\/\/api.github.com\/users\/nickovchinnikov\/repos","events_url":"https:\/\/api.github.com\/users\/nickovchinnikov\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nickovchinnikov\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-07-20T09:08:52Z","updated_at":"2023-07-20T09:09:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nIn  #3627 and  #5543, you tried to fix the host URL in The Pile datasets. But both URLs are not working now:\r\n\r\n`HTTPError: 404 Client Error: Not Found for URL: https:\/\/the-eye.eu\/public\/AI\/pile_preliminary_components\/PUBMED_title_abstracts_2019_baseline.jsonl.zst`\r\nAnd\r\n`ConnectTimeout: HTTPSConnectionPool(host='mystic.the-eye.eu', port=443): Max retries exceeded with url: \/public\/AI\/pile_preliminary_components\/PUBMED_title_abstracts_2019_baseline.jsonl.zst (Caused by ConnectTimeoutError(, 'Connection to mystic.the-eye.eu timed out. (connect timeout=10.0)'))`\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\n# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\r\ndata_files = \"https:\/\/mystic.the-eye.eu\/public\/AI\/pile_preliminary_components\/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\r\npubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\r\npubmed_dataset\r\n```\r\n\r\nResult:\r\n`ConnectTimeout: HTTPSConnectionPool(host='mystic.the-eye.eu', port=443): Max retries exceeded with url: \/public\/AI\/pile_preliminary_components\/PUBMED_title_abstracts_2019_baseline.jsonl.zst (Caused by ConnectTimeoutError(, 'Connection to mystic.the-eye.eu timed out. (connect timeout=10.0)'))`\r\n\r\nAnd\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\n# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\r\ndata_files = \"https:\/\/the-eye.eu\/public\/AI\/pile_preliminary_components\/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\r\npubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\r\npubmed_dataset\r\n```\r\n\r\nResult:\r\n`HTTPError: 404 Client Error: Not Found for URL: https:\/\/the-eye.eu\/public\/AI\/pile_preliminary_components\/PUBMED_title_abstracts_2019_baseline.jsonl.zst`\r\n\r\n### Expected behavior\r\n\r\nDownloading as normal.\r\n\r\n### Environment info\r\n\r\nEnvironment info\r\n\r\n    `datasets` version: 2.9.0\r\n    Platform: Windows\r\n    Python version: 3.9.13\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6055\/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6055\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6054","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6054\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6054\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6054\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6054","id":1813271304,"node_id":"I_kwDODunzps5sFFMI","number":6054,"title":"Multi-processed `Dataset.map` slows down a lot when `import torch`","user":{"login":"ShinoharaHare","id":47121592,"node_id":"MDQ6VXNlcjQ3MTIxNTky","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47121592?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ShinoharaHare","html_url":"https:\/\/github.com\/ShinoharaHare","followers_url":"https:\/\/api.github.com\/users\/ShinoharaHare\/followers","following_url":"https:\/\/api.github.com\/users\/ShinoharaHare\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ShinoharaHare\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ShinoharaHare\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ShinoharaHare\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ShinoharaHare\/orgs","repos_url":"https:\/\/api.github.com\/users\/ShinoharaHare\/repos","events_url":"https:\/\/api.github.com\/users\/ShinoharaHare\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ShinoharaHare\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892865,"node_id":"MDU6TGFiZWwxOTM1ODkyODY1","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/duplicate","name":"duplicate","color":"cfd3d7","default":true,"description":"This issue or pull request already exists"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-20T06:36:14Z","updated_at":"2023-07-21T15:19:37Z","closed_at":"2023-07-21T15:19:37Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen using `Dataset.map` with `num_proc > 1`, the speed slows down much if I add `import torch` to the start of the script even though I don't use it.\r\n\r\nI'm not sure if it's `torch` only or if any other package that is \"large\" will also cause the same result.\r\nBTW, `import lightning` also slows it down.\r\n\r\nBelow are the progress bars of `Dataset.map`, the only difference between them is with or without `import torch`, but the speed varies by 6-7 times.\r\n\r\n- without `import torch` ![image](https:\/\/github.com\/huggingface\/datasets\/assets\/47121592\/0233055a-ced4-424a-9f0f-32a2afd802c2)\r\n- with `import torch` ![image](https:\/\/github.com\/huggingface\/datasets\/assets\/47121592\/463eafb7-b81e-4eb9-91ca-fd7fe20f3d59)\r\n\n\n### Steps to reproduce the bug\n\nBelow is the code I used, but I don't think the dataset and the mapping function have much to do with the phenomenon.\r\n\r\n```python3\r\nfrom datasets import load_from_disk, disable_caching\r\nfrom transformers import AutoTokenizer\r\n# import torch\r\n# import lightning\r\n\r\ndef rearrange_datapoints(\r\n    batch,\r\n    tokenizer,\r\n    sequence_length,\r\n):\r\n    datapoints = []\r\n    \r\n    input_ids = []\r\n    for x in batch['input_ids']:\r\n        input_ids += x\r\n        while len(input_ids) >= sequence_length:\r\n            datapoint = input_ids[:sequence_length]\r\n            datapoints.append(datapoint)\r\n            input_ids[:sequence_length] = []\r\n\r\n    if input_ids:\r\n        paddings = [-1] * (sequence_length - len(input_ids))\r\n        datapoint = paddings + input_ids if tokenizer.padding_side == 'left' else input_ids + paddings\r\n        datapoints.append(datapoint)\r\n    \r\n    batch['input_ids'] = datapoints\r\n    return batch\r\n\r\n\r\nif __name__ == '__main__':\r\n    disable_caching()\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained('...', use_fast=False)\r\n    dataset = load_from_disk('...')\r\n    \r\n    dataset = dataset.map(\r\n        rearrange_datapoints,\r\n        fn_kwargs=dict(\r\n            tokenizer=tokenizer,\r\n            sequence_length=2048,\r\n        ),\r\n        batched=True,\r\n        num_proc=8,\r\n    )\r\n\r\n```\n\n### Expected behavior\n\nThe multi-processed `Dataset.map` function speed between with and without `import torch` should be the same.\n\n### Environment info\n\n- `datasets` version: 2.13.1\r\n- Platform: Linux-3.10.0-1127.el7.x86_64-x86_64-with-glibc2.31\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6054\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6054\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6053","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6053\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6053\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6053\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6053","id":1812635902,"node_id":"I_kwDODunzps5sCqD-","number":6053,"title":"Change package name from \"datasets\" to something less generic","user":{"login":"geajack","id":2124157,"node_id":"MDQ6VXNlcjIxMjQxNTc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2124157?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/geajack","html_url":"https:\/\/github.com\/geajack","followers_url":"https:\/\/api.github.com\/users\/geajack\/followers","following_url":"https:\/\/api.github.com\/users\/geajack\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/geajack\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/geajack\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/geajack\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/geajack\/orgs","repos_url":"https:\/\/api.github.com\/users\/geajack\/repos","events_url":"https:\/\/api.github.com\/users\/geajack\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/geajack\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-19T19:53:28Z","updated_at":"2023-10-03T16:04:09Z","closed_at":"2023-10-03T16:04:09Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nI'm repeatedly finding myself in situations where I want to have a package called `datasets.py` or `evaluate.py` in my code and can't because those names are being taken up by Huggingface packages. While I can understand how (even from the user's perspective) it's aesthetically pleasing to have nice terse library names, ultimately a library hogging simple names like this is something I find short-sighted, impractical and at my most irritable, frankly rude.\r\n\r\nMy preference would be a pattern like what you get with all the other big libraries like numpy or pandas:\r\n\r\n```\r\nimport huggingface as hf\r\n# hf.transformers, hf.datasets, hf.evaluate\r\n```\r\n\r\nor things like\r\n\r\n```\r\nimport huggingface.transformers as tf\r\n# tf.load_model(), etc\r\n```\r\n\r\nIf this isn't possible for some technical reason, at least just call the packages something like `hf_transformers` and so on.\r\n\r\nI realize this is a very big change that's probably been discussed internally already, but I'm making this issue and sister issues on each huggingface project just to start the conversation and begin tracking community feeling on the matter, since I suspect I'm not the only one who feels like this.\r\n\r\nSorry if this has been requested already on this issue tracker, I couldn't find anything looking for terms like \"package name\".\r\n\r\nSister issues:\r\n- [transformers](https:\/\/github.com\/huggingface\/transformers\/issues\/24934)\r\n- **datasets**\r\n- [evaluate](https:\/\/github.com\/huggingface\/evaluate\/issues\/476)\r\n\r\n### Motivation\r\n\r\nNot taking up package names the user is likely to want to use.\r\n\r\n### Your contribution\r\n\r\nNo - more a matter of internal discussion among core library authors.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6053\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6053\/timeline","performed_via_github_app":null,"state_reason":"not_planned","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6052","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6052\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6052\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6052\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6052","id":1812145100,"node_id":"PR_kwDODunzps5V5yOi","number":6052,"title":"Remove `HfFileSystem` and deprecate `S3FileSystem`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2023-07-19T15:00:01Z","updated_at":"2023-07-19T17:39:11Z","closed_at":"2023-07-19T17:27:17Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Remove the legacy `HfFileSystem` and deprecate `S3FileSystem`\r\n\r\n\r\ncc @philschmid for the SageMaker scripts\/notebooks that still use `datasets`' `S3FileSystem`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6052\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6052\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6052","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6052","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6052.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6052.patch","merged_at":"2023-07-19T17:27:17Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6051","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6051\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6051\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6051\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6051","id":1811549650,"node_id":"I_kwDODunzps5r-g3S","number":6051,"title":"Skipping shard in the remote repo and resume upload","user":{"login":"rs9000","id":9029817,"node_id":"MDQ6VXNlcjkwMjk4MTc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9029817?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rs9000","html_url":"https:\/\/github.com\/rs9000","followers_url":"https:\/\/api.github.com\/users\/rs9000\/followers","following_url":"https:\/\/api.github.com\/users\/rs9000\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rs9000\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rs9000\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rs9000\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rs9000\/orgs","repos_url":"https:\/\/api.github.com\/users\/rs9000\/repos","events_url":"https:\/\/api.github.com\/users\/rs9000\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rs9000\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-19T09:25:26Z","updated_at":"2023-07-20T18:16:01Z","closed_at":"2023-07-20T18:16:00Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nFor some reason when I try to resume the upload of my dataset, it is very slow to reach the index of the shard from which to resume the uploading.\r\n\r\nFrom my understanding, the problem is in this part of the code:\r\n\r\narrow_dataset.py\r\n```python\r\nfor index, shard in logging.tqdm(\r\n            enumerate(itertools.chain([first_shard], shards_iter)),\r\n            desc=\"Pushing dataset shards to the dataset hub\",\r\n            total=num_shards,\r\n            disable=not logging.is_progress_bar_enabled(),\r\n        ):\r\n            shard_path_in_repo = path_in_repo(index, shard)\r\n            # Upload a shard only if it doesn't already exist in the repository\r\n            if shard_path_in_repo not in data_files:\r\n```\r\n\r\nIn particular, iterating the generator is slow during the call:\r\n```python\r\nself._select_contiguous(start, length, new_fingerprint=new_fingerprint)\r\n```\r\n\r\nI wonder if it is possible to avoid calling this function for shards that are already uploaded and just start from the correct shard index.\n\n### Steps to reproduce the bug\n\n1. Start the upload\r\n```python\r\ndataset = load_dataset(\"imagefolder\", data_dir=DATA_DIR, split=\"train\", drop_labels=True)    \r\ndataset.push_to_hub(\"repo\/name\")\r\n```\r\n2. Stop and restart the upload after hundreds of shards\n\n### Expected behavior\n\nSkip the uploaded shards faster.\n\n### Environment info\n\n- `datasets` version: 2.5.1\r\n- Platform: Linux-4.18.0-193.el8.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.8.16\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6051\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6051\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6049","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6049\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6049\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6049\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6049","id":1810378706,"node_id":"PR_kwDODunzps5Vz1pd","number":6049,"title":"Update `ruff` version in pre-commit config ","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-18T17:13:50Z","updated_at":"2023-12-01T14:26:19Z","closed_at":"2023-12-01T14:26:19Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"so that it corresponds to the one that is being run in CI","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6049\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6049\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6049","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6049","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6049.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6049.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6048","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6048\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6048\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6048\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6048","id":1809629346,"node_id":"I_kwDODunzps5r3MCi","number":6048,"title":"when i use datasets.load_dataset, i encounter the http connect error!","user":{"login":"yangy1992","id":137855591,"node_id":"U_kgDOCDeCZw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/137855591?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yangy1992","html_url":"https:\/\/github.com\/yangy1992","followers_url":"https:\/\/api.github.com\/users\/yangy1992\/followers","following_url":"https:\/\/api.github.com\/users\/yangy1992\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yangy1992\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yangy1992\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yangy1992\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yangy1992\/orgs","repos_url":"https:\/\/api.github.com\/users\/yangy1992\/repos","events_url":"https:\/\/api.github.com\/users\/yangy1992\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yangy1992\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-18T10:16:34Z","updated_at":"2023-07-18T16:18:39Z","closed_at":"2023-07-18T16:18:39Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n`common_voice_test = load_dataset(\"audiofolder\", data_dir=\".\/dataset\/\",cache_dir=\".\/cache\",split=datasets.Split.TEST)`\r\nwhen i run the code above, i got the error as below:\r\n--------------------------------------------\r\nConnectionError: Couldn't reach https:\/\/raw.githubusercontent.com\/huggingface\/datasets\/2.3.2\/datasets\/audiofolder\/audiofolder.py (ConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: \/huggingface\/datasets\/2.3.2\/datasets\/audiofolder\/audiofolder.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f299ed082e0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\")))\r\n\r\n\r\n--------------------------------------------------\r\nMy all data is on local machine, why does it need to connect the internet? how can i fix it, because my machine cannot connect the internet.\n\n### Steps to reproduce the bug\n\n1\n\n### Expected behavior\n\nno error when i use the load_dataset func\n\n### Environment info\n\npython=3.8.15","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6048\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6048\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6047","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6047\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6047\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6047\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6047","id":1809627947,"node_id":"PR_kwDODunzps5VxRLA","number":6047,"title":"Bump dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-18T10:15:39Z","updated_at":"2023-07-18T10:28:01Z","closed_at":"2023-07-18T10:15:52Z","author_association":"MEMBER","active_lock_reason":null,"body":"workaround to fix an issue with transformers CI\r\n\r\nhttps:\/\/github.com\/huggingface\/transformers\/pull\/24867#discussion_r1266519626","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6047\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6047\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6047","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6047","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6047.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6047.patch","merged_at":"2023-07-18T10:15:52Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6046","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6046\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6046\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6046\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6046","id":1808154414,"node_id":"I_kwDODunzps5rxj8u","number":6046,"title":"Support proxy and user-agent in fsspec calls","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":3761482852,"node_id":"LA_kwDODunzps7gM6xk","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/good%20second%20issue","name":"good second issue","color":"BDE59C","default":false,"description":"Issues a bit more difficult than \"Good First\" issues"}],"state":"open","locked":false,"assignee":{"login":"zutarich","id":95092167,"node_id":"U_kgDOBar9xw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/95092167?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/zutarich","html_url":"https:\/\/github.com\/zutarich","followers_url":"https:\/\/api.github.com\/users\/zutarich\/followers","following_url":"https:\/\/api.github.com\/users\/zutarich\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/zutarich\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/zutarich\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/zutarich\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/zutarich\/orgs","repos_url":"https:\/\/api.github.com\/users\/zutarich\/repos","events_url":"https:\/\/api.github.com\/users\/zutarich\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/zutarich\/received_events","type":"User","site_admin":false},"assignees":[{"login":"zutarich","id":95092167,"node_id":"U_kgDOBar9xw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/95092167?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/zutarich","html_url":"https:\/\/github.com\/zutarich","followers_url":"https:\/\/api.github.com\/users\/zutarich\/followers","following_url":"https:\/\/api.github.com\/users\/zutarich\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/zutarich\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/zutarich\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/zutarich\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/zutarich\/orgs","repos_url":"https:\/\/api.github.com\/users\/zutarich\/repos","events_url":"https:\/\/api.github.com\/users\/zutarich\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/zutarich\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":8,"created_at":"2023-07-17T16:39:26Z","updated_at":"2023-10-09T13:49:14Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Since we switched to the new HfFileSystem we no longer apply user's proxy and user-agent.\r\n\r\nUsing the HTTP_PROXY and HTTPS_PROXY environment variables works though since we use aiohttp to call the HF Hub.\r\n\r\nThis can be implemented in `_prepare_single_hop_path_and_storage_options`.\r\n\r\nThough ideally the `HfFileSystem` could support passing at least the proxies","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6046\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6046\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6045","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6045\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6045\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6045\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6045","id":1808072270,"node_id":"PR_kwDODunzps5Vr-r1","number":6045,"title":"Check if column names match in Parquet loader only when config `features` are specified","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-07-17T15:50:15Z","updated_at":"2023-07-24T14:45:56Z","closed_at":"2023-07-24T14:35:03Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6039 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6045\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6045\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6045","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6045","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6045.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6045.patch","merged_at":"2023-07-24T14:35:03Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6044","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6044\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6044\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6044\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6044","id":1808057906,"node_id":"PR_kwDODunzps5Vr7jr","number":6044,"title":"Rename \"pattern\" to \"path\" in YAML data_files configs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2023-07-17T15:41:16Z","updated_at":"2023-07-19T16:59:55Z","closed_at":"2023-07-19T16:48:06Z","author_association":"MEMBER","active_lock_reason":null,"body":"To make it easier to understand for users.\r\n\r\nThey can use \"path\" to specify a single path, <s>or \"paths\" to use a list of paths.<\/s>\r\n\r\nGlob patterns are still supported though\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6044\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6044\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6044","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6044","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6044.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6044.patch","merged_at":"2023-07-19T16:48:06Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6043","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6043\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6043\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6043\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6043","id":1807771750,"node_id":"I_kwDODunzps5rwGhm","number":6043,"title":"Compression kwargs have no effect when saving datasets as csv","user":{"login":"exs-avianello","id":128361578,"node_id":"U_kgDOB6akag","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/128361578?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/exs-avianello","html_url":"https:\/\/github.com\/exs-avianello","followers_url":"https:\/\/api.github.com\/users\/exs-avianello\/followers","following_url":"https:\/\/api.github.com\/users\/exs-avianello\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/exs-avianello\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/exs-avianello\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/exs-avianello\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/exs-avianello\/orgs","repos_url":"https:\/\/api.github.com\/users\/exs-avianello\/repos","events_url":"https:\/\/api.github.com\/users\/exs-avianello\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/exs-avianello\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-17T13:19:21Z","updated_at":"2023-07-22T17:34:18Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nAttempting to save a dataset as a compressed csv file, the compression kwargs provided to `.to_csv()` that get piped to panda's `pandas.DataFrame.to_csv` do not have any effect - resulting in the dataset not getting compressed.\r\n\r\nA warning is raised if explicitly providing a `compression` kwarg, but no warnings are raised if relying on the defaults. This can lead to datasets secretly not getting compressed for users expecting the behaviour to match panda's `.to_csv()`, where the compression format is automatically inferred from the destination path suffix.\n\n### Steps to reproduce the bug\n\n```python\r\n# dataset is not compressed (but at least a warning is emitted)\r\n\r\nimport datasets\r\n\r\ndataset = datasets.load_dataset(\"rotten_tomatoes\", split=\"train\")\r\n\r\ndataset.to_csv(\"uncompressed.csv\")\r\nprint(os.path.getsize(\"uncompressed.csv\"))  # 1008607\r\n\r\ndataset.to_csv(\"compressed.csv.gz\",  compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1})\r\nprint(os.path.getsize(\"compressed.csv.gz\"))  # 1008607\r\n```\r\n\r\n```shell\r\n>>>\r\nRuntimeWarning: compression has no effect when passing a non-binary object as input.\r\n  csv_str = batch.to_pandas().to_csv(\r\n```\r\n\r\n```python\r\n# dataset is not compressed and no warnings are emitted\r\n\r\ndataset.to_csv(\"compressed.csv.gz\")\r\nprint(os.path.getsize(\"compressed.csv.gz\"))  # 1008607\r\n\r\n# compare with\r\ndataset.to_pandas().to_csv(\"pandas.csv.gz\")\r\nprint(os.path.getsize(\"pandas.csv.gz\"))  # 418561\r\n```\r\n\r\n---\r\n\r\nI think that this is because behind the scenes `pandas.DataFrame.to_csv` is always called with a buf-like `path_or_buf`, but users that are providing a path-like to `datasets.Dataset.to_csv` are likely not to expect \/ know that - leading to a mismatch in their understanding of the expected behaviour of the `compression` kwarg.\n\n### Expected behavior\n\nThe dataset to be saved as a compressed csv file when providing a `compression` kwarg, or when relying on the default `compression='infer'`\n\n### Environment info\n\n`datasets == 2.13.1`\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6043\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6043\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6042","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6042\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6042\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6042\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6042","id":1807516762,"node_id":"PR_kwDODunzps5VqEyb","number":6042,"title":"Fix unused DatasetInfosDict code in push_to_hub","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-17T11:03:09Z","updated_at":"2023-07-18T16:17:52Z","closed_at":"2023-07-18T16:08:42Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6042\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6042\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6042","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6042","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6042.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6042.patch","merged_at":"2023-07-18T16:08:42Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6041","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6041\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6041\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6041\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6041","id":1807441055,"node_id":"PR_kwDODunzps5Vp0GX","number":6041,"title":"Flatten repository_structure docs on yaml","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-17T10:15:10Z","updated_at":"2023-07-17T10:24:51Z","closed_at":"2023-07-17T10:16:22Z","author_association":"MEMBER","active_lock_reason":null,"body":"To have Splits, Configurations and Builder parameters at the same doc level","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6041\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6041\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6041","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6041","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6041.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6041.patch","merged_at":"2023-07-17T10:16:22Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6040","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6040\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6040\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6040\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6040","id":1807410238,"node_id":"PR_kwDODunzps5VptVf","number":6040,"title":"Fix legacy_dataset_infos","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-17T09:56:21Z","updated_at":"2023-07-17T10:24:34Z","closed_at":"2023-07-17T10:16:03Z","author_association":"MEMBER","active_lock_reason":null,"body":"was causing transformers CI to fail\r\n\r\nhttps:\/\/circleci.com\/gh\/huggingface\/transformers\/855105","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6040\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6040\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6040","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6040","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6040.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6040.patch","merged_at":"2023-07-17T10:16:03Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6039","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6039\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6039\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6039\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6039","id":1806508451,"node_id":"I_kwDODunzps5rrSGj","number":6039,"title":"Loading column subset from parquet file produces error since version 2.13","user":{"login":"kklemon","id":1430243,"node_id":"MDQ6VXNlcjE0MzAyNDM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1430243?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kklemon","html_url":"https:\/\/github.com\/kklemon","followers_url":"https:\/\/api.github.com\/users\/kklemon\/followers","following_url":"https:\/\/api.github.com\/users\/kklemon\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kklemon\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kklemon\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kklemon\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kklemon\/orgs","repos_url":"https:\/\/api.github.com\/users\/kklemon\/repos","events_url":"https:\/\/api.github.com\/users\/kklemon\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kklemon\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-07-16T09:13:07Z","updated_at":"2023-07-24T14:35:04Z","closed_at":"2023-07-24T14:35:04Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n`load_dataset` allows loading a subset of columns from a parquet file with the `columns` argument. Since version 2.13, this produces the following error:\r\n\r\n```\r\nTraceback (most recent call last):                      \r\n  File \"\/usr\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1879, in _prepare_split_single\r\n    for _, table in generator:\r\n  File \"\/usr\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/parquet\/parquet.py\", line 68, in _generate_tables\r\n    raise ValueError(\r\nValueError: Tried to load parquet data with columns '['sepal_length']' with mismatching features '{'sepal_length': Value(dtype='float64', id=None), 'sepal_width': Value(dtype='float64', id=None), 'petal_length': Value(dtype='float64', id=None), 'petal_width': Value(dtype='float64', id=None), 'species': Value(dtype='string', id=None)}'\r\n```\r\n\r\nThis seems to occur because `datasets` is checking whether the columns in the schema exactly match the provided list of columns, instead of whether they are a subset.\n\n### Steps to reproduce the bug\n\n```python\r\n# Prepare some sample data\r\n\r\nimport pandas as pd\r\n\r\niris = pd.read_csv('https:\/\/raw.githubusercontent.com\/mwaskom\/seaborn-data\/master\/iris.csv')\r\niris.to_parquet('iris.parquet')\r\n\r\n# ['sepal_length', 'sepal_width', 'petal_length', 'petal_width',  'species']\r\nprint(iris.columns)\r\n\r\n# Load data with datasets\r\n\r\nfrom datasets import load_dataset\r\n\r\n# Load full parquet file\r\ndataset = load_dataset('parquet', data_files='iris.parquet')\r\n\r\n# Load column subset; throws error for datasets>=2.13\r\ndataset = load_dataset('parquet', data_files='iris.parquet', columns=['sepal_length'])\r\n```\n\n### Expected behavior\n\nNo error should be thrown and the given column subset should be loaded.\n\n### Environment info\n\n- `datasets` version: 2.13.0\r\n- Platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.9\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6039\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6039\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6038","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6038\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6038\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6038\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6038","id":1805960244,"node_id":"I_kwDODunzps5rpMQ0","number":6038,"title":"  File \"\/home\/zhizhou\/anaconda3\/envs\/pytorch\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 992, in _download_and_prepare     if str(split_generator.split_info.name).lower() == \"all\": AttributeError: 'str' object has no attribute 'split_info'. Did you mean: 'splitlines'?","user":{"login":"BaiMeiyingxue","id":53547009,"node_id":"MDQ6VXNlcjUzNTQ3MDA5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/53547009?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/BaiMeiyingxue","html_url":"https:\/\/github.com\/BaiMeiyingxue","followers_url":"https:\/\/api.github.com\/users\/BaiMeiyingxue\/followers","following_url":"https:\/\/api.github.com\/users\/BaiMeiyingxue\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/BaiMeiyingxue\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/BaiMeiyingxue\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/BaiMeiyingxue\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/BaiMeiyingxue\/orgs","repos_url":"https:\/\/api.github.com\/users\/BaiMeiyingxue\/repos","events_url":"https:\/\/api.github.com\/users\/BaiMeiyingxue\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/BaiMeiyingxue\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-15T07:58:08Z","updated_at":"2023-07-24T11:54:15Z","closed_at":"2023-07-24T11:54:15Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, I use the code below to load local file\r\n```\r\n    def _split_generators(self, dl_manager):\r\n        # TODO: This method is tasked with downloading\/extracting the data and defining the splits depending on the configuration\r\n        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\r\n\r\n        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLS\r\n        # It can accept any type or nested list\/dict and will give back the same structure with the url replaced with path to local files.\r\n        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\r\n        # urls = _URLS[self.config.name]\r\n        data_dir = dl_manager.download_and_extract(_URLs)\r\n        print(data_dir)\r\n        return [\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TRAIN,\r\n                # These kwargs will be passed to _generate_examples\r\n                gen_kwargs={\r\n                    \"filepath\": os.path.join(data_dir[\"train\"]),\r\n                    \"split\": \"train\",\r\n                },\r\n            ),\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.VALIDATION,\r\n                # These kwargs will be passed to _generate_examples\r\n                gen_kwargs={\r\n                    \"filepath\": os.path.join(data_dir[\"dev\"]),\r\n                    \"split\": \"dev\",\r\n                },\r\n            ),\r\n        ]\r\n```\r\nand error occured\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/zhizhou\/data1\/zhanghao\/huggingface\/FineTuning_Transformer\/load_local_dataset.py\", line 2, in <module>\r\n    dataset = load_dataset(\".\/QA_script.py\",data_files='\/home\/zhizhou\/.cache\/huggingface\/datasets\/conversatiom_corps\/part_file.json')\r\n  File \"\/home\/zhizhou\/anaconda3\/envs\/pytorch\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1809, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/zhizhou\/anaconda3\/envs\/pytorch\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 909, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/zhizhou\/anaconda3\/envs\/pytorch\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1670, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"\/home\/zhizhou\/anaconda3\/envs\/pytorch\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 992, in _download_and_prepare\r\n    if str(split_generator.split_info.name).lower() == \"all\":\r\nAttributeError: 'str' object has no attribute 'split_info'. Did you mean: 'splitlines'?\r\n```\r\nCould you help me?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6038\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6038\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6037","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6037\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6037\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6037\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6037","id":1805887184,"node_id":"I_kwDODunzps5ro6bQ","number":6037,"title":"Documentation links to examples are broken","user":{"login":"david-waterworth","id":5028974,"node_id":"MDQ6VXNlcjUwMjg5NzQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5028974?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/david-waterworth","html_url":"https:\/\/github.com\/david-waterworth","followers_url":"https:\/\/api.github.com\/users\/david-waterworth\/followers","following_url":"https:\/\/api.github.com\/users\/david-waterworth\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/david-waterworth\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/david-waterworth\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/david-waterworth\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/david-waterworth\/orgs","repos_url":"https:\/\/api.github.com\/users\/david-waterworth\/repos","events_url":"https:\/\/api.github.com\/users\/david-waterworth\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/david-waterworth\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-15T04:54:50Z","updated_at":"2023-07-17T22:35:14Z","closed_at":"2023-07-17T15:10:32Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nThe links at the bottom of [add_dataset](https:\/\/huggingface.co\/docs\/datasets\/v1.2.1\/add_dataset.html) to examples of specific datasets are all broken, for example\r\n\r\n- text classification: [ag_news](https:\/\/github.com\/huggingface\/datasets\/blob\/master\/datasets\/ag_news\/ag_news.py) (original data are in csv files)\r\n\r\n### Steps to reproduce the bug\r\n\r\nClick on links to examples from latest documentation\r\n\r\n### Expected behavior\r\n\r\nLinks should be up to date - it might be more stable to link to https:\/\/huggingface.co\/datasets\/ag_news\/blob\/main\/ag_news.py\r\n\r\n### Environment info\r\n\r\ndataset v1.2.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6037\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6037\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6036","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6036\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6036\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6036\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6036","id":1805138898,"node_id":"PR_kwDODunzps5ViKc4","number":6036,"title":"Deprecate search API","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-07-14T16:22:09Z","updated_at":"2023-09-07T16:44:32Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The Search API only supports Faiss and ElasticSearch as vector stores, is somewhat difficult to maintain (e.g., it still doesn't support ElasticSeach 8.0, difficult testing, ...), does not have the best design (adds a bunch of methods to the `Dataset` class that are only useful after creating an index), the usage doesn't seem to be significant and is not integrated with the Hub. Since we have no plans\/bandwidth to improve it and better alternatives such as `langchain` and `docarray` exist, I think it should be deprecated (and eventually removed).\r\n\r\nIf we decide to deprecate\/remove it, the following usage instances need to be addressed:\r\n* [Course](https:\/\/github.com\/huggingface\/course\/blob\/0018bb434204d9750a03592cb0d4e846093218d8\/chapters\/en\/chapter5\/6.mdx#L342 ) and [Blog](https:\/\/github.com\/huggingface\/blog\/blob\/4897c6f73d4492a0955ade503281711d01840e09\/image-search-datasets.md?plain=1#L252) - calling the FAISS API directly should be OK in these instances as it's pretty simple to use for basic scenarios. Alternatively, we can use `langchain`, but this adds an extra dependency\r\n*  [Transformers](https:\/\/github.com\/huggingface\/transformers\/blob\/50726f9ea7afc6113da617f8f4ca1ab264a5e28a\/src\/transformers\/models\/rag\/retrieval_rag.py#L183) - we can use the FAISS API directly and store the index as a separate attribute (and instead of building the `wiki_dpr` index each time the dataset is generated, we can generate it once and push it to the Hub repo, and then read it from there\r\n\r\ncc @huggingface\/datasets @LysandreJik for the opinion","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6036\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6036\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6036","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6036","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6036.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6036.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6035","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6035\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6035\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6035\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6035","id":1805087687,"node_id":"PR_kwDODunzps5Vh_QR","number":6035,"title":"Dataset representation","user":{"login":"Ganryuu","id":63643948,"node_id":"MDQ6VXNlcjYzNjQzOTQ4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/63643948?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Ganryuu","html_url":"https:\/\/github.com\/Ganryuu","followers_url":"https:\/\/api.github.com\/users\/Ganryuu\/followers","following_url":"https:\/\/api.github.com\/users\/Ganryuu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Ganryuu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Ganryuu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Ganryuu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Ganryuu\/orgs","repos_url":"https:\/\/api.github.com\/users\/Ganryuu\/repos","events_url":"https:\/\/api.github.com\/users\/Ganryuu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Ganryuu\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-14T15:42:37Z","updated_at":"2023-07-19T19:41:35Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"__repr__ and _repr_html_ now both are similar to that of Polars ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6035\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6035\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6035","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6035","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6035.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6035.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6034","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6034\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6034\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6034\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6034","id":1804501361,"node_id":"I_kwDODunzps5rjoFx","number":6034,"title":"load_dataset hangs on WSL","user":{"login":"Andy-Zhou2","id":20140522,"node_id":"MDQ6VXNlcjIwMTQwNTIy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20140522?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Andy-Zhou2","html_url":"https:\/\/github.com\/Andy-Zhou2","followers_url":"https:\/\/api.github.com\/users\/Andy-Zhou2\/followers","following_url":"https:\/\/api.github.com\/users\/Andy-Zhou2\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Andy-Zhou2\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Andy-Zhou2\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Andy-Zhou2\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Andy-Zhou2\/orgs","repos_url":"https:\/\/api.github.com\/users\/Andy-Zhou2\/repos","events_url":"https:\/\/api.github.com\/users\/Andy-Zhou2\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Andy-Zhou2\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-14T09:03:10Z","updated_at":"2023-07-14T14:48:29Z","closed_at":"2023-07-14T14:48:29Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nload_dataset simply hangs. It happens once every ~5 times, and interestingly hangs for a multiple of 5 minutes (hangs for 5\/10\/15 minutes). Using the profiler in PyCharm shows that it spends the time at <method 'connect' of '_socket.socket' objects>. However, a local cache is available so I am not sure why socket is needed. ([profiler result](https:\/\/ibb.co\/0Btbbp8))\r\n\r\nIt only happens on WSL for me. It works for native Windows and my MacBook. (cache quickly recognized and loaded within a second).\r\n\r\n\n\n### Steps to reproduce the bug\n\nI am using Ubuntu 22.04.2 LTS (GNU\/Linux 5.15.90.1-microsoft-standard-WSL2 x86_64)\r\n\r\nPython 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] on linux\r\n>>> import datasets\r\n>>> datasets.load_dataset('ai2_arc', 'ARC-Challenge')  # hangs for 5\/10\/15 minutes\n\n### Expected behavior\n\ncache quickly recognized and loaded within a second\n\n### Environment info\n\nPlease let me know if I should provide more environment information.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6034\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6034\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6033","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6033\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6033\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6033\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6033","id":1804482051,"node_id":"I_kwDODunzps5rjjYD","number":6033,"title":"`map` function doesn't fully utilize `input_columns`.","user":{"login":"kwonmha","id":8953934,"node_id":"MDQ6VXNlcjg5NTM5MzQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8953934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kwonmha","html_url":"https:\/\/github.com\/kwonmha","followers_url":"https:\/\/api.github.com\/users\/kwonmha\/followers","following_url":"https:\/\/api.github.com\/users\/kwonmha\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kwonmha\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kwonmha\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kwonmha\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kwonmha\/orgs","repos_url":"https:\/\/api.github.com\/users\/kwonmha\/repos","events_url":"https:\/\/api.github.com\/users\/kwonmha\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kwonmha\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-07-14T08:49:28Z","updated_at":"2023-07-14T09:16:04Z","closed_at":"2023-07-14T09:16:04Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI wanted to select only some columns of data.\r\nAnd I thought that's why the argument `input_columns` exists.\r\nWhat I expected is like this:\r\nIf there are [\"a\", \"b\", \"c\", \"d\"] columns, and if I set `input_columns=[\"a\", \"d\"]`, the data will have only [\"a\", \"d\"] columns.\r\n\r\nBut it doesn't select columns.\r\nIt preserves existing columns.\r\nThe main cause is `update` function of `dictionary` type `transformed_batch`.\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/682d21e94ab1e64c11b583de39dc4c93f0101c5a\/src\/datasets\/iterable_dataset.py#L687-L691\r\n\r\n`transformed_batch` gets all the columns by `transformed_batch = dict(batch)`.\r\nEven `function_args` selects `input_columns`, `update` preserves columns other than `input_columns`.\r\nI think it should take a new dictionary with columns in `input_columns` like this:\r\n```\r\n# transformed_batch = dict(batch)\r\n# transformed_batch.update(self.function(*function_args, **self.fn_kwargs)\r\n\r\n# This is what I think correct.\r\ntransformed_batch = self.function(*function_args, **self.fn_kwargs)\r\n```\r\n\r\nLet me know how to use `input_columns`.\r\n\r\n### Steps to reproduce the bug\r\n\r\nDescribed all above.\r\n\r\n### Expected behavior\r\n\r\nDescribed all above.\r\n\r\n### Environment info\r\n\r\ndatasets: 2.12\r\npython: 3.8","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6033\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6033\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6032","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6032\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6032\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6032\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6032","id":1804358679,"node_id":"I_kwDODunzps5rjFQX","number":6032,"title":"DownloadConfig.proxies not work when load_dataset_builder calling HfApi.dataset_info  ","user":{"login":"codingl2k1","id":138426806,"node_id":"U_kgDOCEA5tg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/138426806?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/codingl2k1","html_url":"https:\/\/github.com\/codingl2k1","followers_url":"https:\/\/api.github.com\/users\/codingl2k1\/followers","following_url":"https:\/\/api.github.com\/users\/codingl2k1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/codingl2k1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/codingl2k1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/codingl2k1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/codingl2k1\/orgs","repos_url":"https:\/\/api.github.com\/users\/codingl2k1\/repos","events_url":"https:\/\/api.github.com\/users\/codingl2k1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/codingl2k1\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-07-14T07:22:55Z","updated_at":"2023-09-11T13:50:41Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\n```python\r\ndownload_config = DownloadConfig(proxies={'https': '<my proxy>'})\r\nbuilder = load_dataset_builder(..., download_config=download_config)\r\n```\r\nBut, when getting the dataset_info from HfApi, the http requests not using the proxies.\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Setup proxies in DownloadConfig.\r\n2. Call `load_dataset_build` with download_config.\r\n3. Inspect the call stack in HfApi.dataset_info.\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/138426806\/33e538a8-2e22-4e63-b634-343febe5324b)\r\n\r\n### Expected behavior\r\n\r\nDownloadConfig.proxies works for getting dataset_info.\r\n\r\n### Environment info\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/commit\/406b2212263c0d33f267e35b917f410ff6b3bc00\r\nPython 3.11.4","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6032\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6032\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6031","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6031\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6031\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6031\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6031","id":1804183858,"node_id":"I_kwDODunzps5riaky","number":6031,"title":"Argument type for map function changes when using `input_columns` for `IterableDataset`","user":{"login":"kwonmha","id":8953934,"node_id":"MDQ6VXNlcjg5NTM5MzQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8953934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kwonmha","html_url":"https:\/\/github.com\/kwonmha","followers_url":"https:\/\/api.github.com\/users\/kwonmha\/followers","following_url":"https:\/\/api.github.com\/users\/kwonmha\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kwonmha\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kwonmha\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kwonmha\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kwonmha\/orgs","repos_url":"https:\/\/api.github.com\/users\/kwonmha\/repos","events_url":"https:\/\/api.github.com\/users\/kwonmha\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kwonmha\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-14T05:11:14Z","updated_at":"2023-07-14T14:44:15Z","closed_at":"2023-07-14T14:44:15Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI wrote `tokenize(examples)` function as an argument for `map` function for `IterableDataset`.\r\nIt process dictionary type `examples` as a parameter.\r\nIt is used in `train_dataset = train_dataset.map(tokenize, batched=True)`\r\nNo error is raised.\r\n\r\nAnd then, I found some unnecessary keys and values in `examples` so I added `input_columns` argument to `map` function to select keys and values.\r\nIt gives me an error saying\r\n```\r\nTypeError: tokenize() takes 1 positional argument but 3 were given.\r\n```\r\n\r\nThe code below matters.\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/406b2212263c0d33f267e35b917f410ff6b3bc00\/src\/datasets\/iterable_dataset.py#L687\r\n\r\nFor example, `inputs = {\"a\":1, \"b\":2, \"c\":3}`.\r\nIf `self.input_coluns` is `None`,\r\n`inputs` is a dictionary type variable and `function_args` becomes a `list` of a single `dict` variable.\r\n`function_args` becomes `[{\"a\":1, \"b\":2, \"c\":3}]`\r\n\r\nOtherwise, lets say `self.input_columns = [\"a\", \"c\"]`\r\n`[inputs[col] for col in self.input_columns]` results in `[1, 3]`.\r\nI think it should be `[{\"a\":1, \"c\":3}]`.\r\n\r\nI want to ask if the resulting format is intended.\r\nMaybe I can modify `tokenize()` to have 2 parameters in this case instead of having 1 dictionary.\r\nBut this is confusing to me.\r\nOr it should be fixed as `[{col:inputs[col] for col in self.input_columns}]`\r\n\r\n### Steps to reproduce the bug\r\n\r\nRun `map` function of `IterableDataset` with `input_columns` argument.\r\n\r\n### Expected behavior\r\n\r\n`function_args` looks better to have same format.\r\nI think it should be `[{\"a\":1, \"c\":3}]`.\r\n\r\n### Environment info\r\n\r\ndataset version: 2.12\r\npython: 3.8","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6031\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6031\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6030","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6030\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6030\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6030\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6030","id":1803864744,"node_id":"PR_kwDODunzps5Vd0ZG","number":6030,"title":"fixed typo in comment","user":{"login":"NightMachinery","id":36224762,"node_id":"MDQ6VXNlcjM2MjI0NzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36224762?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NightMachinery","html_url":"https:\/\/github.com\/NightMachinery","followers_url":"https:\/\/api.github.com\/users\/NightMachinery\/followers","following_url":"https:\/\/api.github.com\/users\/NightMachinery\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NightMachinery\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NightMachinery\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NightMachinery\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NightMachinery\/orgs","repos_url":"https:\/\/api.github.com\/users\/NightMachinery\/repos","events_url":"https:\/\/api.github.com\/users\/NightMachinery\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NightMachinery\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-13T22:49:57Z","updated_at":"2023-07-14T14:21:58Z","closed_at":"2023-07-14T14:13:38Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This mistake was a bit confusing, so I thought it was worth sending a PR over.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6030\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6030\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6030","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6030","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6030.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6030.patch","merged_at":"2023-07-14T14:13:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6029","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6029\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6029\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6029\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6029","id":1803460046,"node_id":"PR_kwDODunzps5VcbPW","number":6029,"title":"[docs] Fix link","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-13T17:24:12Z","updated_at":"2023-07-13T17:47:41Z","closed_at":"2023-07-13T17:38:59Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fixes link to the builder classes :)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6029\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6029\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6029","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6029","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6029.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6029.patch","merged_at":"2023-07-13T17:38:59Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6028","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6028\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6028\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6028\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6028","id":1803294981,"node_id":"PR_kwDODunzps5Vb3LJ","number":6028,"title":"Use new hffs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2023-07-13T15:41:44Z","updated_at":"2023-07-17T17:09:39Z","closed_at":"2023-07-17T17:01:00Z","author_association":"MEMBER","active_lock_reason":null,"body":"Thanks to @janineguo 's work in https:\/\/github.com\/huggingface\/datasets\/pull\/5919 which was needed to support HfFileSystem.\r\n\r\nSwitching to `HfFileSystem` will help implementing optimization in data files resolution\r\n\r\n## Implementation details\r\n\r\nI replaced all the from_hf_repo and from_local_or_remote in data_files.py to only use a new `from_patterns` which works for any fsspec path, including hf:\/\/ paths, https:\/\/ URLs and local paths. This simplifies the codebase since there is no logic duplication anymore when it comes to data files resolution.\r\n\r\nI added `_prepare_path_and_storage_options` which returns the right storage_options to use given a path and a `DownloadConfig`. This is the only place where the logic depends on the filesystem type that must be used.\r\n\r\nI also removed the `get_metadata_data_files_list ` and `get_patterns_and_data_files` functions added recently, since data files resolution is now handled using a common interface.\r\n\r\n## New features\r\n\r\nhf:\/\/ paths are now supported in data_files\r\n\r\n## Breaking changes\r\n\r\nDataFilesList and DataFilesDict:\r\n- use `str` paths instead of `Union[Path, Url]`\r\n- require posix paths for windows paths\r\n\r\nclose https:\/\/github.com\/huggingface\/datasets\/issues\/6017","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6028\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6028\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6028","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6028","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6028.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6028.patch","merged_at":"2023-07-17T17:01:00Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6027","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6027\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6027\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6027\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6027","id":1803008486,"node_id":"PR_kwDODunzps5Va4g3","number":6027,"title":"Delete `task_templates` in `IterableDataset` when they are no longer valid","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-13T13:16:17Z","updated_at":"2023-07-13T14:06:20Z","closed_at":"2023-07-13T13:57:35Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6025 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6027\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6027\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6027","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6027","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6027.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6027.patch","merged_at":"2023-07-13T13:57:35Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6026","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6026\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6026\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6026\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6026","id":1802929222,"node_id":"PR_kwDODunzps5VanI8","number":6026,"title":"Fix style with ruff 0.0.278","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-13T12:34:24Z","updated_at":"2023-07-13T12:46:26Z","closed_at":"2023-07-13T12:37:01Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6026\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6026\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6026","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6026","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6026.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6026.patch","merged_at":"2023-07-13T12:37:01Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6025","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6025\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6025\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6025\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6025","id":1801852601,"node_id":"I_kwDODunzps5rZha5","number":6025,"title":"Using a dataset for a use other than it was intended for.","user":{"login":"surya-narayanan","id":17240858,"node_id":"MDQ6VXNlcjE3MjQwODU4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17240858?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/surya-narayanan","html_url":"https:\/\/github.com\/surya-narayanan","followers_url":"https:\/\/api.github.com\/users\/surya-narayanan\/followers","following_url":"https:\/\/api.github.com\/users\/surya-narayanan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/surya-narayanan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/surya-narayanan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/surya-narayanan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/surya-narayanan\/orgs","repos_url":"https:\/\/api.github.com\/users\/surya-narayanan\/repos","events_url":"https:\/\/api.github.com\/users\/surya-narayanan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/surya-narayanan\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-12T22:33:17Z","updated_at":"2023-07-13T13:57:36Z","closed_at":"2023-07-13T13:57:36Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi, I want to use the rotten tomatoes dataset but for a task other than classification, but when I interleave the dataset, it throws ```'ValueError: Column label is not present in features.'```. It seems that the label_col must be there in the dataset for some reason? \r\n\r\nHere is the full stacktrace\r\n\r\n```\r\n  File \"\/home\/suryahari\/Vornoi\/tryage-handoff-other-datasets.py\", line 276, in create_dataloaders                                                                                                    \r\n    dataset = interleave_datasets(dsfold, stopping_strategy=\"all_exhausted\")                                                                                                                         \r\n  File \"\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/combine.py\", line 134, in interleave_datasets                                                                   \r\n    return _interleave_iterable_datasets(                                                                                                                                                            \r\n  File \"\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/iterable_dataset.py\", line 1833, in _interleave_iterable_datasets                                               \r\n    info = DatasetInfo.from_merge([d.info for d in datasets])                                                                                                                                        \r\n  File \"\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/info.py\", line 275, in from_merge                                                                               \r\n    dataset_infos = [dset_info.copy() for dset_info in dataset_infos if dset_info is not None]                                                                                                       \r\n  File \"\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/info.py\", line 275, in <listcomp>                                                                               \r\n    dataset_infos = [dset_info.copy() for dset_info in dataset_infos if dset_info is not None]                                                                                                       \r\n  File \"\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/info.py\", line 378, in copy                                                                                     \r\n    return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})                                                                                                                 \r\n  File \"<string>\", line 20, in __init__                                                                                                                                                              \r\n  File \"\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/info.py\", line 208, in __post_init__                                                                            \r\n    self.task_templates = [                                                                                                                                                                          \r\n  File \"\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/info.py\", line 209, in <listcomp>                                                                               \r\n    template.align_with_features(self.features) for template in (self.task_templates)                                                                                                                \r\n  File \"\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/tasks\/text_classification.py\", line 20, in align_with_features                                                  \r\n    raise ValueError(f\"Column {self.label_column} is not present in features.\")                                                                                                                      \r\nValueError: Column label is not present in features.    \r\n```\n\n### Steps to reproduce the bug\n\nDelete the column `labels` from the `rotten_tomatoes` dataset. Try to interleave it with other datasets.\n\n### Expected behavior\n\nShould let me use the dataset with just the `text` field\n\n### Environment info\n\nlatest datasets library? I don't think this was an issue in earlier versions.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6025\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6025\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6024","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6024\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6024\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6024\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6024","id":1801708808,"node_id":"PR_kwDODunzps5VWbGe","number":6024,"title":"Don't reference self in Spark._validate_cache_dir","user":{"login":"maddiedawson","id":106995444,"node_id":"U_kgDOBmCe9A","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/106995444?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/maddiedawson","html_url":"https:\/\/github.com\/maddiedawson","followers_url":"https:\/\/api.github.com\/users\/maddiedawson\/followers","following_url":"https:\/\/api.github.com\/users\/maddiedawson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/maddiedawson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/maddiedawson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/maddiedawson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/maddiedawson\/orgs","repos_url":"https:\/\/api.github.com\/users\/maddiedawson\/repos","events_url":"https:\/\/api.github.com\/users\/maddiedawson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/maddiedawson\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-07-12T20:31:16Z","updated_at":"2023-07-13T16:58:32Z","closed_at":"2023-07-13T12:37:09Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix for https:\/\/github.com\/huggingface\/datasets\/issues\/5963","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6024\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6024\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6024","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6024","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6024.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6024.patch","merged_at":"2023-07-13T12:37:09Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6023","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6023\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6023\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6023\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6023","id":1801272420,"node_id":"PR_kwDODunzps5VU7EG","number":6023,"title":"Fix `ClassLabel` min max check for `None` values","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-12T15:46:12Z","updated_at":"2023-07-12T16:29:26Z","closed_at":"2023-07-12T16:18:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #6022 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6023\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6023\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6023","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6023","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6023.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6023.patch","merged_at":"2023-07-12T16:18:04Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6022","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6022\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6022\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6022\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6022","id":1800092589,"node_id":"I_kwDODunzps5rSzut","number":6022,"title":"Batch map raises TypeError: '>=' not supported between instances of 'NoneType' and 'int'","user":{"login":"codingl2k1","id":138426806,"node_id":"U_kgDOCEA5tg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/138426806?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/codingl2k1","html_url":"https:\/\/github.com\/codingl2k1","followers_url":"https:\/\/api.github.com\/users\/codingl2k1\/followers","following_url":"https:\/\/api.github.com\/users\/codingl2k1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/codingl2k1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/codingl2k1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/codingl2k1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/codingl2k1\/orgs","repos_url":"https:\/\/api.github.com\/users\/codingl2k1\/repos","events_url":"https:\/\/api.github.com\/users\/codingl2k1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/codingl2k1\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-12T03:20:17Z","updated_at":"2023-07-12T16:18:06Z","closed_at":"2023-07-12T16:18:05Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen mapping some datasets with `batched=True`, datasets may raise an exeception:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/venv\/lib\/python3.11\/site-packages\/multiprocess\/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n                    ^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/utils\/py_utils.py\", line 1328, in _write_generator_to_queue\r\n    for i, result in enumerate(func(**kwargs)):\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/arrow_dataset.py\", line 3483, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/arrow_writer.py\", line 549, in write_batch\r\n    array = cast_array_to_feature(col_values, col_type) if col_type is not None else col_values\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/table.py\", line 1831, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/table.py\", line 1831, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/table.py\", line 2063, in cast_array_to_feature\r\n    return feature.cast_storage(array)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/features\/features.py\", line 1098, in cast_storage\r\n    if min_max[\"max\"] >= self.num_classes:\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: '>=' not supported between instances of 'NoneType' and 'int'\r\nThe above exception was the direct cause of the following exception:\r\nTraceback (most recent call last):\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/t1.py\", line 33, in <module>\r\n    ds = ds.map(transforms, num_proc=14, batched=True, batch_size=5)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/dataset_dict.py\", line 850, in map\r\n    {\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/dataset_dict.py\", line 851, in <dictcomp>\r\n    k: dataset.map(\r\n       ^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/arrow_dataset.py\", line 577, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/arrow_dataset.py\", line 542, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/arrow_dataset.py\", line 3179, in map\r\n    for rank, done, content in iflatmap_unordered(\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/utils\/py_utils.py\", line 1368, in iflatmap_unordered\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/src\/datasets\/utils\/py_utils.py\", line 1368, in <listcomp>\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/codingl2k1\/Work\/datasets\/venv\/lib\/python3.11\/site-packages\/multiprocess\/pool.py\", line 774, in get\r\n    raise self._value\r\nTypeError: '>=' not supported between instances of 'NoneType' and 'int'\r\n```\n\n### Steps to reproduce the bug\n\n1. Checkout the latest main of datasets.\r\n2. Run the code:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndef transforms(examples):\r\n    # examples[\"pixel_values\"] = [image.convert(\"RGB\").resize((100, 100)) for image in examples[\"image\"]]\r\n    return examples\r\n\r\nds = load_dataset(\"scene_parse_150\")\r\nds = ds.map(transforms, num_proc=14, batched=True, batch_size=5)\r\nprint(ds)\r\n```\n\n### Expected behavior\n\nmap without exception.\n\n### Environment info\n\nDatasets: https:\/\/github.com\/huggingface\/datasets\/commit\/b8067c0262073891180869f700ebef5ac3dc5cce\r\nPython: 3.11.4\r\nSystem: Macos","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6022\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6022\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6021","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6021\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6021\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6021\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6021","id":1799785904,"node_id":"PR_kwDODunzps5VP11Q","number":6021,"title":"[docs] Update return statement of index search","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-11T21:33:32Z","updated_at":"2023-07-12T17:13:02Z","closed_at":"2023-07-12T17:03:00Z","author_association":"MEMBER","active_lock_reason":null,"body":"Clarifies in the return statement of the docstring that the retrieval score is `IndexFlatL2` by default (see [PR](https:\/\/github.com\/huggingface\/transformers\/issues\/24739) and internal Slack [convo](https:\/\/huggingface.slack.com\/archives\/C01229B19EX\/p1689105179711689)), and fixes the formatting because multiple return values are not supported.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6021\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6021\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6021","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6021","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6021.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6021.patch","merged_at":"2023-07-12T17:03:00Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6020","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6020\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6020\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6020\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6020","id":1799720536,"node_id":"I_kwDODunzps5rRY5Y","number":6020,"title":"Inconsistent \"The features can't be aligned\" error when combining map, multiprocessing, and variable length outputs","user":{"login":"kheyer","id":38166299,"node_id":"MDQ6VXNlcjM4MTY2Mjk5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/38166299?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kheyer","html_url":"https:\/\/github.com\/kheyer","followers_url":"https:\/\/api.github.com\/users\/kheyer\/followers","following_url":"https:\/\/api.github.com\/users\/kheyer\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kheyer\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kheyer\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kheyer\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kheyer\/orgs","repos_url":"https:\/\/api.github.com\/users\/kheyer\/repos","events_url":"https:\/\/api.github.com\/users\/kheyer\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kheyer\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-11T20:40:38Z","updated_at":"2023-10-25T14:14:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm using a dataset with map and multiprocessing to run a function that returned a variable length list of outputs. This output list may be empty. Normally this is handled fine, but there is an edge case that crops up when using multiprocessing. In some cases, an empty list result ends up in a dataset shard consisting of a single item. This results in a `The features can't be aligned` error that is difficult to debug because it depends on the number of processes\/shards used.\r\n\r\nI've reproduced a minimal example below. My current workaround is to fill empty results with a dummy value that I filter after, but this was a weird error that took a while to track down.\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\ndataset = datasets.Dataset.from_list([{'idx':i} for i in range(60)])\r\n\r\ndef test_func(row, idx):\r\n    if idx==58:\r\n        return {'output': []}\r\n    else:\r\n        return {'output' : [{'test':1}, {'test':2}]}\r\n\r\n# this works fine\r\ntest1 = dataset.map(lambda row, idx: test_func(row, idx), with_indices=True, num_proc=4)\r\n\r\n# this fails\r\ntest2 = dataset.map(lambda row, idx: test_func(row, idx), with_indices=True, num_proc=32)\r\n>ValueError: The features can't be aligned because the key output of features {'idx': Value(dtype='int64', id=None), 'output': Sequence(feature=Value(dtype='null', id=None), length=-1, id=None)} has unexpected type - Sequence(feature=Value(dtype='null', id=None), length=-1, id=None) (expected either [{'test': Value(dtype='int64', id=None)}] or Value(\"null\").\r\n```\r\n\r\nThe error occurs during the check\r\n\r\n```python\r\n_check_if_features_can_be_aligned([dset.features for dset in dsets])\r\n```\r\n\r\nWhen the multiprocessing splitting lines up just right with the empty return value, one of the `dset` in `dsets` will have a single item with an empty list value, causing the error.\n\n### Expected behavior\n\nExpected behavior is the result would be the same regardless of the `num_proc` value used.\n\n### Environment info\n\nDatasets version 2.11.0\r\nPython 3.9.16","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6020\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6020\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6019","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6019\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6019\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6019\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6019","id":1799532822,"node_id":"PR_kwDODunzps5VPAlD","number":6019,"title":"Improve logging","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2023-07-11T18:30:23Z","updated_at":"2023-07-12T19:34:14Z","closed_at":"2023-07-12T17:19:28Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Adds the StreamHandler (as `hfh` and `transformers` do) to the library's logger to log INFO messages and logs the messages about \"loading a cached result\" (and some other warnings) as INFO\r\n\r\n(Also removes the `leave=False` arg in the progress bars to be consistent with `hfh` and `transformers` - progress bars serve as an indicator that a result is not cached, so it makes more sense not to delete them)\r\n\r\nFix #2832, fix https:\/\/github.com\/huggingface\/datasets\/issues\/1948, fix https:\/\/github.com\/huggingface\/datasets\/issues\/5444","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6019\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6019\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6019","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6019","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6019.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6019.patch","merged_at":"2023-07-12T17:19:28Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6018","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6018\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6018\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6018\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6018","id":1799411999,"node_id":"PR_kwDODunzps5VOmKY","number":6018,"title":"test1","user":{"login":"ognjenovicj","id":139256323,"node_id":"U_kgDOCEziAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/139256323?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ognjenovicj","html_url":"https:\/\/github.com\/ognjenovicj","followers_url":"https:\/\/api.github.com\/users\/ognjenovicj\/followers","following_url":"https:\/\/api.github.com\/users\/ognjenovicj\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ognjenovicj\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ognjenovicj\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ognjenovicj\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ognjenovicj\/orgs","repos_url":"https:\/\/api.github.com\/users\/ognjenovicj\/repos","events_url":"https:\/\/api.github.com\/users\/ognjenovicj\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ognjenovicj\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-11T17:25:49Z","updated_at":"2023-07-20T10:11:41Z","closed_at":"2023-07-20T10:11:41Z","author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6018\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6018\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6018","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6018","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6018.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6018.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6017","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6017\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6017\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6017\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6017","id":1799309132,"node_id":"I_kwDODunzps5rP0dM","number":6017,"title":"Switch to huggingface_hub's HfFileSystem","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-07-11T16:24:40Z","updated_at":"2023-07-17T17:01:01Z","closed_at":"2023-07-17T17:01:01Z","author_association":"MEMBER","active_lock_reason":null,"body":"instead of the current datasets.filesystems.hffilesystem.HfFileSystem which can be slow in some cases\r\n\r\nrelated to https:\/\/github.com\/huggingface\/datasets\/issues\/5846 and https:\/\/github.com\/huggingface\/datasets\/pull\/5919","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6017\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6017\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6016","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6016\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6016\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6016\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6016","id":1798968033,"node_id":"PR_kwDODunzps5VNEvn","number":6016,"title":"Dataset string representation enhancement","user":{"login":"Ganryuu","id":63643948,"node_id":"MDQ6VXNlcjYzNjQzOTQ4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/63643948?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Ganryuu","html_url":"https:\/\/github.com\/Ganryuu","followers_url":"https:\/\/api.github.com\/users\/Ganryuu\/followers","following_url":"https:\/\/api.github.com\/users\/Ganryuu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Ganryuu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Ganryuu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Ganryuu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Ganryuu\/orgs","repos_url":"https:\/\/api.github.com\/users\/Ganryuu\/repos","events_url":"https:\/\/api.github.com\/users\/Ganryuu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Ganryuu\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-11T13:38:25Z","updated_at":"2023-07-16T10:26:18Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"my attempt at #6010  \r\nnot sure if this is the right way to go about it, I will wait for your feedback ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6016\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6016\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6016","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6016","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6016.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6016.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6015","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6015\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6015\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6015\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6015","id":1798807893,"node_id":"PR_kwDODunzps5VMhgB","number":6015,"title":"Add metadata ui screenshot in docs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-11T12:16:29Z","updated_at":"2023-07-11T16:07:28Z","closed_at":"2023-07-11T15:56:46Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6015\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6015\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6015","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6015","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6015.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6015.patch","merged_at":"2023-07-11T15:56:46Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6014","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6014\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6014\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6014\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6014","id":1798213816,"node_id":"I_kwDODunzps5rLpC4","number":6014,"title":"Request to Share\/Update Dataset Viewer Code","user":{"login":"lilyorlilypad","id":105081034,"node_id":"U_kgDOBkNoyg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/105081034?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lilyorlilypad","html_url":"https:\/\/github.com\/lilyorlilypad","followers_url":"https:\/\/api.github.com\/users\/lilyorlilypad\/followers","following_url":"https:\/\/api.github.com\/users\/lilyorlilypad\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lilyorlilypad\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lilyorlilypad\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lilyorlilypad\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lilyorlilypad\/orgs","repos_url":"https:\/\/api.github.com\/users\/lilyorlilypad\/repos","events_url":"https:\/\/api.github.com\/users\/lilyorlilypad\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lilyorlilypad\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892865,"node_id":"MDU6TGFiZWwxOTM1ODkyODY1","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/duplicate","name":"duplicate","color":"cfd3d7","default":true,"description":"This issue or pull request already exists"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-07-11T06:36:09Z","updated_at":"2023-09-25T12:01:27Z","closed_at":"2023-09-25T12:01:17Z","author_association":"NONE","active_lock_reason":null,"body":"\r\nOverview:\r\nThe repository (huggingface\/datasets-viewer) was recently archived and when I tried to run the code, there was the error message \"AttributeError: module 'datasets.load' has no attribute 'prepare_module'\". I could not resolve the issue myself due to lack of documentation of that attribute. \r\n\r\nRequest:\r\nI kindly request the sharing of the code responsible for the dataset preview functionality or help with resolving the error. The dataset viewer on the Hugging Face website is incredibly useful since it is compatible with different types of inputs. It allows users to find datasets that meet their needs more efficiently. If needed, I am willing to contribute to the project by testing, documenting, and providing feedback on the dataset viewer code. \r\n\r\nThank you for considering this request, and I look forward to your response.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6014\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6014\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6013","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6013\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6013\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6013\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6013","id":1796083437,"node_id":"I_kwDODunzps5rDg7t","number":6013,"title":"[FR] `map` should reuse unchanged columns from the previous dataset to avoid disk usage","user":{"login":"NightMachinery","id":36224762,"node_id":"MDQ6VXNlcjM2MjI0NzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36224762?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NightMachinery","html_url":"https:\/\/github.com\/NightMachinery","followers_url":"https:\/\/api.github.com\/users\/NightMachinery\/followers","following_url":"https:\/\/api.github.com\/users\/NightMachinery\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NightMachinery\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NightMachinery\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NightMachinery\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NightMachinery\/orgs","repos_url":"https:\/\/api.github.com\/users\/NightMachinery\/repos","events_url":"https:\/\/api.github.com\/users\/NightMachinery\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NightMachinery\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":3761482852,"node_id":"LA_kwDODunzps7gM6xk","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/good%20second%20issue","name":"good second issue","color":"BDE59C","default":false,"description":"Issues a bit more difficult than \"Good First\" issues"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-10T06:42:20Z","updated_at":"2023-07-10T15:37:52Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nCurrently adding a new column with `map` will cause all the data in the dataset to be duplicated and stored\/cached on the disk again. It should reuse unchanged columns. \n\n### Motivation\n\nThis allows having datasets with different columns but sharing some basic columns. Currently, these datasets would become too expensive to store and one would need some kind of on-the-fly join; which also doesn't seem implemented.\n\n### Your contribution\n\n_","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6013\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6013\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6012","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6012\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6012\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6012\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6012","id":1795575432,"node_id":"I_kwDODunzps5rBk6I","number":6012,"title":"[FR] Transform Chaining, Lazy Mapping","user":{"login":"NightMachinery","id":36224762,"node_id":"MDQ6VXNlcjM2MjI0NzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36224762?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NightMachinery","html_url":"https:\/\/github.com\/NightMachinery","followers_url":"https:\/\/api.github.com\/users\/NightMachinery\/followers","following_url":"https:\/\/api.github.com\/users\/NightMachinery\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NightMachinery\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NightMachinery\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NightMachinery\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NightMachinery\/orgs","repos_url":"https:\/\/api.github.com\/users\/NightMachinery\/repos","events_url":"https:\/\/api.github.com\/users\/NightMachinery\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NightMachinery\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-07-09T21:40:21Z","updated_at":"2023-11-23T10:08:57Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nCurrently using a `map` call processes and duplicates the whole dataset, which takes both time and disk space.\r\n\r\nThe solution is to allow lazy mapping, which is essentially a saved chain of transforms that are applied on the fly whenever a slice of the dataset is requested.\r\n\r\nThe API should look like `map`, as `set_transform` changes the current dataset while `map` returns another dataset.\n\n### Motivation\n\nLazy processing allows lower disk usage and faster experimentation.\n\n### Your contribution\n\n_","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6012\/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6012\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6011","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6011\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6011\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6011\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6011","id":1795296568,"node_id":"I_kwDODunzps5rAg04","number":6011,"title":"Documentation: wiki_dpr Dataset has no metric_type for Faiss Index","user":{"login":"YichiRockyZhang","id":29335344,"node_id":"MDQ6VXNlcjI5MzM1MzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/29335344?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/YichiRockyZhang","html_url":"https:\/\/github.com\/YichiRockyZhang","followers_url":"https:\/\/api.github.com\/users\/YichiRockyZhang\/followers","following_url":"https:\/\/api.github.com\/users\/YichiRockyZhang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/YichiRockyZhang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/YichiRockyZhang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/YichiRockyZhang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/YichiRockyZhang\/orgs","repos_url":"https:\/\/api.github.com\/users\/YichiRockyZhang\/repos","events_url":"https:\/\/api.github.com\/users\/YichiRockyZhang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/YichiRockyZhang\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-07-09T08:30:19Z","updated_at":"2023-07-11T03:02:36Z","closed_at":"2023-07-11T03:02:36Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nAfter loading `wiki_dpr` using:\r\n```py\r\nds = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')\r\nprint(ds.get_index(\"embeddings\").metric_type) # prints nothing because the value is None\r\n```\r\nthe index does not have a defined `metric_type`. This is an issue because I do not know how the `scores` are being computed for `get_nearest_examples()`.\n\n### Steps to reproduce the bug\n\nSystem: Python 3.9.16, Transformers 4.30.2, WSL\r\n\r\nAfter loading `wiki_dpr` using:\r\n```py\r\nds = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')\r\nprint(ds.get_index(\"embeddings\").metric_type) # prints nothing because the value is None\r\n```\r\nthe index does not have a defined `metric_type`. This is an issue because I do not know how the `scores` are being computed for `get_nearest_examples()`.\r\n\r\n```py\r\nfrom transformers import DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\r\n\r\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook\/dpr-question_encoder-multiset-base\")\r\nencoder = DPRQuestionEncoder.from_pretrained(\"facebook\/dpr-question_encoder-multiset-base\")\r\n\r\ndef encode_question(query, tokenizer=tokenizer, encoder=encoder):\r\n    inputs = tokenizer(query, return_tensors='pt')\r\n    question_embedding = encoder(**inputs)[0].detach().numpy()\r\n    return question_embedding\r\n\r\ndef get_knn(query, k=5, tokenizer=tokenizer, encoder=encoder, verbose=False):\r\n    enc_question = encode_question(query, tokenizer, encoder)\r\n    topk_results = ds.get_nearest_examples(index_name='embeddings',\r\n                                           query=enc_question,\r\n                                          k=k)\r\n    \r\n    \r\n    a = torch.tensor(enc_question[0]).reshape(768)\r\n    b = torch.tensor(topk_results.examples['embeddings'][0])\r\n    print(a.shape, b.shape)\r\n    print(torch.dot(a, b))\r\n    print((a-b).pow(2).sum())\r\n\r\n    return topk_results\r\n```\r\n\r\nThe [FAISS documentation](https:\/\/github.com\/facebookresearch\/faiss\/wiki\/MetricType-and-distances) suggests the metric is usually L2 distance (without the square root) or the inner product. I compute both for the sample query:\r\n```py\r\nquery = \"\"\" it catapulted into popular culture along with a line of action figures and other toys by Bandai.[2] By 2001, the media franchise had generated over $6 billion in toy sales.\r\nDespite initial criticism that its action violence targeted child audiences, the franchise has been commercially successful.\"\"\"\r\nget_knn(query,k=5)\r\n```\r\n\r\nHere, I get dot product of 80.6020 and L2 distance of 77.6616 and \r\n```py\r\nNearestExamplesResults(scores=array([76.20431 , 75.312416, 74.945404, 74.866394, 74.68506 ],\r\n      dtype=float32), examples={'id': ['3081096', '2004811', '8908258', '9594124', '286575'], 'text': ['actors, resulting in the \"Power Rangers\" franchise which has continued since then into sequel TV series (with \"Power Rangers Beast Morphers\" set to premiere in 2019), comic books, video games, and three feature films, with a further cinematic universe planned. Following from the success of \"Power Rangers\", Saban acquired the rights to more of Toei\\'s library, creating \"VR Troopers\" and \"Big Bad Beetleborgs\" from several Metal Hero Series shows and \"Masked Rider\" from Kamen Rider Series footage. DIC Entertainment joined this boom by acquiring the rights to \"Gridman the Hyper Agent\" and turning it into \"Superhuman Samurai Syber-Squad\". In 2002,', \r\n```\r\n\r\nDoing `k=1` indicates the higher the outputted number, the better the match, so the metric should not be L2 distance. However, my manually computed inner product (80.6) has a discrepancy with the reported (76.2). Perhaps, this has to do with me using the `compressed` embeddings?\n\n### Expected behavior\n\n```py\r\nds = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')\r\nprint(ds.get_index(\"embeddings\").metric_type) # METRIC_INNER_PRODUCT\r\n```\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Linux-4.18.0-477.13.1.el8_8.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6011\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6011\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6010","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6010\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6010\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6010\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6010","id":1793838152,"node_id":"I_kwDODunzps5q68xI","number":6010,"title":"Improve `Dataset`'s string representation","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-07T16:38:03Z","updated_at":"2023-09-01T03:45:07Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Currently, `Dataset.__repr__` outputs a dataset's column names and the number of rows. We could improve it by printing its features and the first few rows.\r\n\r\nWe should also implement `_repr_html_` to have a rich HTML representation in notebooks\/Streamlit.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6010\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6010\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6009","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6009\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6009\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6009\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6009","id":1792059808,"node_id":"PR_kwDODunzps5U1mus","number":6009,"title":"Fix cast for dictionaries with no keys","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-06T18:48:14Z","updated_at":"2023-07-07T14:13:00Z","closed_at":"2023-07-07T14:01:13Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #5677 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6009\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6009\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6009","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6009","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6009.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6009.patch","merged_at":"2023-07-07T14:01:13Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6008","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6008\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6008\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6008\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6008","id":1789869344,"node_id":"I_kwDODunzps5qrz0g","number":6008,"title":"Dataset.from_generator consistently freezes at ~1000 rows","user":{"login":"andreemic","id":27695722,"node_id":"MDQ6VXNlcjI3Njk1NzIy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27695722?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/andreemic","html_url":"https:\/\/github.com\/andreemic","followers_url":"https:\/\/api.github.com\/users\/andreemic\/followers","following_url":"https:\/\/api.github.com\/users\/andreemic\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/andreemic\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/andreemic\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/andreemic\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/andreemic\/orgs","repos_url":"https:\/\/api.github.com\/users\/andreemic\/repos","events_url":"https:\/\/api.github.com\/users\/andreemic\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/andreemic\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-07-05T16:06:48Z","updated_at":"2023-07-10T13:46:39Z","closed_at":"2023-07-10T13:46:39Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhenever I try to create a dataset which contains images using `Dataset.from_generator`, it freezes around 996 rows. I suppose it has something to do with memory consumption, but there's more memory available. I\r\n\r\nSomehow it worked a few times but mostly this makes the datasets library much more cumbersome to work with because generators are the easiest way to turn an existing dataset into a Hugging Face dataset.\r\n\r\nI've let it run in the frozen state for way longer than it can possibly take to load the actual dataset.\r\n\r\nLet me know if you have ideas how to resolve it!\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Dataset\r\nimport numpy as np\r\n\r\ndef gen():\r\n    for row in range(10000):\r\n        yield {\"i\": np.random.rand(512, 512, 3)}\r\n        \r\nDataset.from_generator(gen)\r\n# -> 90% of the time gets stuck around 1000 rows\r\n```\n\n### Expected behavior\n\nShould continue and go through all the examples yielded by the generator, or at least throw an error or somehow communicate what's going on.\n\n### Environment info\n\n- `datasets` version: 2.8.0\r\n- Platform: Linux-5.15.0-52-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6008\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6008\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6007","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6007\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6007\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6007\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6007","id":1789782693,"node_id":"I_kwDODunzps5qreql","number":6007,"title":"Get an error \"OverflowError: Python int too large to convert to C long\" when loading a large dataset","user":{"login":"silverriver","id":2529049,"node_id":"MDQ6VXNlcjI1MjkwNDk=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2529049?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/silverriver","html_url":"https:\/\/github.com\/silverriver","followers_url":"https:\/\/api.github.com\/users\/silverriver\/followers","following_url":"https:\/\/api.github.com\/users\/silverriver\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/silverriver\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/silverriver\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/silverriver\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/silverriver\/orgs","repos_url":"https:\/\/api.github.com\/users\/silverriver\/repos","events_url":"https:\/\/api.github.com\/users\/silverriver\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/silverriver\/received_events","type":"User","site_admin":false},"labels":[{"id":5705560427,"node_id":"LA_kwDODunzps8AAAABVBPxaw","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/arrow","name":"arrow","color":"c2e0c6","default":false,"description":"Related to Apache Arrow"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-07-05T15:16:50Z","updated_at":"2023-07-10T19:11:17Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen load a large dataset with the following code\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"liwu\/MNBVC\", 'news_peoples_daily', split='train')\r\n```\r\n\r\nWe encountered the error: \"OverflowError: Python int too large to convert to C long\"\r\nThe error look something like:\r\n\r\n```\r\nOverflowError: Python int too large to convert to C long\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nOverflowError                             Traceback (most recent call last)\r\n<ipython-input-7-0ed8700e662d> in <module>\r\n----> 1 dataset = load_dataset(\"liwu\/MNBVC\", 'news_peoples_daily', split='train', cache_dir='\/sfs\/MNBVC\/.cache\/')\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/datasets\/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1749         ignore_verifications=ignore_verifications,\r\n   1750         try_from_hf_gcs=try_from_hf_gcs,\r\n-> 1751         use_auth_token=use_auth_token,\r\n   1752     )\r\n   1753 \r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/datasets\/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    703                     if not downloaded_from_gcs:\r\n    704                         self._download_and_prepare(\r\n--> 705                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    706                         )\r\n    707                     # Sync info\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/datasets\/builder.py in _download_and_prepare(self, dl_manager, verify_infos)\r\n   1225 \r\n   1226     def _download_and_prepare(self, dl_manager, verify_infos):\r\n-> 1227         super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n   1228 \r\n   1229     def _get_examples_iterable_for_split(self, split_generator: SplitGenerator) -> ExamplesIterable:\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/datasets\/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    791             try:\r\n    792                 # Prepare split will record examples associated to the split\r\n--> 793                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    794             except OSError as e:\r\n    795                 raise OSError(\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/datasets\/builder.py in _prepare_split(self, split_generator, check_duplicate_keys)\r\n   1219                     writer.write(example, key)\r\n   1220             finally:\r\n-> 1221                 num_examples, num_bytes = writer.finalize()\r\n   1222 \r\n   1223         split_generator.split_info.num_examples = num_examples\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/datasets\/arrow_writer.py in finalize(self, close_stream)\r\n    536             # Re-intializing to empty list for next batch\r\n    537             self.hkey_record = []\r\n--> 538         self.write_examples_on_file()\r\n    539         if self.pa_writer is None:\r\n    540             if self.schema:\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/datasets\/arrow_writer.py in write_examples_on_file(self)\r\n    407             # Since current_examples contains (example, key) tuples\r\n    408             batch_examples[col] = [row[0][col] for row in self.current_examples]\r\n--> 409         self.write_batch(batch_examples=batch_examples)\r\n    410         self.current_examples = []\r\n    411 \r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/datasets\/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)\r\n    506             col_try_type = try_features[col] if try_features is not None and col in try_features else None\r\n    507             typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)\r\n--> 508             arrays.append(pa.array(typed_sequence))\r\n    509             inferred_features[col] = typed_sequence.get_inferred_type()\r\n    510         schema = inferred_features.arrow_schema if self.pa_writer is None else self.schema\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/pyarrow\/array.pxi in pyarrow.lib.array()\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/pyarrow\/array.pxi in pyarrow.lib._handle_arrow_array_protocol()\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/datasets\/arrow_writer.py in __arrow_array__(self, type)\r\n    180             else:\r\n    181                 trying_cast_to_python_objects = True\r\n--> 182                 out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\r\n    183             # use smaller integer precisions if possible\r\n    184             if self.trying_int_optimization:\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/pyarrow\/array.pxi in pyarrow.lib.array()\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/pyarrow\/array.pxi in pyarrow.lib._sequence_to_array()\r\n\r\n\/sfs\/MNBVC\/venv\/lib64\/python3.6\/site-packages\/pyarrow\/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\nOverflowError: Python int too large to convert to C long\r\n```\r\n\r\nHowever, that dataset can be loaded in a streaming manner:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"liwu\/MNBVC\", 'news_peoples_daily', split='train', streaming=True)\r\n\r\nfor i in dataset:\r\n    pass  # it work well\r\n```\r\n\r\nAnother issue is reported in our dataset hub:\r\nhttps:\/\/huggingface.co\/datasets\/liwu\/MNBVC\/discussions\/2\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"liwu\/MNBVC\", 'news_peoples_daily', split='train')\r\n\r\n### Expected behavior\r\n\r\nthe dataset can be safely loaded\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-3.10.0-1160.an7.x86_64-x86_64-with-centos-7.9\r\n- Python version: 3.6.8\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.1.5","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6007\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6007\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6006","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6006\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6006\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6006\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6006","id":1788855582,"node_id":"I_kwDODunzps5qn8Ue","number":6006,"title":"NotADirectoryError when loading gigawords","user":{"login":"xipq","id":115634163,"node_id":"U_kgDOBuRv8w","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/115634163?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/xipq","html_url":"https:\/\/github.com\/xipq","followers_url":"https:\/\/api.github.com\/users\/xipq\/followers","following_url":"https:\/\/api.github.com\/users\/xipq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/xipq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/xipq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/xipq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/xipq\/orgs","repos_url":"https:\/\/api.github.com\/users\/xipq\/repos","events_url":"https:\/\/api.github.com\/users\/xipq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/xipq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-05T06:23:41Z","updated_at":"2023-07-05T06:31:02Z","closed_at":"2023-07-05T06:31:01Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\ngot `NotADirectoryError` whtn loading gigawords dataset\n\n### Steps to reproduce the bug\n\nWhen running\r\n```\r\nimport datasets\r\ndatasets.load_dataset('gigaword')\r\n```\r\n\r\nGot the following exception:\r\n```bash\r\nTraceback (most recent call last):                                                                                                    [0\/1862]\r\n  File \"\/home\/x\/.conda\/envs\/dataproc\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 1629, in _prepare_split_single          \r\n    for key, record in generator:                  \r\n  File \"\/home\/x\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/gigaword\/ea83a8b819190acac5f2dae011fad51dccf269a0604ec5dd24795b\r\n64efb424b6\/gigaword.py\", line 115, in _generate_examples            \r\n    with open(src_path, encoding=\"utf-8\") as f_d, open(tgt_path, encoding=\"utf-8\") as f_s:\r\n  File \"\/home\/x\/.conda\/envs\/dataproc\/lib\/python3.8\/site-packages\/datasets\/streaming.py\", line 71, in wrapper\r\n    return function(*args, use_auth_token=use_auth_token, **kwargs)\r\n  File \"\/home\/x\/.conda\/envs\/dataproc\/lib\/python3.8\/site-packages\/datasets\/download\/streaming_download_manager.py\", line 493, in xope\r\nn                                     \r\n    return open(main_hop, mode, *args, **kwargs)                                     \r\nNotADirectoryError: [Errno 20] Not a directory: '\/home\/x\/.cache\/huggingface\/datasets\/downloads\/6da52431bb5124d90cf51a0187d2dbee9046e\r\n89780c4be7599794a4f559048ec\/org_data\/train.src.txt'\r\n                                              \r\nThe above exception was the direct cause of the following exception:\r\n                                                                                                                        \r\nTraceback (most recent call last):        \r\n  File \"gigaword.py\", line 38, in <module>                                                                                        \r\n    main()                     \r\n  File \"gigaword.py\", line 35, in main                                                                                              \r\n    train, dev, test = dataset.generate_k_shot_data(k=32, seed=seed, path=\"..\/data\/\")\r\n  File \"\/home\/x\/MICL\/preprocess\/fewshot_gym_dataset.py\", line 199, in generate_k_shot_data                                \r\n    dataset = self.load_dataset()                               \r\n  File \"gigaword.py\", line 29, in load_dataset                                                                               \r\n    return datasets.load_dataset('gigaword')                \r\n  File \"\/home\/x\/.conda\/envs\/dataproc\/lib\/python3.8\/site-packages\/datasets\/load.py\", line 1809, in load_dataset            \r\n    builder_instance.download_and_prepare(                                               \r\n  File \"\/home\/x\/.conda\/envs\/dataproc\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 909, in download_and_prepare\r\n    self._download_and_prepare(                                                                                                               \r\n  File \"\/home\/x\/.conda\/envs\/dataproc\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 1670, in _download_and_prepare\r\n    super()._download_and_prepare(                                                                                  \r\n  File \"\/home\/x\/.conda\/envs\/dataproc\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 1004, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)                                                                              \r\n  File \"\/home\/x\/.conda\/envs\/dataproc\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 1508, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(                                                \r\n  File \"\/home\/x\/.conda\/envs\/dataproc\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 1665, in _prepare_split_single          \r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\n\n### Expected behavior\n\nDownload and process the dataset successfully\n\n### Environment info\n\n- `datasets` version: 2.13.1\r\n- Platform: Linux-5.0.0-1032-azure-x86_64-with-glibc2.10\r\n- Python version: 3.8.0\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6006\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6006\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6005","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6005\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6005\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6005\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6005","id":1788103576,"node_id":"PR_kwDODunzps5UoJ91","number":6005,"title":"Drop Python 3.7 support","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-07-04T15:02:37Z","updated_at":"2023-07-06T15:32:41Z","closed_at":"2023-07-06T15:22:43Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"`hfh` and `transformers` have dropped Python 3.7 support, so we should do the same :).\r\n\r\n(Based on the stats, it seems less than 10% of the users use `datasets` with Python 3.7)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6005\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6005\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6005","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6005","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6005.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6005.patch","merged_at":"2023-07-06T15:22:43Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6004","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6004\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6004\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6004\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6004","id":1786636368,"node_id":"PR_kwDODunzps5UjN2h","number":6004,"title":"Misc improvements","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-07-03T18:29:14Z","updated_at":"2023-07-06T17:04:11Z","closed_at":"2023-07-06T16:55:25Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Contains the following improvements:\r\n\r\n* fixes a \"share dataset\" link in README and modifies the \"hosting\" part in the disclaimer section\r\n* updates `Makefile` to also run the style checks on `utils` and `setup.py`\r\n* deletes a test for GH-hosted datasets (no longer supported)\r\n* deletes `convert_dataset.sh` (outdated)\r\n* aligns `utils\/release.py` with `transformers` (the current version is outdated)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6004\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6004\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6004","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6004","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6004.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6004.patch","merged_at":"2023-07-06T16:55:25Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6003","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6003\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6003\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6003\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/6003","id":1786554110,"node_id":"I_kwDODunzps5qfKb-","number":6003,"title":"interleave_datasets & DataCollatorForLanguageModeling having a conflict ?","user":{"login":"PonteIneptique","id":1929830,"node_id":"MDQ6VXNlcjE5Mjk4MzA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1929830?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/PonteIneptique","html_url":"https:\/\/github.com\/PonteIneptique","followers_url":"https:\/\/api.github.com\/users\/PonteIneptique\/followers","following_url":"https:\/\/api.github.com\/users\/PonteIneptique\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/PonteIneptique\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/PonteIneptique\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/PonteIneptique\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/PonteIneptique\/orgs","repos_url":"https:\/\/api.github.com\/users\/PonteIneptique\/repos","events_url":"https:\/\/api.github.com\/users\/PonteIneptique\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/PonteIneptique\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-07-03T17:15:31Z","updated_at":"2023-07-03T17:15:31Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi everyone :)\r\n\r\nI have two local & custom datasets (1 \"sentence\" per line) which I split along the 95\/5 lines for pre-training a Bert model. I use a modified version of `run_mlm.py` in order to be able to make use of `interleave_dataset`:\r\n\r\n- `tokenize()` runs fine\r\n- `group_text()` runs fine\r\n\r\nEverytime, on step 19, I get \r\n\r\n```pytb\r\n  File \"env\/lib\/python3.9\/site-packages\/transformers\/data\/data_collator.py\", line 779, in torch_mask_tokens\r\n    inputs[indices_random] = random_words[indices_random]\r\nRuntimeError: Index put requires the source and destination dtypes match, got Float for the destination and Long for the source.\r\n```\r\n\r\nI tried:\r\n- training without interleave on dataset 1, it runs\r\n- training without interleave on dataset 2, it runs\r\n- training without `.to_iterable_dataset()`, it hangs then crash\r\n- training without group_text() and padding to max_length seemed to fix the issue, but who knows if this was just because it was an issue that would come much later in terms of steps.\r\n\r\nI might have coded something wrong, but I don't get what \n\n### Steps to reproduce the bug\n\nI have this function:\r\n\r\n```py\r\ndef build_dataset(path: str, percent: str):\r\n    dataset = load_dataset(\r\n        \"text\",\r\n        data_files={\"train\": [path]},\r\n        split=f\"train[{percent}]\"\r\n    )\r\n    dataset = dataset.map(\r\n        lambda examples: tokenize(examples[\"text\"]),\r\n        batched=True,\r\n        num_proc=num_proc,\r\n    )\r\n\r\n    dataset = dataset.map(\r\n        group_texts,\r\n        batched=True,\r\n        num_proc=num_proc,\r\n        desc=f\"Grouping texts in chunks of {tokenizer.max_seq_length}\",\r\n        remove_columns=[\"text\"]\r\n    )\r\n\r\n    print(len(dataset))\r\n    return dataset.to_iterable_dataset()\r\n```\r\n\r\nI hardcoded group_text:\r\n```py\r\n    def group_texts(examples):\r\n        # Concatenate all texts.\r\n        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\r\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\r\n        # We drop the small remainder, and if the total_length < max_seq_length  we exclude this batch and return an empty dict.\r\n        # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\r\n        total_length = (total_length \/\/ 512) * 512\r\n        # Split by chunks of max_len.\r\n        result = {\r\n            k: [t[i: i + 512] for i in range(0, total_length, 512)]\r\n            for k, t in concatenated_examples.items()\r\n        }\r\n        # result = {k: [el for el in elements if el] for k, elements in result.items()}\r\n        return result\r\n```\r\n\r\nAnd then I build datasets using the following code:\r\n\r\n```py\r\ntrain1 = build_dataset(\"d1.txt\", \":95%\")\r\ntrain2 = build_dataset(\"d2.txt\", \":95%\")\r\ndev1 = build_dataset(\"d1.txt\", \"95%:\")\r\ndev2 = build_dataset(\"d2.txt\", \"95%:\")\r\n```\r\n\r\nand finally I run\r\n```py\r\ntrain_dataset = interleave_datasets(\r\n    [train1, train2],\r\n    probabilities=[0.8, 0.2],\r\n    seed=42\r\n)\r\neval_dataset = interleave_datasets(\r\n    [dev1, dev2],\r\n    probabilities=[0.8, 0.2],\r\n    seed=42\r\n)\r\n```\r\n\r\nThen I run the training part which remains mostly untouched:\r\n\r\n> CUDA_VISIBLE_DEVICES=1 python custom_dataset.py --model_type bert --per_device_train_batch_size 32 --do_train --output_dir \/var\/mlm\/training-bert\/model --max_seq_length 512 --save_steps 10000 --save_total_limit 3 --auto_find_batch_size --logging_dir .\/logs-bert --learning_rate 0.0001 --do_train --num_train_epochs 25 --warmup_steps 10000 --max_step 45000 --fp16\n\n### Expected behavior\n\nThe model should then train normally, but fails every time at the same step (19).\r\n\r\nprinting the variables at `inputs[indices_random] = random_words[indices_random]` shows a magnificient empty tensor (, 32) [if I remember well]\n\n### Environment info\n\ntransformers[torch] 4.30.2\r\nUbuntu\r\nA100 0 CUDA 12\r\nDriver Version: 525.116.04","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6003\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6003\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6002","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6002\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6002\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6002\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6002","id":1786053060,"node_id":"PR_kwDODunzps5UhP-Z","number":6002,"title":"Add KLUE-MRC metrics","user":{"login":"ingyuseong","id":37537248,"node_id":"MDQ6VXNlcjM3NTM3MjQ4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/37537248?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ingyuseong","html_url":"https:\/\/github.com\/ingyuseong","followers_url":"https:\/\/api.github.com\/users\/ingyuseong\/followers","following_url":"https:\/\/api.github.com\/users\/ingyuseong\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ingyuseong\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ingyuseong\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ingyuseong\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ingyuseong\/orgs","repos_url":"https:\/\/api.github.com\/users\/ingyuseong\/repos","events_url":"https:\/\/api.github.com\/users\/ingyuseong\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ingyuseong\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-07-03T12:11:10Z","updated_at":"2023-07-09T11:57:20Z","closed_at":"2023-07-09T11:57:20Z","author_association":"NONE","active_lock_reason":null,"body":"## Metrics for KLUE-MRC (Korean Language Understanding Evaluation \u2014 Machine Reading Comprehension)\r\n\r\nAdding metrics for [KLUE-MRC](https:\/\/huggingface.co\/datasets\/klue).\r\nKLUE-MRC is very similar to SQuAD 2.0 but has a slightly different format which is why I added metrics for KLUE-MRC.\r\n\r\nSpecifically, in the case of [LM Eval Harness](https:\/\/github.com\/EleutherAI\/lm-evaluation-harness), it leverages the scoring script of SQuAD to evaluate SQuAD 2.0 and KorQuAD. But the script isn't suitable for KLUE-MRC because KLUE-MRC is a bit different from SQuAD 2.0. And this is why I added the scoring script for KLUE-MRC.\r\n\r\n- [x] All tests passed\r\n- [x] Added a metric card (referred the metric card of SQuAD 2.0)\r\n- [x] Compatibility test with [LM Eval Harness](https:\/\/github.com\/EleutherAI\/lm-evaluation-harness) passed\r\n\r\n### References\r\n- [KLUE: Korean Language Understanding Evaluation](https:\/\/datasets-benchmarks-proceedings.neurips.cc\/paper_files\/paper\/2021\/file\/98dce83da57b0395e163467c9dae521b-Paper-round2.pdf)\r\n- [KLUE on Hugging Face Datasets](https:\/\/huggingface.co\/datasets\/klue)\r\n- #2416","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6002\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6002\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6002","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6002","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6002.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6002.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6001","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6001\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6001\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6001\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6001","id":1782516627,"node_id":"PR_kwDODunzps5UVMMh","number":6001,"title":"Align `column_names` type check with type hint in `sort`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-30T13:15:50Z","updated_at":"2023-06-30T14:18:32Z","closed_at":"2023-06-30T14:11:24Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #5998 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6001\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6001\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6001","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6001","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6001.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6001.patch","merged_at":"2023-06-30T14:11:24Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6000","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6000\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6000\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6000\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6000","id":1782456878,"node_id":"PR_kwDODunzps5UU_FB","number":6000,"title":"Pin `joblib` to avoid `joblibspark` test failures","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-30T12:36:54Z","updated_at":"2023-06-30T13:17:05Z","closed_at":"2023-06-30T13:08:27Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"`joblibspark` doesn't support the latest `joblib` release.\r\n\r\nSee https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/5401870932\/jobs\/9812337078 for the errors","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6000\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/6000\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/6000","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6000","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6000.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/6000.patch","merged_at":"2023-06-30T13:08:27Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5999","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5999\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5999\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5999\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5999","id":1781851513,"node_id":"I_kwDODunzps5qNOV5","number":5999,"title":"Getting a 409 error while loading xglue dataset","user":{"login":"Praful932","id":45713796,"node_id":"MDQ6VXNlcjQ1NzEzNzk2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45713796?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Praful932","html_url":"https:\/\/github.com\/Praful932","followers_url":"https:\/\/api.github.com\/users\/Praful932\/followers","following_url":"https:\/\/api.github.com\/users\/Praful932\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Praful932\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Praful932\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Praful932\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Praful932\/orgs","repos_url":"https:\/\/api.github.com\/users\/Praful932\/repos","events_url":"https:\/\/api.github.com\/users\/Praful932\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Praful932\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-06-30T04:13:54Z","updated_at":"2023-06-30T05:57:23Z","closed_at":"2023-06-30T05:57:22Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nUnable to load xglue dataset\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\ndataset = datasets.load_dataset(\"xglue\", \"ntg\")\r\n```\r\n\r\n> ConnectionError: Couldn't reach https:\/\/xglue.blob.core.windows.net\/xglue\/xglue_full_dataset.tar.gz (error 409)\n\n### Expected behavior\n\nExpected the dataset to load\n\n### Environment info\n\n- `datasets` version: 2.13.1\r\n- Platform: Linux-5.15.107+-x86_64-with-glibc2.31\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5999\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5999\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5998","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5998\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5998\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5998\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5998","id":1781805018,"node_id":"I_kwDODunzps5qNC_a","number":5998,"title":"The current implementation has a potential bug in the sort method","user":{"login":"wangyuxinwhy","id":22192665,"node_id":"MDQ6VXNlcjIyMTkyNjY1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22192665?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wangyuxinwhy","html_url":"https:\/\/github.com\/wangyuxinwhy","followers_url":"https:\/\/api.github.com\/users\/wangyuxinwhy\/followers","following_url":"https:\/\/api.github.com\/users\/wangyuxinwhy\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wangyuxinwhy\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wangyuxinwhy\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wangyuxinwhy\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wangyuxinwhy\/orgs","repos_url":"https:\/\/api.github.com\/users\/wangyuxinwhy\/repos","events_url":"https:\/\/api.github.com\/users\/wangyuxinwhy\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wangyuxinwhy\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-06-30T03:16:57Z","updated_at":"2023-06-30T14:21:03Z","closed_at":"2023-06-30T14:11:25Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nIn the sort method\uff0chere's a piece of code\r\n\r\n```python\r\n# column_names: Union[str, Sequence_[str]]\r\n\r\n# Check proper format of and for duplicates in column_names\r\nif not isinstance(column_names, list):\r\n    column_names = [column_names]\r\n```\r\n\r\nI get an error when I pass in a tuple based on the column_names type annotation, it will raise an errror.As in the example below, while the type annotation implies that a tuple can be passed.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('glue', 'ax')['test']\r\ndataset.sort(column_names=('premise', 'hypothesis'))\r\n# Raise ValueError: Column '('premise', 'hypothesis')' not found in the dataset.\r\n```\r\n\r\nOf course, after I modified the tuple into a list, everything worked fine\r\n\r\nChange the code to the following so there will be no problem\r\n\r\n```python\r\n# Check proper format of and for duplicates in column_names\r\nif not isinstance(column_names, list):\r\n    if isinstance(column_names, str):\r\n        column_names = [column_names]\r\n    else:\r\n        column_names = list(column_names)\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('glue', 'ax')['test']\r\ndataset.sort(column_names=('premise', 'hypothesis'))\r\n# Raise ValueError: Column '('premise', 'hypothesis')' not found in the dataset.\r\n```\r\n\r\n### Expected behavior\r\n\r\nPassing tuple into column_names should be equivalent to passing list\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: macOS-13.1-arm64-arm-64bit\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5998\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5998\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5997","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5997\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5997\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5997\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5997","id":1781582818,"node_id":"I_kwDODunzps5qMMvi","number":5997,"title":"extend the map function so it can wrap around long text that does not fit in the context window","user":{"login":"siddhsql","id":127623723,"node_id":"U_kgDOB5tiKw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/127623723?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/siddhsql","html_url":"https:\/\/github.com\/siddhsql","followers_url":"https:\/\/api.github.com\/users\/siddhsql\/followers","following_url":"https:\/\/api.github.com\/users\/siddhsql\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/siddhsql\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/siddhsql\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/siddhsql\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/siddhsql\/orgs","repos_url":"https:\/\/api.github.com\/users\/siddhsql\/repos","events_url":"https:\/\/api.github.com\/users\/siddhsql\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/siddhsql\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-29T22:15:21Z","updated_at":"2023-07-03T17:58:52Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nI understand `dataset` provides a [`map`](https:\/\/github.com\/huggingface\/datasets\/blob\/main\/src\/datasets\/arrow_dataset.py#L2849) function. This function in turn takes in a callable that is used to tokenize the text on which a model is trained. Frequently this text will not fit within a models's context window. In this case it would be useful to wrap around the text into multiple rows with each row fitting the model's context window. I tried to do it using this code as example which in turn I have borrowed from [here](https:\/\/stackoverflow.com\/a\/76343993\/147530):\r\n\r\n```\r\ndata = data.map(lambda samples: tokenizer(samples[\"text\"], max_length=tokenizer.model_max_length, truncation=True, stride=4, return_overflowing_tokens=True), batched=True)\r\n```\r\n\r\nbut running the code gives me this error:\r\n\r\n```\r\nFile \"\/llm\/fine-tune.py\", line 117, in <module>\r\n    data = data.map(lambda samples: tokenizer(samples[\"text\"], max_length=tokenizer.model_max_length, truncation=True, stride=4, return_overflowing_tokens=True), batched=True)\r\n  File \"\/llm\/.env\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 580, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/llm\/.env\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 545, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/llm\/.env\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 3087, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"\/llm\/.env\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 3480, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"\/llm\/.env\/lib\/python3.9\/site-packages\/datasets\/arrow_writer.py\", line 556, in write_batch\r\n    pa_table = pa.Table.from_arrays(arrays, schema=schema)\r\n  File \"pyarrow\/table.pxi\", line 3798, in pyarrow.lib.Table.from_arrays\r\n  File \"pyarrow\/table.pxi\", line 2962, in pyarrow.lib.Table.validate\r\n  File \"pyarrow\/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Column 1 named input_ids expected length 394 but got length 447\r\n```\r\n\r\nThe lambda function I have provided is correctly chopping up long text so it wraps around (and because of this 394 samples become 447 after wrap around) but the dataset `map` function does not like it.\n\n### Motivation\n\nplease see above\n\n### Your contribution\n\nI'm afraid I don't have much knowledge to help","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5997\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5997\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5996","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5996\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5996\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5996\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5996","id":1779294374,"node_id":"PR_kwDODunzps5UKP0i","number":5996,"title":"Deprecate `use_auth_token` in favor of `token`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-06-28T16:26:38Z","updated_at":"2023-07-05T15:22:20Z","closed_at":"2023-07-03T16:03:33Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"... to be consistent with `transformers` and `huggingface_hub`.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5996\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5996\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5996","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5996","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5996.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5996.patch","merged_at":"2023-07-03T16:03:33Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5995","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5995\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5995\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5995\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5995","id":1777088925,"node_id":"PR_kwDODunzps5UCvYJ","number":5995,"title":"Support returning dataframe in map transform","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-27T14:15:08Z","updated_at":"2023-06-28T13:56:02Z","closed_at":"2023-06-28T13:46:33Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Allow returning Pandas DataFrames in `map` transforms.\r\n\r\n(Plus, raise an error in the non-batched mode if a returned PyArrow table\/Pandas DataFrame has more than one row)\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5995\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5995\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5995","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5995","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5995.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5995.patch","merged_at":"2023-06-28T13:46:33Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5994","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5994\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5994\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5994\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5994","id":1776829004,"node_id":"PR_kwDODunzps5UB1cA","number":5994,"title":"Fix select_columns columns order","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-27T12:32:46Z","updated_at":"2023-06-27T15:40:47Z","closed_at":"2023-06-27T15:32:43Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix the order of the columns in dataset.features when the order changes with `dataset.select_columns()`.\r\n\r\nI also fixed the same issue for `dataset.flatten()`\r\n\r\nClose https:\/\/github.com\/huggingface\/datasets\/issues\/5993","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5994\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5994\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5994","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5994","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5994.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5994.patch","merged_at":"2023-06-27T15:32:43Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5993","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5993\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5993\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5993\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5993","id":1776643555,"node_id":"I_kwDODunzps5p5W3j","number":5993,"title":"ValueError: Table schema does not match schema used to create file","user":{"login":"exs-avianello","id":128361578,"node_id":"U_kgDOB6akag","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/128361578?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/exs-avianello","html_url":"https:\/\/github.com\/exs-avianello","followers_url":"https:\/\/api.github.com\/users\/exs-avianello\/followers","following_url":"https:\/\/api.github.com\/users\/exs-avianello\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/exs-avianello\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/exs-avianello\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/exs-avianello\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/exs-avianello\/orgs","repos_url":"https:\/\/api.github.com\/users\/exs-avianello\/repos","events_url":"https:\/\/api.github.com\/users\/exs-avianello\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/exs-avianello\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-06-27T10:54:07Z","updated_at":"2023-06-27T15:36:42Z","closed_at":"2023-06-27T15:32:44Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nSaving a dataset as parquet fails with a `ValueError: Table schema does not match schema used to create file`  if the dataset was obtained out of a `.select_columns()` call with columns selected out of order.\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\ndataset = datasets.Dataset.from_dict(\r\n    {\r\n        \"x1\": [1, 2, 3],\r\n        \"x2\": [10, 11, 12],\r\n    }\r\n)\r\n\r\nds = dataset.select_columns([\"x2\", \"x1\"])\r\n\r\nds.to_parquet(\"demo.parquet\")\r\n```\r\n\r\n```shell\r\n>>>\r\nValueError: Table schema does not match schema used to create file: \r\ntable:\r\nx2: int64\r\nx1: int64\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"x2\": {\"dtype\": \"int64\", \"_type\": \"V' + 53 vs. \r\nfile:\r\nx1: int64\r\nx2: int64\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"x1\": {\"dtype\": \"int64\", \"_type\": \"V' + 53\r\n```\r\n\r\n--- \r\n\r\nI think this is because after the `.select_columns()` call with out of order columns, the output dataset features' schema ends up being out of sync with the schema of the arrow table backing it. \r\n\r\n```python\r\nds.features.arrow_schema\r\n>>>\r\nx1: int64\r\nx2: int64\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"x1\": {\"dtype\": \"int64\", \"_type\": \"V' + 53\r\n\r\nds.data.schema\r\n>>>\r\nx2: int64\r\nx1: int64\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"x2\": {\"dtype\": \"int64\", \"_type\": \"V' + 53\r\n```\r\n\r\n\r\nSo when we call `.to_parquet()`, the call behind the scenes to `datasets.io.parquet.ParquetDatasetWriter(...).write()` which initialises the backend `pyarrow.parquet.ParquetWriter` with `schema = self.dataset.features.arrow_schema` triggers `pyarrow` on write when [it checks](https:\/\/github.com\/apache\/arrow\/blob\/11b140a734a516e436adaddaeb35d23f30dcce44\/python\/pyarrow\/parquet\/core.py#L1086-L1090) that the `ParquetWriter` schema matches the schema of the table being written \ud83d\ude4c \r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/6ed837325cb539a5deb99129e5ad181d0269e050\/src\/datasets\/io\/parquet.py#L139-L141\r\n\n\n### Expected behavior\n\nThe dataset gets successfully saved as parquet. \r\n\r\n*In the same way as it does if saving it as csv:\r\n\r\n```python\r\nimport datasets\r\n\r\ndataset = datasets.Dataset.from_dict(\r\n    {\r\n        \"x1\": [1, 2, 3],\r\n        \"x2\": [10, 11, 12],\r\n    }\r\n)\r\n\r\nds = dataset.select_columns([\"x2\", \"x1\"])\r\n\r\nds.to_csv(\"demo.csv\")\r\n```\n\n### Environment info\n\n`python==3.11`\r\n`datasets==2.13.1`\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5993\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5993\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5992","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5992\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5992\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5992\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5992","id":1776460964,"node_id":"PR_kwDODunzps5UAk3C","number":5992,"title":"speedup","user":{"login":"qgallouedec","id":45557362,"node_id":"MDQ6VXNlcjQ1NTU3MzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45557362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/qgallouedec","html_url":"https:\/\/github.com\/qgallouedec","followers_url":"https:\/\/api.github.com\/users\/qgallouedec\/followers","following_url":"https:\/\/api.github.com\/users\/qgallouedec\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/qgallouedec\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/qgallouedec\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/qgallouedec\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/qgallouedec\/orgs","repos_url":"https:\/\/api.github.com\/users\/qgallouedec\/repos","events_url":"https:\/\/api.github.com\/users\/qgallouedec\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/qgallouedec\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-06-27T09:17:58Z","updated_at":"2023-06-27T09:23:07Z","closed_at":"2023-06-27T09:18:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5992\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5992\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5992","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5992","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5992.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5992.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5991","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5991\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5991\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5991\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5991","id":1774456518,"node_id":"I_kwDODunzps5pxA7G","number":5991,"title":"`map` with any joblib backend","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-06-26T10:33:42Z","updated_at":"2023-06-26T10:33:42Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"We recently enabled the (experimental) parallel backend switch for data download and extraction but not for `map` yet.\r\n\r\nRight now we're using our `iflatmap_unordered` implementation for multiprocessing that uses a shared Queue to gather progress updates from the subprocesses and show a progress bar in the main process.\r\n\r\nIf a Queue implementation that would work on any joblib backend by leveraging the filesystem that is shared among workers, we can have `iflatmap_unordered` for joblib and therefore a `map` with any joblib backend with a progress bar !\r\n\r\nNote that the Queue doesn't need to be that optimized though since we can choose a small frequency for progress updates (like 1 update per second).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5991\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5991\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5989","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5989\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5989\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5989\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5989","id":1774134091,"node_id":"I_kwDODunzps5pvyNL","number":5989,"title":"Set a rule on the config and split names","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-26T07:34:14Z","updated_at":"2023-07-19T14:22:54Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"> should we actually allow characters like spaces? maybe it's better to add validation for whitespace symbols and directly in datasets and raise\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets-server\/issues\/853\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5989\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5989\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5988","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5988\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5988\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5988\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5988","id":1773257828,"node_id":"I_kwDODunzps5pscRk","number":5988,"title":"ConnectionError: Couldn't reach dataset_infos.json ","user":{"login":"yulingao","id":20674868,"node_id":"MDQ6VXNlcjIwNjc0ODY4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20674868?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yulingao","html_url":"https:\/\/github.com\/yulingao","followers_url":"https:\/\/api.github.com\/users\/yulingao\/followers","following_url":"https:\/\/api.github.com\/users\/yulingao\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yulingao\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yulingao\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yulingao\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yulingao\/orgs","repos_url":"https:\/\/api.github.com\/users\/yulingao\/repos","events_url":"https:\/\/api.github.com\/users\/yulingao\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yulingao\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-06-25T12:39:31Z","updated_at":"2023-07-07T13:20:57Z","closed_at":"2023-07-07T13:20:57Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm trying to load codeparrot\/codeparrot-clean-train, but get the following error:\r\n\r\nConnectionError: Couldn't reach https:\/\/huggingface.co\/datasets\/codeparrot\/codeparrot-clean-train\/resolve\/main\/dataset_infos.json (ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))))\r\n\r\n\n\n### Steps to reproduce the bug\n\ntrain_data = load_dataset('codeparrot\/codeparrot-clean-train', split='train')\r\n\n\n### Expected behavior\n\ndownload the dataset\n\n### Environment info\n\ncentos7","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5988\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5988\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5987","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5987\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5987\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5987\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5987","id":1773047909,"node_id":"I_kwDODunzps5prpBl","number":5987,"title":"Why max_shard_size is not supported in load_dataset and passed to download_and_prepare","user":{"login":"npuichigo","id":11533479,"node_id":"MDQ6VXNlcjExNTMzNDc5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11533479?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/npuichigo","html_url":"https:\/\/github.com\/npuichigo","followers_url":"https:\/\/api.github.com\/users\/npuichigo\/followers","following_url":"https:\/\/api.github.com\/users\/npuichigo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/npuichigo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/npuichigo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/npuichigo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/npuichigo\/orgs","repos_url":"https:\/\/api.github.com\/users\/npuichigo\/repos","events_url":"https:\/\/api.github.com\/users\/npuichigo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/npuichigo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-06-25T04:19:13Z","updated_at":"2023-06-29T16:06:08Z","closed_at":"2023-06-29T16:06:08Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/a8a797cc92e860c8d0df71e0aa826f4d2690713e\/src\/datasets\/load.py#L1809\r\n\r\nWhat I can to is break the `load_dataset` and use `load_datset_builder` + `download_and_prepare` instead.\n\n### Steps to reproduce the bug\n\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/a8a797cc92e860c8d0df71e0aa826f4d2690713e\/src\/datasets\/load.py#L1809\n\n### Expected behavior\n\nUsers can define the max shard size.\n\n### Environment info\n\ndatasets==2.13.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5987\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5987\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5986","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5986\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5986\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5986\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5986","id":1772233111,"node_id":"PR_kwDODunzps5TygOZ","number":5986,"title":"Make IterableDataset.from_spark more efficient","user":{"login":"mathewjacob1002","id":134338709,"node_id":"U_kgDOCAHYlQ","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/134338709?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mathewjacob1002","html_url":"https:\/\/github.com\/mathewjacob1002","followers_url":"https:\/\/api.github.com\/users\/mathewjacob1002\/followers","following_url":"https:\/\/api.github.com\/users\/mathewjacob1002\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mathewjacob1002\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mathewjacob1002\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mathewjacob1002\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mathewjacob1002\/orgs","repos_url":"https:\/\/api.github.com\/users\/mathewjacob1002\/repos","events_url":"https:\/\/api.github.com\/users\/mathewjacob1002\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mathewjacob1002\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-06-23T22:18:20Z","updated_at":"2023-07-07T10:05:58Z","closed_at":"2023-07-07T09:56:09Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Moved the code from using collect() to using toLocalIterator, which allows for prefetching partitions that will be selected next, thus allowing for better performance when iterating. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5986\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5986\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5986","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5986","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5986.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5986.patch","merged_at":"2023-07-07T09:56:09Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5985","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5985\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5985\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5985\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5985","id":1771588158,"node_id":"I_kwDODunzps5pmEo-","number":5985,"title":"Cannot reuse tokenizer object for dataset map","user":{"login":"vikigenius","id":12724810,"node_id":"MDQ6VXNlcjEyNzI0ODEw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12724810?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vikigenius","html_url":"https:\/\/github.com\/vikigenius","followers_url":"https:\/\/api.github.com\/users\/vikigenius\/followers","following_url":"https:\/\/api.github.com\/users\/vikigenius\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vikigenius\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vikigenius\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vikigenius\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vikigenius\/orgs","repos_url":"https:\/\/api.github.com\/users\/vikigenius\/repos","events_url":"https:\/\/api.github.com\/users\/vikigenius\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vikigenius\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892865,"node_id":"MDU6TGFiZWwxOTM1ODkyODY1","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/duplicate","name":"duplicate","color":"cfd3d7","default":true,"description":"This issue or pull request already exists"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-23T14:45:31Z","updated_at":"2023-07-21T14:09:14Z","closed_at":"2023-07-21T14:09:14Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nRelated to https:\/\/github.com\/huggingface\/transformers\/issues\/24441. Not sure if this is a tokenizer issue or caching issue, so filing in both.\r\n\r\nPassing the tokenizer to the dataset map function causes the tokenizer to be fingerprinted weirdly. After calling the tokenizer with arguments like padding and truncation the tokenizer object changes interanally, even though the hash remains the same.\r\n\r\nBut dumps is able to detect that internal change which causes the tokenizer object's fingerprint to change.\r\n\r\n\n\n### Steps to reproduce the bug\n\n```python\r\nfrom transformers import AutoTokenizer\r\nfrom datasets.utils.py_utils import dumps # Huggingface datasets\r\n\r\nt = AutoTokenizer.from_pretrained('bert-base-uncased')\r\nt.save_pretrained(\"tok1\")\r\nth1 = hash(dumps(t))\r\ntext = \"This is an example text\"\r\nttext = t(text, max_length=512, padding=\"max_length\", truncation=True)\r\nt.save_pretrained(\"tok2\")\r\nth2 = hash(dumps(t))\r\n\r\nassert th1 == th2 # Assertion Error\r\n```\r\n\r\nBut if you use just the hash of the object without dumps, the hashes don't change\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\nfrom datasets.utils.py_utils import dumps # Huggingface datasets\r\n\r\nt = AutoTokenizer.from_pretrained('bert-base-uncased')\r\nth1 = hash(t)  # Just hash no dumps\r\ntext = \"This is an example text\"\r\nttext = t(text, max_length=512, padding=\"max_length\", truncation=True)\r\nth2 = hash(t) # Just hash no dumps\r\n\r\nassert th1 == th2 # This is OK\r\n```\r\n\r\nThis causes situations such as the following\r\n\r\n1. Create a text file like this `yes \"This is an example text\" | head -n 10000 > lines.txt`\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\nimport datasets\r\n\r\n\r\nclass TokenizeMapper(object):\r\n    \"\"\"Mapper for tokenizer.\r\n\r\n    This is needed because the caching mechanism of HuggingFace does not work on\r\n    lambdas. Each time a new lambda will be created by a new process which will\r\n    lead to a different hash.\r\n    This way we can have a universal mapper object in init and reuse it with the same\r\n    hash for each process.\r\n    \"\"\"\r\n\r\n    def __init__(self, tokenizer):\r\n        \"\"\"Initialize the tokenizer.\"\"\"\r\n        self.tokenizer = tokenizer\r\n\r\n    def __call__(self, examples, **kwargs):\r\n        \"\"\"Run the mapper.\"\"\"\r\n        texts = examples[\"text\"]\r\n        tt = self.tokenizer(texts, max_length=256, padding=\"max_length\", truncation=True)\r\n        batch_outputs = {\r\n            \"input_ids\": tt.input_ids,\r\n            \"attention_mask\": tt.attention_mask,\r\n        }\r\n        return batch_outputs\r\n\r\n\r\nt = AutoTokenizer.from_pretrained('bert-base-uncased')\r\nmapper = TokenizeMapper(t)\r\n\r\nds = datasets.load_dataset(\"text\", data_files=\"lines.txt\")\r\n\r\nmds1 = ds.map(\r\n    mapper,\r\n    batched=False,\r\n    remove_columns=[\"text\"],\r\n).with_format(\"torch\")\r\n\r\nmds2 = ds.map(\r\n    mapper,\r\n    batched=False,\r\n    remove_columns=[\"text\"],\r\n).with_format(\"torch\")\r\n```\r\n\r\nThe second call to map should reuse the cached processed dataset from mds1, but it instead it redoes the tokenization because of the behavior of dumps.\n\n### Expected behavior\n\nWe should be able to initialize a tokenizer. And reusing it should let us reuse the same map computation for the same dataset.\r\n\r\nThe second call to map should reuse the cached processed dataset from mds1, but it instead it redoes the tokenization because of the behavior of dumps.\n\n### Environment info\n\n- `datasets` version: 2.13.0\r\n- Platform: Linux-6.1.31_1-x86_64-with-glibc2.36\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5985\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5985\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5984","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5984\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5984\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5984\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5984","id":1771571458,"node_id":"I_kwDODunzps5pmAkC","number":5984,"title":"AutoSharding IterableDataset's when num_workers > 1","user":{"login":"mathephysicist","id":25594384,"node_id":"MDQ6VXNlcjI1NTk0Mzg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/25594384?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mathephysicist","html_url":"https:\/\/github.com\/mathephysicist","followers_url":"https:\/\/api.github.com\/users\/mathephysicist\/followers","following_url":"https:\/\/api.github.com\/users\/mathephysicist\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mathephysicist\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mathephysicist\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mathephysicist\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mathephysicist\/orgs","repos_url":"https:\/\/api.github.com\/users\/mathephysicist\/repos","events_url":"https:\/\/api.github.com\/users\/mathephysicist\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mathephysicist\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-06-23T14:34:20Z","updated_at":"2023-12-08T09:04:04Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\n\r\nMinimal Example\r\n\r\n```\r\nimport torch\r\nfrom datasets import IterableDataset\r\n\r\nd = IterableDataset.from_file(<file_name>)\r\ndl = torch.utils.data.dataloader.DataLoader(d,num_workers=3)\r\n\r\nfor sample in dl:\r\n    print(sample)\r\n\r\n```\r\n\r\nWarning:\r\nToo many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.\r\nTo parallelize data loading, we give each process some shards (or data sources) to process. Therefore it's unnecessary to have a number of workers greater than dataset.n_shards=1. To enable more parallelism, please split the dataset in more files than 1.\r\n\r\nExpected Behavior:\r\nDataset is sharded each cpu uses subset (contiguously - so you can do checkpoint loading\/saving) \n\n### Motivation\n\nI have a lot of unused cpu's and would like to be able to shard iterable datasets with pytorch's dataloader when num_workers > 1. This is for a very large single file. I am aware that we can use the `split_dataset_by_node` to ensure that each node (for distributed) gets different shards, but we should extend it so that this also continues for multiple workers. \n\n### Your contribution\n\nIf someone points me to what needs to change, I can create a PR.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5984\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5984\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5983","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5983\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5983\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5983\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5983","id":1770578804,"node_id":"PR_kwDODunzps5TtDdy","number":5983,"title":"replaced PathLike as a variable for save_to_disk for dataset_path wit\u2026","user":{"login":"benjaminbrown038","id":35114142,"node_id":"MDQ6VXNlcjM1MTE0MTQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35114142?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/benjaminbrown038","html_url":"https:\/\/github.com\/benjaminbrown038","followers_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/followers","following_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/orgs","repos_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/repos","events_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-06-23T00:57:05Z","updated_at":"2023-09-11T04:17:17Z","closed_at":"2023-09-11T04:17:17Z","author_association":"NONE","active_lock_reason":null,"body":"\u2026h str like that of load_from_disk","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5983\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5983\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5983","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5983","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5983.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5983.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5982","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5982\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5982\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5982\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5982","id":1770333296,"node_id":"I_kwDODunzps5phSRw","number":5982,"title":"404 on Datasets Documentation Page","user":{"login":"kmulka-bloomberg","id":118509387,"node_id":"U_kgDOBxBPSw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/118509387?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kmulka-bloomberg","html_url":"https:\/\/github.com\/kmulka-bloomberg","followers_url":"https:\/\/api.github.com\/users\/kmulka-bloomberg\/followers","following_url":"https:\/\/api.github.com\/users\/kmulka-bloomberg\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kmulka-bloomberg\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kmulka-bloomberg\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kmulka-bloomberg\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kmulka-bloomberg\/orgs","repos_url":"https:\/\/api.github.com\/users\/kmulka-bloomberg\/repos","events_url":"https:\/\/api.github.com\/users\/kmulka-bloomberg\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kmulka-bloomberg\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-22T20:14:57Z","updated_at":"2023-06-26T15:45:03Z","closed_at":"2023-06-26T15:45:03Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nGetting a 404 from the Hugging Face Datasets docs page:\r\nhttps:\/\/huggingface.co\/docs\/datasets\/index\r\n\n\n### Steps to reproduce the bug\n\n1. Go to URL https:\/\/huggingface.co\/docs\/datasets\/index\r\n2. Notice 404 not found\n\n### Expected behavior\n\nURL should either show docs or redirect to new location\n\n### Environment info\n\nhugginface.co","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5982\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5982\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5981","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5981\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5981\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5981\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5981","id":1770310087,"node_id":"I_kwDODunzps5phMnH","number":5981,"title":"Only two cores are getting used in sagemaker with pytorch 3.10 kernel","user":{"login":"mmr-crexi","id":107141022,"node_id":"U_kgDOBmLXng","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/107141022?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mmr-crexi","html_url":"https:\/\/github.com\/mmr-crexi","followers_url":"https:\/\/api.github.com\/users\/mmr-crexi\/followers","following_url":"https:\/\/api.github.com\/users\/mmr-crexi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mmr-crexi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mmr-crexi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mmr-crexi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mmr-crexi\/orgs","repos_url":"https:\/\/api.github.com\/users\/mmr-crexi\/repos","events_url":"https:\/\/api.github.com\/users\/mmr-crexi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mmr-crexi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-22T19:57:31Z","updated_at":"2023-10-30T06:17:40Z","closed_at":"2023-07-24T11:54:52Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen using the newer pytorch 3.10 kernel, only 2 cores are being used by huggingface filter and map functions.  The Pytorch 3.9 kernel would use as many cores as specified in the num_proc field.\r\n\r\nWe have solved this in our own code by placing the following snippet in the code that is called inside subprocesses:\r\n\r\n```os.sched_setaffinity(0, {i for i in range(1000)})```\r\n\r\nThe problem, as near as we can tell, us that once upon a time, cpu affinity was set using a bitmask (\"0xfffff\" and the like), and affinity recently changed to a list of processors rather than to using the mask.  As such, only processors 1 and 17 are shown to be working in htop.\r\n![Selection_072](https:\/\/github.com\/huggingface\/datasets\/assets\/107141022\/04c5a824-5321-4531-afca-7bc84dff36b4)\r\n\r\n\r\nWhen running functions via `map`, the above resetting of affinity works to spread across the cores.  When using `filter`, however, only two cores are active.\n\n### Steps to reproduce the bug\n\nRepro steps:\r\n\r\n1.  Create an aws sagemaker instance\r\n2. use the pytorch 3_10 kernel\r\n3. Load a dataset\r\n4. run a filter operation\r\n5. watch as only 2 cores are used when num_proc > 2\r\n6. run a map operation\r\n7. watch as only 2 cores are used when num_proc > 2\r\n8. run a map operation with processor affinity reset inside the function called via map\r\n9. Watch as all cores run\r\n\r\n\n\n### Expected behavior\n\nAll specified cores are used via the num_proc argument.\n\n### Environment info\n\nAWS sagemaker with the following init script run in the terminal after instance creation:\r\n\r\nconda init bash\r\nbash\r\nconda activate pytorch_p310\r\npip install Wand PyPDF pytesseract datasets seqeval pdfplumber transformers pymupdf sentencepiece timm donut-python accelerate optimum xgboost\r\npython -m pip install 'git+https:\/\/github.com\/facebookresearch\/detectron2.git'\r\nsudo yum -y install htop\r\nsudo yum -y update\r\nsudo yum -y install wget libstdc++ autoconf automake libtool autoconf-archive pkg-config gcc gcc-c++ make libjpeg-devel libpng-devel libtiff-devel zlib-devel","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5981\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5981\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5980","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5980\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5980\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5980\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5980","id":1770255973,"node_id":"I_kwDODunzps5pg_Zl","number":5980,"title":"Viewing dataset card returns \u201c502 Bad Gateway\u201d","user":{"login":"tbenthompson","id":4241811,"node_id":"MDQ6VXNlcjQyNDE4MTE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/4241811?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/tbenthompson","html_url":"https:\/\/github.com\/tbenthompson","followers_url":"https:\/\/api.github.com\/users\/tbenthompson\/followers","following_url":"https:\/\/api.github.com\/users\/tbenthompson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/tbenthompson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/tbenthompson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/tbenthompson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/tbenthompson\/orgs","repos_url":"https:\/\/api.github.com\/users\/tbenthompson\/repos","events_url":"https:\/\/api.github.com\/users\/tbenthompson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/tbenthompson\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-22T19:14:48Z","updated_at":"2023-06-27T08:38:19Z","closed_at":"2023-06-26T14:42:45Z","author_association":"NONE","active_lock_reason":null,"body":"The url is: https:\/\/huggingface.co\/datasets\/Confirm-Labs\/pile_ngrams_trigrams\r\n\r\nI am able to successfully view the \u201cFiles and versions\u201d tab: [Confirm-Labs\/pile_ngrams_trigrams at main](https:\/\/huggingface.co\/datasets\/Confirm-Labs\/pile_ngrams_trigrams\/tree\/main)\r\n\r\nAny help would be appreciated! Thanks! I hope this is the right place to report an issue like this.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5980\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5980\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5979","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5979\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5979\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5979\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5979","id":1770198250,"node_id":"PR_kwDODunzps5TrxS_","number":5979,"title":"set dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-22T18:32:14Z","updated_at":"2023-06-22T18:42:22Z","closed_at":"2023-06-22T18:32:22Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5979\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5979\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5979","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5979","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5979.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5979.patch","merged_at":"2023-06-22T18:32:22Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5978","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5978\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5978\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5978\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5978","id":1770187053,"node_id":"PR_kwDODunzps5Tru2_","number":5978,"title":"Release: 2.13.1","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-22T18:23:11Z","updated_at":"2023-06-22T18:40:24Z","closed_at":"2023-06-22T18:30:16Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5978\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5978\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5978","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5978","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5978.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5978.patch","merged_at":"2023-06-22T18:30:16Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5976","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5976\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5976\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5976\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5976","id":1768503913,"node_id":"PR_kwDODunzps5TmAFp","number":5976,"title":"Avoid stuck map operation when subprocesses crashes","user":{"login":"pappacena","id":1213561,"node_id":"MDQ6VXNlcjEyMTM1NjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1213561?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/pappacena","html_url":"https:\/\/github.com\/pappacena","followers_url":"https:\/\/api.github.com\/users\/pappacena\/followers","following_url":"https:\/\/api.github.com\/users\/pappacena\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/pappacena\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/pappacena\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/pappacena\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/pappacena\/orgs","repos_url":"https:\/\/api.github.com\/users\/pappacena\/repos","events_url":"https:\/\/api.github.com\/users\/pappacena\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/pappacena\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2023-06-21T21:18:31Z","updated_at":"2023-07-10T09:58:39Z","closed_at":"2023-07-10T09:50:07Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I've been using Dataset.map() with `num_proc=os.cpu_count()` to leverage multicore processing for my datasets, but from time to time I get stuck processes waiting forever. Apparently, when one of the subprocesses is abruptly killed (OOM killer, segfault, SIGKILL, etc), the main process keeps waiting for the async task sent to that child process to finish.\r\n\r\nIt seems to be easy to reproduce the issue with the following script:\r\n\r\n```\r\nimport os\r\nfrom datasets import Dataset, Features, Value\r\n\r\n\r\ndef do_stuck(item):\r\n    os.kill(os.getpid(), 9)\r\n\r\ndata = {\r\n    \"col1\": list(range(5)),\r\n    \"col2\": list(range(5)),\r\n}\r\n\r\nds = Dataset.from_dict(\r\n    data,\r\n    features=Features({\r\n        \"col1\": Value(\"int64\"),\r\n        \"col2\": Value(\"int64\"),\r\n    }),\r\n)\r\n\r\nprint(ds.map(do_stuck, num_proc=4))\r\n```\r\n\r\nThis is an old behavior in Python, which apparently was fixed a few years ago in `concurrent.futures.ProcessPoolExecutor` ([ref](https:\/\/bugs.python.org\/issue9205)), but not in `multiprocessing.pool.Pool` \/ `multiprocess.pool.Pool`, which is used by `Dataset.map` ([ref](https:\/\/bugs.python.org\/issue22393)).\r\n\r\nThis PR is an idea to try to detect when a child process gets killed, and raises a `RuntimeError` warning the dataset.map() caller.\r\n\r\nEDIT: Related proposal for future improvement: https:\/\/github.com\/huggingface\/datasets\/discussions\/5977","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5976\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5976\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5976","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5976","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5976.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5976.patch","merged_at":"2023-07-10T09:50:07Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5975","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5975\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5975\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5975\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5975","id":1768271343,"node_id":"I_kwDODunzps5pZa3v","number":5975,"title":"Streaming Dataset behind Proxy - FileNotFoundError","user":{"login":"Veluchs","id":135350576,"node_id":"U_kgDOCBFJMA","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/135350576?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Veluchs","html_url":"https:\/\/github.com\/Veluchs","followers_url":"https:\/\/api.github.com\/users\/Veluchs\/followers","following_url":"https:\/\/api.github.com\/users\/Veluchs\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Veluchs\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Veluchs\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Veluchs\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Veluchs\/orgs","repos_url":"https:\/\/api.github.com\/users\/Veluchs\/repos","events_url":"https:\/\/api.github.com\/users\/Veluchs\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Veluchs\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-06-21T19:10:02Z","updated_at":"2023-06-30T05:55:39Z","closed_at":"2023-06-30T05:55:38Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen trying to stream a dataset i get the following error after a few minutes of waiting.\r\n\r\n```\r\nFileNotFoundError: https:\/\/huggingface.co\/datasets\/facebook\/voxpopuli\/resolve\/main\/data\/n_files.json\r\nIf the repo is private or gated, make sure to log in with `huggingface-cli login`.\r\n```\r\n\r\nI have already set the proxy environment variables. Downloading a Dataset without streaming works as expected.\r\nStill i suspect that this is connected to being behind a proxy.\r\n\r\nIs there a way to set the proxy for streaming datasets? Possibly a keyword argument that gets passed to ffspec?\r\n\r\n### Steps to reproduce the bug\r\n\r\nThis is the code i use.\r\n\r\n```\r\nimport os\r\nos.environ['http_proxy'] = \"http:\/\/example.com:xxxx\" \r\nos.environ['https_proxy'] = \"http:\/\/example.com:xxxx\" \r\n\r\n\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"facebook\/voxpopuli\", name=\"de\", streaming=True)\r\n```\r\n\r\n### Expected behavior\r\n\r\nI would expect the streaming functionality to use the set proxy settings.\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: Linux-5.15.0-73-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5975\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5975\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5974","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5974\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5974\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5974\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5974","id":1767981231,"node_id":"PR_kwDODunzps5TkXCb","number":5974,"title":"Deprecate `errors` param in favor of `encoding_errors` in text builder","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-21T16:31:38Z","updated_at":"2023-06-26T10:34:43Z","closed_at":"2023-06-26T10:27:40Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"For consistency with the JSON builder and Pandas","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5974\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5974\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5974","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5974","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5974.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5974.patch","merged_at":"2023-06-26T10:27:40Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5972","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5972\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5972\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5972\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5972","id":1767897485,"node_id":"PR_kwDODunzps5TkE7K","number":5972,"title":"Filter unsupported extensions","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-06-21T15:43:01Z","updated_at":"2023-06-22T14:23:29Z","closed_at":"2023-06-22T14:16:26Z","author_association":"MEMBER","active_lock_reason":null,"body":"I used a regex to filter the data files based on their extension for packaged builders.\r\n\r\nI tried and a regex is 10x faster that using `in` to check if the extension is in the list of supported extensions.\r\n\r\nSupersedes https:\/\/github.com\/huggingface\/datasets\/pull\/5850\r\n\r\nClose https:\/\/github.com\/huggingface\/datasets\/issues\/5849\r\n\r\nI also did a small change to favor the parquet module in case of a draw in the extension counter.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5972\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5972\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5972","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5972","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5972.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5972.patch","merged_at":"2023-06-22T14:16:26Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5971","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5971\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5971\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5971\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5971","id":1767053635,"node_id":"I_kwDODunzps5pUxlD","number":5971,"title":"Docs: make \"repository structure\" easier to find","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"open","locked":false,"assignee":{"login":"benjaminbrown038","id":35114142,"node_id":"MDQ6VXNlcjM1MTE0MTQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35114142?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/benjaminbrown038","html_url":"https:\/\/github.com\/benjaminbrown038","followers_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/followers","following_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/orgs","repos_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/repos","events_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/received_events","type":"User","site_admin":false},"assignees":[{"login":"benjaminbrown038","id":35114142,"node_id":"MDQ6VXNlcjM1MTE0MTQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35114142?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/benjaminbrown038","html_url":"https:\/\/github.com\/benjaminbrown038","followers_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/followers","following_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/orgs","repos_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/repos","events_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":5,"created_at":"2023-06-21T08:26:44Z","updated_at":"2023-07-05T06:51:38Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The page https:\/\/huggingface.co\/docs\/datasets\/repository_structure explains how to create a simple repository structure without a dataset script.\r\nIt's the simplest way to create a dataset and should be easier to find, particularly on the docs' first pages.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5971\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5971\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5970","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5970\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5970\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5970\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5970","id":1766010356,"node_id":"I_kwDODunzps5pQy30","number":5970,"title":"description disappearing from Info when Uploading a Dataset Created with `from_dict`","user":{"login":"balisujohn","id":20377292,"node_id":"MDQ6VXNlcjIwMzc3Mjky","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20377292?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/balisujohn","html_url":"https:\/\/github.com\/balisujohn","followers_url":"https:\/\/api.github.com\/users\/balisujohn\/followers","following_url":"https:\/\/api.github.com\/users\/balisujohn\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/balisujohn\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/balisujohn\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/balisujohn\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/balisujohn\/orgs","repos_url":"https:\/\/api.github.com\/users\/balisujohn\/repos","events_url":"https:\/\/api.github.com\/users\/balisujohn\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/balisujohn\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-20T19:18:26Z","updated_at":"2023-06-22T14:23:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen uploading a dataset created locally using `from_dict` with a specified `description` field. It appears before upload, but is missing after upload and re-download.\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nI think the most relevant pattern in the code might be the following lines:\r\n\r\n```\r\ndescription_json_str = json.dumps(\r\n    {\r\n        \"dataset_id\": dataset.spec.dataset_id,\r\n        \"env_name\": dataset.spec.env_spec.id,\r\n        \"action_space\": serialize_space(dataset.spec.action_space),\r\n        \"observation_space\": serialize_space(dataset.spec.observation_space),\r\n    }\r\n)\r\n\r\nhugging_face_dataset = Dataset.from_dict(\r\n    episodes_dict, info=DatasetInfo(description=description_json_str)\r\n)\r\n\r\n```\r\nWhich comes from this function https:\/\/github.com\/balisujohn\/minarai\/blob\/8e023727f0a8488c4451651d9f7a79b981412c40\/minari\/integrations\/hugging_face.py#L39\r\n\r\n\r\n\r\nTo replicate,\r\nclone this branch of my Minari fork https:\/\/github.com\/balisujohn\/minarai\/tree\/dev-huggingface then run\r\n\r\n```\r\npython3.8 -m venv env\r\nsource env\/bin\/activate\r\npython3 -m pip install -e .\r\npython3 -m pip install pytest\r\n```\r\n\r\nThe change the hugging face repo path in the test called  `test_hugging_face_push_and_pull_dataset` in `tests\/integrations\/test_hugging_face.py` to one you have permissions to write to.\r\n\r\nThen run:\r\n\r\n```\r\npytest tests\/integrations\/test_hugging_face.py::test_hugging_face_push_and_pull_dataset\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nDATASET INFO BEFORE UPLOADING\r\nDatasetInfo(description='{\"dataset_id\": \"dummy-combo-test-v0\", \"env_name\": \"DummyComboEnv-v0\", \"action_space\": \"{\\\\\"type\\\\\": \\\\\"Tuple\\\\\", \\\\\"subspaces\\\\\": [{\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [2.0], \\\\\"high\\\\\": [3.0]}, {\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [4.0], \\\\\"high\\\\\": [5.0]}]}\", \"observation_space\": \"{\\\\\"type\\\\\": \\\\\"Tuple\\\\\", \\\\\"subspaces\\\\\": [{\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [2.0], \\\\\"high\\\\\": [3.0]}, {\\\\\"type\\\\\": \\\\\"Tuple\\\\\", \\\\\"subspaces\\\\\": [{\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [2.0], \\\\\"high\\\\\": [3.0]}, {\\\\\"type\\\\\": \\\\\"Dict\\\\\", \\\\\"subspaces\\\\\": {\\\\\"component_1\\\\\": {\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [-1.0], \\\\\"high\\\\\": [1.0]}, \\\\\"component_2\\\\\": {\\\\\"type\\\\\": \\\\\"Dict\\\\\", \\\\\"subspaces\\\\\": {\\\\\"subcomponent_1\\\\\": {\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [2.0], \\\\\"high\\\\\": [3.0]}, \\\\\"subcomponent_2\\\\\": {\\\\\"type\\\\\": \\\\\"Tuple\\\\\", \\\\\"subspaces\\\\\": [{\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [4.0], \\\\\"high\\\\\": [5.0]}, {\\\\\"type\\\\\": \\\\\"Discrete\\\\\", \\\\\"dtype\\\\\": \\\\\"int64\\\\\", \\\\\"start\\\\\": 0, \\\\\"n\\\\\": 10}]}}}}}]}]}\"}', citation='', homepage='', license='', features={'observations': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': {'component_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'component_2': {'subcomponent_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'subcomponent_2': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': Value(dtype='int64', id=None)}}}}}, 'actions': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None)}, 'rewards': Value(dtype='int64', id=None), 'truncations': Value(dtype='bool', id=None), 'terminations': Value(dtype='bool', id=None), 'episode_ids': Value(dtype='int64', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, config_name=None, version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)\r\n...\r\nDATASET INFO AFTER UPLOADING AND DOWNLOADING\r\nDatasetInfo(description='', citation='', homepage='', license='', features={'observations': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': {'component_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'component_2': {'subcomponent_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'subcomponent_2': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': Value(dtype='int64', id=None)}}}}}, 'actions': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None)}, 'rewards': Value(dtype='int64', id=None), 'truncations': Value(dtype='bool', id=None), 'terminations': Value(dtype='bool', id=None), 'episode_ids': Value(dtype='int64', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, config_name=None, version=None, splits={'train': SplitInfo(name='train', num_bytes=4846, num_examples=60, shard_lengths=None, dataset_name='parquet')}, download_checksums={'https:\/\/huggingface.co\/datasets\/balisujohn\/minari_test\/resolve\/8217b614ff9ba5edc1a30c7df430e92a46f65363\/data\/train-00000-of-00001-7c5900b93b35745e.parquet': {'num_bytes': 9052, 'checksum': None}}, download_size=9052, post_processing_size=None, dataset_size=4846, size_in_bytes=13898)\r\n...\r\n\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: Linux-5.15.0-75-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5970\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5970\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5969","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5969\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5969\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5969\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5969","id":1765529905,"node_id":"PR_kwDODunzps5Tcgq4","number":5969,"title":"Add `encoding` and `errors` params to JSON loader","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-20T14:28:35Z","updated_at":"2023-06-21T13:39:50Z","closed_at":"2023-06-21T13:32:22Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"\"Requested\" in https:\/\/discuss.huggingface.co\/t\/utf-16-for-datasets\/43828\/3.\r\n\r\n`pd.read_json` also has these parameters, so it makes sense to be consistent.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5969\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5969\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5969","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5969","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5969.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5969.patch","merged_at":"2023-06-21T13:32:22Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5968","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5968\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5968\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5968\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5968","id":1765252561,"node_id":"I_kwDODunzps5pN53R","number":5968,"title":"Common Voice datasets still need `use_auth_token=True`","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-20T11:58:37Z","updated_at":"2023-07-29T16:08:59Z","closed_at":"2023-07-29T16:08:58Z","author_association":"MEMBER","active_lock_reason":null,"body":"### Describe the bug\n\nWe don't need to pass `use_auth_token=True` anymore to download gated datasets or models, so the following should work if correctly logged in.\r\n\r\n```py\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"mozilla-foundation\/common_voice_6_1\", \"tr\", split=\"train+validation\")\r\n```\r\n\r\nHowever it throws an error - probably because something weird is hardcoded into the dataset loading script.\n\n### Steps to reproduce the bug\n\n1.) \r\n```\r\nhuggingface-cli login\r\n```\r\n\r\n2.) Make sure that you have accepted the license here:\r\nhttps:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_6_1\r\n\r\n3.) Run:\r\n```py\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"mozilla-foundation\/common_voice_6_1\", \"tr\", split=\"train+validation\")\r\n```\r\n\r\n4.)  You'll get:\r\n\r\n```\r\nFile ~\/hf\/lib\/python3.10\/site-packages\/datasets\/builder.py:963, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n    961 split_dict = SplitDict(dataset_name=self.name)\r\n    962 split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 963 split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    965 # Checksums verification\r\n    966 if verification_mode == VerificationMode.ALL_CHECKS and dl_manager.record_checksums:\r\n\r\nFile ~\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/mozilla-foundation--common_voice_6_1\/f4d7854c466f5bd4908988dbd39044ec4fc634d89e0515ab0c51715c0127ffe3\/common_voice_6_1.py:150, in CommonVoice._split_generators(self, dl_manager)\r\n    148 hf_auth_token = dl_manager.download_config.use_auth_token\r\n    149 if hf_auth_token is None:\r\n--> 150     raise ConnectionError(\r\n    151         \"Please set use_auth_token=True or use_auth_token='<TOKEN>' to download this dataset\"\r\n    152     )\r\n    154 bundle_url_template = STATS[\"bundleURLTemplate\"]\r\n    155 bundle_version = bundle_url_template.split(\"\/\")[0]\r\n\r\nConnectionError: Please set use_auth_token=True or use_auth_token='<TOKEN>' to download this dataset\r\n```\n\n### Expected behavior\n\nOne should not have to pass `use_auth_token=True`. Also see discussion here: https:\/\/github.com\/huggingface\/blog\/pull\/1243#discussion_r1235131150\n\n### Environment info\n\n```\r\n- `datasets` version: 2.13.0\r\n- Platform: Linux-6.2.0-76060200-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.16.0.dev0\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5968\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5968\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5967","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5967\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5967\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5967\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5967","id":1763926520,"node_id":"I_kwDODunzps5pI2H4","number":5967,"title":"Config name \/ split name lost after map with multiproc","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-19T17:27:36Z","updated_at":"2023-06-28T08:55:25Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nPerforming a `.map` method on a dataset loses it's config name \/ split name only if run with multiproc\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Audio, load_dataset\r\nfrom transformers import AutoFeatureExtractor\r\nimport numpy as np\r\n\r\n# load dummy dataset\r\nlibri = load_dataset(\"hf-internal-testing\/librispeech_asr_dummy\", \"clean\")\r\n\r\n# make train \/ test splits\r\nlibri = libri[\"validation\"].train_test_split(seed=42, shuffle=True, test_size=0.1)\r\n\r\n# example feature extractor\r\nmodel_id = \"ntu-spml\/distilhubert\"\r\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_id, do_normalize=True, return_attention_mask=True)\r\n\r\nsampling_rate = feature_extractor.sampling_rate\r\n\r\nlibri = libri.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\r\n\r\nmax_duration = 30.0\r\n\r\ndef preprocess_function(examples):\r\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\r\n    inputs = feature_extractor(\r\n        audio_arrays,\r\n        sampling_rate=feature_extractor.sampling_rate,\r\n        max_length=int(feature_extractor.sampling_rate * max_duration),\r\n        truncation=True,\r\n        return_attention_mask=True,\r\n    )\r\n    return inputs\r\n\r\n# single proc map\r\nlibri_encoded = libri.map(\r\n    preprocess_function, remove_columns=[\"audio\", \"file\"], batched=True, num_proc=1\r\n)\r\n\r\nprint(10 * \"=\" ,\"Single processing\", 10 * \"=\")\r\nprint(\"Config name before: \", libri[\"train\"].config_name, \" Split name before: \", libri[\"train\"].split)\r\nprint(\"Config name after: \", libri_encoded[\"train\"].config_name, \" Split name after: \", libri_encoded[\"train\"].split)\r\n\r\n# multi proc map\r\nlibri_encoded = libri.map(\r\n    preprocess_function, remove_columns=[\"audio\", \"file\"], batched=True, num_proc=2\r\n)\r\n\r\nprint(10 * \"=\" ,\"Multi processing\", 10 * \"=\")\r\nprint(\"Config name before: \", libri[\"train\"].config_name, \" Split name before: \", libri[\"train\"].split)\r\nprint(\"Config name after: \", libri_encoded[\"train\"].config_name, \" Split name after: \", libri_encoded[\"train\"].split)\r\n```\r\n\r\n**Print Output:**\r\n```\r\n========== Single processing ==========\r\nConfig name before:  clean  Split name before:  validation\r\nConfig name after:  clean  Split name after:  validation\r\n========== Multi processing ==========\r\nConfig name before:  clean  Split name before:  validation\r\nConfig name after:  None  Split name after:  None\r\n```\r\n\r\n=> we can see that the config\/split names are lost in the multiprocessing setting\r\n\r\n\r\n\n\n### Expected behavior\n\nShould retain both config \/ split names in the multiproc setting\n\n### Environment info\n\n- `datasets` version: 2.13.1.dev0\r\n- Platform: Linux-5.15.0-67-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5967\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5967\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5966","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5966\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5966\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5966\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5966","id":1763885914,"node_id":"PR_kwDODunzps5TXBLP","number":5966,"title":"Fix JSON generation in benchmarks CI","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-19T16:56:06Z","updated_at":"2023-06-19T17:29:11Z","closed_at":"2023-06-19T17:22:10Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Related to changes made in https:\/\/github.com\/iterative\/dvc\/pull\/9475","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5966\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5966\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5966","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5966","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5966.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5966.patch","merged_at":"2023-06-19T17:22:10Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5965","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5965\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5965\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5965\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5965","id":1763648540,"node_id":"I_kwDODunzps5pHyQc","number":5965,"title":"\"Couldn't cast array of type\" in complex datasets","user":{"login":"piercefreeman","id":1712066,"node_id":"MDQ6VXNlcjE3MTIwNjY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1712066?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/piercefreeman","html_url":"https:\/\/github.com\/piercefreeman","followers_url":"https:\/\/api.github.com\/users\/piercefreeman\/followers","following_url":"https:\/\/api.github.com\/users\/piercefreeman\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/piercefreeman\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/piercefreeman\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/piercefreeman\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/piercefreeman\/orgs","repos_url":"https:\/\/api.github.com\/users\/piercefreeman\/repos","events_url":"https:\/\/api.github.com\/users\/piercefreeman\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/piercefreeman\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2023-06-19T14:16:14Z","updated_at":"2023-07-26T15:13:53Z","closed_at":"2023-07-26T15:13:53Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen doing a map of a dataset with complex types, sometimes `datasets` is unable to interpret the valid schema of a returned datasets.map() function. This often comes from conflicting types, like when both empty lists and filled lists are competing for the same field value.\r\n\r\nThis is prone to happen in batch mapping, when the mapper returns a sequence of null\/empty values and other batches are non-null. A workaround is to manually cast the new batch to a pyarrow table (like implemented in this [workaround](https:\/\/github.com\/piercefreeman\/lassen\/pull\/3)) but it feels like this ideally should be solved at the core library level.\r\n\r\nNote that the reproduction case only throws this error if the first datapoint has the empty list. If it is processed later, datasets already detects its representation as list-type and therefore allows the empty list to be provided.\n\n### Steps to reproduce the bug\n\nA trivial reproduction case:\r\n\r\n```python\r\nfrom typing import Iterator, Any\r\nimport pandas as pd\r\nfrom datasets import Dataset\r\n\r\ndef batch_to_examples(batch: dict[str, list[Any]]) -> Iterator[dict[str, Any]]:\r\n    for i in range(next(iter(lengths))):\r\n        yield {feature: values[i] for feature, values in batch.items()}\r\n\r\ndef examples_to_batch(examples) -> dict[str, list[Any]]:\r\n    batch = {}\r\n\r\n    for example in examples:\r\n        for feature, value in example.items():\r\n            if feature not in batch:\r\n                batch[feature] = []\r\n            batch[feature].append(value)\r\n\r\n    return batch\r\n\r\ndef batch_process(examples, explicit_schema: bool):\r\n    new_examples = []\r\n    for example in batch_to_examples(examples):\r\n        new_examples.append(dict(texts=example[\"raw_text\"].split()))\r\n    return examples_to_batch(new_examples)\r\n\r\ndf = pd.DataFrame(\r\n    [\r\n        {\"raw_text\": \"\"},\r\n        {\"raw_text\": \"This is a test\"},\r\n        {\"raw_text\": \"This is another test\"},\r\n    ]\r\n)\r\n\r\ndataset = Dataset.from_pandas(df)\r\n\r\n# datasets won't be able to typehint a dataset that starts with an empty example.\r\nwith pytest.raises(TypeError, match=\"Couldn't cast array of type\"):\r\n    dataset = dataset.map(\r\n        batch_process,\r\n        batched=True,\r\n        batch_size=1,\r\n        num_proc=1,\r\n        remove_columns=dataset.column_names,\r\n    )\r\n```\r\n\r\nThis results in crashes like:\r\n\r\n```bash\r\n  File \"\/Users\/piercefreeman\/Library\/Caches\/pypoetry\/virtualenvs\/example-9kBqeSPy-py3.11\/lib\/python3.11\/site-packages\/datasets\/table.py\", line 1819, in wrapper\r\n    return func(array, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/piercefreeman\/Library\/Caches\/pypoetry\/virtualenvs\/example-9kBqeSPy-py3.11\/lib\/python3.11\/site-packages\/datasets\/table.py\", line 2109, in cast_array_to_feature\r\n    return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/piercefreeman\/Library\/Caches\/pypoetry\/virtualenvs\/example-9kBqeSPy-py3.11\/lib\/python3.11\/site-packages\/datasets\/table.py\", line 1819, in wrapper\r\n    return func(array, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/piercefreeman\/Library\/Caches\/pypoetry\/virtualenvs\/example-9kBqeSPy-py3.11\/lib\/python3.11\/site-packages\/datasets\/table.py\", line 1998, in array_cast\r\n    raise TypeError(f\"Couldn't cast array of type {array.type} to {pa_type}\")\r\nTypeError: Couldn't cast array of type string to null\r\n```\n\n### Expected behavior\n\nThe code should successfully map and create a new dataset without error.\n\n### Environment info\n\nMac OSX, Linux","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5965\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5965\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5964","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5964\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5964\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5964\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5964","id":1763513574,"node_id":"PR_kwDODunzps5TVweZ","number":5964,"title":"Always return list in `list_datasets`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-19T13:07:08Z","updated_at":"2023-06-19T17:29:37Z","closed_at":"2023-06-19T17:22:41Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #5925 \r\n\r\nPlus, deprecate `list_datasets`\/`inspect_dataset` in favor of `huggingface_hub.list_datasets`\/\"git clone workflow\" (downloads data files)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5964\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5964\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5964","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5964","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5964.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5964.patch","merged_at":"2023-06-19T17:22:41Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5963","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5963\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5963\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5963\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5963","id":1762774457,"node_id":"I_kwDODunzps5pEc25","number":5963,"title":"Got an error  _pickle.PicklingError use Dataset.from_spark.","user":{"login":"yanzia12138","id":112800614,"node_id":"U_kgDOBrkzZg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/112800614?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yanzia12138","html_url":"https:\/\/github.com\/yanzia12138","followers_url":"https:\/\/api.github.com\/users\/yanzia12138\/followers","following_url":"https:\/\/api.github.com\/users\/yanzia12138\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yanzia12138\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yanzia12138\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yanzia12138\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yanzia12138\/orgs","repos_url":"https:\/\/api.github.com\/users\/yanzia12138\/repos","events_url":"https:\/\/api.github.com\/users\/yanzia12138\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yanzia12138\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-06-19T05:30:35Z","updated_at":"2023-07-24T11:55:46Z","closed_at":"2023-07-24T11:55:46Z","author_association":"NONE","active_lock_reason":null,"body":"              python 3.9.2\r\nGot an error  _pickle.PicklingError use Dataset.from_spark.\r\n\r\nDid the dataset import load data from spark dataframe using multi-node Spark cluster\r\ndf = spark.read.parquet(args.input_data).repartition(50)\r\nds = Dataset.from_spark(df, keep_in_memory=True,\r\n                        cache_dir=\"\/pnc-data\/data\/nuplan\/t5_spark\/cache_data\")\r\nds.save_to_disk(args.output_data)\r\n\r\nError : \r\n_pickle.PicklingError: Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforma\r\ntion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\r\n23\/06\/16 21:17:20 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\r\n\r\n_Originally posted by @yanzia12138 in https:\/\/github.com\/huggingface\/datasets\/issues\/5701#issuecomment-1594674306_\r\n \r\nW\r\nTraceback (most recent call last):\r\n  File \"\/home\/work\/main.py\", line 100, in <module>\r\n    run(args)\r\n  File \"\/home\/work\/main.py\", line 80, in run\r\n    ds = Dataset.from_spark(df1, keep_in_memory=True,\r\n  File \"\/home\/work\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 1281, in from_spark\r\n    return SparkDatasetReader(\r\n  File \"\/home\/work\/.local\/lib\/python3.9\/site-packages\/datasets\/io\/spark.py\", line 53, in read\r\n    self.builder.download_and_prepare(\r\n  File \"\/home\/work\/.local\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 909, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/work\/.local\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 1004, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/home\/work\/.local\/lib\/python3.9\/site-packages\/datasets\/packaged_modules\/spark\/spark.py\", line 254, in _prepare_split\r\n    self._validate_cache_dir()\r\n  File \"\/home\/work\/.local\/lib\/python3.9\/site-packages\/datasets\/packaged_modules\/spark\/spark.py\", line 122, in _validate_cache_dir\r\n    self._spark.sparkContext.parallelize(range(1), 1).mapPartitions(create_cache_and_write_probe).collect()\r\n  File \"\/home\/work\/.local\/lib\/python3.9\/site-packages\/pyspark\/rdd.py\", line 950, in collect\r\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\r\n  File \"\/home\/work\/.local\/lib\/python3.9\/site-packages\/pyspark\/rdd.py\", line 2951, in _jrdd\r\n    wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\r\n  File \"\/home\/work\/.local\/lib\/python3.9\/site-packages\/pyspark\/rdd.py\", line 2830, in _wrap_function\r\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\r\n  File \"\/home\/work\/.local\/lib\/python3.9\/site-packages\/pyspark\/rdd.py\", line 2816, in _prepare_for_python_RDD\r\n    pickled_command = ser.dumps(command)\r\n  File \"\/home\/work\/.local\/lib\/python3.9\/site-packages\/pyspark\/serializers.py\", line 447, in dumps\r\n    raise pickle.PicklingError(msg)\r\n_pickle.PicklingError: Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. S\r\nparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\r\n23\/06\/19 13:51:21 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5963\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5963\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5962","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5962\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5962\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5962\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5962","id":1761589882,"node_id":"I_kwDODunzps5o_7p6","number":5962,"title":"Issue with train_test_split maintaining the same underlying PyArrow Table","user":{"login":"Oziel14","id":70730520,"node_id":"MDQ6VXNlcjcwNzMwNTIw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/70730520?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Oziel14","html_url":"https:\/\/github.com\/Oziel14","followers_url":"https:\/\/api.github.com\/users\/Oziel14\/followers","following_url":"https:\/\/api.github.com\/users\/Oziel14\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Oziel14\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Oziel14\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Oziel14\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Oziel14\/orgs","repos_url":"https:\/\/api.github.com\/users\/Oziel14\/repos","events_url":"https:\/\/api.github.com\/users\/Oziel14\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Oziel14\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-06-17T02:19:58Z","updated_at":"2023-06-17T02:19:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI've been using the train_test_split method in the datasets module to split my HuggingFace Dataset into separate training, validation, and testing subsets. However, I've noticed an issue where the split datasets appear to maintain the same underlying PyArrow Table.\n\n### Steps to reproduce the bug\n\n1. Load any dataset  ```dataset = load_dataset(\"lhoestq\/demo1\")``` \r\n2. Try the next code:\r\n```python\r\nfrom datasets import Dataset, DatasetDict\r\n\r\ntrain_size = 0.6\r\n\r\nsplit_train = dataset[\"train\"].train_test_split(\r\n    train_size=train_size,\r\n)\r\n\r\nseparate_dataset_dict = DatasetDict({\r\n    \"train\": split_train[\"train\"],\r\n    \"test\": split_train[\"test\"],\r\n})\r\n```\r\n3.  The next code ```print(separate_dataset_dict)``` when printing the dataset it gives the indication that they have 3 and 2 rows respectively.\r\n4. But the next code: \r\n ```python\r\nprint(len(separate_dataset_dict[\"train\"].data['id']))\r\nprint(len(separate_dataset_dict[\"test\"].data['id'])) \r\n```\r\n\r\n Indicates that both tables still have 5 rows.\n\n### Expected behavior\n\nHowever, I've noticed that train_test_split[\"train\"].data, test_val_split[\"train\"].data, and test_val_split[\"test\"].data are identical, suggesting that they all point to the same underlying PyArrow Table. This means that the split datasets are not independent, as I expected.\r\n\r\nI believe this is a bug in the train_test_split implementation, as I would expect this function to return datasets with separate underlying PyArrow Tables. Could you please help me understand if this is expected behavior, or if there's a workaround to create truly independent split datasets?\r\n\r\nI would appreciate any assistance with this issue. Thank you.\n\n### Environment info\n\nI tried in Colab:\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1\r\n\r\nand my PC:\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: Linux-5.15.107+-x86_64-with-glibc2.31\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5962\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5962\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5961","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5961\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5961\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5961\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5961","id":1758525111,"node_id":"I_kwDODunzps5o0Pa3","number":5961,"title":"IterableDataset: split by node and map may preprocess samples that will be skipped anyway","user":{"login":"johnchienbronci","id":27708347,"node_id":"MDQ6VXNlcjI3NzA4MzQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/27708347?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/johnchienbronci","html_url":"https:\/\/github.com\/johnchienbronci","followers_url":"https:\/\/api.github.com\/users\/johnchienbronci\/followers","following_url":"https:\/\/api.github.com\/users\/johnchienbronci\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/johnchienbronci\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/johnchienbronci\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/johnchienbronci\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/johnchienbronci\/orgs","repos_url":"https:\/\/api.github.com\/users\/johnchienbronci\/repos","events_url":"https:\/\/api.github.com\/users\/johnchienbronci\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/johnchienbronci\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-06-15T10:29:10Z","updated_at":"2023-09-01T10:35:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"              There are two ways an iterable dataset can be split by node:\r\n1. if the number of shards is a factor of number of GPUs: in that case the shards are evenly distributed per GPU\r\n2. otherwise, each GPU iterate on the data and at the end keeps 1 sample out of n(GPUs) - skipping the others.\r\n\r\nIn case 2. it's therefore possible to have the same examples passed to `prepare_dataset` for each GPU.\r\n\r\nThis doesn't sound optimized though, because it runs the preprocessing on samples that won't be used in the end.\r\n\r\nCould you open a new issue so that we can discuss about this and find a solution ?\r\n\r\n_Originally posted by @lhoestq in https:\/\/github.com\/huggingface\/datasets\/issues\/5360#issuecomment-1592729051_\r\n            ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5961\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5961\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5959","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5959\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5959\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5959\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5959","id":1757397507,"node_id":"I_kwDODunzps5ov8ID","number":5959,"title":"read metric glue.py from local file ","user":{"login":"JiazhaoLi","id":31148397,"node_id":"MDQ6VXNlcjMxMTQ4Mzk3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/31148397?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/JiazhaoLi","html_url":"https:\/\/github.com\/JiazhaoLi","followers_url":"https:\/\/api.github.com\/users\/JiazhaoLi\/followers","following_url":"https:\/\/api.github.com\/users\/JiazhaoLi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/JiazhaoLi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/JiazhaoLi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/JiazhaoLi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/JiazhaoLi\/orgs","repos_url":"https:\/\/api.github.com\/users\/JiazhaoLi\/repos","events_url":"https:\/\/api.github.com\/users\/JiazhaoLi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/JiazhaoLi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-06-14T17:59:35Z","updated_at":"2023-06-14T18:04:16Z","closed_at":"2023-06-14T18:04:16Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nCurrently, The server is off-line.  I am using the glue metric from the local file downloaded from the hub. \r\nI download \/ cached datasets using `load_dataset('glue','sst2', cache_dir='\/xxx')` to cache them and then in the off-line mode, I use `load_dataset('xxx\/glue.py','sst2', cache_dir='\/xxx')`. I can successfully reuse cached datasets.\r\n\r\nMy problem is about the load_metric. \r\nWhen I run `load_dataset('xxx\/glue_metric.py','sst2',cache_dir='\/xxx')` , it returns \r\n\r\n`  File \"xx\/lib64\/python3.9\/site-packages\/datasets\/utils\/deprecation_utils.py\", line 46, in wrapper\r\n    return deprecated_function(*args, **kwargs)\r\n  File \"xx\/\/lib64\/python3.9\/site-packages\/datasets\/load.py\", line 1392, in load_metric\r\n    metric = metric_cls(\r\nTypeError: 'NoneType' object is not callable`\r\n\r\nThanks in advance for help! \r\n### Steps to reproduce the bug\r\n\r\nN\/A\r\n\r\n### Expected behavior\r\n\r\nN\/A\r\n\r\n### Environment info\r\n\r\n`datasets == 2.12.0`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5959\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5959\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5958","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5958\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5958\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5958\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5958","id":1757265971,"node_id":"PR_kwDODunzps5TA3__","number":5958,"title":"set dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-14T16:26:34Z","updated_at":"2023-06-14T16:34:55Z","closed_at":"2023-06-14T16:26:51Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5958\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5958\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5958","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5958","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5958.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5958.patch","merged_at":"2023-06-14T16:26:51Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5957","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5957\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5957\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5957\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5957","id":1757252466,"node_id":"PR_kwDODunzps5TA1EB","number":5957,"title":"Release: 2.13.0","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-14T16:17:26Z","updated_at":"2023-06-14T16:33:39Z","closed_at":"2023-06-14T16:24:39Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5957\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5957\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5957","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5957","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5957.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5957.patch","merged_at":"2023-06-14T16:24:39Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5956","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5956\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5956\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5956\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5956","id":1756959367,"node_id":"PR_kwDODunzps5S_1o2","number":5956,"title":"Fix ArrowExamplesIterable.shard_data_sources","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-14T13:50:38Z","updated_at":"2023-06-14T14:43:12Z","closed_at":"2023-06-14T14:33:45Z","author_association":"MEMBER","active_lock_reason":null,"body":"ArrowExamplesIterable.shard_data_sources was outdated\r\n\r\nI also fixed a warning message by not using format_type= in with_format()","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5956\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5956\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5956","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5956","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5956.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5956.patch","merged_at":"2023-06-14T14:33:45Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5955","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5955\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5955\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5955\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5955","id":1756827133,"node_id":"I_kwDODunzps5otw39","number":5955,"title":"Strange bug in loading local JSON files, using load_dataset","user":{"login":"Night-Quiet","id":73934131,"node_id":"MDQ6VXNlcjczOTM0MTMx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/73934131?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Night-Quiet","html_url":"https:\/\/github.com\/Night-Quiet","followers_url":"https:\/\/api.github.com\/users\/Night-Quiet\/followers","following_url":"https:\/\/api.github.com\/users\/Night-Quiet\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Night-Quiet\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Night-Quiet\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Night-Quiet\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Night-Quiet\/orgs","repos_url":"https:\/\/api.github.com\/users\/Night-Quiet\/repos","events_url":"https:\/\/api.github.com\/users\/Night-Quiet\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Night-Quiet\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-14T12:46:00Z","updated_at":"2023-06-21T14:42:15Z","closed_at":"2023-06-21T14:42:15Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am using 'load_dataset 'loads a JSON file, but I found a strange bug: an error will be reported when the length of the JSON file exceeds 160000 (uncertain exact number). I have checked the data through the following code and there are no issues. So I cannot determine the true reason for this error. \r\n\r\nThe data is a list containing a dictionary. As follows: \r\n\r\n[\r\n{'input': 'someting...', 'target': 'someting...', 'type': 'someting...', 'history': ['someting...', ...]}, \r\n...\r\n]\n\n### Steps to reproduce the bug\n\n```\r\nimport json\r\nfrom datasets import load_dataset\r\n\r\npath = \"target.json\"\r\ntemp_path = \"temp.json\"\r\n\r\nwith open(path, \"r\") as f:\r\n    data = json.load(f)\r\n    print(f\"\\n-------the JSON file length is: {len(data)}-------\\n\")\r\n\r\nwith open(temp_path, \"w\") as f:\r\n    json.dump(data[:160000], f)\r\ndataset = load_dataset(\"json\", data_files=temp_path)\r\nprint(\"\\n-------This works when the JSON file length is 160000-------\\n\")\r\n\r\nwith open(temp_path, \"w\") as f:\r\n    json.dump(data[160000:], f)\r\ndataset = load_dataset(\"json\", data_files=temp_path)\r\nprint(\"\\n-------This works and eliminates data issues-------\\n\")\r\n\r\nwith open(temp_path, \"w\") as f:\r\n    json.dump(data[:170000], f)\r\ndataset = load_dataset(\"json\", data_files=temp_path)\r\n```\n\n### Expected behavior\n\n```\r\n-------the JSON file length is: 173049-------\r\n\r\nDownloading and preparing dataset json\/default to \/root\/.cache\/huggingface\/datasets\/json\/default-acf3c7f418c5f4b4\/0.0.0\/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 3328.81it\/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 639.47it\/s]\r\nDataset json downloaded and prepared to \/root\/.cache\/huggingface\/datasets\/json\/default-acf3c7f418c5f4b4\/0.0.0\/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 265.85it\/s]\r\n\r\n-------This works when the JSON file length is 160000-------\r\n\r\nDownloading and preparing dataset json\/default to \/root\/.cache\/huggingface\/datasets\/json\/default-a42f04b263ceea6a\/0.0.0\/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 2038.05it\/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 794.83it\/s]\r\nDataset json downloaded and prepared to \/root\/.cache\/huggingface\/datasets\/json\/default-a42f04b263ceea6a\/0.0.0\/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 681.00it\/s]\r\n\r\n-------This works and eliminates data issues-------\r\n\r\nDownloading and preparing dataset json\/default to \/root\/.cache\/huggingface\/datasets\/json\/default-63f391c89599c7b0\/0.0.0\/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 3682.44it\/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 788.70it\/s]\r\nGenerating train split: 0 examples [00:00, ? examples\/s]Failed to read file '\/home\/lakala\/hjc\/code\/pycode\/glm\/temp.json' with error <class 'pyarrow.lib.ArrowInvalid'>: cannot mix list and non-list, non-null values\r\nTraceback (most recent call last):\r\n  File \"\/home\/lakala\/conda\/envs\/glm\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 1858, in _prepare_split_single\r\n    for _, table in generator:\r\n  File \"\/home\/lakala\/conda\/envs\/glm\/lib\/python3.8\/site-packages\/datasets\/packaged_modules\/json\/json.py\", line 146, in _generate_tables\r\n    raise ValueError(f\"Not able to read records in the JSON file at {file}.\") from None\r\nValueError: Not able to read records in the JSON file at \/home\/lakala\/hjc\/code\/pycode\/glm\/temp.json.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/lakala\/hjc\/code\/pycode\/glm\/test.py\", line 22, in <module>\r\n    dataset = load_dataset(\"json\", data_files=temp_path)\r\n  File \"\/home\/lakala\/conda\/envs\/glm\/lib\/python3.8\/site-packages\/datasets\/load.py\", line 1797, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/lakala\/conda\/envs\/glm\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 890, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/lakala\/conda\/envs\/glm\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 985, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/home\/lakala\/conda\/envs\/glm\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 1746, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"\/home\/lakala\/conda\/envs\/glm\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 1891, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\n\n### Environment info\n\n```\r\nUbuntu==22.04\r\n\r\npython==3.8\r\n\r\npytorch-transformers==1.2.0\r\ntransformers== 4.27.1\r\ndatasets==2.12.0\r\nnumpy==1.24.3\r\npandas==1.5.3\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5955\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5955\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5954","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5954\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5954\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5954\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5954","id":1756572994,"node_id":"PR_kwDODunzps5S-hSP","number":5954,"title":"Better filenotfound for gated","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-14T10:33:10Z","updated_at":"2023-06-14T12:33:27Z","closed_at":"2023-06-14T12:26:31Z","author_association":"MEMBER","active_lock_reason":null,"body":"close https:\/\/github.com\/huggingface\/datasets\/issues\/5953\r\n\r\n<img width=\"1292\" alt=\"image\" src=\"https:\/\/github.com\/huggingface\/datasets\/assets\/42851186\/270fe5bc-1739-4878-b7bc-ab6d35336d4d\">\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5954\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5954\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5954","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5954","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5954.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5954.patch","merged_at":"2023-06-14T12:26:31Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5953","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5953\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5953\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5953\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5953","id":1756520523,"node_id":"I_kwDODunzps5osmBL","number":5953,"title":"Bad error message when trying to download gated dataset","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/23423619?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/patrickvonplaten","html_url":"https:\/\/github.com\/patrickvonplaten","followers_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/followers","following_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/orgs","repos_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/repos","events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/patrickvonplaten\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-06-14T10:03:39Z","updated_at":"2023-06-14T16:36:51Z","closed_at":"2023-06-14T12:26:32Z","author_association":"MEMBER","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I attempt to download a model from the Hub that is gated without being logged in, I get a nice error message. E.g.:\r\n\r\nE.g.\r\n```sh\r\nRepository Not Found for url: https:\/\/huggingface.co\/api\/models\/DeepFloyd\/IF-I-XL-v1.0.\r\nPlease make sure you specified the correct `repo_id` and `repo_type`.\r\nIf you are trying to access a private or gated repo, make sure you are authenticated.\r\nInvalid username or password..\r\nWill try to load from local cache.\r\n```\r\n\r\nIf I do the same for a gated dataset on the Hub, I'm not gated a nice error message IMO:\r\n\r\n```sh\r\nFile ~\/hf\/lib\/python3.10\/site-packages\/fsspec\/implementations\/http.py:430, in HTTPFileSystem._info(self, url, **kwargs)\r\n    427     except Exception as exc:\r\n    428         if policy == \"get\":\r\n    429             # If get failed, then raise a FileNotFoundError\r\n--> 430             raise FileNotFoundError(url) from exc\r\n    431         logger.debug(str(exc))\r\n    433 return {\"name\": url, \"size\": None, **info, \"type\": \"file\"}\r\n\r\nFileNotFoundError: https:\/\/huggingface.co\/datasets\/mozilla-foundation\/common_voice_13_0\/resolve\/main\/n_shards.json\r\n```\n\n### Steps to reproduce the bug\n\n```\r\nhuggingface-cli logout\r\n```\r\n\r\nand then:\r\n\r\n```py\r\nfrom datasets import load_dataset, Audio\r\n\r\n# English\r\nstream_data = load_dataset(\"mozilla-foundation\/common_voice_13_0\", \"en\", split=\"test\", streaming=True)\r\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\r\nen_sample = next(iter(stream_data))[\"audio\"][\"array\"]\r\n\r\n# Swahili\r\nstream_data = load_dataset(\"mozilla-foundation\/common_voice_13_0\", \"sw\", split=\"test\", streaming=True)\r\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\r\nsw_sample = next(iter(stream_data))[\"audio\"][\"array\"]\r\n```\n\n### Expected behavior\n\nBetter error message\n\n### Environment info\n\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-6.2.0-76060200-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.16.0.dev0\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5953\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5953\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5952","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5952\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5952\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5952\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5952","id":1756481591,"node_id":"PR_kwDODunzps5S-OIh","number":5952,"title":"Add Arrow builder docs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-14T09:42:46Z","updated_at":"2023-06-14T14:42:31Z","closed_at":"2023-06-14T14:34:39Z","author_association":"MEMBER","active_lock_reason":null,"body":"following https:\/\/github.com\/huggingface\/datasets\/pull\/5944","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5952\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5952\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5952","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5952","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5952.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5952.patch","merged_at":"2023-06-14T14:34:39Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5951","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5951\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5951\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5951\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5951","id":1756363546,"node_id":"I_kwDODunzps5or_sa","number":5951,"title":"What is the Right way to use discofuse dataset??","user":{"login":"akesh1235","id":125154243,"node_id":"U_kgDOB3Wzww","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/125154243?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/akesh1235","html_url":"https:\/\/github.com\/akesh1235","followers_url":"https:\/\/api.github.com\/users\/akesh1235\/followers","following_url":"https:\/\/api.github.com\/users\/akesh1235\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/akesh1235\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/akesh1235\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/akesh1235\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/akesh1235\/orgs","repos_url":"https:\/\/api.github.com\/users\/akesh1235\/repos","events_url":"https:\/\/api.github.com\/users\/akesh1235\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/akesh1235\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-14T08:38:39Z","updated_at":"2023-06-14T13:25:06Z","closed_at":"2023-06-14T12:10:16Z","author_association":"NONE","active_lock_reason":null,"body":"[Click here for Dataset link](https:\/\/huggingface.co\/datasets\/discofuse\/viewer\/discofuse-wikipedia\/train?row=6)\r\n**Below is the following way, as per my understanding , Is it correct :question: :question:**\r\n\r\nThe **columns\/features from `DiscoFuse dataset`** that will be the **input to the `encoder` and `decoder`** are:\r\n\r\n[Click here for Dataset link](https:\/\/huggingface.co\/datasets\/discofuse\/viewer\/discofuse-wikipedia\/train?row=6)\r\n\r\n1. **coherent_first_sentence**\r\n\r\n2. **coherent_second_sentence**\r\n\r\n3. **incoherent_first_sentence**\r\n\r\n4. **incoherent_second_sentence**\r\n\r\n[Click here for Dataset link](https:\/\/huggingface.co\/datasets\/discofuse\/viewer\/discofuse-wikipedia\/train?row=6)\r\n\r\nThe **`encoder` will take these four columns as input and encode them into a sequence of hidden states. The `decoder` will then take these hidden states as input and decode them into a new sentence that fuses the two original sentences together.**\r\n\r\nThe **discourse type, connective_string, has_coref_type_pronoun, and has_coref_type_nominal columns will not be used as input to the encoder or decoder.** These columns are used to provide additional information about the dataset, but they are not necessary for the task of sentence fusion.\r\n\r\nPlease correct me if I am wrong; otherwise, if this understanding is right, how shall I implement this task practically?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5951\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5951\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5950","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5950\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5950\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5950\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5950","id":1755197946,"node_id":"I_kwDODunzps5onjH6","number":5950,"title":"Support for data with instance-wise dictionary as features","user":{"login":"richardwth","id":33274336,"node_id":"MDQ6VXNlcjMzMjc0MzM2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/33274336?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/richardwth","html_url":"https:\/\/github.com\/richardwth","followers_url":"https:\/\/api.github.com\/users\/richardwth\/followers","following_url":"https:\/\/api.github.com\/users\/richardwth\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/richardwth\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/richardwth\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/richardwth\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/richardwth\/orgs","repos_url":"https:\/\/api.github.com\/users\/richardwth\/repos","events_url":"https:\/\/api.github.com\/users\/richardwth\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/richardwth\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-06-13T15:49:00Z","updated_at":"2023-06-14T12:13:38Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nI notice that when loading data instances with feature type of python dictionary, the dictionary keys would be broadcast so that every instance has the same set of keys. Please see an example in the Motivation section.\r\n\r\nIt is possible to avoid this behavior, i.e., load dictionary features as it is and do not broadcast the keys among instances? Please note that these dictionaries would have to be processed dynamically at each training iteration into strings (and tokenized).\n\n### Motivation\n\nI am trying to load a dataset from a json file. Each instance of the dataset has a feature that is a dictionary but its keys depend on the instance. Every two instances may have different keys. For example, imagine a dataset that contains a set of math expressions from a bunch of mutually redundant expressions:\r\n```\r\n{\r\n    \"index\": 0,\r\n    \"feature\": {\r\n        \"2 * x + y >= 3\": [\"2 * x + y >= 3\", \"4 * x + 2 * y >= 6\"],\r\n        ...\r\n    }\r\n},\r\n...\r\n{\r\n    \"index\": 9999,\r\n    \"feature\": {\r\n        \"x >= 6\": [\"x >= 6\", \"x >= 0\", \"x >= -1\"],\r\n        ...\r\n    }\r\n},\r\n...\r\n```\r\nWhen directly loading the dataset using `data = load_dataset(\"json\", data_files=file_paths, split='train')`, each instance would have all the keys from other instances and None as values. That is, instance of index 0 becomes:\r\n```\r\n{\r\n    \"index\": 0,\r\n    \"feature\": {\r\n        \"2 * x + y >= 3\": [\"2 * x + y >= 3\", \"4 * x + 2 * y >= 6\"],\r\n        ...\r\n        \"x >= 6\": None,  # keys from other instances\r\n        ...\r\n    }\r\n},\r\n```\r\nThis is not desirable. Moreover, issue would be raised if I attempt to combine two such datasets using `data = concatenate_datasets(multi_datasets)`, perhaps because their dictionary features contain different keys.\r\n\r\nA solution I can think of is to store the dictionary features as a long string, and evaluate it later. Please kindly suggest any other solution using existing methods of datasets.\n\n### Your contribution\n\nN\/A","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5950\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5950\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5949","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5949\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5949\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5949\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5949","id":1754843717,"node_id":"PR_kwDODunzps5S4oPC","number":5949,"title":"Replace metadata utils with `huggingface_hub`'s RepoCard API","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-06-13T13:03:19Z","updated_at":"2023-06-27T16:47:51Z","closed_at":"2023-06-27T16:38:32Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Use `huggingface_hub`'s RepoCard API instead of `DatasetMetadata` for modifying the card's YAML, and deprecate `datasets.utils.metadata` and `datasets.utils.readme`.\r\n\r\nAfter removing these modules, we can also delete `datasets.utils.resources` since the moon landing repo now stores its own version of these resources for the metadata UI.\r\n\r\nPS: this change requires bumping `huggingface_hub` to 0.13.0 (Transformers requires 0.14.0, so should be ok)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5949\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5949\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5949","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5949","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5949.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5949.patch","merged_at":"2023-06-27T16:38:32Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5948","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5948\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5948\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5948\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5948","id":1754794611,"node_id":"PR_kwDODunzps5S4dUt","number":5948,"title":"Fix sequence of array support for most dtype","user":{"login":"qgallouedec","id":45557362,"node_id":"MDQ6VXNlcjQ1NTU3MzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45557362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/qgallouedec","html_url":"https:\/\/github.com\/qgallouedec","followers_url":"https:\/\/api.github.com\/users\/qgallouedec\/followers","following_url":"https:\/\/api.github.com\/users\/qgallouedec\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/qgallouedec\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/qgallouedec\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/qgallouedec\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/qgallouedec\/orgs","repos_url":"https:\/\/api.github.com\/users\/qgallouedec\/repos","events_url":"https:\/\/api.github.com\/users\/qgallouedec\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/qgallouedec\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-13T12:38:59Z","updated_at":"2023-06-14T15:11:55Z","closed_at":"2023-06-14T15:03:33Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fixes #5936 \r\nAlso, a related fix to #5927 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5948\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5948\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5948","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5948","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5948.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5948.patch","merged_at":"2023-06-14T15:03:33Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5947","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5947\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5947\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5947\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5947","id":1754359316,"node_id":"I_kwDODunzps5okWYU","number":5947,"title":"Return the audio filename when decoding fails due to corrupt files","user":{"login":"wetdog","id":8949105,"node_id":"MDQ6VXNlcjg5NDkxMDU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8949105?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wetdog","html_url":"https:\/\/github.com\/wetdog","followers_url":"https:\/\/api.github.com\/users\/wetdog\/followers","following_url":"https:\/\/api.github.com\/users\/wetdog\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wetdog\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wetdog\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wetdog\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wetdog\/orgs","repos_url":"https:\/\/api.github.com\/users\/wetdog\/repos","events_url":"https:\/\/api.github.com\/users\/wetdog\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wetdog\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-13T08:44:09Z","updated_at":"2023-06-14T12:45:01Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nReturn the audio filename when the audio decoding fails. Although currently there are some checks for mp3 and opus formats with the library version there are still cases when the audio decoding could fail, eg. Corrupt file. \r\n\r\n### Motivation\r\n\r\nWhen you try to load an object file dataset and the decoding fails you can't know which file is corrupt\r\n```\r\n\r\nraise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\r\nsoundfile.LibsndfileError: Error opening <_io.BytesIO object at 0x7f5ab7e38290>: Format not recognised.\r\n```\r\n\r\n### Your contribution\r\n\r\nMake a PR to Add exceptions for LIbsndfileError to return the audio filename or path when soundfile decoding fails.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5947\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5947\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5946","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5946\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5946\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5946\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5946","id":1754234469,"node_id":"I_kwDODunzps5oj35l","number":5946,"title":"IndexError Not Solving -> IndexError: Invalid key: ?? is out of bounds for size 0 or ??","user":{"login":"syngokhan","id":70565543,"node_id":"MDQ6VXNlcjcwNTY1NTQz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/70565543?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/syngokhan","html_url":"https:\/\/github.com\/syngokhan","followers_url":"https:\/\/api.github.com\/users\/syngokhan\/followers","following_url":"https:\/\/api.github.com\/users\/syngokhan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/syngokhan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/syngokhan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/syngokhan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/syngokhan\/orgs","repos_url":"https:\/\/api.github.com\/users\/syngokhan\/repos","events_url":"https:\/\/api.github.com\/users\/syngokhan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/syngokhan\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-06-13T07:34:15Z","updated_at":"2023-07-14T12:04:48Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nin <cell line: 1>:1                                                                              \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/trainer.py:1537 in train                    \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1534 \u2502   \u2502   inner_training_loop = find_executable_batch_size(                                 \u2502\r\n\u2502   1535 \u2502   \u2502   \u2502   self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size  \u2502\r\n\u2502   1536 \u2502   \u2502   )                                                                                 \u2502\r\n\u2502 \u2771 1537 \u2502   \u2502   return inner_training_loop(                                                       \u2502\r\n\u2502   1538 \u2502   \u2502   \u2502   args=args,                                                                    \u2502\r\n\u2502   1539 \u2502   \u2502   \u2502   resume_from_checkpoint=resume_from_checkpoint,                                \u2502\r\n\u2502   1540 \u2502   \u2502   \u2502   trial=trial,                                                                  \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/transformers\/trainer.py:1789 in _inner_training_loop     \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1786 \u2502   \u2502   \u2502   \u2502   rng_to_sync = True                                                        \u2502\r\n\u2502   1787 \u2502   \u2502   \u2502                                                                                 \u2502\r\n\u2502   1788 \u2502   \u2502   \u2502   step = -1                                                                     \u2502\r\n\u2502 \u2771 1789 \u2502   \u2502   \u2502   for step, inputs in enumerate(epoch_iterator):                                \u2502\r\n\u2502   1790 \u2502   \u2502   \u2502   \u2502   total_batched_samples += 1                                                \u2502\r\n\u2502   1791 \u2502   \u2502   \u2502   \u2502   if rng_to_sync:                                                           \u2502\r\n\u2502   1792 \u2502   \u2502   \u2502   \u2502   \u2502   self._load_rng_state(resume_from_checkpoint)                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/accelerate\/data_loader.py:377 in __iter__                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   374 \u2502   \u2502   dataloader_iter = super().__iter__()                                               \u2502\r\n\u2502   375 \u2502   \u2502   # We iterate one batch ahead to check when we are at the end                       \u2502\r\n\u2502   376 \u2502   \u2502   try:                                                                               \u2502\r\n\u2502 \u2771 377 \u2502   \u2502   \u2502   current_batch = next(dataloader_iter)                                          \u2502\r\n\u2502   378 \u2502   \u2502   except StopIteration:                                                              \u2502\r\n\u2502   379 \u2502   \u2502   \u2502   yield                                                                          \u2502\r\n\u2502   380                                                                                            \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/torch\/utils\/data\/dataloader.py:633 in __next__           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502    630 \u2502   \u2502   \u2502   if self._sampler_iter is None:                                                \u2502\r\n\u2502    631 \u2502   \u2502   \u2502   \u2502   # TODO(https:\/\/github.com\/pytorch\/pytorch\/issues\/76750)                   \u2502\r\n\u2502    632 \u2502   \u2502   \u2502   \u2502   self._reset()  # type: ignore[call-arg]                                   \u2502\r\n\u2502 \u2771  633 \u2502   \u2502   \u2502   data = self._next_data()                                                      \u2502\r\n\u2502    634 \u2502   \u2502   \u2502   self._num_yielded += 1                                                        \u2502\r\n\u2502    635 \u2502   \u2502   \u2502   if self._dataset_kind == _DatasetKind.Iterable and \\                          \u2502\r\n\u2502    636 \u2502   \u2502   \u2502   \u2502   \u2502   self._IterableDataset_len_called is not None and \\                    \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/torch\/utils\/data\/dataloader.py:677 in _next_data         \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502    674 \u2502                                                                                         \u2502\r\n\u2502    675 \u2502   def _next_data(self):                                                                 \u2502\r\n\u2502    676 \u2502   \u2502   index = self._next_index()  # may raise StopIteration                             \u2502\r\n\u2502 \u2771  677 \u2502   \u2502   data = self._dataset_fetcher.fetch(index)  # may raise StopIteration              \u2502\r\n\u2502    678 \u2502   \u2502   if self._pin_memory:                                                              \u2502\r\n\u2502    679 \u2502   \u2502   \u2502   data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)            \u2502\r\n\u2502    680 \u2502   \u2502   return data                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/torch\/utils\/data\/_utils\/fetch.py:49 in fetch             \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   46 \u2502   def fetch(self, possibly_batched_index):                                                \u2502\r\n\u2502   47 \u2502   \u2502   if self.auto_collation:                                                             \u2502\r\n\u2502   48 \u2502   \u2502   \u2502   if hasattr(self.dataset, \"__getitems__\") and self.dataset.__getitems__:         \u2502\r\n\u2502 \u2771 49 \u2502   \u2502   \u2502   \u2502   data = self.dataset.__getitems__(possibly_batched_index)                    \u2502\r\n\u2502   50 \u2502   \u2502   \u2502   else:                                                                           \u2502\r\n\u2502   51 \u2502   \u2502   \u2502   \u2502   data = [self.dataset[idx] for idx in possibly_batched_index]                \u2502\r\n\u2502   52 \u2502   \u2502   else:                                                                               \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:2782 in __getitems__           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   2779 \u2502                                                                                         \u2502\r\n\u2502   2780 \u2502   def __getitems__(self, keys: List) -> List:                                           \u2502\r\n\u2502   2781 \u2502   \u2502   \"\"\"Can be used to get a batch using a list of integers indices.\"\"\"                \u2502\r\n\u2502 \u2771 2782 \u2502   \u2502   batch = self.__getitem__(keys)                                                    \u2502\r\n\u2502   2783 \u2502   \u2502   n_examples = len(batch[next(iter(batch))])                                        \u2502\r\n\u2502   2784 \u2502   \u2502   return [{col: array[i] for col, array in batch.items()} for i in range(n_example  \u2502\r\n\u2502   2785                                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:2778 in __getitem__            \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   2775 \u2502                                                                                         \u2502\r\n\u2502   2776 \u2502   def __getitem__(self, key):  # noqa: F811                                             \u2502\r\n\u2502   2777 \u2502   \u2502   \"\"\"Can be used to index columns (by string names) or rows (by integer index or i  \u2502\r\n\u2502 \u2771 2778 \u2502   \u2502   return self._getitem(key)                                                         \u2502\r\n\u2502   2779 \u2502                                                                                         \u2502\r\n\u2502   2780 \u2502   def __getitems__(self, keys: List) -> List:                                           \u2502\r\n\u2502   2781 \u2502   \u2502   \"\"\"Can be used to get a batch using a list of integers indices.\"\"\"                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/arrow_dataset.py:2762 in _getitem               \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   2759 \u2502   \u2502   format_kwargs = kwargs[\"format_kwargs\"] if \"format_kwargs\" in kwargs else self._  \u2502\r\n\u2502   2760 \u2502   \u2502   format_kwargs = format_kwargs if format_kwargs is not None else {}                \u2502\r\n\u2502   2761 \u2502   \u2502   formatter = get_formatter(format_type, features=self._info.features, **format_kw  \u2502\r\n\u2502 \u2771 2762 \u2502   \u2502   pa_subtable = query_table(self._data, key, indices=self._indices if self._indice  \u2502\r\n\u2502   2763 \u2502   \u2502   formatted_output = format_table(                                                  \u2502\r\n\u2502   2764 \u2502   \u2502   \u2502   pa_subtable, key, formatter=formatter, format_columns=format_columns, output  \u2502\r\n\u2502   2765 \u2502   \u2502   )                                                                                 \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py:578 in query_table     \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   575 \u2502   \u2502   _check_valid_column_key(key, table.column_names)                                   \u2502\r\n\u2502   576 \u2502   else:                                                                                  \u2502\r\n\u2502   577 \u2502   \u2502   size = indices.num_rows if indices is not None else table.num_rows                 \u2502\r\n\u2502 \u2771 578 \u2502   \u2502   _check_valid_index_key(key, size)                                                  \u2502\r\n\u2502   579 \u2502   # Query the main table                                                                 \u2502\r\n\u2502   580 \u2502   if indices is None:                                                                    \u2502\r\n\u2502   581 \u2502   \u2502   pa_subtable = _query_table(table, key)                                             \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py:531 in                 \u2502\r\n\u2502 _check_valid_index_key                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   528 \u2502   \u2502   \u2502   _check_valid_index_key(min(key), size=size)                                    \u2502\r\n\u2502   529 \u2502   elif isinstance(key, Iterable):                                                        \u2502\r\n\u2502   530 \u2502   \u2502   if len(key) > 0:                                                                   \u2502\r\n\u2502 \u2771 531 \u2502   \u2502   \u2502   _check_valid_index_key(int(max(key)), size=size)                               \u2502\r\n\u2502   532 \u2502   \u2502   \u2502   _check_valid_index_key(int(min(key)), size=size)                               \u2502\r\n\u2502   533 \u2502   else:                                                                                  \u2502\r\n\u2502   534 \u2502   \u2502   _raise_bad_key_type(key)                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/formatting\/formatting.py:521 in                 \u2502\r\n\u2502 _check_valid_index_key                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   518 def _check_valid_index_key(key: Union[int, slice, range, Iterable], size: int) -> None:    \u2502\r\n\u2502   519 \u2502   if isinstance(key, int):                                                               \u2502\r\n\u2502   520 \u2502   \u2502   if (key < 0 and key + size < 0) or (key >= size):                                  \u2502\r\n\u2502 \u2771 521 \u2502   \u2502   \u2502   raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")       \u2502\r\n\u2502   522 \u2502   \u2502   return                                                                             \u2502\r\n\u2502   523 \u2502   elif isinstance(key, slice):                                                           \u2502\r\n\u2502   524 \u2502   \u2502   pass                                             \n\n### Steps to reproduce the bug\n\n``\r\nimport json\r\nimport os\r\nfrom pprint import pprint\r\n\r\nimport bitsandbytes as bnb\r\nimport pandas as pd\r\nimport torch\r\nimport torch.nn as nn\r\nimport transformers\r\nfrom datasets import Dataset,load_dataset\r\n\r\nfrom peft import (\r\n    LoraConfig,\r\n    PeftConfig,\r\n    PeftModel,\r\n    get_peft_model,\r\n    prepare_model_for_kbit_training\r\n)\r\n\r\nfrom transformers import (\r\n    AutoConfig,\r\n    AutoModelForCausalLM,\r\n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n\r\n)\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\ndef print_trainable_parameters(model):\r\n    \"\"\"\r\n    Prints the number of trainable parameters in the model.\r\n    \"\"\"\r\n    trainable_params = 0\r\n    all_param = 0\r\n    for _, param in model.named_parameters():\r\n        all_param += param.numel()\r\n        if param.requires_grad:\r\n            trainable_params += param.numel()\r\n    print(\r\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params \/ all_param}\"\r\n    )\r\n\r\n\r\nMODEL_NAME = \"tiiuae\/falcon-7b\"\r\n\r\nbnb_config = BitsAndBytesConfig(\r\n    load_in_4bit = True,\r\n    bnb_4bit_use_double_quant=True,\r\n    bnb_4bit_quant_type=\"nf4\",\r\n    bnb_4bit_compute_dtype=torch.bfloat16,\r\n)\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    MODEL_NAME,\r\n    device_map = \"auto\",\r\n    trust_remote_code = True,\r\n    quantization_config = bnb_config\r\n)\r\n\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\r\ntokenizer.pad_token = tokenizer.eos_token\r\n\r\nmodel.gradient_checkpointing_enable()\r\nmodel = prepare_model_for_kbit_training(model)\r\n\r\n\r\nconfig = LoraConfig(\r\n    r = 16,\r\n    lora_alpha = 32,\r\n    target_modules = [\"query_key_value\"],\r\n    lora_dropout = 0.05,\r\n    bias = \"none\",\r\n    task_type = \"CASUAL_LM\"\r\n)\r\n\r\nmodel = get_peft_model(model,config)\r\nprint_trainable_parameters(model)\r\n\r\ndef generate_prompt(data_point):\r\n  return f\"\"\"\r\n<human>: {data_point[\"question\"]}\r\n<assistant>: {data_point[\"answer\"]}  \r\n\"\"\".strip()\r\n\r\ndef generate_and_tokenize_prompt(data_point):\r\n    full_prompt = generate_prompt(data_point)\r\n    tokenized_full_prompt = tokenizer(full_prompt, padding = True, truncation = True,return_tensors = None)\r\n    return dict({\r\n        \"input_ids\" : tokenized_full_prompt[\"input_ids\"],\r\n        \"attention_mask\" : tokenized_full_prompt[\"attention_mask\"]\r\n\r\n    })\r\n\r\n\r\ndata = data[\"train\"].shuffle().map(generate_and_tokenize_prompt, batched = False)  \r\n\r\nOUTPUT_DIR = \"experiments\"\r\n\r\ntrainings_args = transformers.TrainingArguments(\r\n    per_device_train_batch_size = 1,\r\n    gradient_accumulation_steps = 4,\r\n    num_train_epochs = 1,\r\n    learning_rate = 2e-4,\r\n    fp16 = True,\r\n    save_total_limit = 3,\r\n    logging_steps = 1,\r\n    output_dir = OUTPUT_DIR,\r\n    max_steps = 80,\r\n    optim = \"paged_adamw_8bit\",\r\n    lr_scheduler_type = \"cosine\",\r\n    warmup_ratio = 0.05,\r\n    #remove_unused_columns=True\r\n)\r\n\r\ntrainer = transformers.Trainer(\r\n    model = model,\r\n    train_dataset = data,\r\n    args = trainings_args, \r\n    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\r\n\r\n)\r\n\r\nmodel.config.use_cache = False\r\n\r\ntrainer.train()\r\n\r\n\r\nIndexError: Invalid key: 32 is out of bounds for size 0\r\n\r\nDataSet Format is like : \r\n[{\"question\": \"How can I create an account?\", \"answer\": \"To create an account, click on the 'Sign Up' button on the top right corner of our website and follow the instructions to complete the registration process.\"}, .... ]\n\n### Expected behavior\n\n-\n\n### Environment info\n\n\r\n!pip install -q pip \r\n!pip install -q bitsandbytes==0.39.0 \r\n!pip install -q torch==2.0.1\r\n\r\n!pip install -q git+https:\/\/github.com\/huggingface\/transformers.git \r\n!pip install -q git+https:\/\/github.com\/huggingface\/peft.git \r\n!pip install -q git+https:\/\/github.com\/huggingface\/accelerate.git \r\n\r\n!pip install -q datasets \r\n!pip install -q loralib==0.1.1 \r\n!pip install -q einops==0.6.1 \r\n\r\n\r\nimport json\r\nimport os\r\nfrom pprint import pprint\r\n\r\nimport bitsandbytes as bnb\r\nimport pandas as pd\r\nimport torch\r\nimport torch.nn as nn\r\nimport transformers\r\nfrom datasets import Dataset,load_dataset\r\n\r\nfrom peft import (\r\n    LoraConfig,\r\n    PeftConfig,\r\n    PeftModel,\r\n    get_peft_model,\r\n    prepare_model_for_kbit_training\r\n)\r\n\r\nfrom transformers import (\r\n    AutoConfig,\r\n    AutoModelForCausalLM,\r\n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n\r\n)\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5946\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5946\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5945","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5945\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5945\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5945\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5945","id":1754084577,"node_id":"I_kwDODunzps5ojTTh","number":5945,"title":"Failing to upload dataset to the hub","user":{"login":"Ar770","id":77382661,"node_id":"MDQ6VXNlcjc3MzgyNjYx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/77382661?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Ar770","html_url":"https:\/\/github.com\/Ar770","followers_url":"https:\/\/api.github.com\/users\/Ar770\/followers","following_url":"https:\/\/api.github.com\/users\/Ar770\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Ar770\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Ar770\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Ar770\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Ar770\/orgs","repos_url":"https:\/\/api.github.com\/users\/Ar770\/repos","events_url":"https:\/\/api.github.com\/users\/Ar770\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Ar770\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-13T05:46:46Z","updated_at":"2023-07-24T11:56:40Z","closed_at":"2023-07-24T11:56:40Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nTrying to upload a dataset of hundreds of thousands of audio samples (the total volume is not very large, 60 gb) to the hub with push_to_hub, it doesn't work.\r\nFrom time to time one piece of the data (parquet) gets pushed and then I get RemoteDisconnected even though my internet is stable.\r\nPlease help.\r\nI'm trying to upload the dataset for almost a week.\r\nThanks\n\n### Steps to reproduce the bug\n\nnot relevant \n\n### Expected behavior\n\nBe able to upload thedataset\n\n### Environment info\n\npython: 3.9","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5945\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5945\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5944","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5944\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5944\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5944\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5944","id":1752882200,"node_id":"PR_kwDODunzps5Sx7O4","number":5944,"title":"Arrow dataset builder to be able to load and stream Arrow datasets","user":{"login":"mariusz-jachimowicz-83","id":10278877,"node_id":"MDQ6VXNlcjEwMjc4ODc3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10278877?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83","html_url":"https:\/\/github.com\/mariusz-jachimowicz-83","followers_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/followers","following_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/repos","events_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-12T14:21:49Z","updated_at":"2023-06-13T17:36:02Z","closed_at":"2023-06-13T17:29:01Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This adds a Arrow dataset builder to be able to load and stream from already preprocessed Arrow files.\r\nIt's related to https:\/\/github.com\/huggingface\/datasets\/issues\/3035","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5944\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5944\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5944","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5944","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5944.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5944.patch","merged_at":"2023-06-13T17:29:01Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5942","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5942\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5942\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5942\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5942","id":1752021681,"node_id":"PR_kwDODunzps5Su-V4","number":5942,"title":"Pass datasets-cli additional args as kwargs to DatasetBuilder in `run_beam.py`","user":{"login":"graelo","id":84066822,"node_id":"MDQ6VXNlcjg0MDY2ODIy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/84066822?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/graelo","html_url":"https:\/\/github.com\/graelo","followers_url":"https:\/\/api.github.com\/users\/graelo\/followers","following_url":"https:\/\/api.github.com\/users\/graelo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/graelo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/graelo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/graelo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/graelo\/orgs","repos_url":"https:\/\/api.github.com\/users\/graelo\/repos","events_url":"https:\/\/api.github.com\/users\/graelo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/graelo\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-06-12T06:50:50Z","updated_at":"2023-06-30T09:15:00Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\n\r\nFollowing this <https:\/\/discuss.huggingface.co\/t\/how-to-preprocess-a-wikipedia-dataset-using-dataflowrunner\/41991\/3>, here is a simple PR to pass any additional args to datasets-cli as kwargs in the DatasetBuilder in `run_beam.py`.\r\n\r\nI also took the liberty to add missing setup steps to the `beam.mdx` docs in order to help everyone.\r\n\r\n@lhoestq ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5942\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5942\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5942","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5942","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5942.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5942.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5941","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5941\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5941\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5941\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5941","id":1751838897,"node_id":"I_kwDODunzps5oavCx","number":5941,"title":"Load Data Sets Too Slow In Train Seq2seq Model","user":{"login":"xyx361100238","id":19569322,"node_id":"MDQ6VXNlcjE5NTY5MzIy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/19569322?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/xyx361100238","html_url":"https:\/\/github.com\/xyx361100238","followers_url":"https:\/\/api.github.com\/users\/xyx361100238\/followers","following_url":"https:\/\/api.github.com\/users\/xyx361100238\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/xyx361100238\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/xyx361100238\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/xyx361100238\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/xyx361100238\/orgs","repos_url":"https:\/\/api.github.com\/users\/xyx361100238\/repos","events_url":"https:\/\/api.github.com\/users\/xyx361100238\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/xyx361100238\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2023-06-12T03:58:43Z","updated_at":"2023-08-15T02:52:22Z","closed_at":"2023-08-15T02:52:22Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n\r\nstep 'Generating train split' in load_dataset is too slow\uff1a\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/19569322\/d9b08eee-95fe-4741-a346-b70416c948f8)\r\n\n\n### Steps to reproduce the bug\n\nData\uff1a own data\uff0c16K16B Mono wav\r\nOficial Script:[ run_speech_recognition_seq2seq.py](https:\/\/github.com\/huggingface\/transformers\/blob\/main\/examples\/pytorch\/speech-recognition\/run_speech_recognition_seq2seq.py)\r\nAdd Code\uff1a\r\n    if data_args.data_path is not None:\r\n        print(data_args.data_path)\r\n        raw_datasets = load_dataset(\"audiofolder\", data_dir=data_args.data_path, cache_dir=model_args.cache_dir)\r\n        raw_datasets = raw_datasets.cast_column(\"audio\", Audio(sampling_rate=16000))\r\n        raw_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.005, shuffle=True)\r\n \uff08change cache_dir to other path \uff0cex:\/DATA\/cache\uff09\n\n### Expected behavior\n\nload data fast,at least 1000+\r\n`Generating train split: 387875 examples [32:24:45, 1154.83 examples\/s]`\n\n### Environment info\n\n\r\n- `transformers` version: 4.28.0.dev0\r\n- Platform: Linux-5.4.0-149-generic-x86_64-with-debian-bullseye-sid\r\n- Python version: 3.7.16\r\n- Huggingface_hub version: 0.13.2\r\n- PyTorch version (GPU?): 1.13.1+cu116 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5941\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5941\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5990","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5990\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5990\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5990\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5990","id":1774389854,"node_id":"I_kwDODunzps5pwwpe","number":5990,"title":"Pushing a large dataset on the hub consistently hangs","user":{"login":"AntreasAntoniou","id":10792502,"node_id":"MDQ6VXNlcjEwNzkyNTAy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10792502?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/AntreasAntoniou","html_url":"https:\/\/github.com\/AntreasAntoniou","followers_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/followers","following_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/orgs","repos_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/repos","events_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":44,"created_at":"2023-06-10T14:46:47Z","updated_at":"2023-08-17T09:54:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nOnce I have locally built a large dataset that I want to push to hub, I use the recommended approach of .push_to_hub to get the dataset on the hub, and after pushing a few shards, it consistently hangs. This has happened over 40 times over the past week, and despite my best efforts to try and catch this happening and kill a process and restart, it seems to be extremely time wasting -- so I came to you to report this and to seek help. \r\n\r\nI already tried installing hf_transfer, but it doesn't support Byte file uploads so I uninstalled it.\n\n### Reproduction\n\n```python\r\nimport multiprocessing as mp\r\nimport pathlib\r\nfrom math import ceil\r\n\r\nimport datasets\r\nimport numpy as np\r\nfrom tqdm.auto import tqdm\r\n\r\nfrom tali.data.data import select_subtitles_between_timestamps\r\nfrom tali.utils import load_json\r\n\r\ntali_dataset_dir = \"\/data\/\"\r\n\r\nif __name__ == \"__main__\":\r\n    full_dataset = datasets.load_dataset(\r\n        \"Antreas\/TALI\", num_proc=mp.cpu_count(), cache_dir=tali_dataset_dir\r\n    )\r\n\r\n    def data_generator(set_name, percentage: float = 1.0):\r\n        dataset = full_dataset[set_name]\r\n\r\n        for item in tqdm(dataset):\r\n            video_list = item[\"youtube_content_video\"]\r\n            video_list = np.random.choice(\r\n                video_list, int(ceil(len(video_list) * percentage))\r\n            )\r\n            if len(video_list) == 0:\r\n                continue\r\n            captions = item[\"youtube_subtitle_text\"]\r\n            captions = select_subtitles_between_timestamps(\r\n                subtitle_dict=load_json(\r\n                    captions.replace(\r\n                        \"\/data\/\",\r\n                        tali_dataset_dir,\r\n                    )\r\n                ),\r\n                starting_timestamp=0,\r\n                ending_timestamp=100000000,\r\n            )\r\n\r\n            for video_path in video_list:\r\n                temp_path = video_path.replace(\"\/data\/\", tali_dataset_dir)\r\n                video_path_actual: pathlib.Path = pathlib.Path(temp_path)\r\n\r\n                if video_path_actual.exists():\r\n                    item[\"youtube_content_video\"] = open(video_path_actual, \"rb\").read()\r\n                    item[\"youtube_subtitle_text\"] = captions\r\n                    yield item\r\n\r\n    train_generator = lambda: data_generator(\"train\", percentage=0.1)\r\n    val_generator = lambda: data_generator(\"val\")\r\n    test_generator = lambda: data_generator(\"test\")\r\n\r\n    train_data = datasets.Dataset.from_generator(\r\n        train_generator,\r\n        num_proc=mp.cpu_count(),\r\n        writer_batch_size=5000,\r\n        cache_dir=tali_dataset_dir,\r\n    )\r\n\r\n    val_data = datasets.Dataset.from_generator(\r\n        val_generator,\r\n        writer_batch_size=5000,\r\n        num_proc=mp.cpu_count(),\r\n        cache_dir=tali_dataset_dir,\r\n    )\r\n\r\n    test_data = datasets.Dataset.from_generator(\r\n        test_generator,\r\n        writer_batch_size=5000,\r\n        num_proc=mp.cpu_count(),\r\n        cache_dir=tali_dataset_dir,\r\n    )\r\n\r\n    dataset = datasets.DatasetDict(\r\n        {\r\n            \"train\": train_data,\r\n            \"val\": val_data,\r\n            \"test\": test_data,\r\n        }\r\n    )\r\n    succesful_competion = False\r\n    while not succesful_competion:\r\n        try:\r\n            dataset.push_to_hub(repo_id=\"Antreas\/TALI-small\", max_shard_size=\"5GB\")\r\n            succesful_competion = True\r\n        except Exception as e:\r\n            print(e)\r\n```\n\n### Logs\n\n```shell\nPushing dataset shards to the dataset hub:  33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                            | 7\/21 [24:33<49:06, 210.45s\/it]\r\nError while uploading 'data\/val-00007-of-00021-6b216a984af1a4c8.parquet' to the Hub.                                                                                                               \r\nPushing split train to the Hub.                                                                                                                                                                    \r\nResuming upload of the dataset shards.                                                                                                                                                             \r\nPushing dataset shards to the dataset hub: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46\/46 [42:10<00:00, 55.01s\/it]\r\nPushing split val to the Hub.                                                                                                                                                                      \r\nResuming upload of the dataset shards.                                                                                                                                                             \r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:01<00:00,  1.55ba\/s]\r\nUpload 1 LFS files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:23<00:00, 23.51s\/it]\r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:02<00:00,  1.39ba\/s]\r\nUpload 1 LFS files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:30<00:00, 30.19s\/it]\r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:02<00:00,  1.28ba\/s]\r\nUpload 1 LFS files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:24<00:00, 24.08s\/it]\r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:02<00:00,  1.42ba\/s]\r\nUpload 1 LFS files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:23<00:00, 23.97s\/it]\r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:02<00:00,  1.49ba\/s]\r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:02<00:00,  1.54ba\/s^\r\nUpload 1 LFS files:   0%|                                                                                                                                                    | 0\/1 [04:42<?, ?it\/s]\r\nPushing dataset shards to the dataset hub:  52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                      | 11\/21 [17:23<15:48, 94.82s\/it]\r\n\r\nThat's where it got stuck\n```\n\n\n### System info\n\n```shell\n- huggingface_hub version: 0.15.1\r\n- Platform: Linux-5.4.0-147-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.11\r\n- Running in iPython ?: No\r\n- Running in notebook ?: No\r\n- Running in Google Colab ?: No\r\n- Token path ?: \/root\/.cache\/huggingface\/token\r\n- Has saved token ?: True\r\n- Who am I ?: Antreas\r\n- Configured git credential helpers: store\r\n- FastAI: N\/A\r\n- Tensorflow: N\/A\r\n- Torch: 2.1.0.dev20230606+cu121\r\n- Jinja2: 3.1.2\r\n- Graphviz: N\/A\r\n- Pydot: N\/A\r\n- Pillow: 9.5.0\r\n- hf_transfer: N\/A\r\n- gradio: N\/A\r\n- numpy: 1.24.3\r\n- ENDPOINT: https:\/\/huggingface.co\r\n- HUGGINGFACE_HUB_CACHE: \/root\/.cache\/huggingface\/hub\r\n- HUGGINGFACE_ASSETS_CACHE: \/root\/.cache\/huggingface\/assets\r\n- HF_TOKEN_PATH: \/root\/.cache\/huggingface\/token\r\n- HF_HUB_OFFLINE: False\r\n- HF_HUB_DISABLE_TELEMETRY: False\r\n- HF_HUB_DISABLE_PROGRESS_BARS: None\r\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\r\n- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\r\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\r\n- HF_HUB_ENABLE_HF_TRANSFER: False\n```\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5990\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5990\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5939","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5939\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5939\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5939\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5939","id":1749955883,"node_id":"I_kwDODunzps5oTjUr","number":5939,"title":".","user":{"login":"flckv","id":103381497,"node_id":"U_kgDOBil5-Q","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/103381497?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/flckv","html_url":"https:\/\/github.com\/flckv","followers_url":"https:\/\/api.github.com\/users\/flckv\/followers","following_url":"https:\/\/api.github.com\/users\/flckv\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/flckv\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/flckv\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/flckv\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/flckv\/orgs","repos_url":"https:\/\/api.github.com\/users\/flckv\/repos","events_url":"https:\/\/api.github.com\/users\/flckv\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/flckv\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-06-09T14:01:34Z","updated_at":"2023-06-12T12:19:34Z","closed_at":"2023-06-12T12:19:19Z","author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5939\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5939\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5938","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5938\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5938\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5938\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5938","id":1749462851,"node_id":"PR_kwDODunzps5SmbkI","number":5938,"title":"Make get_from_cache use custom temp filename that is locked","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-09T09:01:13Z","updated_at":"2023-06-14T13:35:38Z","closed_at":"2023-06-14T13:27:24Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR ensures that the temporary filename created is the same as the one that is locked, while writing to the cache.\r\n\r\nThis PR stops using `tempfile` to generate the temporary filename.\r\n\r\nAdditionally, the behavior now is aligned for both `resume_download` `True` and `False`.\r\n\r\nRefactor temp_file_manager so that it uses the filename that is locked: \r\n- Use: `cache_path + \".incomplete\"`, when the locked one is `cache_path + \".lock\"`\r\n\r\nBefore it was using `tempfile` inside `cache_dir`, which was not locked: although very improbable name collision (8 random characters), this was not impossible when huge number of multiple processes.\r\n\r\nMaybe related to \"Stale file handle\" issues caused by `tempfile`: \r\n- [ ] https:\/\/huggingface.co\/datasets\/tapaco\/discussions\/4\r\n- [ ] https:\/\/huggingface.co\/datasets\/xcsr\/discussions\/1\r\n- [ ] https:\/\/huggingface.co\/datasets\/covost2\/discussions\/3\r\n```\r\nError code:   ConfigNamesError\r\nException:    OSError\r\nMessage:      [Errno 116] Stale file handle\r\nTraceback:    Traceback (most recent call last):\r\n                File \"\/src\/services\/worker\/src\/worker\/job_runners\/dataset\/config_names.py\", line 61, in compute_config_names_response\r\n                  for config in sorted(get_dataset_config_names(path=dataset, use_auth_token=use_auth_token))\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/inspect.py\", line 323, in get_dataset_config_names\r\n                  dataset_module = dataset_module_factory(\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1219, in dataset_module_factory\r\n                  raise e1 from None\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1188, in dataset_module_factory\r\n                  return HubDatasetModuleFactoryWithScript(\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 907, in get_module\r\n                  dataset_readme_path = self.download_dataset_readme_file()\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 896, in download_dataset_readme_file\r\n                  return cached_path(\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/utils\/file_utils.py\", line 183, in cached_path\r\n                  output_path = get_from_cache(\r\n                File \"\/src\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/utils\/file_utils.py\", line 611, in get_from_cache\r\n                  http_get(\r\n                File \"\/usr\/local\/lib\/python3.9\/tempfile.py\", line 496, in __exit__\r\n                  result = self.file.__exit__(exc, value, tb)\r\n              OSError: [Errno 116] Stale file handle\r\n```\r\n- the stale file handle error can be raised when `tempfile` tries to close (when exiting its context manager) a filename that has been already closed by other process\r\n  - note that `tempfile` filenames are randomly generated but not locked in our code\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5938\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5938\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5938","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5938","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5938.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5938.patch","merged_at":"2023-06-14T13:27:24Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5937","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5937\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5937\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5937\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5937","id":1749388597,"node_id":"PR_kwDODunzps5SmLIs","number":5937,"title":"Avoid parallel redownload in cache","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-09T08:18:36Z","updated_at":"2023-06-14T12:30:59Z","closed_at":"2023-06-14T12:23:57Z","author_association":"MEMBER","active_lock_reason":null,"body":"Avoid parallel redownload in cache by retrying inside the lock if path exists.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5937\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5937\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5937","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5937","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5937.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5937.patch","merged_at":"2023-06-14T12:23:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5936","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5936\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5936\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5936\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5936","id":1748424388,"node_id":"I_kwDODunzps5oNtbE","number":5936,"title":"Sequence of array not supported for most dtype","user":{"login":"qgallouedec","id":45557362,"node_id":"MDQ6VXNlcjQ1NTU3MzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45557362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/qgallouedec","html_url":"https:\/\/github.com\/qgallouedec","followers_url":"https:\/\/api.github.com\/users\/qgallouedec\/followers","following_url":"https:\/\/api.github.com\/users\/qgallouedec\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/qgallouedec\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/qgallouedec\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/qgallouedec\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/qgallouedec\/orgs","repos_url":"https:\/\/api.github.com\/users\/qgallouedec\/repos","events_url":"https:\/\/api.github.com\/users\/qgallouedec\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/qgallouedec\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-08T18:18:07Z","updated_at":"2023-06-14T15:03:34Z","closed_at":"2023-06-14T15:03:34Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nCreate a dataset composed of sequence of array fails for most dtypes (see code below).\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Sequence, Array2D, Features, Dataset\r\n\r\nimport numpy as np\r\n\r\nfor dtype in [\r\n    \"bool\",  # ok\r\n    \"int8\",  # failed\r\n    \"int16\",  # failed\r\n    \"int32\",  # failed\r\n    \"int64\",  # ok\r\n    \"uint8\",  # failed\r\n    \"uint16\",  # failed\r\n    \"uint32\",  # failed\r\n    \"uint64\",  # failed\r\n    \"float16\",  # failed\r\n    \"float32\",  # failed\r\n    \"float64\",  # ok\r\n]:\r\n    features = Features({\"foo\": Sequence(Array2D(dtype=dtype, shape=(2, 2)))})\r\n    sequence = [\r\n        [[1.0, 2.0], [3.0, 4.0]],\r\n        [[5.0, 6.0], [7.0, 8.0]],\r\n    ]\r\n    array = np.array(sequence, dtype=dtype)\r\n    try:\r\n        dataset = Dataset.from_dict({\"foo\": [array]}, features=features)\r\n    except Exception as e:\r\n        print(f\"Failed for dtype={dtype}\")\r\n```\r\n\r\nTraceback for `dtype=\"int8\"`:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/qgallouedec\/datasets\/a.py\", line 29, in <module>\r\n    raise e\r\n  File \"\/home\/qgallouedec\/datasets\/a.py\", line 26, in <module>\r\n    dataset = Dataset.from_dict({\"foo\": [array]}, features=features)\r\n  File \"\/home\/qgallouedec\/env\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 899, in from_dict\r\n    pa_table = InMemoryTable.from_pydict(mapping=mapping)\r\n  File \"\/home\/qgallouedec\/env\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 799, in from_pydict\r\n    return cls(pa.Table.from_pydict(*args, **kwargs))\r\n  File \"pyarrow\/table.pxi\", line 3725, in pyarrow.lib.Table.from_pydict\r\n  File \"pyarrow\/table.pxi\", line 5254, in pyarrow.lib._from_pydict\r\n  File \"pyarrow\/array.pxi\", line 350, in pyarrow.lib.asarray\r\n  File \"pyarrow\/array.pxi\", line 236, in pyarrow.lib.array\r\n  File \"pyarrow\/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"\/home\/qgallouedec\/env\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 204, in __arrow_array__\r\n    out = cast_array_to_feature(out, type, allow_number_to_str=not self.trying_type)\r\n  File \"\/home\/qgallouedec\/env\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1833, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"\/home\/qgallouedec\/env\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 2091, in cast_array_to_feature\r\n    casted_values = _c(array.values, feature.feature)\r\n  File \"\/home\/qgallouedec\/env\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1833, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"\/home\/qgallouedec\/env\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 2139, in cast_array_to_feature\r\n    return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n  File \"\/home\/qgallouedec\/env\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1833, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"\/home\/qgallouedec\/env\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1967, in array_cast\r\n    return pa_type.wrap_array(array)\r\n  File \"pyarrow\/types.pxi\", line 879, in pyarrow.lib.BaseExtensionType.wrap_array\r\nTypeError: Incompatible storage type for extension<arrow.py_extension_type<Array2DExtensionType>>: expected list<item: list<item: int8>>, got list<item: list<item: int64>>\r\n```\n\n### Expected behavior\n\nNot to fail.\n\n### Environment info\n\n\r\n- Python 3.10.6\r\n- datasets: master branch\r\n- Numpy: 1.23.4","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5936\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5936\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5935","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5935\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5935\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5935\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5935","id":1748090220,"node_id":"PR_kwDODunzps5Sh9Mg","number":5935,"title":"Better row group size in push_to_hub","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2023-06-08T15:01:15Z","updated_at":"2023-06-09T17:47:37Z","closed_at":"2023-06-09T17:40:09Z","author_association":"MEMBER","active_lock_reason":null,"body":"This is a very simple change that improves `to_parquet` to use a more reasonable row group size for image and audio datasets.\r\n\r\nThis is especially useful for `push_to_hub` and will provide a better experience with the dataset viewer on HF","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5935\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5935\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5935","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5935","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5935.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5935.patch","merged_at":"2023-06-09T17:40:09Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5934","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5934\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5934\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5934\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5934","id":1747904840,"node_id":"PR_kwDODunzps5ShUxQ","number":5934,"title":"Modify levels of some logging messages","user":{"login":"Laurent2916","id":21087104,"node_id":"MDQ6VXNlcjIxMDg3MTA0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/21087104?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Laurent2916","html_url":"https:\/\/github.com\/Laurent2916","followers_url":"https:\/\/api.github.com\/users\/Laurent2916\/followers","following_url":"https:\/\/api.github.com\/users\/Laurent2916\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Laurent2916\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Laurent2916\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Laurent2916\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Laurent2916\/orgs","repos_url":"https:\/\/api.github.com\/users\/Laurent2916\/repos","events_url":"https:\/\/api.github.com\/users\/Laurent2916\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Laurent2916\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-08T13:31:44Z","updated_at":"2023-07-12T18:21:03Z","closed_at":"2023-07-12T18:21:02Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Some warning messages didn't quite sound like warnings so I modified their logging levels to info.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5934\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5934\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5934","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5934","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5934.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5934.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5933","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5933\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5933\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5933\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5933","id":1747382500,"node_id":"PR_kwDODunzps5Sfi5J","number":5933,"title":"Fix `to_numpy` when None values in the sequence","user":{"login":"qgallouedec","id":45557362,"node_id":"MDQ6VXNlcjQ1NTU3MzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45557362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/qgallouedec","html_url":"https:\/\/github.com\/qgallouedec","followers_url":"https:\/\/api.github.com\/users\/qgallouedec\/followers","following_url":"https:\/\/api.github.com\/users\/qgallouedec\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/qgallouedec\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/qgallouedec\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/qgallouedec\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/qgallouedec\/orgs","repos_url":"https:\/\/api.github.com\/users\/qgallouedec\/repos","events_url":"https:\/\/api.github.com\/users\/qgallouedec\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/qgallouedec\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-08T08:38:56Z","updated_at":"2023-06-09T13:49:41Z","closed_at":"2023-06-09T13:23:48Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Closes #5927 \r\nI've realized that the error was overlooked during testing due to the presence of only one None value in the sequence.\r\nUnfortunately, it was the only case where the function works as expected. When the sequence contained more than one None value, the function failed. Consequently, I've updated the tests to include sequences with multiple None values.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5933\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5933\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5933","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5933","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5933.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5933.patch","merged_at":"2023-06-09T13:23:48Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5932","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5932\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5932\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5932\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5932","id":1746249161,"node_id":"PR_kwDODunzps5Sbrzo","number":5932,"title":"[doc build] Use secrets","user":{"login":"mishig25","id":11827707,"node_id":"MDQ6VXNlcjExODI3NzA3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11827707?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mishig25","html_url":"https:\/\/github.com\/mishig25","followers_url":"https:\/\/api.github.com\/users\/mishig25\/followers","following_url":"https:\/\/api.github.com\/users\/mishig25\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mishig25\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mishig25\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mishig25\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mishig25\/orgs","repos_url":"https:\/\/api.github.com\/users\/mishig25\/repos","events_url":"https:\/\/api.github.com\/users\/mishig25\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mishig25\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-07T16:09:39Z","updated_at":"2023-06-09T10:16:58Z","closed_at":"2023-06-09T09:53:16Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Companion pr to https:\/\/github.com\/huggingface\/doc-builder\/pull\/379","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5932\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5932\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5932","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5932","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5932.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5932.patch","merged_at":"2023-06-09T09:53:16Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5931","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5931\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5931\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5931\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5931","id":1745408784,"node_id":"I_kwDODunzps5oCNMQ","number":5931,"title":"`datasets.map` not reusing cached copy by default","user":{"login":"bhavitvyamalik","id":19718818,"node_id":"MDQ6VXNlcjE5NzE4ODE4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/19718818?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/bhavitvyamalik","html_url":"https:\/\/github.com\/bhavitvyamalik","followers_url":"https:\/\/api.github.com\/users\/bhavitvyamalik\/followers","following_url":"https:\/\/api.github.com\/users\/bhavitvyamalik\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/bhavitvyamalik\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/bhavitvyamalik\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/bhavitvyamalik\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/bhavitvyamalik\/orgs","repos_url":"https:\/\/api.github.com\/users\/bhavitvyamalik\/repos","events_url":"https:\/\/api.github.com\/users\/bhavitvyamalik\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/bhavitvyamalik\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-06-07T09:03:33Z","updated_at":"2023-06-21T16:15:40Z","closed_at":"2023-06-21T16:15:40Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen I load the dataset from local directory, it's cached copy is picked up after first time. However, for `map` operation, the operation is applied again and cached copy is not picked up. Is there any way to pick cached copy instead of processing it again? The only solution I could think of was to use `save_to_disk` after my last transform and then use that in my DataLoader pipeline. Are there any other solutions for the same?\r\n\r\nOne more thing, my dataset is occupying 6GB storage memory after I use `map`, is there any way I can reduce that memory usage?\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\n# make sure that dataset decodes audio with correct sampling rate\r\ndataset_sampling_rate = next(iter(self.raw_datasets.values())).features[\"audio\"].sampling_rate\r\nif dataset_sampling_rate != self.feature_extractor.sampling_rate:\r\n    self.raw_datasets = self.raw_datasets.cast_column(\r\n        \"audio\", datasets.features.Audio(sampling_rate=self.feature_extractor.sampling_rate)\r\n    )\r\n\r\nvectorized_datasets = self.raw_datasets.map(\r\n    self.prepare_dataset,\r\n    remove_columns=next(iter(self.raw_datasets.values())).column_names,\r\n    num_proc=self.num_workers,\r\n    desc=\"preprocess datasets\",\r\n)\r\n# filter data that is longer than max_input_length\r\nself.vectorized_datasets = vectorized_datasets.filter(\r\n    self.is_audio_in_length_range,\r\n    num_proc=self.num_workers,\r\n    input_columns=[\"input_length\"],\r\n        )\r\n\r\ndef prepare_dataset(self, batch):\r\n    # load audio\r\n    sample = batch[\"audio\"]\r\n    inputs = self.feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\r\n    batch[\"input_values\"] = inputs.input_values[0]\r\n    batch[\"input_length\"] = len(batch[\"input_values\"])\r\n\r\n    batch[\"labels\"] = self.tokenizer(batch[\"target_text\"]).input_ids\r\n    return batch\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\n`map` to use cached copy and if possible an alternative technique to reduce memory usage after using `map`\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.8.16\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5931\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5931\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5930","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5930\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5930\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5930\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5930","id":1745184395,"node_id":"I_kwDODunzps5oBWaL","number":5930,"title":"loading private custom dataset script - authentication error","user":{"login":"flckv","id":103381497,"node_id":"U_kgDOBil5-Q","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/103381497?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/flckv","html_url":"https:\/\/github.com\/flckv","followers_url":"https:\/\/api.github.com\/users\/flckv\/followers","following_url":"https:\/\/api.github.com\/users\/flckv\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/flckv\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/flckv\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/flckv\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/flckv\/orgs","repos_url":"https:\/\/api.github.com\/users\/flckv\/repos","events_url":"https:\/\/api.github.com\/users\/flckv\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/flckv\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-06-07T06:58:23Z","updated_at":"2023-06-15T14:49:21Z","closed_at":"2023-06-15T14:49:20Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nTrain model with my custom dataset stored in HuggingFace and loaded with the loading script requires authentication but I am not sure how ?\r\n\r\n\r\nI am logged in in the terminal, in the browser. I receive this error: \r\n\r\n\r\n\/python3.8\/site-packages\/datasets\/utils\/file_utils.py\", line 566, in get_from_cache\r\n    raise ConnectionError(f\"Couldn't reach {url} ({repr(head_error)})\")\r\nConnectionError: Couldn't reach https:\/\/huggingface.co\/datasets\/fkov\/s\/blob\/main\/data\/s\/train\/labels `(ConnectionError('Unauthorized for URL `https:\/\/huggingface.co\/datasets\/fkov\/s\/blob\/main\/data\/s\/train\/labels. Please use the parameter `**`use_auth_token=True`**` after logging in with `**`huggingface-cli login`**`'))\r\n\r\nwhen I added: `use_auth_token=True` and logged in via terminal then I received error:\r\n\r\nor the same error in different format: \r\nraise ConnectionError(f\"`Couldn't reach {url} (error {response.status_code}`)\")\r\nConnectionError: Couldn't reach https:\/\/huggingface.co\/datasets\/fkov\/s\/blob\/main\/data\/s\/train\/labels (`error 401`)\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\n1.  cloned transformers library locally:\r\nhttps:\/\/huggingface.co\/docs\/transformers\/v4.15.0\/examples :\r\n\r\n> git clone https:\/\/github.com\/huggingface\/transformers\r\n> cd transformers\r\n> pip install .\r\n> cd \/transformers\/examples\/pytorch\/audio-classification\r\n> pip install -r requirements.txt\r\n\r\n2. created **loading script** \r\n> https:\/\/huggingface.co\/docs\/datasets\/dataset_script  added next to dataset:\r\n\r\n3. uploaded **private custom dataset** with loading script to HuggingFace\r\n> https:\/\/huggingface.co\/docs\/datasets\/dataset_script\r\n\r\n4. added dataset loading script to **local directory** in the above cloned transformers library:\r\n> cd \/transformers\/examples\/pytorch\/audio-classification\r\n\r\n5. logged in to HuggingFace on local terminal with :\r\n> **huggingface-cli login**\r\n\r\n6.  run the model with the custom dataset stored on HuggingFace with code:  https:\/\/github.com\/huggingface\/transformers\/blob\/main\/examples\/pytorch\/audio-classification\/README.md\r\n\r\n     cd \/transformers\/examples\/pytorch\/audio-classification\r\n>  python run_audio_classification.py \\\r\n>     --model_name_or_path facebook\/wav2vec2-base \\\r\n>     --output_dir l\/users\/flck\/outputs\/wav2vec2-base-s \\\r\n>     --overwrite_output_dir \\\r\n>     --dataset_name s \\\r\n>     --dataset_config_name s \\\r\n>     --remove_unused_columns False \\\r\n>     --do_train \\\r\n>     --do_eval \\\r\n>     --fp16 \\\r\n>     --learning_rate 3e-5 \\\r\n>     --max_length_seconds 1 \\\r\n>     --attention_mask False \\\r\n>     --warmup_ratio 0.1 \\\r\n>     --num_train_epochs 5 \\\r\n>     --per_device_train_batch_size 32 \\\r\n>     --gradient_accumulation_steps 4 \\\r\n>     --per_device_eval_batch_size 32 \\\r\n>     --dataloader_num_workers 4 \\\r\n>     --logging_strategy steps \\\r\n>     --logging_steps 10 \\\r\n>     --evaluation_strategy epoch \\\r\n>     --save_strategy epoch \\\r\n>     --load_best_model_at_end True \\\r\n>     --metric_for_best_model accuracy \\\r\n>     --save_total_limit 3 \\\r\n>     --seed 0 \\\r\n>     --push_to_hub \\\r\n>     **--use_auth_token=True**  \r\n\r\n\n\n### Expected behavior\n\nBe able to train a model the https:\/\/github.com\/huggingface\/transformers\/blob\/main\/examples\/pytorch\/audio-classification\/ run_audio_classification.py with private custom dataset stored on HuggingFace.\n\n### Environment info\n\n- datasets version:                2.12.0                  \r\n- `transformers` version: 4.30.0.dev0\r\n- Platform: Linux-5.4.204-ql-generic-12.0-19-x86_64-with-glibc2.17\r\n- Python version: 3.8.12\r\n- Huggingface_hub version: 0.15.1\r\n- Safetensors version: 0.3.1\r\n- PyTorch version (GPU?): 2.0.1+cu117 (True)\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.3\r\n[pip3] torch==2.0.1\r\n[pip3] torchaudio==2.0.2\r\n[conda] numpy                     1.24.3                   pypi_0    pypi\r\n[conda] torch                     2.0.1                    pypi_0    pypi\r\n[conda] torchaudio                2.0.2                    pypi_0    pypi\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5930\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5930\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5929","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5929\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5929\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5929\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5929","id":1744478456,"node_id":"I_kwDODunzps5n-qD4","number":5929,"title":"Importing PyTorch reduces multiprocessing performance for map","user":{"login":"Maxscha","id":12814709,"node_id":"MDQ6VXNlcjEyODE0NzA5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12814709?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Maxscha","html_url":"https:\/\/github.com\/Maxscha","followers_url":"https:\/\/api.github.com\/users\/Maxscha\/followers","following_url":"https:\/\/api.github.com\/users\/Maxscha\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Maxscha\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Maxscha\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Maxscha\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Maxscha\/orgs","repos_url":"https:\/\/api.github.com\/users\/Maxscha\/repos","events_url":"https:\/\/api.github.com\/users\/Maxscha\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Maxscha\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-06T19:42:25Z","updated_at":"2023-06-16T13:09:12Z","closed_at":"2023-06-16T13:09:12Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI noticed that the performance of my dataset preprocessing with `map(...,num_proc=32)` decreases when PyTorch is imported.\r\n\r\n### Steps to reproduce the bug\r\n\r\nI created two example scripts to reproduce this behavior:\r\n\r\n```\r\nimport datasets\r\ndatasets.disable_caching()\r\n\r\nfrom datasets import Dataset\r\nimport time\r\n    \r\nPROC=32\r\n\r\nif __name__ == \"__main__\":\r\n    dataset = [True] * 10000000\r\n    dataset = Dataset.from_dict({'train': dataset})\r\n    \r\n\r\n    start = time.time()\r\n    dataset.map(lambda x: x, num_proc=PROC)\r\n    end = time.time()\r\n    print(end - start)\r\n```\r\nTakes around 4 seconds on my machine.\r\n\r\nWhile the same code, but with an `import torch`:\r\n```\r\nimport datasets\r\ndatasets.disable_caching()\r\n\r\nfrom datasets import Dataset\r\nimport time\r\nimport torch\r\n    \r\nPROC=32\r\n\r\nif __name__ == \"__main__\":\r\n    dataset = [True] * 10000000\r\n    dataset = Dataset.from_dict({'train': dataset})\r\n    \r\n\r\n    start = time.time()\r\n    dataset.map(lambda x: x, num_proc=PROC)\r\n    end = time.time()\r\n    print(end - start)\r\n```\r\ntakes around 22 seconds.\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect that the import of torch to not have such a significant effect on the performance of map using multiprocessing.\r\n\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.15.0-56-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.3\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.2\r\n- torch: 2.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5929\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5929\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5928","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5928\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5928\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5928\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5928","id":1744098371,"node_id":"PR_kwDODunzps5SUXPC","number":5928,"title":"Fix link to quickstart docs in README.md","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-06T15:23:01Z","updated_at":"2023-06-06T15:52:34Z","closed_at":"2023-06-06T15:43:53Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5928\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5928\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5928","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5928","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5928.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5928.patch","merged_at":"2023-06-06T15:43:53Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5927","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5927\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5927\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5927\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5927","id":1744009032,"node_id":"I_kwDODunzps5n83dI","number":5927,"title":"`IndexError` when indexing `Sequence` of `Array2D` with `None` values","user":{"login":"qgallouedec","id":45557362,"node_id":"MDQ6VXNlcjQ1NTU3MzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45557362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/qgallouedec","html_url":"https:\/\/github.com\/qgallouedec","followers_url":"https:\/\/api.github.com\/users\/qgallouedec\/followers","following_url":"https:\/\/api.github.com\/users\/qgallouedec\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/qgallouedec\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/qgallouedec\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/qgallouedec\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/qgallouedec\/orgs","repos_url":"https:\/\/api.github.com\/users\/qgallouedec\/repos","events_url":"https:\/\/api.github.com\/users\/qgallouedec\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/qgallouedec\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-06T14:36:22Z","updated_at":"2023-06-13T12:39:39Z","closed_at":"2023-06-09T13:23:50Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nHaving `None` values in a `Sequence` of `ArrayND` fails.\r\n\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Array2D, Dataset, Features, Sequence\r\n\r\ndata = [\r\n    [\r\n        [[0]],\r\n        None,\r\n        None,\r\n    ]\r\n]\r\nfeature = Sequence(Array2D((1, 1), dtype=\"int64\"))\r\ndataset = Dataset.from_dict({\"a\": data}, features=Features({\"a\": feature}))\r\n\r\ndataset[0]  # error raised only when indexing\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/quentingallouedec\/gia\/c.py\", line 13, in <module>\r\n    dataset[0]  # error raised only when indexing\r\n  File \"\/Users\/quentingallouedec\/gia\/env\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2658, in __getitem__\r\n    return self._getitem(key)\r\n  File \"\/Users\/quentingallouedec\/gia\/env\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 2643, in _getitem\r\n    formatted_output = format_table(\r\n  File \"\/Users\/quentingallouedec\/gia\/env\/lib\/python3.10\/site-packages\/datasets\/formatting\/formatting.py\", line 634, in format_table\r\n    return formatter(pa_table, query_type=query_type)\r\n  File \"\/Users\/quentingallouedec\/gia\/env\/lib\/python3.10\/site-packages\/datasets\/formatting\/formatting.py\", line 406, in __call__\r\n    return self.format_row(pa_table)\r\n  File \"\/Users\/quentingallouedec\/gia\/env\/lib\/python3.10\/site-packages\/datasets\/formatting\/formatting.py\", line 441, in format_row\r\n    row = self.python_arrow_extractor().extract_row(pa_table)\r\n  File \"\/Users\/quentingallouedec\/gia\/env\/lib\/python3.10\/site-packages\/datasets\/formatting\/formatting.py\", line 144, in extract_row\r\n    return _unnest(pa_table.to_pydict())\r\n  File \"pyarrow\/table.pxi\", line 4146, in pyarrow.lib.Table.to_pydict\r\n  File \"pyarrow\/table.pxi\", line 1312, in pyarrow.lib.ChunkedArray.to_pylist\r\n  File \"pyarrow\/array.pxi\", line 1521, in pyarrow.lib.Array.to_pylist\r\n  File \"pyarrow\/scalar.pxi\", line 675, in pyarrow.lib.ListScalar.as_py\r\n  File \"\/Users\/quentingallouedec\/gia\/env\/lib\/python3.10\/site-packages\/datasets\/features\/features.py\", line 760, in to_pylist\r\n    return self.to_numpy(zero_copy_only=zero_copy_only).tolist()\r\n  File \"\/Users\/quentingallouedec\/gia\/env\/lib\/python3.10\/site-packages\/datasets\/features\/features.py\", line 725, in to_numpy\r\n    numpy_arr = np.insert(numpy_arr.astype(np.float64), null_indices, np.nan, axis=0)\r\n  File \"<__array_function__ internals>\", line 200, in insert\r\n  File \"\/Users\/quentingallouedec\/gia\/env\/lib\/python3.10\/site-packages\/numpy\/lib\/function_base.py\", line 5426, in insert\r\n    old_mask[indices] = False\r\nIndexError: index 3 is out of bounds for axis 0 with size 3\r\n```\r\n\r\nAFAIK, the problem only occurs when you use a `Sequence` of `ArrayND`.\r\n\r\nI strongly suspect that the problem comes from this line, or `np.insert` is misused:\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/02ee418831aba68d0be93227bce8b3f42ef8980f\/src\/datasets\/features\/features.py#L729\r\n\r\nTo put t simply, you want something that do that:\r\n\r\n```python\r\nimport numpy as np\r\nnumpy_arr = np.zeros((1, 1, 1))\r\nnull_indices = np.array([1, 2])\r\nnp.insert(numpy_arr, null_indices, np.nan, axis=0)\r\n# raise an error, instead of outputting \r\n# array([[[ 0.]],\r\n#        [[nan]],\r\n#        [[nan]]])\r\n```\r\n\r\n\n\n### Expected behavior\n\nThe previous code should not raise an error.\n\n### Environment info\n\n- Python 3.10.11\r\n- datasets 2.10.0\r\n- pyarrow 12.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5927\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5927\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5926","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5926\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5926\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5926\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5926","id":1743922028,"node_id":"I_kwDODunzps5n8iNs","number":5926,"title":"Uncaught exception when generating the splits from a dataset that miss data","user":{"login":"severo","id":1676121,"node_id":"MDQ6VXNlcjE2NzYxMjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1676121?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/severo","html_url":"https:\/\/github.com\/severo","followers_url":"https:\/\/api.github.com\/users\/severo\/followers","following_url":"https:\/\/api.github.com\/users\/severo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/severo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/severo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/severo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/severo\/orgs","repos_url":"https:\/\/api.github.com\/users\/severo\/repos","events_url":"https:\/\/api.github.com\/users\/severo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/severo\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-06-06T13:51:01Z","updated_at":"2023-06-07T07:53:16Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nDataset https:\/\/huggingface.co\/datasets\/blog_authorship_corpus has an issue with its hosting platform, since https:\/\/drive.google.com\/u\/0\/uc?id=1cGy4RNDV87ZHEXbiozABr9gsSrZpPaPz&export=download returns 404 error.\r\n\r\nBut when trying to generate the split names, we get an exception which is now correctly caught.\r\n\r\nSeen originally in https:\/\/github.com\/huggingface\/datasets-server\/blob\/adbdcd6710ffed4e2eb2e4cd905b5e0dff530a15\/services\/worker\/src\/worker\/job_runners\/config\/parquet_and_info.py#L435\n\n### Steps to reproduce the bug\n\n```python\r\n>>> from datasets import StreamingDownloadManager, load_dataset_builder\r\n>>> builder = load_dataset_builder(path=\"blog_authorship_corpus\")\r\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.60k\/5.60k [00:00<00:00, 23.1MB\/s]\r\nDownloading metadata: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.81k\/2.81k [00:00<00:00, 14.7MB\/s]\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.30k\/7.30k [00:00<00:00, 30.8MB\/s]\r\n>>> dl_manager = StreamingDownloadManager(base_path=builder.base_path)\r\n>>> builder._split_generators(dl_manager)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/slesage\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/blog_authorship_corpus\/6f5d78241afd8313111956f877a57db7a0e9fc6718255dc85df0928197feb683\/blog_authorship_corpus.py\", line 79, in _split_generators\r\n    data = dl_manager.download_and_extract(_DATA_URL)\r\n  File \"\/home\/slesage\/hf\/datasets-server\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/download\/streaming_download_manager.py\", line 1087, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"\/home\/slesage\/hf\/datasets-server\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/download\/streaming_download_manager.py\", line 1039, in extract\r\n    urlpaths = map_nested(self._extract, url_or_urls, map_tuple=True)\r\n  File \"\/home\/slesage\/hf\/datasets-server\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/utils\/py_utils.py\", line 435, in map_nested\r\n    return function(data_struct)\r\n  File \"\/home\/slesage\/hf\/datasets-server\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/download\/streaming_download_manager.py\", line 1044, in _extract\r\n    protocol = _get_extraction_protocol(urlpath, use_auth_token=self.download_config.use_auth_token)\r\n  File \"\/home\/slesage\/hf\/datasets-server\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/datasets\/download\/streaming_download_manager.py\", line 433, in _get_extraction_protocol\r\n    with fsspec.open(urlpath, **kwargs) as f:\r\n  File \"\/home\/slesage\/hf\/datasets-server\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/fsspec\/core.py\", line 439, in open\r\n    return open_files(\r\n  File \"\/home\/slesage\/hf\/datasets-server\/services\/worker\/.venv\/lib\/python3.9\/site-packages\/fsspec\/core.py\", line 194, in __getitem__\r\n    out = super().__getitem__(item)\r\nIndexError: list index out of range\r\n```\n\n### Expected behavior\n\nWe should have an Exception raised by the datasets library.\n\n### Environment info\n\n\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.19.0-1026-aws-x86_64-with-glibc2.35\r\n- Python version: 3.9.15\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5926\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5926\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5925","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5925\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5925\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5925\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5925","id":1741941436,"node_id":"I_kwDODunzps5n0-q8","number":5925,"title":"Breaking API change in datasets.list_datasets caused by change in HfApi.list_datasets","user":{"login":"mtkinit","id":78868366,"node_id":"MDQ6VXNlcjc4ODY4MzY2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/78868366?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mtkinit","html_url":"https:\/\/github.com\/mtkinit","followers_url":"https:\/\/api.github.com\/users\/mtkinit\/followers","following_url":"https:\/\/api.github.com\/users\/mtkinit\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mtkinit\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mtkinit\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mtkinit\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mtkinit\/orgs","repos_url":"https:\/\/api.github.com\/users\/mtkinit\/repos","events_url":"https:\/\/api.github.com\/users\/mtkinit\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mtkinit\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-06-05T14:46:04Z","updated_at":"2023-06-19T17:22:43Z","closed_at":"2023-06-19T17:22:43Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi all,\r\n\r\nafter an update of the `datasets` library, we observer crashes in our code. We relied on `datasets.list_datasets` returning a `list`. Now, after the API of the HfApi.list_datasets was changed and it returns a `list` instead of an `Iterable`, the `datasets.list_datasets` now sometimes returns a `list` and somesimes an `Iterable`. \r\n\r\nIt would be helpful to indicate that by the return type of the `datasets.list_datasets` function.\r\n\r\nThanks,\r\nMartin \n\n### Steps to reproduce the bug\n\nHere, the code crashed after we updated the `datasets` library:\r\n\r\n```python\r\n# list_datasets no longer returns a list, which leads to an error when one tries to slice it\r\nfor datasets.list_datasets(with_details=True)[:limit]:\r\n    ...\r\n```\n\n### Expected behavior\n\nIt would be helpful to indicate that by the return type of the `datasets.list_datasets` function.\n\n### Environment info\n\nUbuntu 22.04\r\ndatasets 2.12.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5925\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5925\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5924","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5924\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5924\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5924\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5924","id":1738889236,"node_id":"PR_kwDODunzps5SCiFv","number":5924,"title":"Add parallel module using joblib for Spark","user":{"login":"es94129","id":12763339,"node_id":"MDQ6VXNlcjEyNzYzMzM5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12763339?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/es94129","html_url":"https:\/\/github.com\/es94129","followers_url":"https:\/\/api.github.com\/users\/es94129\/followers","following_url":"https:\/\/api.github.com\/users\/es94129\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/es94129\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/es94129\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/es94129\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/es94129\/orgs","repos_url":"https:\/\/api.github.com\/users\/es94129\/repos","events_url":"https:\/\/api.github.com\/users\/es94129\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/es94129\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-06-02T22:25:25Z","updated_at":"2023-06-14T10:25:10Z","closed_at":"2023-06-14T10:15:46Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Discussion in https:\/\/github.com\/huggingface\/datasets\/issues\/5798","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5924\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5924\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5924","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5924","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5924.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5924.patch","merged_at":"2023-06-14T10:15:46Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5923","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5923\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5923\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5923\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5923","id":1737436227,"node_id":"I_kwDODunzps5njyxD","number":5923,"title":"Cannot import datasets - ValueError: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility","user":{"login":"ehuangc","id":71412682,"node_id":"MDQ6VXNlcjcxNDEyNjgy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/71412682?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ehuangc","html_url":"https:\/\/github.com\/ehuangc","followers_url":"https:\/\/api.github.com\/users\/ehuangc\/followers","following_url":"https:\/\/api.github.com\/users\/ehuangc\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ehuangc\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ehuangc\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ehuangc\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ehuangc\/orgs","repos_url":"https:\/\/api.github.com\/users\/ehuangc\/repos","events_url":"https:\/\/api.github.com\/users\/ehuangc\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ehuangc\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":18,"created_at":"2023-06-02T04:16:32Z","updated_at":"2023-12-13T15:53:52Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen trying to import datasets, I get a pyarrow ValueError:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/edward\/test\/test.py\", line 1, in <module>\r\n    import datasets\r\n  File \"\/Users\/edward\/opt\/anaconda3\/envs\/cs235\/lib\/python3.9\/site-packages\/datasets\/__init__.py\", line 43, in <module>\r\n    from .arrow_dataset import Dataset\r\n  File \"\/Users\/edward\/opt\/anaconda3\/envs\/cs235\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 65, in <module>\r\n    from .arrow_reader import ArrowReader\r\n  File \"\/Users\/edward\/opt\/anaconda3\/envs\/cs235\/lib\/python3.9\/site-packages\/datasets\/arrow_reader.py\", line 28, in <module>\r\n    import pyarrow.parquet as pq\r\n  File \"\/Users\/edward\/opt\/anaconda3\/envs\/cs235\/lib\/python3.9\/site-packages\/pyarrow\/parquet\/__init__.py\", line 20, in <module>\r\n    from .core import *\r\n  File \"\/Users\/edward\/opt\/anaconda3\/envs\/cs235\/lib\/python3.9\/site-packages\/pyarrow\/parquet\/core.py\", line 45, in <module>\r\n    from pyarrow.fs import (LocalFileSystem, FileSystem, FileType,\r\n  File \"\/Users\/edward\/opt\/anaconda3\/envs\/cs235\/lib\/python3.9\/site-packages\/pyarrow\/fs.py\", line 49, in <module>\r\n    from pyarrow._gcsfs import GcsFileSystem  # noqa\r\n  File \"pyarrow\/_gcsfs.pyx\", line 1, in init pyarrow._gcsfs\r\nValueError: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 88 from C header, got 72 from PyObject\r\n\r\n### Steps to reproduce the bug\r\n\r\n`import datasets`\r\n\r\n### Expected behavior\r\n\r\nSuccessful import\r\n\r\n### Environment info\r\n\r\nConda environment, MacOS\r\npython 3.9.12\r\ndatasets 2.12.0\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5923\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5923\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5922","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5922\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5922\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5922\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5922","id":1736898953,"node_id":"I_kwDODunzps5nhvmJ","number":5922,"title":"Length of table does not accurately reflect the split","user":{"login":"amogkam","id":8068268,"node_id":"MDQ6VXNlcjgwNjgyNjg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8068268?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/amogkam","html_url":"https:\/\/github.com\/amogkam","followers_url":"https:\/\/api.github.com\/users\/amogkam\/followers","following_url":"https:\/\/api.github.com\/users\/amogkam\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/amogkam\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/amogkam\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/amogkam\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/amogkam\/orgs","repos_url":"https:\/\/api.github.com\/users\/amogkam\/repos","events_url":"https:\/\/api.github.com\/users\/amogkam\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/amogkam\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892913,"node_id":"MDU6TGFiZWwxOTM1ODkyOTEz","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/wontfix","name":"wontfix","color":"ffffff","default":true,"description":"This will not be worked on"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-06-01T18:56:26Z","updated_at":"2023-06-02T16:13:31Z","closed_at":"2023-06-02T16:13:31Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI load a Huggingface Dataset and do `train_test_split`. I'm expecting the underlying table for the dataset to also be split, but it's not.\n\n### Steps to reproduce the bug\n\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/8068268\/83e5768f-8b4c-422a-945c-832a7585afff)\r\n\n\n### Expected behavior\n\nThe expected behavior is when `len(hf_dataset[\"train\"].data)` should match the length of the train split, and not be the entire unsplit dataset.\n\n### Environment info\n\ndatasets 2.10.1\r\npython 3.10.11","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5922\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5922\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5921","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5921\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5921\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5921\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5921","id":1736563023,"node_id":"PR_kwDODunzps5R6j-y","number":5921,"title":"Fix streaming parquet with image feature in schema","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-06-01T15:23:10Z","updated_at":"2023-06-02T10:02:54Z","closed_at":"2023-06-02T09:53:11Z","author_association":"MEMBER","active_lock_reason":null,"body":"It was not reading the feature type from the parquet arrow schema","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5921\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5921\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5921","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5921","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5921.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5921.patch","merged_at":"2023-06-02T09:53:11Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5920","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5920\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5920\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5920\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5920","id":1736196991,"node_id":"PR_kwDODunzps5R5TRB","number":5920,"title":"Optimize IterableDataset.from_file using ArrowExamplesIterable","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-06-01T12:14:36Z","updated_at":"2023-06-01T12:42:10Z","closed_at":"2023-06-01T12:35:14Z","author_association":"MEMBER","active_lock_reason":null,"body":"following https:\/\/github.com\/huggingface\/datasets\/pull\/5893","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5920\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5920\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5920","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5920","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5920.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5920.patch","merged_at":"2023-06-01T12:35:14Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5919","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5919\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5919\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5919\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5919","id":1735519227,"node_id":"PR_kwDODunzps5R2_EK","number":5919,"title":"add support for storage_options for load_dataset API","user":{"login":"janineguo","id":59083384,"node_id":"MDQ6VXNlcjU5MDgzMzg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59083384?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/janineguo","html_url":"https:\/\/github.com\/janineguo","followers_url":"https:\/\/api.github.com\/users\/janineguo\/followers","following_url":"https:\/\/api.github.com\/users\/janineguo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/janineguo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/janineguo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/janineguo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/janineguo\/orgs","repos_url":"https:\/\/api.github.com\/users\/janineguo\/repos","events_url":"https:\/\/api.github.com\/users\/janineguo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/janineguo\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":12,"created_at":"2023-06-01T05:52:32Z","updated_at":"2023-07-18T06:14:32Z","closed_at":"2023-07-17T17:02:00Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"to solve the issue in #5880 \r\n\r\n1.  add s3 support in the link check step, previous we only check `http` and `https`,\r\n\r\n2. change the parameter of `use_auth_token` to `download_config` to support both `storage_options` and `use_auth_token` parameter when trying to handle(list, open, read, etc,.) the remote files.\r\n\r\n3.  integrate the check part's duplicate code to make adding or deleting other sources easier.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5919\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5919\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5919","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5919","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5919.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5919.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5918","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5918\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5918\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5918\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5918","id":1735313549,"node_id":"I_kwDODunzps5nbsiN","number":5918,"title":"File not found for audio dataset","user":{"login":"RobertBaruch","id":1783950,"node_id":"MDQ6VXNlcjE3ODM5NTA=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1783950?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/RobertBaruch","html_url":"https:\/\/github.com\/RobertBaruch","followers_url":"https:\/\/api.github.com\/users\/RobertBaruch\/followers","following_url":"https:\/\/api.github.com\/users\/RobertBaruch\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/RobertBaruch\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/RobertBaruch\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/RobertBaruch\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/RobertBaruch\/orgs","repos_url":"https:\/\/api.github.com\/users\/RobertBaruch\/repos","events_url":"https:\/\/api.github.com\/users\/RobertBaruch\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/RobertBaruch\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-06-01T02:15:29Z","updated_at":"2023-06-11T06:02:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nAfter loading an audio dataset, and looking at a sample entry, the `path` element, which is supposed to be the path to the audio file, doesn't actually exist.\r\n\n\n### Steps to reproduce the bug\n\nRun bug.py:\r\n\r\n```py\r\nimport os.path\r\n\r\nfrom datasets import load_dataset\r\n\r\ndef run() -> None:\r\n    cv13 = load_dataset(\r\n        \"mozilla-foundation\/common_voice_13_0\",\r\n        \"hi\",\r\n        split=\"train\",\r\n    )\r\n\r\n    print(cv13[0])\r\n    audio_file = cv13[0][\"path\"]\r\n    if not os.path.exists(audio_file):\r\n        raise ValueError(f'File {audio_file} does not exist.')\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nThe result (on my machine):\r\n\r\n```json\r\n{'client_id': '0f018a99663f33afbb7d38aee281fb1afcfd07f9e7acd00383f604e1e17c38d6ed8adf1bd2ccbf927a52c5adefb8ac4b158ce27a7c2ed9581e71202eb302dfb3', 'path': 'C:\\\\Users\\\\rober\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\8d1479bc09b4609bc2675bd02d6869a4d5e09f7e6616f540bd55eacef46c6e2b\\\\common_voice_hi_26008353.mp3', 'audio': {'path': 'C:\\\\Users\\\\rober\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\8d1479bc09b4609bc2675bd02d6869a4d5e09f7e6616f540bd55eacef46c6e2b\\\\common_voice_hi_26008353.mp3', 'array': array([ 6.46234854e-26, -1.35709319e-25, -8.07793567e-26, ...,\r\n        1.06425944e-07,  4.46417090e-08,  2.61451660e-09]), 'sampling_rate': 48000}, 'sentence': '\u0939\u092e\u0928\u0947 \u0909\u0938\u0915\u093e \u091c\u0928\u094d\u092e\u0926\u093f\u0928 \u092e\u0928\u093e\u092f\u093e\u0964', 'up_votes': 2, 'down_votes': 0, 'age': '', 'gender': '', 'accent': '', 'locale': 'hi', 'segment': '' ', 'variant': ''}\r\n```\r\n\r\n```txt\r\nTraceback (most recent call last):\r\n  File \"F:\\eo-reco\\bug.py\", line 18, in <module>\r\n    run()\r\n  File \"F:\\eo-reco\\bug.py\", line 15, in run\r\n    raise ValueError(f'File {audio_file} does not exist.')\r\nValueError: File C:\\Users\\rober\\.cache\\huggingface\\datasets\\downloads\\extracted\\8d1479bc09b4609bc2675bd02d6869a4d5e09f7e6616f540bd55eacef46c6e2b\\common_voice_hi_26008353.mp3 does not exist.\r\n```\r\n\n\n### Expected behavior\n\nThe `path` element points to the correct file, which happens to be:\r\n\r\n```\r\nC:\\Users\\rober\\.cache\\huggingface\\datasets\\downloads\\extracted\\8d1479bc09b4609bc2675bd02d6869a4d5e09f7e6616f540bd55eacef46c6e2b\\hi_train_0\\common_voice_hi_26008353.mp3\r\n```\r\n\r\nThat is, there's an extra directory `hi_train_0` that is not in the `path` element.\r\n\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.11.3\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1\r\n- ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5918\/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5918\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5917","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5917\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5917\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5917\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5917","id":1733661588,"node_id":"PR_kwDODunzps5RwoRU","number":5917,"title":"Refactor extensions","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-31T08:33:02Z","updated_at":"2023-05-31T13:34:35Z","closed_at":"2023-05-31T13:25:57Z","author_association":"MEMBER","active_lock_reason":null,"body":"Related to:\r\n- #5850","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5917\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5917\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5917","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5917","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5917.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5917.patch","merged_at":"2023-05-31T13:25:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5916","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5916\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5916\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5916\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5916","id":1732456392,"node_id":"PR_kwDODunzps5RskTb","number":5916,"title":"Unpin responses","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-05-30T14:59:48Z","updated_at":"2023-05-30T18:03:10Z","closed_at":"2023-05-30T17:53:29Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #5906","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5916\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5916\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5916","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5916","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5916.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5916.patch","merged_at":"2023-05-30T17:53:29Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5915","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5915\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5915\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5915\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5915","id":1732389984,"node_id":"PR_kwDODunzps5RsVzj","number":5915,"title":"Raise error in `DatasetBuilder.as_dataset` when `file_format` is not `\"arrow\"`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-05-30T14:27:55Z","updated_at":"2023-05-31T13:31:21Z","closed_at":"2023-05-31T13:23:54Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Raise an error in  `DatasetBuilder.as_dataset` when `file_format != \"arrow\"` (and fix the docstring)\r\n\r\nFix #5874 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5915\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5915\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5915","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5915","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5915.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5915.patch","merged_at":"2023-05-31T13:23:54Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5914","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5914\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5914\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5914\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5914","id":1731483996,"node_id":"I_kwDODunzps5nNFlc","number":5914,"title":"array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size in Datasets","user":{"login":"ravenouse","id":85110830,"node_id":"MDQ6VXNlcjg1MTEwODMw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/85110830?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ravenouse","html_url":"https:\/\/github.com\/ravenouse","followers_url":"https:\/\/api.github.com\/users\/ravenouse\/followers","following_url":"https:\/\/api.github.com\/users\/ravenouse\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ravenouse\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ravenouse\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ravenouse\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ravenouse\/orgs","repos_url":"https:\/\/api.github.com\/users\/ravenouse\/repos","events_url":"https:\/\/api.github.com\/users\/ravenouse\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ravenouse\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-05-30T04:25:00Z","updated_at":"2023-05-30T04:25:00Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen using the `filter` or `map` function to preprocess a dataset, a ValueError is encountered with the error message \"array is too big; arr.size * arr.dtype.itemsize is larger than the maximum possible size.\" \r\n\r\nDetailed error message:\r\nTraceback (most recent call last):\r\n  File \"data_processing.py\", line 26, in <module>\r\n    processed_dataset[split] = samromur_children[split].map(prepare_dataset, cache_file_name=cache_dict[split],writer_batch_size = 50)\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/datasets\/arrow_dataset.py\", line 2405, in map\r\n    desc=desc,\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/datasets\/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/datasets\/arrow_dataset.py\", line 524, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/datasets\/fingerprint.py\", line 480, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/datasets\/arrow_dataset.py\", line 2756, in _map_single\r\n    example = apply_function_on_filtered_inputs(example, i, offset=offset)\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/datasets\/arrow_dataset.py\", line 2655, in apply_function_on_filtered_inputs\r\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/datasets\/arrow_dataset.py\", line 2347, in decorated\r\n    result = f(decorated_item, *args, **kwargs)\r\n  File \"data_processing.py\", line 11, in prepare_dataset\r\n    audio = batch[\"audio\"]\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/datasets\/arrow_dataset.py\", line 123, in __getitem__\r\n    value = decode_nested_example(self.features[key], value) if value is not None else None\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/datasets\/features\/features.py\", line 1260, in decode_nested_example\r\n    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id) if obj is not None else None\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/datasets\/features\/audio.py\", line 156, in decode_example\r\n    array, sampling_rate = self._decode_non_mp3_path_like(path, token_per_repo_id=token_per_repo_id)\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/datasets\/features\/audio.py\", line 257, in _decode_non_mp3_path_like\r\n    array, sampling_rate = librosa.load(f, sr=self.sampling_rate, mono=self.mono)\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/librosa\/core\/audio.py\", line 176, in load\r\n    y, sr_native = __soundfile_load(path, offset, duration, dtype)\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/librosa\/core\/audio.py\", line 222, in __soundfile_load\r\n    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/soundfile.py\", line 891, in read\r\n    out = self._create_empty_array(frames, always_2d, dtype)\r\n  File \"\/projects\/zhwa3087\/software\/anaconda\/envs\/mycustomenv\/lib\/python3.7\/site-packages\/soundfile.py\", line 1323, in _create_empty_array\r\n    return np.empty(shape, dtype, order='C')\r\nValueError: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.\r\n\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import load_dataset, DatasetDict\r\nfrom transformers import WhisperFeatureExtractor\r\nfrom transformers import WhisperTokenizer\r\n\r\nsamromur_children= load_dataset(\"language-and-voice-lab\/samromur_children\")\r\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai\/whisper-small\")\r\ntokenizer = WhisperTokenizer.from_pretrained(\"openai\/whisper-small\", language=\"icelandic\", task=\"transcribe\")\r\n\r\ndef prepare_dataset(batch):\r\n    # load and resample audio data from 48 to 16kHz\r\n    audio = batch[\"audio\"]\r\n\r\n    # compute log-Mel input features from input audio array \r\n    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=16000).input_features[0]\r\n\r\n    # encode target text to label ids \r\n    batch[\"labels\"] = tokenizer(batch[\"normalized_text\"]).input_ids\r\n    return batch\r\n\r\ncache_dict = {\"train\": \".\/cache\/audio_train.cache\", \\\r\n              \"validation\": \".\/cache\/audio_validation.cache\", \\\r\n              \"test\": \".\/cache\/audio_test.cache\"}\r\nfilter_cache_dict = {\"train\": \".\/cache\/filter_train.arrow\", \\\r\n                \"validation\": \".\/cache\/filter_validation.arrow\", \\\r\n                \"test\": \".\/cache\/filter_test.arrow\"}\r\n\r\nprint(\"before filtering\")\r\nprint(samromur_children)\r\n#filter the dataset to only include examples with more than 2 seconds of audio\r\nsamromur_children = samromur_children.filter(lambda example: example[\"audio\"][\"array\"].shape[0] > 16000*2, cache_file_names=filter_cache_dict)   \r\nprint(\"after filtering\")\r\nprint(samromur_children)\r\nprocessed_dataset = DatasetDict()\r\n# processed_dataset = samromur_children.map(prepare_dataset, cache_file_names=cache_dict, num_proc=10,)\r\nfor split in [\"train\", \"validation\", \"test\"]:\r\n    processed_dataset[split] = samromur_children[split].map(prepare_dataset, cache_file_name=cache_dict[split])\r\n```\n\n### Expected behavior\n\nThe dataset is successfully processed and ready to train the model.\n\n### Environment info\n\nPython version:  3.7.13\r\ndatasets package version: 2.4.0\r\nlibrosa package version: 0.10.0.post2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5914\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5914\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5913","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5913\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5913\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5913\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5913","id":1731427484,"node_id":"I_kwDODunzps5nM3yc","number":5913,"title":"I tried to load a custom dataset using the following statement: dataset = load_dataset('json', data_files=data_files). The dataset contains 50 million text-image pairs, but an error occurred.","user":{"login":"cjt222","id":17508662,"node_id":"MDQ6VXNlcjE3NTA4NjYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17508662?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/cjt222","html_url":"https:\/\/github.com\/cjt222","followers_url":"https:\/\/api.github.com\/users\/cjt222\/followers","following_url":"https:\/\/api.github.com\/users\/cjt222\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/cjt222\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/cjt222\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/cjt222\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/cjt222\/orgs","repos_url":"https:\/\/api.github.com\/users\/cjt222\/repos","events_url":"https:\/\/api.github.com\/users\/cjt222\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/cjt222\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-30T02:55:26Z","updated_at":"2023-07-24T12:00:38Z","closed_at":"2023-07-24T12:00:38Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nFile \"\/home\/kas\/.conda\/envs\/diffusers\/lib\/python3.7\/site-packages\/datasets\/builder.py\", line 1858, in _prepare_split_single\r\nDownloading and preparing dataset json\/default to \/home\/kas\/diffusers\/examples\/dreambooth\/cache_data\/datasets\/json\/default-acf423d8c6ef99d0\/0.0.0\/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\r\nDownloading data files: 0%| | 0\/1 [00:00<?, ?it\/s] Downloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 84.35it\/s]\r\nExtracting data files: 0%| | 0\/1 [00:00<?, ?it\/s] for _, table in generator:\r\nFile \"\/home\/kas\/.conda\/envs\/diffusers\/lib\/python3.7\/site-packages\/datasets\/packaged_modules\/json\/json.py\", line 114, in _generate_tables\r\nio.BytesIO(batch), read_options=paj.ReadOptions(block_size=block_size)\r\nFile \"pyarrow\/_json.pyx\", line 258, in pyarrow._json.read_json\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 27.72it\/s]\r\nGenerating train split: 0 examples [00:00, ? examples\/s] File \"pyarrow\/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\nFile \"pyarrow\/error.pxi\", line 125, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowCapacityError: array cannot contain more than 2147483646 bytes, have 2390448764\n\n### Steps to reproduce the bug\n\n1\u3001data_files = [\"1.json\", \"2.json\", \"3.json\"]\r\n2\u3001dataset = load_dataset('json', data_files=data_files)\n\n### Expected behavior\n\nRead the dataset normally.\r\n\r\n\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Linux-4.15.0-29-generic-x86_64-with-debian-buster-sid\r\n- Python version: 3.7.16\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 1.3.5","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5913\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5913\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5912","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5912\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5912\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5912\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5912","id":1730299852,"node_id":"I_kwDODunzps5nIkfM","number":5912,"title":"Missing elements in `map` a batched dataset","user":{"login":"sachinruk","id":1410927,"node_id":"MDQ6VXNlcjE0MTA5Mjc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1410927?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sachinruk","html_url":"https:\/\/github.com\/sachinruk","followers_url":"https:\/\/api.github.com\/users\/sachinruk\/followers","following_url":"https:\/\/api.github.com\/users\/sachinruk\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sachinruk\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sachinruk\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sachinruk\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sachinruk\/orgs","repos_url":"https:\/\/api.github.com\/users\/sachinruk\/repos","events_url":"https:\/\/api.github.com\/users\/sachinruk\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sachinruk\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-29T08:09:19Z","updated_at":"2023-07-26T15:48:15Z","closed_at":"2023-07-26T15:48:15Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nAs outlined [here](https:\/\/discuss.huggingface.co\/t\/length-error-using-map-with-datasets\/40969\/3?u=sachin), the following collate function drops 5 out of possible 6 elements in the batch (it is 6 because out of the eight, two are bad links in laion). A reproducible [kaggle kernel ](https:\/\/www.kaggle.com\/sachin\/laion-hf-dataset\/edit) can be found here.\r\n\r\nThe weirdest part is when inspecting the sizes of the tensors as shown below, both `tokenized_captions[\"input_ids\"]` and `image_features` show the correct shapes. Simply the output only has one element (with the batch dimension squeezed out).\r\n\r\n```python\r\nclass CollateFn:        \r\n    def get_image(self, url):\r\n        try:\r\n            response = requests.get(url)\r\n            return Image.open(io.BytesIO(response.content)).convert(\"RGB\")\r\n        except PIL.UnidentifiedImageError:\r\n            logger.info(f\"Reading error: Could not transform f{url}\")\r\n            return None\r\n        except requests.exceptions.ConnectionError:\r\n            logger.info(f\"Connection error: Could not transform f{url}\")\r\n            return None\r\n        \r\n        \r\n    def __call__(self, batch):\r\n        images = [self.get_image(url) for url in batch[\"url\"]]\r\n        captions = [caption for caption, image in zip(batch[\"caption\"], images) if image is not None]\r\n        images = [image for image in images if image is not None]\r\n        \r\n        tokenized_captions = tokenizer(\r\n            captions,\r\n            padding=\"max_length\",\r\n            truncation=True,\r\n            max_length=tokenizer.model_max_length,\r\n            return_tensors=\"pt\",\r\n        )\r\n        \r\n        image_features = torch.stack([torch.Tensor(feature_extractor(image)[\"pixel_values\"][0]) for image in images])\r\n#         import pdb; pdb.set_trace()\r\n        return {\"input_ids\": tokenized_captions[\"input_ids\"], \"images\": image_features}\r\n\r\ncollate_fn = CollateFn()\r\nlaion_ds = datasets.load_dataset(\"laion\/laion400m\", split=\"train\", streaming=True)\r\nlaion_ds_batched = laion_ds.map(collate_fn, batched=True, batch_size=8, remove_columns=next(iter(laion_ds)).keys())\r\n```\n\n### Steps to reproduce the bug\n\nA reproducible [kaggle kernel ](https:\/\/www.kaggle.com\/sachin\/laion-hf-dataset\/edit) can be found here.\n\n### Expected behavior\n\nWould expect `next(iter(laion_ds_batched))` to produce two tensors of shape `(batch_size, 77)` and `batch_size, image_shape`.\n\n### Environment info\n\ndatasets==2.12.0\r\npython==3.10","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5912\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5912\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5910","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5910\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5910\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5910\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5910","id":1728909790,"node_id":"I_kwDODunzps5nDRHe","number":5910,"title":"Cannot use both set_format and set_transform","user":{"login":"ybouane","id":14046002,"node_id":"MDQ6VXNlcjE0MDQ2MDAy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14046002?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ybouane","html_url":"https:\/\/github.com\/ybouane","followers_url":"https:\/\/api.github.com\/users\/ybouane\/followers","following_url":"https:\/\/api.github.com\/users\/ybouane\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ybouane\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ybouane\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ybouane\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ybouane\/orgs","repos_url":"https:\/\/api.github.com\/users\/ybouane\/repos","events_url":"https:\/\/api.github.com\/users\/ybouane\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ybouane\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-05-27T19:22:23Z","updated_at":"2023-07-09T21:40:54Z","closed_at":"2023-06-16T14:41:24Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI need to process some data using the set_transform method but I also need the data to be formatted for pytorch before processing it.\r\n\r\nI don't see anywhere in the documentation something that says that both methods cannot be used at the same time.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"mnist\", split=\"train\")\r\nds.set_format(type=\"torch\")\r\ndef transform(entry):\r\n    return entry[\"image\"].double()\r\nds.set_transform(transform)\r\n\r\nprint(ds[0])\r\n```\r\n\r\n### Expected behavior\r\n\r\nIt should print the pytorch tensor image as a double, but it errors because \"entry\" in the transform function doesn't receive a pytorch tensor to begin with, it receives a PIL Image -> entry.double() errors because entry isn't a pytorch tensor.\r\n\r\n### Environment info\r\nLatest versions.\r\n\r\n\r\n### Note:\r\nIt would be at least handy to have access to a function that can do the dataset.set_format in the set_transform function.\r\n\r\nSomething like:\r\n```\r\nfrom datasets import load_dataset, do_format\r\nds = load_dataset(\"mnist\", split=\"train\")\r\ndef transform(entry):\r\n    entry = do_format(entry, type=\"torch\")\r\n    return entry[\"image\"].double()\r\nds.set_transform(transform)\r\n\r\nprint(ds[0])\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5910\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5910\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5909","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5909\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5909\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5909\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5909","id":1728900068,"node_id":"PR_kwDODunzps5Rgga6","number":5909,"title":"Use more efficient and idiomatic way to construct list.","user":{"login":"ttsugriy","id":172294,"node_id":"MDQ6VXNlcjE3MjI5NA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/172294?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ttsugriy","html_url":"https:\/\/github.com\/ttsugriy","followers_url":"https:\/\/api.github.com\/users\/ttsugriy\/followers","following_url":"https:\/\/api.github.com\/users\/ttsugriy\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ttsugriy\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ttsugriy\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ttsugriy\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ttsugriy\/orgs","repos_url":"https:\/\/api.github.com\/users\/ttsugriy\/repos","events_url":"https:\/\/api.github.com\/users\/ttsugriy\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ttsugriy\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-27T18:54:47Z","updated_at":"2023-05-31T15:37:11Z","closed_at":"2023-05-31T13:28:29Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Using `*` is ~2X faster according to [benchmark](https:\/\/colab.research.google.com\/gist\/ttsugriy\/c964a2604edf70c41911b10335729b6a\/for-vs-mult.ipynb) with just 4 patterns. This doesn't matter much since this tiny difference is not going to be noticeable, but why not?","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5909\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5909\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5909","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5909","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5909.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5909.patch","merged_at":"2023-05-31T13:28:28Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5908","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5908\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5908\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5908\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5908","id":1728653935,"node_id":"I_kwDODunzps5nCSpv","number":5908,"title":"Unbearably slow sorting on big mapped datasets","user":{"login":"maximxlss","id":29152154,"node_id":"MDQ6VXNlcjI5MTUyMTU0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/29152154?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/maximxlss","html_url":"https:\/\/github.com\/maximxlss","followers_url":"https:\/\/api.github.com\/users\/maximxlss\/followers","following_url":"https:\/\/api.github.com\/users\/maximxlss\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/maximxlss\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/maximxlss\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/maximxlss\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/maximxlss\/orgs","repos_url":"https:\/\/api.github.com\/users\/maximxlss\/repos","events_url":"https:\/\/api.github.com\/users\/maximxlss\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/maximxlss\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-05-27T11:08:32Z","updated_at":"2023-06-13T17:45:10Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nFor me, with ~40k lines, sorting took 3.5 seconds on a flattened dataset (including the flatten operation) and 22.7 seconds on a mapped dataset (right after sharding), which is about x5 slowdown. Moreover, it seems like it slows down exponentially with bigger datasets (wasn't able to sort 700k lines at all, with flattening takes about a minute).\n\n### Steps to reproduce the bug\n\n```Python\r\nfrom datasets import load_dataset\r\nimport time\r\n\r\ndataset = load_dataset(\"xnli\", \"en\", split=\"train\")\r\n\r\ndataset = dataset.shard(10, 0)\r\n\r\nprint(len(dataset))\r\n\r\nt = time.time()\r\n\r\n# dataset = dataset.flatten_indices()  # uncomment this line and it's fast\r\n\r\ndataset = dataset.sort(\"label\", reverse=True, load_from_cache_file=False)\r\n\r\nprint(f\"finished in {time.time() - t:.4f} seconds\")\r\n\r\n```\n\n### Expected behavior\n\nExpect sorting to take the same or less time than flattening and then sorting.\n\n### Environment info\n\n- `datasets` version: 2.12.1.dev0 (same with 2.12.0 too)\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.10.10\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5908\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5908\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5907","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5907\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5907\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5907\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5907","id":1728648560,"node_id":"PR_kwDODunzps5RfqUU","number":5907,"title":"Add `flatten_indices` to `DatasetDict`","user":{"login":"maximxlss","id":29152154,"node_id":"MDQ6VXNlcjI5MTUyMTU0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/29152154?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/maximxlss","html_url":"https:\/\/github.com\/maximxlss","followers_url":"https:\/\/api.github.com\/users\/maximxlss\/followers","following_url":"https:\/\/api.github.com\/users\/maximxlss\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/maximxlss\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/maximxlss\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/maximxlss\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/maximxlss\/orgs","repos_url":"https:\/\/api.github.com\/users\/maximxlss\/repos","events_url":"https:\/\/api.github.com\/users\/maximxlss\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/maximxlss\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-27T10:55:44Z","updated_at":"2023-06-01T11:46:35Z","closed_at":"2023-06-01T11:39:36Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Add `flatten_indices` to `DatasetDict` for convinience","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5907\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5907\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5907","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5907","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5907.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5907.patch","merged_at":"2023-06-01T11:39:35Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5906","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5906\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5906\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5906\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5906","id":1728171113,"node_id":"I_kwDODunzps5nAcxp","number":5906,"title":"Could you unpin responses version?","user":{"login":"kenimou","id":47789026,"node_id":"MDQ6VXNlcjQ3Nzg5MDI2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47789026?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kenimou","html_url":"https:\/\/github.com\/kenimou","followers_url":"https:\/\/api.github.com\/users\/kenimou\/followers","following_url":"https:\/\/api.github.com\/users\/kenimou\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kenimou\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kenimou\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kenimou\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kenimou\/orgs","repos_url":"https:\/\/api.github.com\/users\/kenimou\/repos","events_url":"https:\/\/api.github.com\/users\/kenimou\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kenimou\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-05-26T20:02:14Z","updated_at":"2023-05-30T17:53:31Z","closed_at":"2023-05-30T17:53:31Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nCould you unpin [this](https:\/\/github.com\/huggingface\/datasets\/blob\/main\/setup.py#L139) or move it to test requirements? This is a testing library and we also use it for our tests as well. We do not want to use a very outdated version.\n\n### Steps to reproduce the bug\n\ncould not install this library due to dependency conflict.\n\n### Expected behavior\n\ncan install datasets\n\n### Environment info\n\nlinux 64","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5906\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5906\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5905","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5905\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5905\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5905\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5905","id":1727541392,"node_id":"I_kwDODunzps5m-DCQ","number":5905,"title":"Offer an alternative to Iterable Dataset that allows lazy loading and processing while skipping batches efficiently","user":{"login":"Hubert-Bonisseur","id":48770768,"node_id":"MDQ6VXNlcjQ4NzcwNzY4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/48770768?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur","html_url":"https:\/\/github.com\/Hubert-Bonisseur","followers_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/followers","following_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/orgs","repos_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/repos","events_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-26T12:33:02Z","updated_at":"2023-06-15T13:34:18Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\r\n\r\nI would like a way to resume training from a checkpoint without waiting for a very long time when using an iterable dataset.\r\n\r\n### Motivation\r\n\r\nI am training models on the speech-recognition task. I have very large datasets that I can't comfortably store on a disk and also quite computationally intensive audio processing to do. As a result I want to load data from my remote when it is needed and perform all processing on the fly.\r\n\r\nI am currently using the iterable dataset feature of _datasets_. It does everything I need with one exception. My issue is that when resuming training at a step n, we have to download all the data and perform the processing of steps < n, just to get the iterable at the right step. In my case it takes almost as long as training for the same steps, which make resuming training from a checkpoint useless in practice.\r\n\r\nI understand that the nature of iterators make it probably nearly impossible to quickly resume training.\r\n\r\nI thought about a possible solution nonetheless : \r\n\r\nI could in fact index my large dataset and make it a mapped dataset. Then I could use set_transform to perform the processing on the fly. Finally, if I'm not mistaken, the _accelerate_ package allows to [skip steps efficiently](https:\/\/github.com\/huggingface\/accelerate\/blob\/a73898027a211c3f6dc4460351b0ec246aa824aa\/src\/accelerate\/data_loader.py#L827) for a mapped dataset.\r\n\r\nIs it possible to lazily load samples of a mapped dataset ? I'm used to [dataset scripts](https:\/\/huggingface.co\/docs\/datasets\/dataset_script), maybe something can be done there.\r\nIf not, I could do it using a plain _Pytorch_ dataset. Then I would need to convert it to a _datasets_' dataset to get all the features of _datasets_. Is it something possible ?\r\n\r\n### Your contribution\r\n\r\nI could provide a PR to allow lazy loading of mapped dataset or the conversion of a mapped _Pytorch_ dataset into a _Datasets_ dataset if you think it is an useful new feature.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5905\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5905\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5904","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5904\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5904\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5904\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5904","id":1727415626,"node_id":"PR_kwDODunzps5Rbfks","number":5904,"title":"Validate name parameter in make_file_instructions","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-26T11:12:46Z","updated_at":"2023-05-31T07:43:32Z","closed_at":"2023-05-31T07:34:57Z","author_association":"MEMBER","active_lock_reason":null,"body":"Validate `name` parameter in `make_file_instructions`.\r\n\r\nThis way users get more informative error messages, instead of:\r\n```stacktrace\r\n...\/huggingface\/datasets\/src\/datasets\/arrow_reader.py in make_file_instructions(name, split_infos, instruction, filetype_suffix, prefix_path)\r\n    110     name2len = {info.name: info.num_examples for info in split_infos}\r\n    111     name2shard_lengths = {info.name: info.shard_lengths for info in split_infos}\r\n--> 112     name2filenames = {\r\n    113         info.name: filenames_for_dataset_split(\r\n    114             path=prefix_path,\r\n\r\n...\/huggingface\/datasets\/src\/datasets\/arrow_reader.py in <dictcomp>(.0)\r\n    111     name2shard_lengths = {info.name: info.shard_lengths for info in split_infos}\r\n    112     name2filenames = {\r\n--> 113         info.name: filenames_for_dataset_split(\r\n    114             path=prefix_path,\r\n    115             dataset_name=name,\r\n\r\n...\/huggingface\/datasets\/src\/datasets\/naming.py in filenames_for_dataset_split(path, dataset_name, split, filetype_suffix, shard_lengths)\r\n     68 \r\n     69 def filenames_for_dataset_split(path, dataset_name, split, filetype_suffix=None, shard_lengths=None):\r\n---> 70     prefix = filename_prefix_for_split(dataset_name, split)\r\n     71     prefix = os.path.join(path, prefix)\r\n     72 \r\n\r\n...\/huggingface\/datasets\/src\/datasets\/naming.py in filename_prefix_for_split(name, split)\r\n     52 \r\n     53 def filename_prefix_for_split(name, split):\r\n---> 54     if os.path.basename(name) != name:\r\n     55         raise ValueError(f\"Should be a dataset name, not a path: {name}\")\r\n     56     if not re.match(_split_re, split):\r\n\r\n...\/lib\/python3.9\/posixpath.py in basename(p)\r\n    140 def basename(p):\r\n    141     \"\"\"Returns the final component of a pathname\"\"\"\r\n--> 142     p = os.fspath(p)\r\n    143     sep = _get_sep(p)\r\n    144     i = p.rfind(sep) + 1\r\n\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n```\r\n\r\nRelated to #5895.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5904\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5904\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5904","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5904","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5904.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5904.patch","merged_at":"2023-05-31T07:34:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5903","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5903\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5903\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5903\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5903","id":1727372549,"node_id":"PR_kwDODunzps5RbV82","number":5903,"title":"Relax `ci.yml` trigger for `pull_request` based on modified paths","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-26T10:46:52Z","updated_at":"2023-09-07T15:52:36Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## What's in this PR?\r\n\r\nAs of a previous PR at #5902, I've seen that the CI was automatically trigger on any file, in that case when modifying a Jupyter Notebook (.ipynb), which IMO could be skipped, as the modification on the Jupyter Notebook has no effect\/impact on the `ci.yml` outcome. So this PR controls the paths that trigger the `ci.yml` to avoid wasting resources when not needed.\r\n\r\n## What's pending in this PR?\r\n\r\nI would like to confirm whether this should affect both `push` and `pull_request`, since just modifications in those files won't change the `ci.yml` outcome, so maybe it's worth skipping it too in the `push` trigger.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5903\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5903\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5903","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5903","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5903.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5903.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5902","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5902\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5902\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5902\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5902","id":1727342194,"node_id":"PR_kwDODunzps5RbPS9","number":5902,"title":"Fix `Overview.ipynb` & detach Jupyter Notebooks from `datasets` repository","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2023-05-26T10:25:01Z","updated_at":"2023-07-25T13:50:06Z","closed_at":"2023-07-25T13:38:33Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## What's in this PR?\r\n\r\nThis PR solves #5887 since there was a mismatch between the tokenizer and the model used, since the tokenizer was `bert-base-cased` while the model was `distilbert-base-case` both for the PyTorch and TensorFlow alternatives. Since DistilBERT doesn't use\/need the `token_type_ids`, the `**batch` was failing, as the batch contained `input_ids`, `attention_mask`, `token_type_ids`, `start_positions` and `end_positions`, and `token_type_ids` was not required.\r\n\r\nBesides that, at the end `seqeval` was being used to evaluate the model predictions, and just `evaluate` was being installed, so I've also included the `seqeval` installation.\r\n\r\nFinally, I've re-run everything in Google Colab, and every cell was successfully executed!\r\n\r\n## What was done on top of the original PR?\r\n\r\nBased on the comments from @mariosasko and @stevhliu, I've updated the contents of this PR to also review the `quickstart.mdx` and update what was needed, besides that, we may eventually move the `Overview.ipynb` dataset to `huggingface\/notebooks` following @stevhliu suggestions.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5902\/reactions","total_count":2,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5902\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5902","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5902","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5902.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5902.patch","merged_at":"2023-07-25T13:38:33Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5901","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5901\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5901\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5901\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5901","id":1727179016,"node_id":"PR_kwDODunzps5Rarux","number":5901,"title":"Make prepare_split more robust if errors in metadata dataset_info splits","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-26T08:48:22Z","updated_at":"2023-06-02T06:06:38Z","closed_at":"2023-06-01T13:39:40Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR uses `split_generator.split_info` as default value for `split_info` if any exception is raised while trying to get `split_generator.name` from `self.info.splits` (this may happen if there is any error in the metadata dataset_info splits).\r\n\r\nPlease note that `split_info` is only used by the logger.\r\n\r\nFix #5895 if passed `verification_mode=\"no_checks\"`:\r\n```python\r\nds = load_dataset(\r\n    \"ArmelR\/stack-exchange-instruction\", \r\n    data_dir=\"data\/finetune\", \r\n    split=\"train\", \r\n    verification_mode=\"no_checks\", \r\n    revision=\"c609f1caade5cfbf3b9fe9cfa17d7cb000b457bd\",\r\n)\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5901\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5901\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5901","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5901","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5901.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5901.patch","merged_at":"2023-06-01T13:39:39Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5900","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5900\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5900\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5900\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5900","id":1727129617,"node_id":"PR_kwDODunzps5RahTR","number":5900,"title":"Fix minor typo in docs loading.mdx","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-26T08:10:54Z","updated_at":"2023-05-26T09:34:15Z","closed_at":"2023-05-26T09:25:12Z","author_association":"MEMBER","active_lock_reason":null,"body":"Minor fix.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5900\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5900\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5900","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5900","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5900.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5900.patch","merged_at":"2023-05-26T09:25:12Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5899","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5899\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5899\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5899\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5899","id":1726279011,"node_id":"PR_kwDODunzps5RXods","number":5899,"title":"canonicalize data dir in config ID hash","user":{"login":"kylrth","id":5044802,"node_id":"MDQ6VXNlcjUwNDQ4MDI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5044802?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kylrth","html_url":"https:\/\/github.com\/kylrth","followers_url":"https:\/\/api.github.com\/users\/kylrth\/followers","following_url":"https:\/\/api.github.com\/users\/kylrth\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kylrth\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kylrth\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kylrth\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kylrth\/orgs","repos_url":"https:\/\/api.github.com\/users\/kylrth\/repos","events_url":"https:\/\/api.github.com\/users\/kylrth\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kylrth\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-25T18:17:10Z","updated_at":"2023-06-02T16:02:15Z","closed_at":"2023-06-02T15:52:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"fixes #5871 \r\n\r\nThe second commit is optional but improves readability.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5899\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5899\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5899","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5899","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5899.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5899.patch","merged_at":"2023-06-02T15:52:04Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5898","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5898\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5898\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5898\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5898","id":1726190481,"node_id":"I_kwDODunzps5m45OR","number":5898,"title":"Loading The flores data set for specific language","user":{"login":"106AbdulBasit","id":36159918,"node_id":"MDQ6VXNlcjM2MTU5OTE4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36159918?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/106AbdulBasit","html_url":"https:\/\/github.com\/106AbdulBasit","followers_url":"https:\/\/api.github.com\/users\/106AbdulBasit\/followers","following_url":"https:\/\/api.github.com\/users\/106AbdulBasit\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/106AbdulBasit\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/106AbdulBasit\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/106AbdulBasit\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/106AbdulBasit\/orgs","repos_url":"https:\/\/api.github.com\/users\/106AbdulBasit\/repos","events_url":"https:\/\/api.github.com\/users\/106AbdulBasit\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/106AbdulBasit\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-25T17:08:55Z","updated_at":"2023-05-25T17:21:38Z","closed_at":"2023-05-25T17:21:37Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am trying to load the Flores data set\r\n\r\nthe code which is given is\r\n```\r\n\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"facebook\/flores\")\r\n\r\n```\r\nThis gives the error of config name \r\n\r\n\"\"ValueError: Config name is missing\"\r\n\r\nNow if I add some config it gives me the some error\r\n\r\n\"HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'facebook\/flores, 'ace_Arab''.\r\n\"\r\n\r\nHow I can load the data of the specific language ?\r\n\r\nCouldn't find any tutorial\r\n\r\nany one can help me out?\n\n### Steps to reproduce the bug\n\nstep one load the data set \r\n\r\n`from datasets import load_dataset\r\n\r\ndataset = load_dataset(\"facebook\/flores\")`\r\n\r\nit gives the error of config\r\n\r\nonce config is given \r\n\r\nit gives the error of\r\n\r\n\"HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'facebook\/flores, 'ace_Arab''.\r\n\"\n\n### Expected behavior\n\nData set should be loaded but  I am receiving error\n\n### Environment info\n\nDatasets , python , ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5898\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5898\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5897","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5897\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5897\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5897\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5897","id":1726135494,"node_id":"PR_kwDODunzps5RXJaY","number":5897,"title":"Fix `FixedSizeListArray` casting","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-05-25T16:26:33Z","updated_at":"2023-05-26T12:22:04Z","closed_at":"2023-05-26T11:57:16Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix cast on sliced `FixedSizeListArray`s.\r\n\r\nFix #5866","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5897\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5897\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5897","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5897","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5897.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5897.patch","merged_at":"2023-05-26T11:57:16Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5896","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5896\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5896\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5896\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5896","id":1726022500,"node_id":"I_kwDODunzps5m4QNk","number":5896,"title":"HuggingFace does not cache downloaded files aggressively\/early enough","user":{"login":"geajack","id":2124157,"node_id":"MDQ6VXNlcjIxMjQxNTc=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2124157?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/geajack","html_url":"https:\/\/github.com\/geajack","followers_url":"https:\/\/api.github.com\/users\/geajack\/followers","following_url":"https:\/\/api.github.com\/users\/geajack\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/geajack\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/geajack\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/geajack\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/geajack\/orgs","repos_url":"https:\/\/api.github.com\/users\/geajack\/repos","events_url":"https:\/\/api.github.com\/users\/geajack\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/geajack\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-25T15:14:36Z","updated_at":"2023-12-20T06:51:45Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI wrote the following script:\r\n\r\n```\r\nimport datasets\r\n\r\ndataset = datasets.load.load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:10000]\")\r\n```\r\n\r\nI ran it and spent 90 minutes downloading a 20GB file. Then I saw:\r\n\r\n```\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.3G\/20.3G [1:30:29<00:00, 3.73MB\/s]\r\nTraceback (most recent call last):\r\n  File \"\/home\/jack\/Code\/Projects\/Transformers\/Codebase\/main.py\", line 5, in <module>\r\n    dataset = datasets.load.load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:10000]\")\r\n  File \"\/home\/jack\/.local\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1782, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/jack\/.local\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 883, in download_and_prepare\r\n    self._save_info()\r\n  File \"\/home\/jack\/.local\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 2037, in _save_info\r\n    import apache_beam as beam\r\nModuleNotFoundError: No module named 'apache_beam'\r\n```\r\n\r\nAnd the 20GB of data was seemingly instantly gone forever, because when I ran the script again, it had to do the download again.\n\n### Steps to reproduce the bug\n\nSee above\n\n### Expected behavior\n\nSee above\n\n### Environment info\n\ndatasets 2.10.1\r\nPython 3.10","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5896\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5896\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5895","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5895\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5895\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5895\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5895","id":1725467252,"node_id":"I_kwDODunzps5m2Ip0","number":5895,"title":"The dir name and split strings are confused when loading ArmelR\/stack-exchange-instruction dataset","user":{"login":"DongHande","id":45357817,"node_id":"MDQ6VXNlcjQ1MzU3ODE3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45357817?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/DongHande","html_url":"https:\/\/github.com\/DongHande","followers_url":"https:\/\/api.github.com\/users\/DongHande\/followers","following_url":"https:\/\/api.github.com\/users\/DongHande\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/DongHande\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/DongHande\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/DongHande\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/DongHande\/orgs","repos_url":"https:\/\/api.github.com\/users\/DongHande\/repos","events_url":"https:\/\/api.github.com\/users\/DongHande\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/DongHande\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-25T09:39:06Z","updated_at":"2023-05-29T02:32:12Z","closed_at":"2023-05-29T02:32:12Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen I load the ArmelR\/stack-exchange-instruction dataset, I encounter a bug that may be raised by confusing the dir name string and the split string about the dataset. \r\n\r\nWhen I use the script \"datasets.load_dataset('ArmelR\/stack-exchange-instruction', data_dir=\"data\/finetune\", split=\"train\", use_auth_token=True)\", it fails. But it succeeds when I add the \"streaming = True\" parameter. \r\n\r\nThe website of the dataset is https:\/\/huggingface.co\/datasets\/ArmelR\/stack-exchange-instruction\/ .\r\n\r\nThe traceback logs are as below: \r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/xxx\/miniconda3\/envs\/code\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1797, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/xxx\/miniconda3\/envs\/code\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 890, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/xxx\/miniconda3\/envs\/code\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 985, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/home\/xxx\/miniconda3\/envs\/code\/lib\/python3.9\/site-packages\/datasets\/builder.py\", line 1706, in _prepare_split\r\n    split_info = self.info.splits[split_generator.name]\r\n  File \"\/home\/xxx\/miniconda3\/envs\/code\/lib\/python3.9\/site-packages\/datasets\/splits.py\", line 530, in __getitem__\r\n    instructions = make_file_instructions(\r\n  File \"\/home\/xxx\/miniconda3\/envs\/code\/lib\/python3.9\/site-packages\/datasets\/arrow_reader.py\", line 112, in make_file_instructions\r\n    name2filenames = {\r\n  File \"\/home\/xxx\/miniconda3\/envs\/code\/lib\/python3.9\/site-packages\/datasets\/arrow_reader.py\", line 113, in <dictcomp>\r\n    info.name: filenames_for_dataset_split(\r\n  File \"\/home\/xxx\/miniconda3\/envs\/code\/lib\/python3.9\/site-packages\/datasets\/naming.py\", line 70, in filenames_for_dataset_split\r\n    prefix = filename_prefix_for_split(dataset_name, split)\r\n  File \"\/home\/xxx\/miniconda3\/envs\/code\/lib\/python3.9\/site-packages\/datasets\/naming.py\", line 54, in filename_prefix_for_split\r\n    if os.path.basename(name) != name:\r\n  File \"\/home\/xxx\/miniconda3\/envs\/code\/lib\/python3.9\/posixpath.py\", line 142, in basename\r\n    p = os.fspath(p)\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n\r\n\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. import datasets library function: ```from datasets import load_dataset```\r\n2. load dataset: ```ds=load_dataset('ArmelR\/stack-exchange-instruction', data_dir=\"data\/finetune\", split=\"train\", use_auth_token=True)```\r\n\r\n### Expected behavior\r\n\r\nThe dataset can be loaded successfully without the streaming setting. \r\n\r\n### Environment info\r\n\r\nLinux, \r\npython=3.9\r\ndatasets=2.12.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5895\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5895\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5894","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5894\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5894\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5894\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5894","id":1724774910,"node_id":"PR_kwDODunzps5RSjot","number":5894,"title":"Force overwrite existing filesystem protocol","user":{"login":"baskrahmer","id":24520725,"node_id":"MDQ6VXNlcjI0NTIwNzI1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24520725?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/baskrahmer","html_url":"https:\/\/github.com\/baskrahmer","followers_url":"https:\/\/api.github.com\/users\/baskrahmer\/followers","following_url":"https:\/\/api.github.com\/users\/baskrahmer\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/baskrahmer\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/baskrahmer\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/baskrahmer\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/baskrahmer\/orgs","repos_url":"https:\/\/api.github.com\/users\/baskrahmer\/repos","events_url":"https:\/\/api.github.com\/users\/baskrahmer\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/baskrahmer\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-24T21:41:53Z","updated_at":"2023-05-25T06:52:08Z","closed_at":"2023-05-25T06:42:33Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #5876","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5894\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5894\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5894","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5894","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5894.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5894.patch","merged_at":"2023-05-25T06:42:33Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5893","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5893\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5893\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5893\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5893","id":1722519056,"node_id":"PR_kwDODunzps5RK40K","number":5893,"title":"Load cached dataset as iterable ","user":{"login":"mariusz-jachimowicz-83","id":10278877,"node_id":"MDQ6VXNlcjEwMjc4ODc3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10278877?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83","html_url":"https:\/\/github.com\/mariusz-jachimowicz-83","followers_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/followers","following_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/repos","events_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariusz-jachimowicz-83\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-05-23T17:40:35Z","updated_at":"2023-06-01T11:58:24Z","closed_at":"2023-06-01T11:51:29Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"To be used to train models it allows to load an IterableDataset from the cached Arrow file.  \r\nSee https:\/\/github.com\/huggingface\/datasets\/issues\/5481","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5893\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5893\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5893","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5893","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5893.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5893.patch","merged_at":"2023-06-01T11:51:29Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5892","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5892\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5892\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5892\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5892","id":1722503824,"node_id":"I_kwDODunzps5mq1KQ","number":5892,"title":"User access requests with manual review do not notify the dataset owner","user":{"login":"leondz","id":121934,"node_id":"MDQ6VXNlcjEyMTkzNA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/121934?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/leondz","html_url":"https:\/\/github.com\/leondz","followers_url":"https:\/\/api.github.com\/users\/leondz\/followers","following_url":"https:\/\/api.github.com\/users\/leondz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/leondz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/leondz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/leondz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/leondz\/orgs","repos_url":"https:\/\/api.github.com\/users\/leondz\/repos","events_url":"https:\/\/api.github.com\/users\/leondz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/leondz\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-23T17:27:46Z","updated_at":"2023-07-21T13:55:37Z","closed_at":"2023-07-21T13:55:36Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nWhen a user access requests are enabled, and new requests are set to Manual Review, the dataset owner should be notified of the pending requests. However, instead, currently nothing happens, and so the dataset request can go unanswered for quite some time until the owner happens to check that particular dataset's Settings pane.\n\n### Steps to reproduce the bug\n\n1. Enable a dataset's user access requests\r\n2. Set to Manual Review\r\n3. Ask another HF user to request access to the dataset\r\n4. Dataset owner is not notified\n\n### Expected behavior\n\nThe dataset owner should receive some kind of notification, perhaps in their HF site inbox, or by email, when a dataset access request is made and manual review is enabled.\n\n### Environment info\n\nn\/a","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5892\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5892\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5891","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5891\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5891\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5891\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5891","id":1722384135,"node_id":"PR_kwDODunzps5RKchn","number":5891,"title":"Make split slicing consisten with list slicing","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-23T16:04:33Z","updated_at":"2023-05-23T16:11:12Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #1774, fix #5875 \r\n\r\nTODO: a test","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5891\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5891\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5891","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5891","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5891.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5891.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5889","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5889\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5889\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5889\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5889","id":1722373618,"node_id":"I_kwDODunzps5mqVXy","number":5889,"title":"Token Alignment for input and output data over train and test batch\/dataset.","user":{"login":"akesh1235","id":125154243,"node_id":"U_kgDOB3Wzww","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/125154243?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/akesh1235","html_url":"https:\/\/github.com\/akesh1235","followers_url":"https:\/\/api.github.com\/users\/akesh1235\/followers","following_url":"https:\/\/api.github.com\/users\/akesh1235\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/akesh1235\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/akesh1235\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/akesh1235\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/akesh1235\/orgs","repos_url":"https:\/\/api.github.com\/users\/akesh1235\/repos","events_url":"https:\/\/api.github.com\/users\/akesh1235\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/akesh1235\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-05-23T15:58:55Z","updated_at":"2023-05-23T15:58:55Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"`data`\r\n> DatasetDict({\r\n    train: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: 4500\r\n    })\r\n    test: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: 500\r\n    })\r\n})\r\n\r\n**# input (in-correct sentence)**\r\n`data['train'][0]['input']`\r\n**>>** 'We are meet sunday 10am12pmET in Crown Heights Brooklyn New York'\r\n**# output (correct sentence)**\r\n`data['train'][0]['output']`\r\n**>>** 'We meet Sundays 10am-12pmET in Crown Heights, Brooklyn, New York.'\r\n\r\n**I Want to align the output tokens with input**\r\n\r\n```\r\n`# tokenize both inputs and targets\r\ndef tokenize_fn(batch):\r\n    # tokenize the input sequence first\r\n    # this populates input_ids, attention_mask, etc.\r\n\r\n    tokenized_inputs = tokenizer(\r\n    batch['input']\r\n    )\r\n    \r\n    labels_batch = tokenizer.tokenize(batch['output']) # original targets\r\n\r\n    aligned_labels_batch = []\r\n    for i, labels in enumerate(labels_batch):\r\n        word_ids = tokenized_inputs[i].word_ids()\r\n        aligned_labels_batch.append(align_targets(labels, word_ids))  # align_targets is another user defined function which is been called here\r\n\r\n    # recall: the 'target' must be stored in key called 'labels'\r\n    tokenized_inputs['labels'] = aligned_labels_batch\r\n\r\n    return tokenized_inputs`\r\n```\r\n```\r\ndata.map(\r\n  tokenize_fn,\r\n  batched=True,\r\n  remove_columns=data['train'].column_names,\r\n)\r\n```\r\n\r\nWhen this user defined function is mapped to every records of train and test batch am getting following error:\r\n\r\n**1.** **raise DatasetTransformationNotAllowedError(\r\n   3457         \"Using `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn't create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\"**\r\n\r\n**2.** **TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]**","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5889\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5889\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5887","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5887\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5887\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5887\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5887","id":1722166382,"node_id":"I_kwDODunzps5mpixu","number":5887,"title":"HuggingsFace dataset example give error","user":{"login":"donhuvy","id":1328316,"node_id":"MDQ6VXNlcjEzMjgzMTY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1328316?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/donhuvy","html_url":"https:\/\/github.com\/donhuvy","followers_url":"https:\/\/api.github.com\/users\/donhuvy\/followers","following_url":"https:\/\/api.github.com\/users\/donhuvy\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/donhuvy\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/donhuvy\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/donhuvy\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/donhuvy\/orgs","repos_url":"https:\/\/api.github.com\/users\/donhuvy\/repos","events_url":"https:\/\/api.github.com\/users\/donhuvy\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/donhuvy\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"assignees":[{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2023-05-23T14:09:05Z","updated_at":"2023-07-25T14:01:01Z","closed_at":"2023-07-25T14:01:00Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/1328316\/1f4f0086-3db9-4c79-906b-05a375357cce)\r\n\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/1328316\/733ebd3d-89b9-4ece-b80a-00ab5b0a4122)\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nUse link as reference document written https:\/\/colab.research.google.com\/github\/huggingface\/datasets\/blob\/main\/notebooks\/Overview.ipynb#scrollTo=biqDH9vpvSVz\r\n\r\n```python\r\n# Now let's train our model\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\nmodel.train().to(device)\r\nfor i, batch in enumerate(dataloader):\r\n    batch.to(device)\r\n    outputs = model(**batch)\r\n    loss = outputs.loss\r\n    loss.backward()\r\n    optimizer.step()\r\n    model.zero_grad()\r\n    print(f'Step {i} - loss: {loss:.3}')\r\n    if i > 5:\r\n        break\r\n```\r\n\r\nError\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-44-7040b885f382>](https:\/\/localhost:8080\/#) in <cell line: 5>()\r\n      5 for i, batch in enumerate(dataloader):\r\n      6     batch.to(device)\r\n----> 7     outputs = model(**batch)\r\n      8     loss = outputs.loss\r\n      9     loss.backward()\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/torch\/nn\/modules\/module.py](https:\/\/localhost:8080\/#) in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks or _global_backward_hooks\r\n   1500                 or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args, **kwargs)\r\n   1502         # Do not call functions when jit is used\r\n   1503         full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\nTypeError: DistilBertForQuestionAnswering.forward() got an unexpected keyword argument 'token_type_ids'\r\n```\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/assets\/1328316\/5d8b1d61-9337-4d59-8423-4f37f834c156\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nRun success on Google Colab (free)\r\n\r\n### Environment info\r\n\r\nWindows 11 x64, Google Colab free (my Google Drive just empty about 200 MB, but I don't think it cause problem)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5887\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5887\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5886","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5886\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5886\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5886\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5886","id":1721070225,"node_id":"I_kwDODunzps5mlXKR","number":5886,"title":"Use work-stealing algorithm when parallel computing","user":{"login":"1014661165","id":46060451,"node_id":"MDQ6VXNlcjQ2MDYwNDUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/46060451?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/1014661165","html_url":"https:\/\/github.com\/1014661165","followers_url":"https:\/\/api.github.com\/users\/1014661165\/followers","following_url":"https:\/\/api.github.com\/users\/1014661165\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/1014661165\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/1014661165\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/1014661165\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/1014661165\/orgs","repos_url":"https:\/\/api.github.com\/users\/1014661165\/repos","events_url":"https:\/\/api.github.com\/users\/1014661165\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/1014661165\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-23T03:08:44Z","updated_at":"2023-05-24T15:30:09Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nwhen i used Dataset.map api to process data concurrently, i found that\r\n it gets slower and slower as it gets closer to completion. Then i read the source code of arrow_dataset.py and found that it shard the dataset and use multiprocessing pool to execute each shard.It may cause the slowest task to drag out the entire program's execution time,especially when processing huge dataset.\r\n\r\n### Motivation\r\n\r\nusing work-stealing algorithm instead of sharding and parallel computing to optimize performance. \r\n\r\n### Your contribution\r\n\r\njust an idea.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5886\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5886\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5885","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5885\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5885\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5885\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5885","id":1720954440,"node_id":"PR_kwDODunzps5RFjTL","number":5885,"title":"Modify `is_remote_filesystem` to return True for FUSE-mounted paths","user":{"login":"maddiedawson","id":106995444,"node_id":"U_kgDOBmCe9A","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/106995444?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/maddiedawson","html_url":"https:\/\/github.com\/maddiedawson","followers_url":"https:\/\/api.github.com\/users\/maddiedawson\/followers","following_url":"https:\/\/api.github.com\/users\/maddiedawson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/maddiedawson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/maddiedawson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/maddiedawson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/maddiedawson\/orgs","repos_url":"https:\/\/api.github.com\/users\/maddiedawson\/repos","events_url":"https:\/\/api.github.com\/users\/maddiedawson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/maddiedawson\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-05-23T01:04:54Z","updated_at":"2023-05-25T08:50:48Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5885\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5885\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5885","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5885","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5885.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5885.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5888","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5888\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5888\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5888\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5888","id":1722290363,"node_id":"I_kwDODunzps5mqBC7","number":5888,"title":"A way to upload and visualize .mp4 files (millions of them) as part of a dataset","user":{"login":"AntreasAntoniou","id":10792502,"node_id":"MDQ6VXNlcjEwNzkyNTAy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10792502?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/AntreasAntoniou","html_url":"https:\/\/github.com\/AntreasAntoniou","followers_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/followers","following_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/orgs","repos_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/repos","events_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/AntreasAntoniou\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-05-22T18:05:26Z","updated_at":"2023-06-23T03:37:16Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nI recently chose to use huggingface hub as the home for a large multi modal dataset I've been building. https:\/\/huggingface.co\/datasets\/Antreas\/TALI\r\n\r\nIt combines images, text, audio and video. Now, I could very easily upload a dataset made via datasets.Dataset.from_generator, as long as it did not include video files. I found that including .mp4 files in the entries would not auto-upload those files. \r\n\r\nHence I tried to upload them myself. I quickly found out that uploading many small files is a very bad way to use git lfs, and that it would take ages, so, I resorted to using 7z to pack them all up. But then I had a new problem.\r\n\r\nMy dataset had a size of 1.9TB. Trying to upload such a large file with the default huggingface_hub API always resulted in time outs etc. So I decided to split the large files into chunks of 5GB each and reupload. \r\n\r\nSo, eventually it all worked out. But now the dataset can't be properly and natively used by the datasets API because of all the needed preprocessing -- and furthermore the hub is unable to visualize things. \r\n\r\n**Describe the solution you'd like**\r\nA native way to upload large datasets that include .mp4 or other video types.\r\n\r\n**Describe alternatives you've considered**\r\nAlready explained earlier\r\n\r\n**Additional context**\r\nhttps:\/\/huggingface.co\/datasets\/Antreas\/TALI\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5888\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5888\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5884","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5884\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5884\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5884\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5884","id":1719548172,"node_id":"I_kwDODunzps5mfjkM","number":5884,"title":"`Dataset.to_tf_dataset` fails when strings cannot be encoded as `np.bytes_`","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"assignees":[{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-05-22T12:03:06Z","updated_at":"2023-06-09T16:04:56Z","closed_at":"2023-06-09T16:04:55Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nWhen loading any dataset that contains a column with strings that are not ASCII-compatible, looping over those records raises the following exception e.g. for `\u00e9` character `UnicodeEncodeError: 'ascii' codec can't encode character '\\xe9' in position 0: ordinal not in range(128)`.\n\n### Steps to reproduce the bug\n\nRunning the following script will eventually fail, when reaching to the batch that contains non-ASCII compatible strings.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"imdb\", split=\"train\")\r\ntfds = ds.to_tf_dataset(batch_size=16)\r\n\r\nfor batch in tfds:\r\n  print(batch)\r\n>>> UnicodeEncodeError: 'ascii' codec can't encode character '\\xe9' in position 0: ordinal not in range(128)\r\n```\n\n### Expected behavior\n\nThe following script to run properly, making sure that the strings are either `numpy.unicode_` or `numpy.string` instead of `numpy.bytes_` since some characters are not ASCII compatible and that would lead to an issue when applying the `map`.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"imdb\", split=\"train\")\r\ntfds = ds.to_tf_dataset(batch_size=16)\r\n\r\nfor batch in tfds:\r\n  print(batch)\r\n```\n\n### Environment info\n\n- `datasets` version: 2.12.1.dev0\r\n- Platform: macOS-13.3.1-arm64-arm-64bit\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5884\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5884\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5883","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5883\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5883\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5883\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5883","id":1719527597,"node_id":"PR_kwDODunzps5RAkYi","number":5883,"title":"Fix string-encoding, make `batch_size` optional, and minor improvements in `Dataset.to_tf_dataset`","user":{"login":"alvarobartt","id":36760800,"node_id":"MDQ6VXNlcjM2NzYwODAw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/36760800?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alvarobartt","html_url":"https:\/\/github.com\/alvarobartt","followers_url":"https:\/\/api.github.com\/users\/alvarobartt\/followers","following_url":"https:\/\/api.github.com\/users\/alvarobartt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alvarobartt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alvarobartt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alvarobartt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alvarobartt\/orgs","repos_url":"https:\/\/api.github.com\/users\/alvarobartt\/repos","events_url":"https:\/\/api.github.com\/users\/alvarobartt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alvarobartt\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":29,"created_at":"2023-05-22T11:51:07Z","updated_at":"2023-06-08T11:09:03Z","closed_at":"2023-06-06T16:49:15Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## What's in this PR?\r\n\r\nThis PR addresses some minor fixes and general improvements in the `to_tf_dataset` method of `datasets.Dataset`, to convert a \ud83e\udd17HuggingFace Dataset as a TensorFlow Dataset.\r\n\r\nThe main bug solved in this PR comes with the string-encoding, since for safety purposes the internal conversion of `numpy.arrays` when `dtype` is unicode\/string, is to convert it into `numpy.bytes`, more information in the docstring of https:\/\/github.com\/tensorflow\/tensorflow\/blob\/388d952114e59a1aeda440ed4737b29f8b7c6e8a\/tensorflow\/python\/ops\/script_ops.py#L210. That's triggered when using `tensorflow.numpy_function` as it's applying another type cast besides the one that `datasets` does, so the casting is applied at least twice per entry\/batch. So this means that the definition of the `numpy.unicode_` dtype when the data in the batch is a string, is ignored, and replaced by `numpy.bytes_`.\r\n\r\nBesides that, some other minor things have been fixed:\r\n\r\n* Made `batch_size` an optional parameter in `to_tf_dataset`\r\n* Map the `tensorflow` output dtypes just once, and not in every `tf.function` call during `map`\r\n* Keep `numpy` formatting in the `datasets.Dataset` if already formatted like it, no need to format it again as `numpy`\r\n* Docstring indentation in `dataset_to_tf` and `multiprocess_dataset_to_tf`\r\n\r\n## What's missing in this PR?\r\n\r\nI can include some integration tests if needed, to validate that `batch_size` is optional, and that the tensors in the TF-Dataset can be looped over with no issues as before.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5883\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5883\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5883","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5883","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5883.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5883.patch","merged_at":"2023-06-06T16:49:15Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5881","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5881\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5881\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5881\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5881","id":1719402643,"node_id":"I_kwDODunzps5mfACT","number":5881,"title":"Split dataset by node: index error when sharding iterable dataset","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-22T10:36:13Z","updated_at":"2023-05-23T08:32:14Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nContext: we're splitting an iterable dataset by node and then passing it to a torch data loader with multiple workers\r\n\r\nWhen we iterate over it for 5 steps, we don't get an error\r\n\r\nWhen we instead iterate over it for 8 steps, we get an `IndexError` when fetching the data if we have too many workers\r\n\r\n### Steps to reproduce the bug\r\n\r\nHere, we have 2 JAX processes (`jax.process_count() = 2`) which we split the dataset over. The dataset loading script can be found here: https:\/\/huggingface.co\/datasets\/distil-whisper\/librispeech_asr\/blob\/c6a1e805cbfeed5057400ac5937327d7e30281b8\/librispeech_asr.py#L310\r\n\r\n<details>\r\n\r\n<summary> Code to reproduce <\/summary>\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nimport jax\r\nfrom datasets.distributed import split_dataset_by_node\r\nfrom torch.utils.data import DataLoader\r\nfrom tqdm import tqdm\r\n\r\n# load an example dataset (https:\/\/huggingface.co\/datasets\/distil-whisper\/librispeech_asr)\r\ndataset = load_dataset(\"distil-whisper\/librispeech_asr\", \"all\", split=\"train.clean.100\", streaming=True)\r\n# just keep the text column -> no need to define a collator\r\ndataset_text = dataset.remove_columns(set(dataset.features.keys()) - {\"text\"})\r\n\r\n# define some constants\r\nbatch_size = 256\r\nnum_examples = 5  # works for 5 examples, doesn't for 8\r\nnum_workers = dataset_text.n_shards\r\n\r\n# try with multiple workers\r\ndataloader = DataLoader(dataset_text, batch_size=batch_size, num_workers=num_workers, drop_last=True)\r\n\r\nfor i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Multiple workers\"):\r\n    if i == num_examples:\r\n        break\r\n\r\n# try splitting by node (we can't do this with `dataset_text` since `split_dataset_by_node` expects the Audio column for an ASR dataset)\r\ndataset = split_dataset_by_node(dataset, rank=jax.process_index(), world_size=jax.process_count())\r\n# remove the text column again\r\ndataset_text = dataset.remove_columns(set(dataset.features.keys()) - {\"text\"})\r\ndataloader = DataLoader(dataset_text, batch_size=16, num_workers=num_workers \/\/ 2, drop_last=True)\r\n\r\nfor i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Split by node\"):\r\n    if i == num_examples:\r\n        break\r\n\r\n# too many workers\r\ndataloader = DataLoader(dataset_text, batch_size=256, num_workers=num_workers, drop_last=True)\r\nfor i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Too many workers\"):\r\n    if i == num_examples:\r\n        break\r\n```\r\n\r\n<\/details>\r\n\r\n<details>\r\n\r\n<summary> With 5 examples: <\/summary>\r\n\r\n```\r\nMultiple workers: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5\/5 [00:16<00:00,  3.33s\/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.                                                       \r\nSplit by node: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5\/5 [00:13<00:00,  2.76s\/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.                                                       \r\nToo many dataloader workers: 14 (max is dataset.n_shards=7). Stopping 7 dataloader workers.                             \r\nTo parallelize data loading, we give each process some shards (or data sources) to process. Therefore it's unnecessary t\r\no have a number of workers greater than dataset.n_shards=7. To enable more parallelism, please split the dataset in more\r\n files than 7.                                                                                                          \r\nToo many workers: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5\/5 [00:15<00:00,  3.03s\/it]\r\n```\r\n\r\n<\/details>\r\n\r\n<details>\r\n\r\n<summary> With 7 examples: <\/summary>\r\n\r\n```\r\nMultiple workers: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8\/8 [00:13<00:00,  1.71s\/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.\r\nSplit by node: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8\/8 [00:11<00:00,  1.38s\/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.\r\nToo many dataloader workers: 14 (max is dataset.n_shards=7). Stopping 7 dataloader workers.\r\nTo parallelize data loading, we give each process some shards (or data sources) to process. Therefore it's unnecessary to have a number of workers greater than dataset.n_shards=7. To enable more parallelism, please split the dataset in more files than 7.\r\nToo many workers:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b        | 7\/8 [00:13<00:01,  1.89s\/it]\r\nTraceback (most recent call last):\r\n  File \"distil-whisper\/test_librispeech.py\", line 36, in <module>\r\n    for i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Too many workers\"):\r\n  File \"\/home\/sanchitgandhi\/hf\/lib\/python3.8\/site-packages\/tqdm\/std.py\", line 1178, in __iter__\r\n    for obj in iterable:\r\n  File \"\/home\/sanchitgandhi\/hf\/lib\/python3.8\/site-packages\/torch\/utils\/data\/dataloader.py\", line 633, in __next__\r\n    data = self._next_data()\r\n  File \"\/home\/sanchitgandhi\/hf\/lib\/python3.8\/site-packages\/torch\/utils\/data\/dataloader.py\", line 1325, in _next_data\r\n    return self._process_data(data)\r\n  File \"\/home\/sanchitgandhi\/hf\/lib\/python3.8\/site-packages\/torch\/utils\/data\/dataloader.py\", line 1371, in _process_data\r\n    data.reraise()\r\n  File \"\/home\/sanchitgandhi\/hf\/lib\/python3.8\/site-packages\/torch\/_utils.py\", line 644, in reraise\r\n    raise exception\r\nIndexError: Caught IndexError in DataLoader worker process 7.\r\nOriginal Traceback (most recent call last):\r\n  File \"\/home\/sanchitgandhi\/hf\/lib\/python3.8\/site-packages\/torch\/utils\/data\/_utils\/worker.py\", line 308, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"\/home\/sanchitgandhi\/hf\/lib\/python3.8\/site-packages\/torch\/utils\/data\/_utils\/fetch.py\", line 32, in fetch\r\n    data.append(next(self.dataset_iter))\r\n  File \"\/home\/sanchitgandhi\/datasets\/src\/datasets\/iterable_dataset.py\", line 986, in __iter__\r\n    yield from self._iter_pytorch(ex_iterable)\r\n  File \"\/home\/sanchitgandhi\/datasets\/src\/datasets\/iterable_dataset.py\", line 920, in _iter_pytorch\r\n    for key, example in ex_iterable.shard_data_sources(worker_info.id, worker_info.num_workers):\r\n  File \"\/home\/sanchitgandhi\/datasets\/src\/datasets\/iterable_dataset.py\", line 540, in shard_data_sources\r\n    self.ex_iterable.shard_data_sources(worker_id, num_workers),\r\n  File \"\/home\/sanchitgandhi\/datasets\/src\/datasets\/iterable_dataset.py\", line 796, in shard_data_sources\r\n    self.ex_iterable.shard_data_sources(worker_id, num_workers),\r\n  File \"\/home\/sanchitgandhi\/datasets\/src\/datasets\/iterable_dataset.py\", line 126, in shard_data_sources\r\n    requested_gen_kwargs = _merge_gen_kwargs([gen_kwargs_list[i] for i in shard_indices])\r\n  File \"\/home\/sanchitgandhi\/datasets\/src\/datasets\/utils\/sharding.py\", line 76, in _merge_gen_kwargs\r\n    for key in gen_kwargs_list[0]\r\nIndexError: list index out of range\r\n```\r\n\r\n<\/details>\r\n\r\n### Expected behavior\r\n\r\nShould pass for both 5 and 7 examples\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.12.1.dev0\r\n- Platform: Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5881\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5881\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5880","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5880\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5880\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5880\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5880","id":1719090101,"node_id":"I_kwDODunzps5mdzu1","number":5880,"title":"load_dataset from s3 file system through streaming can't not iterate data ","user":{"login":"janineguo","id":59083384,"node_id":"MDQ6VXNlcjU5MDgzMzg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59083384?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/janineguo","html_url":"https:\/\/github.com\/janineguo","followers_url":"https:\/\/api.github.com\/users\/janineguo\/followers","following_url":"https:\/\/api.github.com\/users\/janineguo\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/janineguo\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/janineguo\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/janineguo\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/janineguo\/orgs","repos_url":"https:\/\/api.github.com\/users\/janineguo\/repos","events_url":"https:\/\/api.github.com\/users\/janineguo\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/janineguo\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-05-22T07:40:27Z","updated_at":"2023-05-26T12:52:08Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nI have a JSON file in my s3 file system(minio), I can use load_dataset to get the file link, but I can't iterate it\r\n<img width=\"816\" alt=\"image\" src=\"https:\/\/github.com\/huggingface\/datasets\/assets\/59083384\/cc0778d3-36f3-45b5-ac68-4e7c664c2ed0\">\r\n<img width=\"1144\" alt=\"image\" src=\"https:\/\/github.com\/huggingface\/datasets\/assets\/59083384\/76872af3-8b3c-42ff-9f55-528c920a7af1\">\r\n\r\nwe can change 4 lines to fix this bug, you can check whether it is ok for us.\r\n<img width=\"941\" alt=\"image\" src=\"https:\/\/github.com\/huggingface\/datasets\/assets\/59083384\/5a22155a-ece7-496c-8506-047e5c235cd3\">\n\n### Steps to reproduce the bug\n\n1. storage a file in you s3 file system\r\n2. use load_dataset to read it through streaming\r\n3. iterate it\n\n### Expected behavior\n\ncan iterate it successfully\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.16\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5880\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5880\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5878","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5878\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5878\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5878\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5878","id":1718203843,"node_id":"I_kwDODunzps5mabXD","number":5878,"title":"Prefetching for IterableDataset","user":{"login":"vyeevani","id":30946190,"node_id":"MDQ6VXNlcjMwOTQ2MTkw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/30946190?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vyeevani","html_url":"https:\/\/github.com\/vyeevani","followers_url":"https:\/\/api.github.com\/users\/vyeevani\/followers","following_url":"https:\/\/api.github.com\/users\/vyeevani\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vyeevani\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vyeevani\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vyeevani\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vyeevani\/orgs","repos_url":"https:\/\/api.github.com\/users\/vyeevani\/repos","events_url":"https:\/\/api.github.com\/users\/vyeevani\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vyeevani\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-05-20T15:25:40Z","updated_at":"2023-06-01T17:40:00Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nAdd support for prefetching the next n batches through iterabledataset to reduce batch loading bottleneck in training loop.\r\n\r\n### Motivation\r\n\r\nThe primary motivation behind this is to use hardware accelerators alongside a streaming dataset. This is required when you are in a low ram or low disk space setting as well as quick iteration where you're iterating though different accelerator environments (e.x changing ec2 instances quickly to figure out batch\/sec for a particular architecture). \r\n\r\nCurrently, using the IterableDataset results in accelerators becoming basically useless due to the massive bottleneck induced by the dataset lazy loading\/transform\/mapping.\r\n\r\nI've considered two alternatives:\r\nPyTorch dataloader that handles this. However, I'm using jax, and I believe this is a piece of functionality that should live in the stream class.\r\n\r\nReplicating the \"num_workers\" part of the PyTorch DataLoader to eagerly load batches and apply the transform so Arrow caching will automatically cache results and make them accessible.\r\n\r\n### Your contribution\r\n\r\nI may or may not have time to do this. Currently, I've written the basic multiprocessor approach to handle the eager DataLoader for my own use case with code that's not integrated to datasets. I'd definitely see this as being the default over the regular Dataset for most people given that they wouldn't have to wait on the datasets while also not worrying about performance.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5878\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5878\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5877","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5877\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5877\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5877\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5877","id":1717983961,"node_id":"I_kwDODunzps5mZlrZ","number":5877,"title":"Request for text deduplication feature","user":{"login":"SupreethRao99","id":55043035,"node_id":"MDQ6VXNlcjU1MDQzMDM1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/55043035?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/SupreethRao99","html_url":"https:\/\/github.com\/SupreethRao99","followers_url":"https:\/\/api.github.com\/users\/SupreethRao99\/followers","following_url":"https:\/\/api.github.com\/users\/SupreethRao99\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/SupreethRao99\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/SupreethRao99\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/SupreethRao99\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/SupreethRao99\/orgs","repos_url":"https:\/\/api.github.com\/users\/SupreethRao99\/repos","events_url":"https:\/\/api.github.com\/users\/SupreethRao99\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/SupreethRao99\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-20T01:56:00Z","updated_at":"2023-07-26T21:42:14Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nIt would be great if there would be support for high performance, highly scalable text deduplication algorithms as part of the datasets library.\n\n### Motivation\n\nMotivated by this blog post https:\/\/huggingface.co\/blog\/dedup and this library https:\/\/github.com\/google-research\/deduplicate-text-datasets, but slightly frustrated by how its not very easy to work with these tools I am proposing this feature.\n\n### Your contribution\n\nI would be happy to contribute to the development effort of this feature. would love to collaborate with others in the development effort. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5877\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5877\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5876","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5876\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5876\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5876\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5876","id":1717978985,"node_id":"I_kwDODunzps5mZkdp","number":5876,"title":"Incompatibility with DataLab","user":{"login":"helpmefindaname","id":26192135,"node_id":"MDQ6VXNlcjI2MTkyMTM1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/26192135?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/helpmefindaname","html_url":"https:\/\/github.com\/helpmefindaname","followers_url":"https:\/\/api.github.com\/users\/helpmefindaname\/followers","following_url":"https:\/\/api.github.com\/users\/helpmefindaname\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/helpmefindaname\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/helpmefindaname\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/helpmefindaname\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/helpmefindaname\/orgs","repos_url":"https:\/\/api.github.com\/users\/helpmefindaname\/repos","events_url":"https:\/\/api.github.com\/users\/helpmefindaname\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/helpmefindaname\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892877,"node_id":"MDU6TGFiZWwxOTM1ODkyODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/good%20first%20issue","name":"good first issue","color":"7057ff","default":true,"description":"Good for newcomers"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-20T01:39:11Z","updated_at":"2023-05-25T06:42:34Z","closed_at":"2023-05-25T06:42:34Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHello,\r\nI am currently working on a project where both [DataLab](https:\/\/github.com\/ExpressAI\/DataLab) and [datasets](https:\/\/github.com\/huggingface\/datasets) are subdependencies.\r\nI noticed that I cannot import both libraries, as they both register FileSystems in `fsspec`, expecting the FileSystems not being registered before.\r\n\r\nWhen running the code below, I get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\__init__.py\", line 28, in <module>\r\n    from datalabs.arrow_dataset import concatenate_datasets, Dataset\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\arrow_dataset.py\", line 60, in <module>\r\n    from datalabs.arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\arrow_writer.py\", line 28, in <module>\r\n    from datalabs.features import (\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\features\\__init__.py\", line 2, in <module>\r\n    from datalabs.features.audio import Audio\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\features\\audio.py\", line 21, in <module>\r\n    from datalabs.utils.streaming_download_manager import xopen\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\utils\\streaming_download_manager.py\", line 16, in <module>\r\n    from datalabs.filesystems import COMPRESSION_FILESYSTEMS\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\filesystems\\__init__.py\", line 37, in <module>\r\n    fsspec.register_implementation(fs_class.protocol, fs_class)\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\fsspec\\registry.py\", line 51, in register_implementation\r\n    raise ValueError(\r\nValueError: Name (bz2) already in the registry and clobber is False\r\n```\r\n\r\nI think as simple solution would be to just set `clobber=True` in https:\/\/github.com\/huggingface\/datasets\/blob\/main\/src\/datasets\/filesystems\/__init__.py#L28. This allows the register to discard previous registrations. This should work, as the datalabs FileSystems are copies of the datasets FileSystems. However, I don't know if it is guaranteed to be compatible with other libraries that might use the same protocols.\r\n\r\n\r\nI am linking the symmetric issue on [DataLab](https:\/\/github.com\/ExpressAI\/DataLab\/issues\/425) as ideally the issue is solved in both libraries the same way. Otherwise, it could lead to different behaviors depending on which library gets imported first.\r\n\n\n### Steps to reproduce the bug\n\n1. Run `pip install datalabs==0.4.15 datasets==2.12.0`\r\n2. Run the following python code:\r\n   ```\r\n      import datalabs\r\n      import datasets\r\n   ```\n\n### Expected behavior\n\nIt should be possible to import both libraries without getting a Value Error\n\n### Environment info\n\ndatalabs==0.4.15\r\ndatasets==2.12.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5876\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5876\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5875","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5875\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5875\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5875\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5875","id":1716770394,"node_id":"I_kwDODunzps5mU9Za","number":5875,"title":"Why split slicing doesn't behave like list slicing ?","user":{"login":"astariul","id":43774355,"node_id":"MDQ6VXNlcjQzNzc0MzU1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/43774355?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/astariul","html_url":"https:\/\/github.com\/astariul","followers_url":"https:\/\/api.github.com\/users\/astariul\/followers","following_url":"https:\/\/api.github.com\/users\/astariul\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/astariul\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/astariul\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/astariul\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/astariul\/orgs","repos_url":"https:\/\/api.github.com\/users\/astariul\/repos","events_url":"https:\/\/api.github.com\/users\/astariul\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/astariul\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892865,"node_id":"MDU6TGFiZWwxOTM1ODkyODY1","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/duplicate","name":"duplicate","color":"cfd3d7","default":true,"description":"This issue or pull request already exists"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-19T07:21:10Z","updated_at":"2023-05-23T16:02:14Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nIf I want to get the first 10 samples of my dataset, I can do :\r\n\r\n```\r\nds = datasets.load_dataset('mnist', split='train[:10]')\r\n```\r\n\r\nBut if I exceed the number of samples in the dataset, an exception is raised : \r\n\r\n```\r\nds = datasets.load_dataset('mnist', split='train[:999999999]')\r\n```\r\n\r\n> ValueError: Requested slice [:999999999] incompatible with 60000 examples.\n\n### Steps to reproduce the bug\n\n```\r\nds = datasets.load_dataset('mnist', split='train[:999999999]')\r\n```\n\n### Expected behavior\n\nI would expect it to behave like python lists (no exception raised, the whole list is kept) : \r\n\r\n```\r\nd = list(range(1000))[:999999]\r\nprint(len(d))  # > 1000\r\n```\n\n### Environment info\n\n- `datasets` version: 2.9.0\r\n- Platform: macOS-12.6-arm64-arm-64bit\r\n- Python version: 3.9.12\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5875\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5875\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5874","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5874\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5874\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5874\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5874","id":1715708930,"node_id":"I_kwDODunzps5mQ6QC","number":5874,"title":"Using as_dataset on a \"parquet\" builder  ","user":{"login":"rems75","id":9039058,"node_id":"MDQ6VXNlcjkwMzkwNTg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/9039058?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rems75","html_url":"https:\/\/github.com\/rems75","followers_url":"https:\/\/api.github.com\/users\/rems75\/followers","following_url":"https:\/\/api.github.com\/users\/rems75\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rems75\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rems75\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rems75\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rems75\/orgs","repos_url":"https:\/\/api.github.com\/users\/rems75\/repos","events_url":"https:\/\/api.github.com\/users\/rems75\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rems75\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-18T14:09:03Z","updated_at":"2023-05-31T13:23:55Z","closed_at":"2023-05-31T13:23:55Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI used a custom builder to ``download_and_prepare`` a dataset. The first (very minor) issue is that the doc seems to suggest ``download_and_prepare`` will return the dataset, while it does not ([builder.py](https:\/\/github.com\/huggingface\/datasets\/blob\/main\/src\/datasets\/builder.py#L718-L738)).\r\n```\r\n        >>> from datasets import load_dataset_builder\r\n        >>> builder = load_dataset_builder(\"rotten_tomatoes\")\r\n        >>> ds = builder.download_and_prepare(\".\/output_dir\", file_format=\"parquet\")\r\n```\r\n\r\nThe main issue I am facing is loading the dataset from those parquet files. I used the `as_dataset` method suggested by the doc, however it returns:\r\n`\r\nFileNotFoundError: [Errno 2] Failed to open local file 'output_dir\/__main__-train-00000-of-00245.arrow'. Detail:\r\n[errno 2] No such file or directory.\r\n` \n\n### Steps to reproduce the bug\n\n1. Create a custom builder of some sort: `builder = CustomBuilder()`.\r\n2. Run `download_and_prepare` with the parquet format: `builder.download_and_prepare(\".\/output_dir\", file_format=\"parquet\")`.\r\n3. Run `dataset = builder.as_dataset()`.  \n\n### Expected behavior\n\nI guess I'd expect `as_dataset` to generate the dataset in arrow format if it has to, or to suggest an alternative way to load the dataset (I've also tried other methods with `load_dataset` to no avail, probably due to misunderstandings on my part). \n\n### Environment info\n\n```\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.15.0-1027-gcp-x86_64-with-glibc2.31\r\n- Python version: 3.10.0\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.5.3\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5874\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5874\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5873","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5873\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5873\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5873\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5873","id":1713269724,"node_id":"I_kwDODunzps5mHmvc","number":5873,"title":"Allow setting the environment variable for the lock file path","user":{"login":"xin3he","id":83260933,"node_id":"MDQ6VXNlcjgzMjYwOTMz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/83260933?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/xin3he","html_url":"https:\/\/github.com\/xin3he","followers_url":"https:\/\/api.github.com\/users\/xin3he\/followers","following_url":"https:\/\/api.github.com\/users\/xin3he\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/xin3he\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/xin3he\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/xin3he\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/xin3he\/orgs","repos_url":"https:\/\/api.github.com\/users\/xin3he\/repos","events_url":"https:\/\/api.github.com\/users\/xin3he\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/xin3he\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-05-17T07:10:02Z","updated_at":"2023-05-17T07:11:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nAdd an environment variable to replace the default lock file path.\r\n\r\n### Motivation\r\n\r\nUsually, dataset path is a read-only path while the lock file needs to be modified each time. It would be convenient if the path can be reset individually.\r\n\r\n### Your contribution\r\n\r\n```\/src\/datasets\/utils\/filelock.py\r\nclass UnixFileLock(BaseFileLock):\r\n    def __init__(self, lock_file, timeout=-1, max_filename_length=None):\r\n        #-------------------\r\n        if os.getenv('DS_TMP_PATH'):\r\n            file_name = str(lock_file).split('\/')[-1]\r\n            dataset_tmp_path = os.getenv('DS_TMP_PATH')\r\n            lock_file = os.path.join(dataset_tmp_path, file_name)\r\n        #-------------------\r\n        max_filename_length = os.statvfs(os.path.dirname(lock_file)).f_namemax\r\n        super().__init__(lock_file, timeout=timeout, max_filename_length=max_filename_length)\r\n```\r\nA simple demo is as upper. Thanks.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5873\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5873\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5872","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5872\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5872\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5872\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5872","id":1713174662,"node_id":"PR_kwDODunzps5QrQ5o","number":5872,"title":"Fix infer module for uppercase extensions","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-17T05:56:45Z","updated_at":"2023-05-17T14:26:59Z","closed_at":"2023-05-17T14:19:18Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix the `infer_module_for_data_files` and `infer_module_for_data_files_in_archives` functions when passed a data file name with uppercase extension, e.g. `filename.TXT`.\r\n\r\nBefore, `None` module was returned.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5872\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5872\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5872","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5872","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5872.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5872.patch","merged_at":"2023-05-17T14:19:18Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5871","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5871\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5871\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5871\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5871","id":1712573073,"node_id":"I_kwDODunzps5mE8qR","number":5871,"title":"data configuration hash suffix depends on uncanonicalized data_dir","user":{"login":"kylrth","id":5044802,"node_id":"MDQ6VXNlcjUwNDQ4MDI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5044802?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kylrth","html_url":"https:\/\/github.com\/kylrth","followers_url":"https:\/\/api.github.com\/users\/kylrth\/followers","following_url":"https:\/\/api.github.com\/users\/kylrth\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kylrth\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kylrth\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kylrth\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kylrth\/orgs","repos_url":"https:\/\/api.github.com\/users\/kylrth\/repos","events_url":"https:\/\/api.github.com\/users\/kylrth\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kylrth\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892877,"node_id":"MDU6TGFiZWwxOTM1ODkyODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/good%20first%20issue","name":"good first issue","color":"7057ff","default":true,"description":"Good for newcomers"}],"state":"closed","locked":false,"assignee":{"login":"kylrth","id":5044802,"node_id":"MDQ6VXNlcjUwNDQ4MDI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5044802?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kylrth","html_url":"https:\/\/github.com\/kylrth","followers_url":"https:\/\/api.github.com\/users\/kylrth\/followers","following_url":"https:\/\/api.github.com\/users\/kylrth\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kylrth\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kylrth\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kylrth\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kylrth\/orgs","repos_url":"https:\/\/api.github.com\/users\/kylrth\/repos","events_url":"https:\/\/api.github.com\/users\/kylrth\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kylrth\/received_events","type":"User","site_admin":false},"assignees":[{"login":"kylrth","id":5044802,"node_id":"MDQ6VXNlcjUwNDQ4MDI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5044802?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kylrth","html_url":"https:\/\/github.com\/kylrth","followers_url":"https:\/\/api.github.com\/users\/kylrth\/followers","following_url":"https:\/\/api.github.com\/users\/kylrth\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kylrth\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kylrth\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kylrth\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kylrth\/orgs","repos_url":"https:\/\/api.github.com\/users\/kylrth\/repos","events_url":"https:\/\/api.github.com\/users\/kylrth\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kylrth\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2023-05-16T18:56:04Z","updated_at":"2023-06-02T15:52:05Z","closed_at":"2023-06-02T15:52:05Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI am working with the `recipe_nlg` dataset, which requires manual download. Once it's downloaded, I've noticed that the hash in the custom data configuration is different if I add a trailing `\/` to my `data_dir`. It took me a while to notice that the hashes were different, and to understand that that was the cause of my dataset being processed anew instead of the cached version being used.\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Follow the steps to manually download the `recipe_nlg` dataset to `\/data\/recipenlg`.\r\n2. Load it using `load_dataset`, once without a trailing slash and once with one:\r\n\r\n    ```python\r\n    >>> ds = load_dataset(\"recipe_nlg\", data_dir=\"\/data\/recipenlg\")\r\n    Using custom data configuration default-082278caeea85765\r\n    Downloading and preparing dataset recipe_nlg\/default to \/home\/kyle\/.cache\/huggingface\/datasets\/recipe_nlg\/default-082278caeea85765\/1.0.0\/aa4f120223637bedf7360cecb70a9bd108acfd64e38207ca90c9f385d21e5e74...\r\n    Dataset recipe_nlg downloaded and prepared to \/home\/kyle\/.cache\/huggingface\/datasets\/recipe_nlg\/default-082278caeea85765\/1.0.0\/aa4f120223637bedf7360cecb70a9bd108acfd64e38207ca90c9f385d21e5e74. Subsequent calls will reuse this data.\r\n    100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:01<00:00,  1.10s\/it]\r\n    DatasetDict({\r\n        train: Dataset({\r\n            features: ['id', 'title', 'ingredients', 'directions', 'link', 'source', 'ner'],\r\n            num_rows: 2231142\r\n        })\r\n    })\r\n    >>> ds = load_dataset(\"recipe_nlg\", data_dir=\"\/data\/recipenlg\/\")\r\n    Using custom data configuration default-83e87680785d0493\r\n    Downloading and preparing dataset recipe_nlg\/default to \/home\/user\/.cache\/huggingface\/datasets\/recipe_nlg\/default-83e87680785d0493\/1.0.0\/aa4f120223637bedf7360cecb70a9bd108acfd64e38207ca90c9f385d21e5e74...\r\n    Generating train split:   1%|          | 12701\/2231142 [00:04<13:15, 2790.25 examples\/s\r\n    ^C\r\n    ```\r\n\r\n3. Observe that the hash suffix in the custom data configuration changes due to the altered string.\r\n\r\n### Expected behavior\r\n\r\nI think I would expect the hash to remain constant if it actually points to the same location on disk. I would expect the use of `os.path.normpath` to canonicalize the paths.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.8.0\r\n- Platform: Linux-5.4.0-147-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.8\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5871\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5871\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5870","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5870\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5870\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5870\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5870","id":1712156282,"node_id":"I_kwDODunzps5mDW56","number":5870,"title":"Behaviour difference between datasets.map and IterableDatasets.map","user":{"login":"llStringll","id":30209072,"node_id":"MDQ6VXNlcjMwMjA5MDcy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/30209072?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/llStringll","html_url":"https:\/\/github.com\/llStringll","followers_url":"https:\/\/api.github.com\/users\/llStringll\/followers","following_url":"https:\/\/api.github.com\/users\/llStringll\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/llStringll\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/llStringll\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/llStringll\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/llStringll\/orgs","repos_url":"https:\/\/api.github.com\/users\/llStringll\/repos","events_url":"https:\/\/api.github.com\/users\/llStringll\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/llStringll\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-16T14:32:57Z","updated_at":"2023-05-16T14:36:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nAll the examples in all the docs mentioned throughout huggingface datasets correspond to datasets object, and not IterableDatasets object. At one point of time, they might have been in sync, but the code for datasets version >=2.9.0 is very different as compared to the docs. \r\nI basically need to .map() a transform on images in an iterable dataset, which was made using a custom databuilder config.\r\nThis works very good in map-styles datasets, but the .map() fails in IterableDatasets, show behvaiour as such:\r\n\"pixel_values\" key not found, KeyError in examples object\/dict passed into transform function for map, which works fine with map style, even as batch.\r\nIn iterable style, the object\/dict passed into map() paramter callable function is completely different as what is mentioned in all examples.\r\nPlease look into this. Thank you\r\n\r\nMy databuilder class is inherited as such:\r\n\r\n    def _info(self):\r\n        print (\"Config: \",self.config.__dict__.keys())\r\n        return datasets.DatasetInfo(\r\n            description=_DESCRIPTION,\r\n            features=datasets.Features(\r\n                {\r\n                    \"labels\": datasets.Sequence(datasets.Value(\"uint16\")),\r\n                    # \"labels_name\": datasets.Value(\"string\"),\r\n                    # \"pixel_values\": datasets.Array3D(shape=(3, 1280, 960), dtype=\"float32\"),\r\n                    \"pixel_values\": datasets.Array3D(shape=(1280, 960, 3), dtype=\"uint8\"),\r\n                    \"image_s3_path\": datasets.Value(\"string\"),\r\n                }\r\n            ),\r\n            supervised_keys=None,\r\n            homepage=\"none\",\r\n            citation=\"\",\r\n        )\r\n\r\n    def _split_generators(self, dl_manager):\r\n        records_train = list(db.mini_set.find({'split':'train'},{'image_s3_path':1, 'ocwen_template_name':1}))[:10000]\r\n        records_val = list(db.mini_set.find({'split':'val'},{'image_s3_path':1, 'ocwen_template_name':1}))[:1000]\r\n        # print (len(records),self.config.num_shards)\r\n        # shard_size_train = len(records_train)\/\/self.config.num_shards\r\n        # sharded_records_train = [records_train[i:i+shard_size_train] for i in range(0,len(records_train),shard_size_train)]\r\n        # shard_size_val = len(records_val)\/\/self.config.num_shards\r\n        # sharded_records_val = [records_val[i:i+shard_size_val] for i in range(0,len(records_val),shard_size_val)]\r\n        return [\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TRAIN, gen_kwargs={\"records\":records_train} # passing list of records, for sharding to take over\r\n            ),\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.VALIDATION, gen_kwargs={\"records\":records_val} # passing list of records, for sharding to take over\r\n            ),\r\n        ]\r\n\r\n    def _generate_examples(self, records):\r\n        # print (\"Generating examples for [{}] shards\".format(len(shards)))\r\n        # initiate_db_connection()\r\n        # records = list(db.mini_set.find({'split':split},{'image_s3_path':1, 'ocwen_template_name':1}))[:10]\r\n        id_ = 0\r\n        # for records in shards:\r\n        for i,rec in enumerate(records):\r\n            img_local_path = fetch_file(rec['image_s3_path'],self.config.buffer_dir)\r\n            # t = self.config.processor(Image.open(img_local_path), random_padding=True, return_tensors=\"np\").pixel_values.squeeze()\r\n            # print (t.shape, type(t),type(t[0][0][0]))\r\n            # sys.exit()\r\n            pvs = np.array(Image.open(img_local_path).resize((1280,960))) # image object is wxh, so resize as per that, numpy array of it is hxwxc, transposing to cxwxh\r\n            # pvs = self.config.processor(Image.open(img_local_path), random_padding=True, return_tensors=\"np\").pixel_values.astype(np.float16).squeeze()\r\n            # print (type(pvs[0][0][0]))\r\n            lblids = self.config.processor.tokenizer('<s_class>'+rec['ocwen_template_name']+'<\/s_class>'+'<\/s>', add_special_tokens=False, padding=False, truncation=False, return_tensors=\"np\")[\"input_ids\"].squeeze(0)  # take padding later, as per batch collating\r\n            # print (len(lblids),type(lblids[0]))\r\n            # print (type(pvs),pvs.shape,type(pvs[0][0][0]), type(lblids))\r\n            yield id_, {\"labels\":lblids,\"pixel_values\":pvs,\"image_s3_path\":rec['image_s3_path']}\r\n            id_+=1\r\n            os.remove(img_local_path)\r\n\r\nand I load it inside my trainer script as such\r\n`ds = load_dataset(\"\/tmp\/DonutDS\/dataset\/\", split=\"train\", streaming=True) # iterable dataset, where .map() falls`\r\nor also as\r\n`ds = load_from_disk('\/tmp\/DonutDS\/dataset\/') #map style dataset`\r\n\r\nThank you to the team for having such a great library, and for this bug fix in advance!\n\n### Steps to reproduce the bug\n\nAbove config can allow one to reproduce the said bug\n\n### Expected behavior\n\n.map() should show some consistency b\/w map-style and iterable-style datasets, or atleast the docs should address iterable-style datasets behaviour and examples. I honestly do not figure the use of such docs.\n\n### Environment info\n\ndatasets==2.9.0\r\ntransformers==4.26.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5870\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5870\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5869","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5869\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5869\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5869\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5869","id":1711990003,"node_id":"I_kwDODunzps5mCuTz","number":5869,"title":"Image Encoding Issue when submitting a Parquet Dataset","user":{"login":"PhilippeMoussalli","id":47530815,"node_id":"MDQ6VXNlcjQ3NTMwODE1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47530815?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/PhilippeMoussalli","html_url":"https:\/\/github.com\/PhilippeMoussalli","followers_url":"https:\/\/api.github.com\/users\/PhilippeMoussalli\/followers","following_url":"https:\/\/api.github.com\/users\/PhilippeMoussalli\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/PhilippeMoussalli\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/PhilippeMoussalli\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/PhilippeMoussalli\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/PhilippeMoussalli\/orgs","repos_url":"https:\/\/api.github.com\/users\/PhilippeMoussalli\/repos","events_url":"https:\/\/api.github.com\/users\/PhilippeMoussalli\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/PhilippeMoussalli\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":16,"created_at":"2023-05-16T09:42:58Z","updated_at":"2023-06-16T12:48:38Z","closed_at":"2023-06-16T09:30:48Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nHello,\r\n\r\nI'd like to report an issue related to pushing a dataset represented as a Parquet file to a dataset repository using Dask. Here are the details:\r\n\r\nWe attempted to load an example dataset in Parquet format from the Hugging Face (HF) filesystem using Dask with the following code snippet:\r\n```\r\nimport dask.dataframe as dd\r\ndf = dd.read_parquet(\"hf:\/\/datasets\/lambdalabs\/pokemon-blip-captions\",index=False)\r\n```\r\nIn this dataset, the \"image\" column is represented as a dictionary\/struct with the format:\r\n\r\n```\r\ndf = df.compute()\r\ndf[\"image\"].iloc[0].keys()\r\n-> dict_keys(['bytes', 'path'])\r\n```\r\nI think this is the format encoded by the [`Image`](https:\/\/huggingface.co\/docs\/datasets\/v2.0.0\/en\/package_reference\/main_classes#datasets.Image) feature extractor from datasets to format suitable for Arrow. \r\n\r\nThe next step was to push the dataset to a repository that I created:\r\n```\r\ndd.to_parquet(dask_df, path = \"hf:\/\/datasets\/philippemo\/dummy_dataset\/data\")\r\n```\r\n\r\nHowever, after pushing the dataset using Dask, the \"image\" column is now represented as the encoded dictionary `(['bytes', 'path'])`, and the images are not properly visualized. You can find the dataset here: [Link to the problematic dataset](https:\/\/huggingface.co\/datasets\/philippemo\/dummy_dataset).\r\n\r\nIt's worth noting that both the original dataset and the one submitted with Dask have the same schema with minor alterations related to metadata:\r\n\r\n**[ Schema of original dummy example.](https:\/\/huggingface.co\/datasets\/lambdalabs\/pokemon-blip-captions\/blob\/main\/data\/train-00000-of-00001-566cc9b19d7203f8.parquet)**   \r\n```\r\nimage: struct<bytes: binary, path: null>\r\n  child 0, bytes: binary\r\n  child 1, path: null\r\ntext: string\r\n```\r\n**[ Schema of pushed dataset with dask](https:\/\/huggingface.co\/datasets\/philippemo\/dummy_dataset\/blob\/main\/data\/part.0.parquet)**\r\n```\r\nimage: struct<bytes: binary, path: null>\r\n  child 0, bytes: binary\r\n  child 1, path: null\r\ntext: string\r\n```\r\n\r\nThis issue seems to be related to an encoding type that occurs when pushing a model to the hub. Normally, models should be represented as an HF dataset before pushing, but we are working with an example where we need to push large datasets using Dask.\r\n\r\nCould you please provide clarification on how to resolve this issue?\r\n\r\nThank you!\r\n\r\n\r\n### Reproduction\r\n\r\nTo get the schema I downloaded the parquet files and used pyarrow.parquet to read the schema\r\n```\r\nimport pyarrow.parquet\r\npyarrow.parquet.read_schema(<path_to_parquet>, memory_map=True)\r\n```\r\n\r\n### Logs\r\n\r\n_No response_\r\n\r\n### System info\r\n\r\n```shell\r\n- huggingface_hub version: 0.14.1\r\n- Platform: Linux-5.19.0-41-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Running in iPython ?: No\r\n- Running in notebook ?: No\r\n- Running in Google Colab ?: No\r\n- Token path ?: \/home\/philippe\/.cache\/huggingface\/token\r\n- Has saved token ?: True\r\n- Who am I ?: philippemo\r\n- Configured git credential helpers: cache\r\n- FastAI: N\/A\r\n- Tensorflow: N\/A\r\n- Torch: N\/A\r\n- Jinja2: 3.1.2\r\n- Graphviz: N\/A\r\n- Pydot: N\/A\r\n- Pillow: 9.4.0\r\n- hf_transfer: N\/A\r\n- gradio: N\/A\r\n- ENDPOINT: https:\/\/huggingface.co\r\n- HUGGINGFACE_HUB_CACHE: \/home\/philippe\/.cache\/huggingface\/hub\r\n- HUGGINGFACE_ASSETS_CACHE: \/home\/philippe\/.cache\/huggingface\/assets\r\n- HF_TOKEN_PATH: \/home\/philippe\/.cache\/huggingface\/token\r\n- HF_HUB_OFFLINE: False\r\n- HF_HUB_DISABLE_TELEMETRY: False\r\n- HF_HUB_DISABLE_PROGRESS_BARS: None\r\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\r\n- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\r\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\r\n- HF_HUB_ENABLE_HF_TRANSFER: False\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5869\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5869\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5868","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5868\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5868\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5868\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5868","id":1711173098,"node_id":"I_kwDODunzps5l_m3q","number":5868,"title":"Is it possible to change a cached file and 're-cache' it instead of re-generating?","user":{"login":"zyh3826","id":31238754,"node_id":"MDQ6VXNlcjMxMjM4NzU0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/31238754?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/zyh3826","html_url":"https:\/\/github.com\/zyh3826","followers_url":"https:\/\/api.github.com\/users\/zyh3826\/followers","following_url":"https:\/\/api.github.com\/users\/zyh3826\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/zyh3826\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/zyh3826\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/zyh3826\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/zyh3826\/orgs","repos_url":"https:\/\/api.github.com\/users\/zyh3826\/repos","events_url":"https:\/\/api.github.com\/users\/zyh3826\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/zyh3826\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-16T03:45:42Z","updated_at":"2023-05-17T11:21:36Z","closed_at":"2023-05-17T11:21:36Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nHi,\r\nI have a huge cached file using `map`(over 500GB), and I want to change an attribution of each element, is there possible to do it using some method instead of re-generating, because `map` takes over 24 hours\n\n### Motivation\n\nFor large datasets, I think it is very important because we always face the problem which is changing something in the original cache without re-generating it.\n\n### Your contribution\n\nFor now, I can't help, sorry.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5868\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5868\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5867","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5867\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5867\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5867\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5867","id":1710656067,"node_id":"PR_kwDODunzps5QizOn","number":5867,"title":"Add logic for hashing modules\/functions optimized with `torch.compile`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-05-15T19:03:35Z","updated_at":"2023-11-27T20:03:32Z","closed_at":"2023-11-27T20:03:31Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix https:\/\/github.com\/huggingface\/datasets\/issues\/5839\r\n\r\nPS: The `Pickler.save` method is becoming a bit messy, so I plan to refactor the pickler a bit at some point.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5867\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5867\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5867","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5867","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5867.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5867.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5866","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5866\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5866\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5866\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5866","id":1710496993,"node_id":"I_kwDODunzps5l9Bzh","number":5866,"title":"Issue with Sequence features","user":{"login":"alialamiidrissi","id":14365168,"node_id":"MDQ6VXNlcjE0MzY1MTY4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14365168?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alialamiidrissi","html_url":"https:\/\/github.com\/alialamiidrissi","followers_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/followers","following_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/orgs","repos_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/repos","events_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-15T17:13:29Z","updated_at":"2023-05-26T11:57:17Z","closed_at":"2023-05-26T11:57:17Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nSequences features  sometimes causes errors when the specified length is not -1\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nimport numpy as np\r\nfrom datasets import Features, ClassLabel, Sequence, Value, Dataset\r\nfeats = Features(**{'target': ClassLabel(names=[0, 1]),'x': Sequence(feature=Value(dtype='float64',id=None), length=2, id=None)})\r\nDataset.from_dict({\"target\": np.ones(2000).astype(int), \"x\": np.random.rand(2000,2)},features = feats).flatten_indices()\r\n```\r\nThrows:\r\n```\r\n  TypeError: Couldn't cast array of type\r\n  fixed_size_list<item: double>[2]\r\n  to\r\n  Sequence(feature=Value(dtype='float64', id=None), length=2, id=None)\r\n```\r\nThe same code works without any issues when `length = -1`\r\n\r\nEDIT: The error seems to happen only when the length of the dataset is bigger than 1000 for some reason\r\n### Expected behavior\r\n\r\nNo exception\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.10.1\r\n- Python version: 3.9.5\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.4.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5866\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5866\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5865","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5865\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5865\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5865\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5865","id":1710455738,"node_id":"PR_kwDODunzps5QiHnw","number":5865,"title":"Deprecate task api","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-05-15T16:48:24Z","updated_at":"2023-07-10T12:33:59Z","closed_at":"2023-07-10T12:24:01Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The task API is not well adopted in the ecosystem, so this PR deprecates it. The `train_eval_index` is a newer, more flexible solution that should be used instead (I think?).\r\n\r\nThese are the projects that still use the task API :\r\n* the image classification example in Transformers: [here](https:\/\/github.com\/huggingface\/transformers\/blob\/8f76dc8e5aaad58f2df7748b6d6970376f315a9a\/examples\/pytorch\/image-classification\/run_image_classification_no_trainer.py#L262) and [here](https:\/\/github.com\/huggingface\/transformers\/blob\/8f76dc8e5aaad58f2df7748b6d6970376f315a9a\/examples\/tensorflow\/image-classification\/run_image_classification.py#L277)\r\n* autotrain: [here](https:\/\/github.com\/huggingface\/autotrain-backend\/blob\/455e274004b56f9377d64db4ab03671508fcc4cd\/zeus\/zeus\/run\/utils.py#L666)\r\n* api-inference-community: [here](https:\/\/github.com\/huggingface\/api-inference-community\/blob\/fb8fb29d577a5bf01c82944db745489a6d6ed3d4\/manage.py#L64) (but the rest of the code does not call the `resolve_dataset` function)\r\n\r\nSo we need to update these files after the merge.\r\n\r\ncc @lewtun ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5865\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5865\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5865","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5865","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5865.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5865.patch","merged_at":"2023-07-10T12:24:01Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5864","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5864\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5864\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5864\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5864","id":1710450047,"node_id":"I_kwDODunzps5l82V_","number":5864,"title":"Slow iteration over Torch tensors","user":{"login":"crisostomi","id":51738205,"node_id":"MDQ6VXNlcjUxNzM4MjA1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/51738205?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/crisostomi","html_url":"https:\/\/github.com\/crisostomi","followers_url":"https:\/\/api.github.com\/users\/crisostomi\/followers","following_url":"https:\/\/api.github.com\/users\/crisostomi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/crisostomi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/crisostomi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/crisostomi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/crisostomi\/orgs","repos_url":"https:\/\/api.github.com\/users\/crisostomi\/repos","events_url":"https:\/\/api.github.com\/users\/crisostomi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/crisostomi\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-15T16:43:58Z","updated_at":"2023-05-16T03:27:38Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI have a problem related to this [issue](https:\/\/github.com\/huggingface\/datasets\/issues\/5841): I get a way slower iteration when using a Torch dataloader if I use vanilla Numpy tensors or if I first apply a ToTensor transform to the input. In particular, it takes 5 seconds to iterate over the vanilla input and ~30s after the transformation.\n\n### Steps to reproduce the bug\n\nHere is the minimum code to reproduce the problem\r\n\r\n```python\r\nimport numpy as np\r\nfrom datasets import Dataset, DatasetDict, load_dataset, Array3D, Image, Features\r\nfrom torch.utils.data import DataLoader\r\nfrom tqdm import tqdm\r\nimport torchvision \r\nfrom torchvision.transforms import ToTensor, Normalize\r\n\r\n\r\n#################################\r\n# Without transform\r\n#################################\r\n    \r\ntrain_dataset = load_dataset(\r\n    'cifar100',\r\n    split='train',\r\n    use_auth_token=True,\r\n)\r\n\r\ntrain_dataset.set_format(type=\"numpy\", columns=[\"img\", \"fine_label\"])\r\n\r\ntrain_loader= DataLoader(\r\n    train_dataset,\r\n    batch_size=100,\r\n    pin_memory=False,\r\n    shuffle=True,\r\n    num_workers=8,\r\n)\r\n\r\nfor batch in tqdm(train_loader, desc=\"Loading data, no transform\"):\r\n    pass\r\n\r\n\r\n#################################\r\n# With transform\r\n#################################\r\n\r\ntransform_func = torchvision.transforms.Compose([\r\n    ToTensor(), \r\n    Normalize(mean=[0.485, 0.456, 0.406], std= [0.229, 0.224, 0.225]),]          \r\n)\r\n    \r\ntrain_dataset = train_dataset.map(\r\n    desc=f\"Preprocessing samples\",\r\n    function=lambda x: {\"img\": transform_func(x[\"img\"])},\r\n)\r\n\r\ntrain_dataset.set_format(type=\"numpy\", columns=[\"img\", \"fine_label\"])\r\n\r\n\r\ntrain_loader= DataLoader(\r\n    train_dataset,\r\n    batch_size=100,\r\n    pin_memory=False,\r\n    shuffle=True,\r\n    num_workers=8,\r\n)\r\n\r\n\r\nfor batch in tqdm(train_loader, desc=\"Loading data after transform\"):\r\n    pass \r\n```\r\n\r\nI have also tried converting the Image column to an Array3D\r\n```python\r\nimg_shape = train_dataset[0][\"img\"].shape\r\n\r\nfeatures = train_dataset.features.copy()\r\nfeatures[\"x\"] = Array3D(shape=img_shape, dtype=\"float32\")\r\n\r\ntrain_dataset = train_dataset.map(\r\n    desc=f\"Preprocessing samples\",\r\n    function=lambda x: {\"x\": np.array(x[\"img\"], dtype=np.uint8)},\r\n    features=features,\r\n)\r\ntrain_dataset.cast_column(\"x\", Array3D(shape=img_shape, dtype=\"float32\"))\r\ntrain_dataset.set_format(type=\"numpy\", columns=[\"x\", \"fine_label\"])\r\n```\r\nbut to no avail. Any clue?\n\n### Expected behavior\n\nThe iteration should take approximately the same time with or without the transformation, as it doesn't change the shape of the input. What may be the issue here?\n\n### Environment info\n\n```\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.4.0-137-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5864\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5864\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5863","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5863\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5863\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5863\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5863","id":1710335905,"node_id":"PR_kwDODunzps5QhtlM","number":5863,"title":"Use a new low-memory approach for tf dataset index shuffling","user":{"login":"Rocketknight1","id":12866554,"node_id":"MDQ6VXNlcjEyODY2NTU0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12866554?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Rocketknight1","html_url":"https:\/\/github.com\/Rocketknight1","followers_url":"https:\/\/api.github.com\/users\/Rocketknight1\/followers","following_url":"https:\/\/api.github.com\/users\/Rocketknight1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Rocketknight1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Rocketknight1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Rocketknight1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Rocketknight1\/orgs","repos_url":"https:\/\/api.github.com\/users\/Rocketknight1\/repos","events_url":"https:\/\/api.github.com\/users\/Rocketknight1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Rocketknight1\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":36,"created_at":"2023-05-15T15:28:34Z","updated_at":"2023-06-08T16:40:18Z","closed_at":"2023-06-08T16:32:51Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR tries out a new approach to generating the index tensor in `to_tf_dataset`, which should reduce memory usage for very large datasets. I'll need to do some testing before merging it!\r\n\r\nFixes #5855","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5863\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5863\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5863","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5863","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5863.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5863.patch","merged_at":"2023-06-08T16:32:50Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5862","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5862\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5862\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5862\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5862","id":1710140646,"node_id":"I_kwDODunzps5l7qzm","number":5862,"title":"IndexError: list index out of range with data hosted on Zenodo","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-05-15T13:47:19Z","updated_at":"2023-09-25T12:09:51Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"The dataset viewer sometimes raises an `IndexError`:\r\n```\r\nIndexError: list index out of range\r\n```\r\nSee:\r\n- huggingface\/datasets-server#1151\r\n  - https:\/\/huggingface.co\/datasets\/reddit\/discussions\/5\r\n- huggingface\/datasets-server#1118\r\n  - https:\/\/huggingface.co\/datasets\/krr-oxford\/OntoLAMA\/discussions\/1\r\n- https:\/\/huggingface.co\/datasets\/hyperpartisan_news_detection\/discussions\/3\r\n- https:\/\/huggingface.co\/datasets\/um005\/discussions\/2\r\n- https:\/\/huggingface.co\/datasets\/tapaco\/discussions\/2\r\n- https:\/\/huggingface.co\/datasets\/common_language\/discussions\/3\r\n- https:\/\/huggingface.co\/datasets\/pass\/discussions\/1\r\n\r\nAfter investigation:\r\n- This happens with data files hosted on Zenodo\r\n- Indeed, there is an underlying 429 HTTP error: Too Many Requests\r\n\r\nNote that some time ago, it also happened with data files hosted on Google Drive. See:\r\n  - #4581\r\n  - #4580 \r\n\r\nThe reason then was that there was a 403 HTTP error: Forbidden\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5862\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5862\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5861","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5861\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5861\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5861\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5861","id":1709807340,"node_id":"PR_kwDODunzps5Qf55q","number":5861,"title":"Better error message when combining dataset dicts instead of datasets","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-05-15T10:36:24Z","updated_at":"2023-05-23T10:40:13Z","closed_at":"2023-05-23T10:32:58Z","author_association":"MEMBER","active_lock_reason":null,"body":"close https:\/\/github.com\/huggingface\/datasets\/issues\/5851","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5861\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5861\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5861","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5861","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5861.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5861.patch","merged_at":"2023-05-23T10:32:58Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5860","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5860\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5860\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5860\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5860","id":1709727460,"node_id":"PR_kwDODunzps5QfojD","number":5860,"title":"Minor tqdm optim","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-15T09:49:37Z","updated_at":"2023-05-17T18:46:46Z","closed_at":"2023-05-17T18:39:35Z","author_association":"MEMBER","active_lock_reason":null,"body":"Don't create a tqdm progress bar when `disable_tqdm` is passed to `map_nested`.\r\n\r\nOn my side it sped up some iterable datasets by ~30% when `map_nested` is used extensively to recursively tensorize python dicts.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5860\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5860\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5860","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5860","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5860.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5860.patch","merged_at":"2023-05-17T18:39:35Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5859","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5859\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5859\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5859\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5859","id":1709554829,"node_id":"PR_kwDODunzps5QfDLC","number":5859,"title":"Raise TypeError when indexing a dataset with bool","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-05-15T08:08:42Z","updated_at":"2023-05-25T16:31:24Z","closed_at":"2023-05-25T16:23:17Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix #5858.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5859\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5859\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5859","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5859","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5859.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5859.patch","merged_at":"2023-05-25T16:23:17Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5858","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5858\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5858\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5858\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5858","id":1709332632,"node_id":"I_kwDODunzps5l4liY","number":5858,"title":"Throw an error when dataset improperly indexed","user":{"login":"sarahwie","id":8027676,"node_id":"MDQ6VXNlcjgwMjc2NzY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8027676?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sarahwie","html_url":"https:\/\/github.com\/sarahwie","followers_url":"https:\/\/api.github.com\/users\/sarahwie\/followers","following_url":"https:\/\/api.github.com\/users\/sarahwie\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sarahwie\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sarahwie\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sarahwie\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sarahwie\/orgs","repos_url":"https:\/\/api.github.com\/users\/sarahwie\/repos","events_url":"https:\/\/api.github.com\/users\/sarahwie\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sarahwie\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-05-15T05:15:53Z","updated_at":"2023-05-25T16:23:19Z","closed_at":"2023-05-25T16:23:19Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nPandas-style subset indexing on dataset does not throw an error, when maybe it should. Instead returns the first instance of the dataset regardless of index condition.\n\n### Steps to reproduce the bug\n\nSteps to reproduce the behavior:\r\n\r\n1. `squad = datasets.load_dataset(\"squad_v2\", split=\"validation\")`\r\n2. `item = squad[squad['question'] ==  \"Who was the Norse leader?\"]`\r\nor `it = squad[squad['id'] == '56ddde6b9a695914005b962b']`\r\n3.  returns the first item in the dataset, which does not satisfy the above conditions:\r\n\r\n`{'id': '56ddde6b9a695914005b9628', 'title': 'Normans', 'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.', 'question': 'In what country is Normandy located?', 'answers': {'text': ['France', 'France', 'France', 'France'], 'answer_start': [159, 159, 159, 159]}}`\n\n### Expected behavior\n\nShould either throw an error message, or return the dataset item that satisfies the condition.\n\n### Environment info\n\n- `datasets` version: 2.9.0\r\n- Platform: macOS-13.3.1-arm64-arm-64bit\r\n- Python version: 3.10.8\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5858\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5858\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5857","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5857\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5857\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5857\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5857","id":1709326622,"node_id":"I_kwDODunzps5l4kEe","number":5857,"title":"Adding chemistry dataset\/models in huggingface","user":{"login":"knc6","id":16902896,"node_id":"MDQ6VXNlcjE2OTAyODk2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16902896?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/knc6","html_url":"https:\/\/github.com\/knc6","followers_url":"https:\/\/api.github.com\/users\/knc6\/followers","following_url":"https:\/\/api.github.com\/users\/knc6\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/knc6\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/knc6\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/knc6\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/knc6\/orgs","repos_url":"https:\/\/api.github.com\/users\/knc6\/repos","events_url":"https:\/\/api.github.com\/users\/knc6\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/knc6\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-15T05:09:49Z","updated_at":"2023-07-21T13:45:40Z","closed_at":"2023-07-21T13:45:40Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nHuggingface is really amazing platform for open science.\r\n\r\nIn addition to computer vision, video and NLP, would it be of interest to add chemistry\/materials science dataset\/models in Huggingface? Or, if its already done, can you provide some pointers.\r\n\r\nWe have been working on a comprehensive benchmark on this topic: [JARVIS-Leaderboard](https:\/\/pages.nist.gov\/jarvis_leaderboard\/) and I am wondering if we could contribute\/integrate this project as a part of huggingface. \n\n### Motivation\n\nSimilar to the main stream AI field, there is need of large scale benchmarks\/models\/infrastructure for chemistry\/materials data.\n\n### Your contribution\n\nWe can start adding datasets as our [benchmarks](https:\/\/github.com\/usnistgov\/jarvis_leaderboard\/tree\/main\/jarvis_leaderboard\/benchmarks) should be easily convertible to the dataset format.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5857\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5857\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5856","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5856\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5856\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5856\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5856","id":1709218242,"node_id":"I_kwDODunzps5l4JnC","number":5856,"title":"Error loading natural_questions","user":{"login":"Crownor","id":19185508,"node_id":"MDQ6VXNlcjE5MTg1NTA4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/19185508?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Crownor","html_url":"https:\/\/github.com\/Crownor","followers_url":"https:\/\/api.github.com\/users\/Crownor\/followers","following_url":"https:\/\/api.github.com\/users\/Crownor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Crownor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Crownor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Crownor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Crownor\/orgs","repos_url":"https:\/\/api.github.com\/users\/Crownor\/repos","events_url":"https:\/\/api.github.com\/users\/Crownor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Crownor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-15T02:46:04Z","updated_at":"2023-06-05T09:11:19Z","closed_at":"2023-06-05T09:11:18Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen try to load natural_questions through datasets == 2.12.0 with python == 3.8.9:\r\n\r\n```python\r\nimport datasets\r\ndatasets.load_dataset('natural_questions',beam_runner='DirectRunner')\r\n```\r\n\r\nIt failed with following info:\r\n\r\n`pyarrow.lib.ArrowNotImplementedError: Nested data conversions not implemented for chunked array outputs`\n\n### Steps to reproduce the bug\n\nIn python console:\r\n\r\n```python\r\nimport datasets\r\ndatasets.load_dataset('natural_questions',beam_runner='DirectRunner')\r\n```\r\n\r\nThen the trace is:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/nlp\/.cache\/pypoetry\/virtualenvs\/drg-W3LF4Ol9-py3.8\/lib\/python3.8\/site-packages\/datasets\/load.py\", line 1797, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/nlp\/.cache\/pypoetry\/virtualenvs\/drg-W3LF4Ol9-py3.8\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 890, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/nlp\/.cache\/pypoetry\/virtualenvs\/drg-W3LF4Ol9-py3.8\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 2019, in _download_and_prepare\r\n    num_examples, num_bytes = beam_writer.finalize(metrics.query(m_filter))\r\n  File \"\/home\/nlp\/.cache\/pypoetry\/virtualenvs\/drg-W3LF4Ol9-py3.8\/lib\/python3.8\/site-packages\/datasets\/arrow_writer.py\", line 694, in finalize\r\n    shard_num_bytes, _ = parquet_to_arrow(source, destination)\r\n  File \"\/home\/nlp\/.cache\/pypoetry\/virtualenvs\/drg-W3LF4Ol9-py3.8\/lib\/python3.8\/site-packages\/datasets\/arrow_writer.py\", line 737, in parquet_to_arrow\r\n    for record_batch in parquet_file.iter_batches():\r\n  File \"pyarrow\/_parquet.pyx\", line 1323, in iter_batches\r\n  File \"pyarrow\/error.pxi\", line 121, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowNotImplementedError: Nested data conversions not implemented for chunked array outputs\r\n```\n\n### Expected behavior\n\nload natural_question questions\n\n### Environment info\n\n```\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-3.10.0-1160.42.2.el7.x86_64-x86_64-with-glibc2.2.5\r\n- Python version: 3.8.9\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.1\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5856\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5856\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5855","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5855\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5855\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5855\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5855","id":1708784943,"node_id":"I_kwDODunzps5l2f0v","number":5855,"title":"`to_tf_dataset` consumes too much memory","user":{"login":"massquantity","id":28751760,"node_id":"MDQ6VXNlcjI4NzUxNzYw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/28751760?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/massquantity","html_url":"https:\/\/github.com\/massquantity","followers_url":"https:\/\/api.github.com\/users\/massquantity\/followers","following_url":"https:\/\/api.github.com\/users\/massquantity\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/massquantity\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/massquantity\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/massquantity\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/massquantity\/orgs","repos_url":"https:\/\/api.github.com\/users\/massquantity\/repos","events_url":"https:\/\/api.github.com\/users\/massquantity\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/massquantity\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-05-14T01:22:29Z","updated_at":"2023-06-08T16:32:52Z","closed_at":"2023-06-08T16:32:52Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nHi, I'm using `to_tf_dataset` to convert a _large_ dataset to `tf.data.Dataset`. I observed that the data loading *before* training took a lot of time and memory, even with `batch_size=1`.\r\n\r\nAfter some digging, i believe the reason lies in the shuffle behavior. The [source code](https:\/\/github.com\/huggingface\/datasets\/blob\/main\/src\/datasets\/utils\/tf_utils.py#L185) uses `len(dataset)` as the `buffer_size`, which may load all the data into the memory, and the [tf.data doc](https:\/\/www.tensorflow.org\/guide\/data#randomly_shuffling_input_data) also states that \"While large buffer_sizes shuffle more thoroughly, they can take a lot of memory, and significant time to fill\".\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import Dataset\r\n\r\ndef gen():  # some large data\r\n    for i in range(50000000):\r\n        yield {\"data\": i}\r\n\r\nds = Dataset.from_generator(gen, cache_dir=\".\/huggingface\")\r\n\r\ntf_ds = ds.to_tf_dataset(\r\n    batch_size=64,\r\n    shuffle=False,  # no shuffle\r\n    drop_remainder=False,\r\n    prefetch=True,\r\n)\r\n\r\n# fast and memory friendly \ud83e\udd17\r\nfor batch in tf_ds: \r\n    ...\r\n\r\ntf_ds_shuffle = ds.to_tf_dataset(\r\n    batch_size=64,\r\n    shuffle=True,\r\n    drop_remainder=False,\r\n    prefetch=True,\r\n)\r\n\r\n# slow and memory hungry for simple iteration \ud83d\ude31\r\nfor batch in tf_ds_shuffle: \r\n    ...\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nShuffling should not load all the data into the memory. Would adding a `buffer_size` parameter in the `to_tf_dataset` API alleviate the problem?\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.11.0\r\n- Platform: Linux-5.17.1-051701-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.13\r\n- Huggingface_hub version: 0.13.4\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.4.3\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5855\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5855\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5854","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5854\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5854\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5854\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5854","id":1708779300,"node_id":"I_kwDODunzps5l2eck","number":5854,"title":"Can not load audiofolder dataset on kaggle","user":{"login":"ILG2021","id":93691919,"node_id":"U_kgDOBZWgDw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93691919?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ILG2021","html_url":"https:\/\/github.com\/ILG2021","followers_url":"https:\/\/api.github.com\/users\/ILG2021\/followers","following_url":"https:\/\/api.github.com\/users\/ILG2021\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ILG2021\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ILG2021\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ILG2021\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ILG2021\/orgs","repos_url":"https:\/\/api.github.com\/users\/ILG2021\/repos","events_url":"https:\/\/api.github.com\/users\/ILG2021\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ILG2021\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-14T00:50:47Z","updated_at":"2023-08-16T13:35:36Z","closed_at":"2023-07-21T13:53:45Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\nIt's crash log:\r\nFileNotFoundError: Couldn't find a dataset script at \/kaggle\/working\/audiofolder\/audiofolder.py or any data file in the same directory. Couldn't find 'audiofolder' on the Hugging Face Hub either: FileNotFoundError: Couldn't find file at https:\/\/raw.githubusercontent.com\/huggingface\/datasets\/master\/datasets\/audiofolder\/audiofolder.py\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n![image](https:\/\/github.com\/huggingface\/datasets\/assets\/93691919\/a2829d27-d15c-4acc-86fb-d1987c760468)\r\ncommon_voice = load_dataset(\"audiofolder\", data_dir=\"\/kaggle\/working\/data\")\r\n\r\n### Expected behavior\r\n\r\nload dataset without error. It works ok on colab, but on kaggle it happends.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.15.109+-x86_64-with-glibc2.31\r\n- Python version: 3.10.10\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5854\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5854\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5853","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5853\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5853\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5853\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5853","id":1708092786,"node_id":"PR_kwDODunzps5QaZLP","number":5853,"title":"[docs] Redirects, migrated from nginx","user":{"login":"julien-c","id":326577,"node_id":"MDQ6VXNlcjMyNjU3Nw==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/326577?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/julien-c","html_url":"https:\/\/github.com\/julien-c","followers_url":"https:\/\/api.github.com\/users\/julien-c\/followers","following_url":"https:\/\/api.github.com\/users\/julien-c\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/julien-c\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/julien-c\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/julien-c\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/julien-c\/orgs","repos_url":"https:\/\/api.github.com\/users\/julien-c\/repos","events_url":"https:\/\/api.github.com\/users\/julien-c\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/julien-c\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-12T19:19:27Z","updated_at":"2023-05-15T10:37:19Z","closed_at":"2023-05-15T10:30:14Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5853\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5853\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5853","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5853","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5853.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5853.patch","merged_at":"2023-05-15T10:30:14Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5852","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5852\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5852\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5852\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5852","id":1707927165,"node_id":"PR_kwDODunzps5QZ1lj","number":5852,"title":"Iterable torch formatting","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2023-05-12T16:48:49Z","updated_at":"2023-06-13T16:04:05Z","closed_at":"2023-06-13T15:57:05Z","author_association":"MEMBER","active_lock_reason":null,"body":"Used the TorchFormatter to get torch tensors in iterable dataset with format set to \"torch\".\r\n\r\nIt uses the data from Arrow if possible, otherwise applies recursive_tensorize.\r\n\r\nWhen set back to format_type=None, cast_to_python_objects is used.\r\n\r\nrequires https:\/\/github.com\/huggingface\/datasets\/pull\/5821\r\n\r\nclose https:\/\/github.com\/huggingface\/datasets\/issues\/5793","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5852\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5852\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5852","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5852","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5852.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5852.patch","merged_at":"2023-06-13T15:57:05Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5850","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5850\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5850\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5850\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5850","id":1707678911,"node_id":"PR_kwDODunzps5QZALv","number":5850,"title":"Make packaged builders skip non-supported file formats","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2023-05-12T13:52:34Z","updated_at":"2023-06-07T12:26:38Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"This PR makes packaged builders skip non-supported file formats:\r\n- Csv builder skips non-CSV files\r\n- Analogously for the other builders\r\n\r\nFix #5849.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5850\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5850\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5850","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5850","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5850.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5850.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5849","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5849\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5849\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5849\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5849","id":1707551511,"node_id":"I_kwDODunzps5lxysX","number":5849,"title":"CSV datasets should only read the CSV data files in the repo","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-05-12T12:29:53Z","updated_at":"2023-06-22T14:16:27Z","closed_at":"2023-06-22T14:16:27Z","author_association":"MEMBER","active_lock_reason":null,"body":"When a no-script dataset has many CSV files and a JPG file, the library infers to use the Csv builder, but tries to read as CSV all files in the repo, also the JPG file.\r\n\r\nI think the Csv builder should filter out non-CSV files when reading.\r\n\r\nAn analogue solution should be implemented for other packaged builders.\r\n\r\nRelated to:\r\n- https:\/\/huggingface.co\/datasets\/abidlabs\/img2text\/discussions\/1\r\n- https:\/\/github.com\/gradio-app\/gradio\/pull\/3973#issuecomment-1545409061\r\n\r\nCC: @abidlabs @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5849\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5849\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5848","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5848\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5848\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5848\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5848","id":1707506734,"node_id":"PR_kwDODunzps5QYa1B","number":5848,"title":"Add `accelerate` as metric's test dependency to fix CI error","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-12T12:01:01Z","updated_at":"2023-05-12T13:48:47Z","closed_at":"2023-05-12T13:39:06Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"The `frugalscore` metric uses Transformers' Trainer, which requires `accelerate` (as of recently).\r\n\r\nFixes the following [CI error](https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/4950900048\/jobs\/8855148703?pr=5845).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5848\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5848\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5848","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5848","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5848.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5848.patch","merged_at":"2023-05-12T13:39:06Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5847","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5847\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5847\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5847\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5847","id":1706616634,"node_id":"I_kwDODunzps5luOc6","number":5847,"title":"Streaming IterableDataset not working with translation pipeline","user":{"login":"jlquinn","id":826841,"node_id":"MDQ6VXNlcjgyNjg0MQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/826841?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jlquinn","html_url":"https:\/\/github.com\/jlquinn","followers_url":"https:\/\/api.github.com\/users\/jlquinn\/followers","following_url":"https:\/\/api.github.com\/users\/jlquinn\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jlquinn\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jlquinn\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jlquinn\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jlquinn\/orgs","repos_url":"https:\/\/api.github.com\/users\/jlquinn\/repos","events_url":"https:\/\/api.github.com\/users\/jlquinn\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jlquinn\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-05-11T21:52:38Z","updated_at":"2023-05-16T15:59:55Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm trying to use a streaming dataset for translation inference to avoid downloading the training data.\r\n\r\nI'm using a pipeline and a dataset, and following the guidance in the tutorial.  \r\n\r\nInstead I get an exception that IterableDataset has no len().\n\n### Steps to reproduce the bug\n\nCODE:\r\n```\r\nfrom transformers import pipeline\r\nfrom transformers.pipelines.pt_utils import KeyDataset\r\nfrom datasets import load_dataset\r\nds = load_dataset(path=\"wmt14\", name=\"fr-en\", split=\"test\", streaming=True)\r\nbs=1\r\nmt = pipeline(\"translation_en_to_fr\", model=\"t5-base\", batch_size=bs)\r\n#print(mt(\"hello\"))    THIS WORKS                                                                                                                                                                       \r\nks = KeyDataset(ds, \"translation\")\r\nprint(f\"{ks}\")\r\nxx= mt(ks)\r\nfor x in xx:\r\n    print(x)\r\n```\r\n\r\nRUN:\r\n```\r\n(watnlp) [jlquinn@bertdev01 hf]$ python ende.t5.pipe.py \r\n2023-05-11 16:48:08.817572: I tensorflow\/core\/util\/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-05-11 16:48:08.821388: W tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2023-05-11 16:48:08.821407: I tensorflow\/stream_executor\/cuda\/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n<transformers.pipelines.pt_utils.KeyDataset object at 0x7f61ed5da9d0>\r\nTraceback (most recent call last):\r\n  File \"\/home\/jlquinn\/models\/hf\/ende.t5.pipe.py\", line 11, in <module>\r\n    for x in xx:\r\n  File \"\/home\/jlquinn\/miniconda3\/envs\/watnlp\/lib\/python3.9\/site-packages\/transformers\/pipelines\/pt_utils.py\", line 111, in __next__\r\n    item = next(self.iterator)\r\n  File \"\/home\/jlquinn\/miniconda3\/envs\/watnlp\/lib\/python3.9\/site-packages\/transformers\/pipelines\/pt_utils.py\", line 111, in __next__\r\n    item = next(self.iterator)\r\n  File \"\/home\/jlquinn\/miniconda3\/envs\/watnlp\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 681, in __next__\r\n    data = self._next_data()\r\n  File \"\/home\/jlquinn\/miniconda3\/envs\/watnlp\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 720, in _next_data\r\n    index = self._next_index()  # may raise StopIteration\r\n  File \"\/home\/jlquinn\/miniconda3\/envs\/watnlp\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 671, in _next_index\r\n    return next(self._sampler_iter)  # may raise StopIteration\r\n  File \"\/home\/jlquinn\/miniconda3\/envs\/watnlp\/lib\/python3.9\/site-packages\/torch\/utils\/data\/sampler.py\", line 247, in __iter__\r\n    for idx in self.sampler:\r\n  File \"\/home\/jlquinn\/miniconda3\/envs\/watnlp\/lib\/python3.9\/site-packages\/torch\/utils\/data\/sampler.py\", line 76, in __iter__\r\n    return iter(range(len(self.data_source)))\r\n  File \"\/home\/jlquinn\/miniconda3\/envs\/watnlp\/lib\/python3.9\/site-packages\/transformers\/pipelines\/pt_utils.py\", line 13, in __len__\r\n    return len(self.dataset)\r\n  File \"\/home\/jlquinn\/miniconda3\/envs\/watnlp\/lib\/python3.9\/site-packages\/transformers\/pipelines\/pt_utils.py\", line 289, in __len__\r\n    return len(self.dataset)\r\nTypeError: object of type 'IterableDataset' has no len()\r\n```\n\n### Expected behavior\n\nI'm expecting french translations of the english test set to be printed.\r\n\n\n### Environment info\n\nRun on CPU with no GPU.\r\n\r\nRHEL 8.7 x86_64\r\npython 3.9.0\r\ntransformers 4.17.0\r\ndatasets 2.0.0\r\ntokenizers 0.12.1\r\n\r\n```\r\n(watnlp) [jlquinn@bertdev01 hf]$ datasets-cli env\r\n\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.0.0\r\n- Platform: Linux-4.18.0-372.19.1.el8_6.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.9.0\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.4\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5847\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5847\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5851","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5851\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5851\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5851\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5851","id":1707907048,"node_id":"I_kwDODunzps5lzJfo","number":5851,"title":"Error message not clear in interleaving datasets","user":{"login":"surya-narayanan","id":17240858,"node_id":"MDQ6VXNlcjE3MjQwODU4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17240858?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/surya-narayanan","html_url":"https:\/\/github.com\/surya-narayanan","followers_url":"https:\/\/api.github.com\/users\/surya-narayanan\/followers","following_url":"https:\/\/api.github.com\/users\/surya-narayanan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/surya-narayanan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/surya-narayanan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/surya-narayanan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/surya-narayanan\/orgs","repos_url":"https:\/\/api.github.com\/users\/surya-narayanan\/repos","events_url":"https:\/\/api.github.com\/users\/surya-narayanan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/surya-narayanan\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-05-11T20:52:13Z","updated_at":"2023-05-23T10:32:59Z","closed_at":"2023-05-23T10:32:59Z","author_association":"NONE","active_lock_reason":null,"body":"### System Info\n\nstandard env\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [X] My own task or dataset (give details below)\n\n### Reproduction\n\nI'm trying to interleave 'sciq', 'wiki' and the 'pile-enron' dataset. I think the error I made was that I loaded the train split of one, but for the other but the error is not too helpful- \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n[\/home\/suryahari\/Vornoi\/save_model_ops.py](https:\/\/vscode-remote+ssh-002dremote-002bthomsonlab-002d2-002ejamesgornet-002ecom.vscode-resource.vscode-cdn.net\/home\/suryahari\/Vornoi\/save_model_ops.py) in line 3\r\n      [41](file:\/\/\/home\/suryahari\/Vornoi\/save_model_ops.py?line=40) # %%\r\n----> [43](file:\/\/\/home\/suryahari\/Vornoi\/save_model_ops.py?line=42) dataset = interleave_datasets(datasets, stopping_strategy=\"all_exhausted\")\r\n\r\nFile [~\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/combine.py:124](https:\/\/vscode-remote+ssh-002dremote-002bthomsonlab-002d2-002ejamesgornet-002ecom.vscode-resource.vscode-cdn.net\/home\/suryahari\/~\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/combine.py:124), in interleave_datasets(datasets, probabilities, seed, info, split, stopping_strategy)\r\n    [122](file:\/\/\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/combine.py?line=121) for dataset in datasets[1:]:\r\n    [123](file:\/\/\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/combine.py?line=122)     if (map_style and not isinstance(dataset, Dataset)) or (iterable and not isinstance(dataset, IterableDataset)):\r\n--> [124](file:\/\/\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/combine.py?line=123)         raise ValueError(\r\n    [125](file:\/\/\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/combine.py?line=124)             f\"Unable to interleave a {type(datasets[0])} with a {type(dataset)}. Expected a list of Dataset objects or a list of IterableDataset objects.\"\r\n    [126](file:\/\/\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/combine.py?line=125)         )\r\n    [127](file:\/\/\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/combine.py?line=126) if stopping_strategy not in [\"first_exhausted\", \"all_exhausted\"]:\r\n    [128](file:\/\/\/home\/suryahari\/miniconda3\/envs\/vornoi\/lib\/python3.10\/site-packages\/datasets\/combine.py?line=127)     raise ValueError(f\"{stopping_strategy} is not supported. Please enter a valid stopping_strategy.\")\r\n\r\nValueError: Unable to interleave a  with a . Expected a list of Dataset objects or a list of IterableDataset objects.\r\n```\n\n### Expected behavior\n\nthe error message should hopefully be more clear","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5851\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5851\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5846","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5846\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5846\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5846\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5846","id":1706289290,"node_id":"I_kwDODunzps5ls-iK","number":5846,"title":"load_dataset('bigcode\/the-stack-dedup', streaming=True) very slow!","user":{"login":"tbenthompson","id":4241811,"node_id":"MDQ6VXNlcjQyNDE4MTE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/4241811?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/tbenthompson","html_url":"https:\/\/github.com\/tbenthompson","followers_url":"https:\/\/api.github.com\/users\/tbenthompson\/followers","following_url":"https:\/\/api.github.com\/users\/tbenthompson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/tbenthompson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/tbenthompson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/tbenthompson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/tbenthompson\/orgs","repos_url":"https:\/\/api.github.com\/users\/tbenthompson\/repos","events_url":"https:\/\/api.github.com\/users\/tbenthompson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/tbenthompson\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2023-05-11T17:58:57Z","updated_at":"2023-05-16T03:23:46Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nRunning\r\n\r\n```\r\nimport datasets\r\nds = datasets.load_dataset('bigcode\/the-stack-dedup', streaming=True)\r\n```\r\n\r\ntakes about 2.5 minutes! \r\n\r\nI would expect this to be near instantaneous. With other datasets, the runtime is one or two seconds.\r\n\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.11.0\r\n- Platform: macOS-13.3.1-arm64-arm-64bit\r\n- Python version: 3.10.10\r\n- Huggingface_hub version: 0.13.4\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5846\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5846\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5845","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5845\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5845\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5845\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5845","id":1706253251,"node_id":"PR_kwDODunzps5QUMjS","number":5845,"title":"Add `date_format` param to the CSV reader","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-05-11T17:29:57Z","updated_at":"2023-05-15T07:39:13Z","closed_at":"2023-05-12T15:14:48Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Adds the `date_format` param introduced in Pandas 2.0 to the CSV reader and improves its type hints.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5845\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5845\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5845","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5845","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5845.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5845.patch","merged_at":"2023-05-12T15:14:48Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5844","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5844\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5844\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5844\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5844","id":1705907812,"node_id":"I_kwDODunzps5lrhZk","number":5844,"title":"TypeError: Couldn't cast array of type struct<answer: struct<unanswerable: bool, answerType: string, free_form_answer: string, evidence: list<item: string>, evidenceAnnotate: list<item: string>, highlighted_evidence: list<item: string>>> to ...","user":{"login":"chen-coding","id":54010030,"node_id":"MDQ6VXNlcjU0MDEwMDMw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54010030?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/chen-coding","html_url":"https:\/\/github.com\/chen-coding","followers_url":"https:\/\/api.github.com\/users\/chen-coding\/followers","following_url":"https:\/\/api.github.com\/users\/chen-coding\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/chen-coding\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/chen-coding\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/chen-coding\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/chen-coding\/orgs","repos_url":"https:\/\/api.github.com\/users\/chen-coding\/repos","events_url":"https:\/\/api.github.com\/users\/chen-coding\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/chen-coding\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-05-11T14:15:01Z","updated_at":"2023-05-11T14:15:01Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nTypeError: Couldn't cast array of type struct<answer: struct<unanswerable: bool, answerType: string, free_form_answer: string, evidence: list<item: string>, evidenceAnnotate: list<item: string>, highlighted_evidence: list<item: string>>> to {'answer': {'unanswerable': Value(dtype='bool', id=None), 'answerType': Value(dtype='string', id=None), 'free_form_answer': Value(dtype='string', id=None), 'evidence': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'evidenceAnnotate': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'highlighted_evidence': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'unanswerable': Value(dtype='bool', id=None), 'answerType': Value(dtype='string', id=None), 'free_form_answer': Value(dtype='string', id=None), 'evidence': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'evidenceAnnotate': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'highlighted_evidence': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\r\n\r\nWhen I use _load_dataset()_ I get the error\r\n\r\n`from datasets import load_dataset\r\ndatafiles = {'train': '.\/data\/train.json', 'validation': '.\/data\/validation.json', 'test': '.\/data\/test.json'}\r\nraw_data = load_dataset(\"json\", data_files=datafiles, cache_dir=\".\/cache\")\r\n`\r\nDetailed error information is as follows\uff1a\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\/Users\/CHENJIALEI\/Desktop\/NLPCC2023\/NLPCC23_SciMRC-main\/test2.py\", line 9, in <module>\r\n    raw_data = load_dataset(\"json\", data_files=datafiles, cache_dir=\".\/cache\")\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\load.py\", line 1747, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\builder.py\", line 814, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\builder.py\", line 905, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\builder.py\", line 1521, in _prepare_split\r\n    writer.write_table(table)\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\arrow_writer.py\", line 540, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 2069, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 2031, in cast_table_to_schema\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 2031, in <listcomp>\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 1740, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 1740, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 1867, in cast_array_to_feature\r\n    casted_values = _c(array.values, feature[0])\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 1742, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 1862, in cast_array_to_feature\r\n    arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 1862, in <listcomp>\r\n    arrays = [_c(array.field(name), subfeature) for name, subfeature in feature.items()]\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 1742, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 1867, in cast_array_to_feature\r\n    casted_values = _c(array.values, feature[0])\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 1742, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"D:\\Environment\\anaconda3\\envs\\test\\lib\\site-packages\\datasets\\table.py\", line 1913, in cast_array_to_feature\r\n    raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\n\r\nIt is successful when I load the data separately\r\n\r\n`raw_data = load_dataset(\"json\", data_files=\".\/data\/train.json\", cache_dir=\".\/cache\")`\r\n\r\n\n\n### Steps to reproduce the bug\n\n1.from datasets import load_dataset\r\n2.datafiles = {'train': '.\/data\/train.json', 'validation': '.\/data\/validation.json', 'test': '.\/data\/test.json'}\r\n3.raw_data = load_dataset(\"json\", data_files=datafiles, cache_dir=\".\/cache\")\n\n### Expected behavior\n\n Successfully load dataset\n\n### Environment info\n\ndatasets == 2.6.1\r\npyarrow == 8.0.0\r\npython == 3.8\r\n\r\nplatform:windows11","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5844\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5844\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5841","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5841\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5841\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5841\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5841","id":1705286639,"node_id":"I_kwDODunzps5lpJvv","number":5841,"title":"Abusurdly slow on iteration","user":{"login":"fecet","id":41792945,"node_id":"MDQ6VXNlcjQxNzkyOTQ1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/41792945?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/fecet","html_url":"https:\/\/github.com\/fecet","followers_url":"https:\/\/api.github.com\/users\/fecet\/followers","following_url":"https:\/\/api.github.com\/users\/fecet\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/fecet\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/fecet\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/fecet\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/fecet\/orgs","repos_url":"https:\/\/api.github.com\/users\/fecet\/repos","events_url":"https:\/\/api.github.com\/users\/fecet\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/fecet\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-05-11T08:04:09Z","updated_at":"2023-05-15T15:38:13Z","closed_at":"2023-05-15T15:38:13Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am attempting to iterate through an image dataset, but I am encountering a significant slowdown in the iteration speed. In order to investigate this issue, I conducted the following experiment:\r\n\r\n\r\n```python\r\na=torch.randn(100,224)\r\na=torch.stack([a] * 10000)\r\na.shape\r\n\r\n# %%\r\nds=Dataset.from_dict({\"tensor\":a})\r\nfor i in tqdm(ds.with_format(\"numpy\")):\r\n    pass\r\n\r\nfor i in tqdm(ds.with_format(\"torch\")):\r\n    pass\r\n```\r\nI noticed that the dataset in numpy format performs significantly faster than the one in torch format. My hypothesis is that the dataset undergoes a transformation process of torch->python->numpy(torch) in the background, which might be causing the slowdown. Is there any way to expedite the process by bypassing such transformations?\r\n\r\nFurthermore, if I increase the size of a to an image shape, like:\r\n```python\r\na=torch.randn(3,224,224)\r\n```\r\nthe iteration speed becomes absurdly slow, around 100 iterations per second, whereas the speed with numpy format is approximately 250 iterations per second. This level of speed would be unacceptable for large image datasets, as it could take several hours just to iterate through a single epoch.\n\n### Steps to reproduce the bug\n\n ```python\r\na=torch.randn(100,224)\r\na=torch.stack([a] * 10000)\r\na.shape\r\n\r\n# %%\r\nds=Dataset.from_dict({\"tensor\":a})\r\nfor i in tqdm(ds.with_format(\"numpy\")):\r\n    pass\r\n\r\nfor i in tqdm(ds.with_format(\"torch\")):\r\n    pass\r\n```\n\n### Expected behavior\n\niteration faster\n\n### Environment info\n\n - `datasets` version: 2.11.0\r\n- Platform: Linux-5.4.0-148-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.16\r\n- Huggingface_hub version: 0.13.4\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5841\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5841\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5840","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5840\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5840\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5840\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5840","id":1705212085,"node_id":"I_kwDODunzps5lo3i1","number":5840,"title":"load model error.","user":{"login":"LanShanPi","id":58167546,"node_id":"MDQ6VXNlcjU4MTY3NTQ2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/58167546?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/LanShanPi","html_url":"https:\/\/github.com\/LanShanPi","followers_url":"https:\/\/api.github.com\/users\/LanShanPi\/followers","following_url":"https:\/\/api.github.com\/users\/LanShanPi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/LanShanPi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/LanShanPi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/LanShanPi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/LanShanPi\/orgs","repos_url":"https:\/\/api.github.com\/users\/LanShanPi\/repos","events_url":"https:\/\/api.github.com\/users\/LanShanPi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/LanShanPi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-11T07:12:38Z","updated_at":"2023-05-12T13:44:07Z","closed_at":"2023-05-12T13:44:06Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI had trained one model use deepspeed, when I load the final load I get the follow error:\r\nOSError: Can't load tokenizer for '\/XXX\/DeepSpeedExamples\/applications\/DeepSpeed-Chat\/output\/step3-models\/1.3b\/actor'. If you were trying to load it from 'https:\/\/huggingface.co\/models', make sure you don't have a local directory with the same name. Otherwise, make sure '\/home\/fm001\/hzl\/Project\/DeepSpeedExamples\/applications\/DeepSpeed-Chat\/output\/step3-models\/1.3b\/actor' is the correct path to a directory containing all relevant files for a BloomTokenizerFast tokenizer.\r\n\r\n\r\nmy load code is : python chat.py --path \/XXX\/DeepSpeedExamples\/applications\/DeepSpeed-Chat\/output\/step3-models\/1.3b\/actor\/\r\n\r\n\n\n### Steps to reproduce the bug\n\n\u3002\u3002\u3002\n\n### Expected behavior\n\n\u3002\u3002\u3002\n\n### Environment info\n\n\u3002\u3002\u3002","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5840\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5840\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5842","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5842\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5842\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5842\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5842","id":1705510602,"node_id":"I_kwDODunzps5lqAbK","number":5842,"title":"Remove columns in interable dataset","user":{"login":"surya-narayanan","id":17240858,"node_id":"MDQ6VXNlcjE3MjQwODU4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17240858?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/surya-narayanan","html_url":"https:\/\/github.com\/surya-narayanan","followers_url":"https:\/\/api.github.com\/users\/surya-narayanan\/followers","following_url":"https:\/\/api.github.com\/users\/surya-narayanan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/surya-narayanan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/surya-narayanan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/surya-narayanan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/surya-narayanan\/orgs","repos_url":"https:\/\/api.github.com\/users\/surya-narayanan\/repos","events_url":"https:\/\/api.github.com\/users\/surya-narayanan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/surya-narayanan\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-11T03:48:46Z","updated_at":"2023-06-21T16:36:42Z","closed_at":"2023-06-21T16:36:41Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nRight now, remove_columns() produces a NotImplementedError for iterable style datasets\n\n### Motivation\n\nIt would be great to have the same functionality irrespective of whether one is using an iterable or a map-style dataset\n\n### Your contribution\n\nhope and courage.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5842\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5842\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5843","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5843\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5843\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5843\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5843","id":1705514551,"node_id":"I_kwDODunzps5lqBY3","number":5843,"title":"Can't add iterable datasets to a Dataset Dict.","user":{"login":"surya-narayanan","id":17240858,"node_id":"MDQ6VXNlcjE3MjQwODU4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17240858?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/surya-narayanan","html_url":"https:\/\/github.com\/surya-narayanan","followers_url":"https:\/\/api.github.com\/users\/surya-narayanan\/followers","following_url":"https:\/\/api.github.com\/users\/surya-narayanan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/surya-narayanan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/surya-narayanan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/surya-narayanan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/surya-narayanan\/orgs","repos_url":"https:\/\/api.github.com\/users\/surya-narayanan\/repos","events_url":"https:\/\/api.github.com\/users\/surya-narayanan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/surya-narayanan\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-11T02:09:29Z","updated_at":"2023-05-25T04:51:59Z","closed_at":"2023-05-25T04:51:59Z","author_association":"NONE","active_lock_reason":null,"body":"### System Info\n\nstandard env\n\n### Who can help?\n\n_No response_\n\n### Information\n\n- [ ] The official example scripts\n- [X] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE\/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nGet the following error:\r\n\r\nTypeError: Values in `DatasetDict` should be of type `Dataset` but got type '<class 'datasets.iterable_dataset.IterableDataset'>'\r\n\n\n### Expected behavior\n\nshould be able to add iterable datasets to a dataset dict","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5843\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5843\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5839","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5839\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5839\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5839\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5839","id":1704554718,"node_id":"I_kwDODunzps5lmXDe","number":5839,"title":"Make models\/functions optimized with `torch.compile` hashable","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-05-10T20:02:08Z","updated_at":"2023-11-28T16:29:33Z","closed_at":"2023-11-28T16:29:33Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"As reported in https:\/\/github.com\/huggingface\/datasets\/issues\/5819, hashing functions\/transforms that reference a model, or a function, optimized with `torch.compile` currently fails due to them not being picklable (the concrete error can be found in the linked issue).\r\n\r\nThe solutions to consider:\r\n1. hashing\/pickling the original, uncompiled version of a compiled model\/function (attributes `_orig_mod`\/`_torchdynamo_orig_callable`) (less precise than the 2nd option as it ignores the other params of `torch.compute`)\r\n2. wait for https:\/\/github.com\/pytorch\/pytorch\/issues\/101107 to be resolved\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5839\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5839\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5838","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5838\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5838\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5838\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5838","id":1703210848,"node_id":"I_kwDODunzps5lhO9g","number":5838,"title":"Streaming support for `load_from_disk`","user":{"login":"Nilabhra","id":5437792,"node_id":"MDQ6VXNlcjU0Mzc3OTI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5437792?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Nilabhra","html_url":"https:\/\/github.com\/Nilabhra","followers_url":"https:\/\/api.github.com\/users\/Nilabhra\/followers","following_url":"https:\/\/api.github.com\/users\/Nilabhra\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Nilabhra\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Nilabhra\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Nilabhra\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Nilabhra\/orgs","repos_url":"https:\/\/api.github.com\/users\/Nilabhra\/repos","events_url":"https:\/\/api.github.com\/users\/Nilabhra\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Nilabhra\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2023-05-10T06:25:22Z","updated_at":"2023-05-12T09:37:45Z","closed_at":"2023-05-12T09:37:45Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nSupport for streaming datasets stored in object stores in `load_from_disk`. \n\n### Motivation\n\nThe `load_from_disk` function supports fetching datasets stored in object stores such as `s3`. In many cases, the datasets that are stored in object stores are very large and being able to stream the data from the buckets becomes essential.\n\n### Your contribution\n\nI'd be happy to contribute this feature if I could get the guidance on how to do so.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5838\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5838\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5837","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5837\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5837\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5837\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5837","id":1703019816,"node_id":"I_kwDODunzps5lggUo","number":5837,"title":"Use DeepSpeed load myself \" .csv \" dataset.","user":{"login":"LanShanPi","id":58167546,"node_id":"MDQ6VXNlcjU4MTY3NTQ2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/58167546?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/LanShanPi","html_url":"https:\/\/github.com\/LanShanPi","followers_url":"https:\/\/api.github.com\/users\/LanShanPi\/followers","following_url":"https:\/\/api.github.com\/users\/LanShanPi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/LanShanPi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/LanShanPi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/LanShanPi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/LanShanPi\/orgs","repos_url":"https:\/\/api.github.com\/users\/LanShanPi\/repos","events_url":"https:\/\/api.github.com\/users\/LanShanPi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/LanShanPi\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-10T02:39:28Z","updated_at":"2023-05-15T03:51:36Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I use DeepSpeed train a model with my own \" XXX.csv\" dataset I got the follow question:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/fm001\/.conda\/envs\/hzl\/lib\/python3.8\/site-packages\/datasets\/load.py\", line 1767, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"\/home\/fm001\/.conda\/envs\/hzl\/lib\/python3.8\/site-packages\/datasets\/load.py\", line 1498, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"\/home\/fm001\/.conda\/envs\/hzl\/lib\/python3.8\/site-packages\/datasets\/load.py\", line 1217, in dataset_module_factory\r\n    raise FileNotFoundError(\r\nFileNotFoundError: Couldn't find a dataset script at \/home\/fm001\/hzl\/Data\/qa.csv\/qa.csv.py or any data file in the same directory.\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\nmy code is :\r\nfrom datasets import load_dataset\r\nmydata = load_dataset(\"\/home\/fm001\/hzl\/Data\/qa.csv\")\n\n### Expected behavior\n\n\u3002\u3002\u3002\n\n### Environment info\n\n\u3002\u3002\u3002","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5837\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5837\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5836","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5836\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5836\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5836\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5836","id":1702773316,"node_id":"PR_kwDODunzps5QIgzu","number":5836,"title":"[docs] Custom decoding transforms","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-05-09T21:21:41Z","updated_at":"2023-05-15T07:36:12Z","closed_at":"2023-05-10T20:23:03Z","author_association":"MEMBER","active_lock_reason":null,"body":"Adds custom decoding transform solution to the docs to fix #5782.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5836\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5836\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5836","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5836","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5836.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5836.patch","merged_at":"2023-05-10T20:23:03Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5835","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5835\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5835\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5835\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5835","id":1702522620,"node_id":"PR_kwDODunzps5QHquR","number":5835,"title":"Always set nullable fields in the writer","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-05-09T18:16:59Z","updated_at":"2023-05-23T16:10:29Z","closed_at":"2023-05-19T13:04:30Z","author_association":"MEMBER","active_lock_reason":null,"body":"This fixes loading of e.g. parquet data with non-nullable fields.\r\n\r\nIndeed `datasets.Features` doesn't support non-nullable fields, which can lead to data not concatenable due to arrow schema mismatch.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5835\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5835\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5835","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5835","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5835.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5835.patch","merged_at":"2023-05-19T13:04:30Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5834","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5834\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5834\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5834\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5834","id":1702448892,"node_id":"I_kwDODunzps5leU78","number":5834,"title":"Is uint8 supported?","user":{"login":"ryokan0123","id":17979572,"node_id":"MDQ6VXNlcjE3OTc5NTcy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17979572?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ryokan0123","html_url":"https:\/\/github.com\/ryokan0123","followers_url":"https:\/\/api.github.com\/users\/ryokan0123\/followers","following_url":"https:\/\/api.github.com\/users\/ryokan0123\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ryokan0123\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ryokan0123\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ryokan0123\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ryokan0123\/orgs","repos_url":"https:\/\/api.github.com\/users\/ryokan0123\/repos","events_url":"https:\/\/api.github.com\/users\/ryokan0123\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ryokan0123\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-05-09T17:31:13Z","updated_at":"2023-05-13T05:04:21Z","closed_at":"2023-05-13T05:04:21Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI expect the dataset to store the data in the `uint8` data type, but it's returning `int64` instead.\r\nWhile I've found that `datasets` doesn't yet support float16 (https:\/\/github.com\/huggingface\/datasets\/issues\/4981), I'm wondering if this is the case for other data types as well.\r\nIs there a way to store vector data as `uint8` and then upload it to the hub?\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Features, Dataset, Sequence, Value\r\nimport numpy as np\r\n\r\ndataset = Dataset.from_dict(\r\n    {\"vector\": [np.array([0, 1, 2], dtype=np.uint8)]}, features=Features({\"vector\": Sequence(Value(\"uint8\"))})\r\n).with_format(\"numpy\")\r\n\r\nprint(dataset[0][\"vector\"].dtype)\r\n```\n\n### Expected behavior\n\nExpected: `uint8`\r\nActual: `int64`\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: macOS-12.1-x86_64-i386-64bit\r\n- Python version: 3.8.12\r\n- Huggingface_hub version: 0.12.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5834\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5834\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5833","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5833\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5833\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5833\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5833","id":1702280682,"node_id":"I_kwDODunzps5ldr3q","number":5833,"title":"Unable to push dataset - `create_pr` problem","user":{"login":"agombert","id":17645711,"node_id":"MDQ6VXNlcjE3NjQ1NzEx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17645711?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/agombert","html_url":"https:\/\/github.com\/agombert","followers_url":"https:\/\/api.github.com\/users\/agombert\/followers","following_url":"https:\/\/api.github.com\/users\/agombert\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/agombert\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/agombert\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/agombert\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/agombert\/orgs","repos_url":"https:\/\/api.github.com\/users\/agombert\/repos","events_url":"https:\/\/api.github.com\/users\/agombert\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/agombert\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":14,"created_at":"2023-05-09T15:32:55Z","updated_at":"2023-10-24T18:22:29Z","closed_at":"2023-10-24T18:22:29Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI can't upload to the hub the dataset I manually created locally (Image dataset). I have a problem when using the method `.push_to_hub` which asks for a `create_pr` attribute which is not compatible.\n\n### Steps to reproduce the bug\n\nhere what I have:\r\n\r\n```python\r\ndataset.push_to_hub(\"agomberto\/FrenchCensus-handwritten-texts\")\r\n```\r\nOutput:\r\n```python\r\nPushing split train to the Hub.\r\nPushing dataset shards to the dataset hub:   0%|                                                                                                                                      | 0\/2 [00:00<?, ?it\/s]\r\nCreating parquet from Arrow format:   0%|                                                                                                                                             | 0\/3 [00:00<?, ?ba\/s]\r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3\/3 [00:00<00:00, 12.70ba\/s]\r\nPushing dataset shards to the dataset hub:   0%|                                                                                                                                      | 0\/2 [00:01<?, ?it\/s]\r\n\r\n---------------------------------------------------------------------------\r\nHTTPError                                 Traceback (most recent call last)\r\nFile ~\/miniconda3\/envs\/hwocr\/lib\/python3.8\/site-packages\/huggingface_hub\/utils\/_errors.py:259, in hf_raise_for_status(response, endpoint_name)\r\n    258 try:\r\n--> 259     response.raise_for_status()\r\n    260 except HTTPError as e:\r\n\r\nFile ~\/miniconda3\/envs\/hwocr\/lib\/python3.8\/site-packages\/requests\/models.py:1021, in Response.raise_for_status(self)\r\n   1020 if http_error_msg:\r\n-> 1021     raise HTTPError(http_error_msg, response=self)\r\n\r\nHTTPError: 403 Client Error: Forbidden for url: https:\/\/huggingface.co\/api\/datasets\/agomberto\/FrenchCensus-handwritten-texts\/commit\/main\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nHfHubHTTPError                            Traceback (most recent call last)\r\nCell In[7], line 1\r\n----> 1 dataset.push_to_hub(\"agomberto\/FrenchCensus-handwritten-texts\")\r\n\r\nFile ~\/miniconda3\/envs\/hwocr\/lib\/python3.8\/site-packages\/datasets\/dataset_dict.py:1583, in DatasetDict.push_to_hub(self, repo_id, private, token, branch, max_shard_size, num_shards, embed_external_files)\r\n   1581 logger.warning(f\"Pushing split {split} to the Hub.\")\r\n   1582 # The split=key needs to be removed before merging\r\n-> 1583 repo_id, split, uploaded_size, dataset_nbytes, _, _ = self[split]._push_parquet_shards_to_hub(\r\n   1584     repo_id,\r\n   1585     split=split,\r\n   1586     private=private,\r\n   1587     token=token,\r\n   1588     branch=branch,\r\n   1589     max_shard_size=max_shard_size,\r\n   1590     num_shards=num_shards.get(split),\r\n   1591     embed_external_files=embed_external_files,\r\n   1592 )\r\n   1593 total_uploaded_size += uploaded_size\r\n   1594 total_dataset_nbytes += dataset_nbytes\r\n\r\nFile ~\/miniconda3\/envs\/hwocr\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py:5275, in Dataset._push_parquet_shards_to_hub(self, repo_id, split, private, token, branch, max_shard_size, num_shards, embed_external_files)\r\n   5273         shard.to_parquet(buffer)\r\n   5274         uploaded_size += buffer.tell()\r\n-> 5275         _retry(\r\n   5276             api.upload_file,\r\n   5277             func_kwargs={\r\n   5278                 \"path_or_fileobj\": buffer.getvalue(),\r\n   5279                 \"path_in_repo\": shard_path_in_repo,\r\n   5280                 \"repo_id\": repo_id,\r\n   5281                 \"token\": token,\r\n   5282                 \"repo_type\": \"dataset\",\r\n   5283                 \"revision\": branch,\r\n   5284             },\r\n   5285             exceptions=HTTPError,\r\n   5286             status_codes=[504],\r\n   5287             base_wait_time=2.0,\r\n   5288             max_retries=5,\r\n   5289             max_wait_time=20.0,\r\n   5290         )\r\n   5291     shards_path_in_repo.append(shard_path_in_repo)\r\n   5293 # Cleanup to remove unused files\r\n\r\nFile ~\/miniconda3\/envs\/hwocr\/lib\/python3.8\/site-packages\/datasets\/utils\/file_utils.py:285, in _retry(func, func_args, func_kwargs, exceptions, status_codes, max_retries, base_wait_time, max_wait_time)\r\n    283 except exceptions as err:\r\n    284     if retry >= max_retries or (status_codes and err.response.status_code not in status_codes):\r\n--> 285         raise err\r\n    286     else:\r\n    287         sleep_time = min(max_wait_time, base_wait_time * 2**retry)  # Exponential backoff\r\n\r\nFile ~\/miniconda3\/envs\/hwocr\/lib\/python3.8\/site-packages\/datasets\/utils\/file_utils.py:282, in _retry(func, func_args, func_kwargs, exceptions, status_codes, max_retries, base_wait_time, max_wait_time)\r\n    280 while True:\r\n    281     try:\r\n--> 282         return func(*func_args, **func_kwargs)\r\n    283     except exceptions as err:\r\n    284         if retry >= max_retries or (status_codes and err.response.status_code not in status_codes):\r\n\r\nFile ~\/miniconda3\/envs\/hwocr\/lib\/python3.8\/site-packages\/huggingface_hub\/utils\/_validators.py:120, in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)\r\n    117 if check_use_auth_token:\r\n    118     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\r\n--> 120 return fn(*args, **kwargs)\r\n\r\nFile ~\/miniconda3\/envs\/hwocr\/lib\/python3.8\/site-packages\/huggingface_hub\/hf_api.py:2998, in HfApi.upload_file(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit)\r\n   2990 commit_message = (\r\n   2991     commit_message if commit_message is not None else f\"Upload {path_in_repo} with huggingface_hub\"\r\n   2992 )\r\n   2993 operation = CommitOperationAdd(\r\n   2994     path_or_fileobj=path_or_fileobj,\r\n   2995     path_in_repo=path_in_repo,\r\n   2996 )\r\n-> 2998 commit_info = self.create_commit(\r\n   2999     repo_id=repo_id,\r\n   3000     repo_type=repo_type,\r\n   3001     operations=[operation],\r\n   3002     commit_message=commit_message,\r\n   3003     commit_description=commit_description,\r\n   3004     token=token,\r\n   3005     revision=revision,\r\n   3006     create_pr=create_pr,\r\n   3007     parent_commit=parent_commit,\r\n   3008 )\r\n   3010 if commit_info.pr_url is not None:\r\n   3011     revision = quote(_parse_revision_from_pr_url(commit_info.pr_url), safe=\"\")\r\n\r\nFile ~\/miniconda3\/envs\/hwocr\/lib\/python3.8\/site-packages\/huggingface_hub\/utils\/_validators.py:120, in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)\r\n    117 if check_use_auth_token:\r\n    118     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\r\n--> 120 return fn(*args, **kwargs)\r\n\r\nFile ~\/miniconda3\/envs\/hwocr\/lib\/python3.8\/site-packages\/huggingface_hub\/hf_api.py:2548, in HfApi.create_commit(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit)\r\n   2546 try:\r\n   2547     commit_resp = get_session().post(url=commit_url, headers=headers, data=data, params=params)\r\n-> 2548     hf_raise_for_status(commit_resp, endpoint_name=\"commit\")\r\n   2549 except RepositoryNotFoundError as e:\r\n   2550     e.append_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\r\n\r\nFile ~\/miniconda3\/envs\/hwocr\/lib\/python3.8\/site-packages\/huggingface_hub\/utils\/_errors.py:301, in hf_raise_for_status(response, endpoint_name)\r\n    297     raise BadRequestError(message, response=response) from e\r\n    299 # Convert `HTTPError` into a `HfHubHTTPError` to display request information\r\n    300 # as well (request id and\/or server error message)\r\n--> 301 raise HfHubHTTPError(str(e), response=response) from e\r\n\r\nHfHubHTTPError: 403 Client Error: Forbidden for url: https:\/\/huggingface.co\/api\/datasets\/agomberto\/FrenchCensus-handwritten-texts\/commit\/main (Request ID: Root=1-645a66bf-255ad91602a6404e6cb70fba)\r\n\r\nForbidden: pass `create_pr=1` as a query parameter to create a Pull Request\r\n```\r\n\r\nAnd then when I do\r\n\r\n```python\r\ndataset.push_to_hub(\"agomberto\/FrenchCensus-handwritten-texts\", create_pr=1)\r\n```\r\n\r\nI get \r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[8], line 1\r\n----> 1 dataset.push_to_hub(\"agomberto\/FrenchCensus-handwritten-texts\", create_pr=1)\r\n\r\nTypeError: push_to_hub() got an unexpected keyword argument 'create_pr'\r\n```\n\n### Expected behavior\n\nI would like to have the dataset updloaded [here](https:\/\/huggingface.co\/datasets\/agomberto\/FrenchCensus-handwritten-texts).\n\n### Environment info\n\n```bash\r\n- `datasets` version: 2.12.0\r\n- Platform: macOS-13.3.1-arm64-arm-64bit\r\n- Python version: 3.8.16\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 1.5.3\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5833\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5833\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5832","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5832\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5832\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5832\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5832","id":1702135336,"node_id":"I_kwDODunzps5ldIYo","number":5832,"title":"404 Client Error: Not Found for url: https:\/\/huggingface.co\/api\/models\/bert-large-cased","user":{"login":"varungupta31","id":51288316,"node_id":"MDQ6VXNlcjUxMjg4MzE2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/51288316?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/varungupta31","html_url":"https:\/\/github.com\/varungupta31","followers_url":"https:\/\/api.github.com\/users\/varungupta31\/followers","following_url":"https:\/\/api.github.com\/users\/varungupta31\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/varungupta31\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/varungupta31\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/varungupta31\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/varungupta31\/orgs","repos_url":"https:\/\/api.github.com\/users\/varungupta31\/repos","events_url":"https:\/\/api.github.com\/users\/varungupta31\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/varungupta31\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-09T14:14:59Z","updated_at":"2023-05-09T14:25:59Z","closed_at":"2023-05-09T14:25:59Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nRunning [Bert-Large-Cased](https:\/\/huggingface.co\/bert-large-cased) model causes `HTTPError`, with the following traceback-\r\n\r\n```\r\nHTTPError                                 Traceback (most recent call last)\r\n<ipython-input-6-5c580443a1ad> in <module>\r\n----> 1 tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\r\n\r\n~\/miniconda3\/envs\/cmd-chall\/lib\/python3.7\/site-packages\/transformers\/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\r\n   1646             # At this point pretrained_model_name_or_path is either a directory or a model identifier name\r\n   1647             fast_tokenizer_file = get_fast_tokenizer_file(\r\n-> 1648                 pretrained_model_name_or_path, revision=revision, use_auth_token=use_auth_token\r\n   1649             )\r\n   1650             additional_files_names = {\r\n\r\n~\/miniconda3\/envs\/cmd-chall\/lib\/python3.7\/site-packages\/transformers\/tokenization_utils_base.py in get_fast_tokenizer_file(path_or_repo, revision, use_auth_token)\r\n   3406     \"\"\"\r\n   3407     # Inspect all files from the repo\/folder.\r\n-> 3408     all_files = get_list_of_files(path_or_repo, revision=revision, use_auth_token=use_auth_token)\r\n   3409     tokenizer_files_map = {}\r\n   3410     for file_name in all_files:\r\n\r\n~\/miniconda3\/envs\/cmd-chall\/lib\/python3.7\/site-packages\/transformers\/file_utils.py in get_list_of_files(path_or_repo, revision, use_auth_token)\r\n   1685         token = None\r\n   1686     model_info = HfApi(endpoint=HUGGINGFACE_CO_RESOLVE_ENDPOINT).model_info(\r\n-> 1687         path_or_repo, revision=revision, token=token\r\n   1688     )\r\n   1689     return [f.rfilename for f in model_info.siblings]\r\n\r\n~\/miniconda3\/envs\/cmd-chall\/lib\/python3.7\/site-packages\/huggingface_hub\/hf_api.py in model_info(self, repo_id, revision, token)\r\n    246         )\r\n    247         r = requests.get(path, headers=headers)\r\n--> 248         r.raise_for_status()\r\n    249         d = r.json()\r\n    250         return ModelInfo(**d)\r\n\r\n~\/miniconda3\/envs\/cmd-chall\/lib\/python3.7\/site-packages\/requests\/models.py in raise_for_status(self)\r\n    951 \r\n    952         if http_error_msg:\r\n--> 953             raise HTTPError(http_error_msg, response=self)\r\n    954 \r\n    955     def close(self):\r\n\r\nHTTPError: 404 Client Error: Not Found for url: https:\/\/huggingface.co\/api\/models\/bert-large-cased\r\n```\r\n\r\nI have also tried running in offline mode, as [discussed here](https:\/\/huggingface.co\/docs\/transformers\/installation#offline-mode)\r\n```\r\nHF_DATASETS_OFFLINE=1 \r\nTRANSFORMERS_OFFLINE=1\r\n```\n\n### Steps to reproduce the bug\n\n1.  `from transformers import BertTokenizer, BertModel`\r\n2.  `tokenizer = BertTokenizer.from_pretrained('bert-large-cased')`\n\n### Expected behavior\n\nRun without the HTTP error.\n\n### Environment info\n\n| # Name             | Version    | Build                       | Channel |   |\r\n|--------------------|------------|-----------------------------|---------|---|\r\n| _libgcc_mutex      | 0.1        | main                        |         |   |\r\n| _openmp_mutex      | 4.5        | 1_gnu                       |         |   |\r\n| _pytorch_select    | 0.1        | cpu_0                       |         |   |\r\n| appdirs            | 1.4.4      | pypi_0                      | pypi    |   |\r\n| backcall           | 0.2.0      | pypi_0                      | pypi    |   |\r\n| blas               | 1.0        | mkl                         |         |   |\r\n| bzip2              | 1.0.8      | h7b6447c_0                  |         |   |\r\n| ca-certificates    | 2021.7.5   | h06a4308_1                  |         |   |\r\n| certifi            | 2021.5.30  | py37h06a4308_0              |         |   |\r\n| cffi               | 1.14.6     | py37h400218f_0              |         |   |\r\n| charset-normalizer | 2.0.3      | pypi_0                      | pypi    |   |\r\n| click              | 8.0.1      | pypi_0                      | pypi    |   |\r\n| colorama           | 0.4.4      | pypi_0                      | pypi    |   |\r\n| cudatoolkit        | 11.1.74    | h6bb024c_0                  | nvidia  |   |\r\n| cycler             | 0.11.0     | pypi_0                      | pypi    |   |\r\n| decorator          | 5.0.9      | pypi_0                      | pypi    |   |\r\n| docker-pycreds     | 0.4.0      | pypi_0                      | pypi    |   |\r\n| docopt             | 0.6.2      | pypi_0                      | pypi    |   |\r\n| dominate           | 2.6.0      | pypi_0                      | pypi    |   |\r\n| ffmpeg             | 4.3        | hf484d3e_0                  | pytorch |   |\r\n| filelock           | 3.0.12     | pypi_0                      | pypi    |   |\r\n| fonttools          | 4.38.0     | pypi_0                      | pypi    |   |\r\n| freetype           | 2.10.4     | h5ab3b9f_0                  |         |   |\r\n| gitdb              | 4.0.7      | pypi_0                      | pypi    |   |\r\n| gitpython          | 3.1.18     | pypi_0                      | pypi    |   |\r\n| gmp                | 6.2.1      | h2531618_2                  |         |   |\r\n| gnutls             | 3.6.15     | he1e5248_0                  |         |   |\r\n| huggingface-hub    | 0.0.12     | pypi_0                      | pypi    |   |\r\n| humanize           | 3.10.0     | pypi_0                      | pypi    |   |\r\n| idna               | 3.2        | pypi_0                      | pypi    |   |\r\n| importlib-metadata | 4.6.1      | pypi_0                      | pypi    |   |\r\n| intel-openmp       | 2019.4     | 243                         |         |   |\r\n| ipdb               | 0.13.9     | pypi_0                      | pypi    |   |\r\n| ipython            | 7.25.0     | pypi_0                      | pypi    |   |\r\n| ipython-genutils   | 0.2.0      | pypi_0                      | pypi    |   |\r\n| jedi               | 0.18.0     | pypi_0                      | pypi    |   |\r\n| joblib             | 1.0.1      | pypi_0                      | pypi    |   |\r\n| jpeg               | 9b         | h024ee3a_2                  |         |   |\r\n| jsonpickle         | 1.5.2      | pypi_0                      | pypi    |   |\r\n| kiwisolver         | 1.4.4      | pypi_0                      | pypi    |   |\r\n| lame               | 3.100      | h7b6447c_0                  |         |   |\r\n| lcms2              | 2.12       | h3be6417_0                  |         |   |\r\n| ld_impl_linux-64   | 2.35.1     | h7274673_9                  |         |   |\r\n| libffi             | 3.3        | he6710b0_2                  |         |   |\r\n| libgcc-ng          | 9.3.0      | h5101ec6_17                 |         |   |\r\n| libgomp            | 9.3.0      | h5101ec6_17                 |         |   |\r\n| libiconv           | 1.15       | h63c8f33_5                  |         |   |\r\n| libidn2            | 2.3.2      | h7f8727e_0                  |         |   |\r\n| libmklml           | 2019.0.5   | 0                           |         |   |\r\n| libpng             | 1.6.37     | hbc83047_0                  |         |   |\r\n| libstdcxx-ng       | 9.3.0      | hd4cf53a_17                 |         |   |\r\n| libtasn1           | 4.16.0     | h27cfd23_0                  |         |   |\r\n| libtiff            | 4.2.0      | h85742a9_0                  |         |   |\r\n| libunistring       | 0.9.10     | h27cfd23_0                  |         |   |\r\n| libuv              | 1.40.0     | h7b6447c_0                  |         |   |\r\n| libwebp-base       | 1.2.0      | h27cfd23_0                  |         |   |\r\n| lz4-c              | 1.9.3      | h2531618_0                  |         |   |\r\n| matplotlib         | 3.5.3      | pypi_0                      | pypi    |   |\r\n| matplotlib-inline  | 0.1.2      | pypi_0                      | pypi    |   |\r\n| mergedeep          | 1.3.4      | pypi_0                      | pypi    |   |\r\n| mkl                | 2020.2     | 256                         |         |   |\r\n| mkl-service        | 2.3.0      | py37he8ac12f_0              |         |   |\r\n| mkl_fft            | 1.3.0      | py37h54f3939_0              |         |   |\r\n| mkl_random         | 1.1.1      | py37h0573a6f_0              |         |   |\r\n| msgpack            | 1.0.2      | pypi_0                      | pypi    |   |\r\n| munch              | 2.5.0      | pypi_0                      | pypi    |   |\r\n| ncurses            | 6.2        | he6710b0_1                  |         |   |\r\n| nettle             | 3.7.3      | hbbd107a_1                  |         |   |\r\n| ninja              | 1.10.2     | hff7bd54_1                  |         |   |\r\n| nltk               | 3.8.1      | pypi_0                      | pypi    |   |\r\n| numpy              | 1.19.2     | py37h54aff64_0              |         |   |\r\n| numpy-base         | 1.19.2     | py37hfa32c7d_0              |         |   |\r\n| olefile            | 0.46       | py37_0                      |         |   |\r\n| openh264           | 2.1.0      | hd408876_0                  |         |   |\r\n| openjpeg           | 2.3.0      | h05c96fa_1                  |         |   |\r\n| openssl            | 1.1.1k     | h27cfd23_0                  |         |   |\r\n| packaging          | 21.0       | pypi_0                      | pypi    |   |\r\n| pandas             | 1.3.1      | pypi_0                      | pypi    |   |\r\n| parso              | 0.8.2      | pypi_0                      | pypi    |   |\r\n| pathtools          | 0.1.2      | pypi_0                      | pypi    |   |\r\n| pexpect            | 4.8.0      | pypi_0                      | pypi    |   |\r\n| pickleshare        | 0.7.5      | pypi_0                      | pypi    |   |\r\n| pillow             | 8.3.1      | py37h2c7a002_0              |         |   |\r\n| pip                | 21.1.3     | py37h06a4308_0              |         |   |\r\n| prompt-toolkit     | 3.0.19     | pypi_0                      | pypi    |   |\r\n| protobuf           | 4.21.12    | pypi_0                      | pypi    |   |\r\n| psutil             | 5.8.0      | pypi_0                      | pypi    |   |\r\n| ptyprocess         | 0.7.0      | pypi_0                      | pypi    |   |\r\n| py-cpuinfo         | 8.0.0      | pypi_0                      | pypi    |   |\r\n| pycparser          | 2.20       | py_2                        |         |   |\r\n| pygments           | 2.9.0      | pypi_0                      | pypi    |   |\r\n| pyparsing          | 2.4.7      | pypi_0                      | pypi    |   |\r\n| python             | 3.7.10     | h12debd9_4                  |         |   |\r\n| python-dateutil    | 2.8.2      | pypi_0                      | pypi    |   |\r\n| pytorch            | 1.9.0      | py3.7_cuda11.1_cudnn8.0.5_0 | pytorch |   |\r\n| pytz               | 2021.1     | pypi_0                      | pypi    |   |\r\n| pyyaml             | 5.4.1      | pypi_0                      | pypi    |   |\r\n| readline           | 8.1        | h27cfd23_0                  |         |   |\r\n| regex              | 2022.10.31 | pypi_0                      | pypi    |   |\r\n| requests           | 2.26.0     | pypi_0                      | pypi    |   |\r\n| sacred             | 0.8.2      | pypi_0                      | pypi    |   |\r\n| sacremoses         | 0.0.45     | pypi_0                      | pypi    |   |\r\n| scikit-learn       | 0.24.2     | pypi_0                      | pypi    |   |\r\n| scipy              | 1.7.0      | pypi_0                      | pypi    |   |\r\n| sentry-sdk         | 1.15.0     | pypi_0                      | pypi    |   |\r\n| setproctitle       | 1.3.2      | pypi_0                      | pypi    |   |\r\n| setuptools         | 52.0.0     | py37h06a4308_0              |         |   |\r\n| six                | 1.16.0     | pyhd3eb1b0_0                |         |   |\r\n| smmap              | 4.0.0      | pypi_0                      | pypi    |   |\r\n| sqlite             | 3.36.0     | hc218d9a_0                  |         |   |\r\n| threadpoolctl      | 2.2.0      | pypi_0                      | pypi    |   |\r\n| tk                 | 8.6.10     | hbc83047_0                  |         |   |\r\n| tokenizers         | 0.10.3     | pypi_0                      | pypi    |   |\r\n| toml               | 0.10.2     | pypi_0                      | pypi    |   |\r\n| torchaudio         | 0.9.0      | py37                        | pytorch |   |\r\n| torchvision        | 0.10.0     | py37_cu111                  | pytorch |   |\r\n| tqdm               | 4.61.2     | pypi_0                      | pypi    |   |\r\n| traitlets          | 5.0.5      | pypi_0                      | pypi    |   |\r\n| transformers       | 4.9.1      | pypi_0                      | pypi    |   |\r\n| typing-extensions  | 3.10.0.0   | hd3eb1b0_0                  |         |   |\r\n| typing_extensions  | 3.10.0.0   | pyh06a4308_0                |         |   |\r\n| urllib3            | 1.26.14    | pypi_0                      | pypi    |   |\r\n| wandb              | 0.13.10    | pypi_0                      | pypi    |   |\r\n| wcwidth            | 0.2.5      | pypi_0                      | pypi    |   |\r\n| wheel              | 0.36.2     | pyhd3eb1b0_0                |         |   |\r\n| wrapt              | 1.12.1     | pypi_0                      | pypi    |   |\r\n| xz                 | 5.2.5      | h7b6447c_0                  |         |   |\r\n| zipp               | 3.5.0      | pypi_0                      | pypi    |   |\r\n| zlib               | 1.2.11     | h7b6447c_3                  |         |   |\r\n| zstd               | 1.4.9      | haebb681_0                  |         |   |","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5832\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5832\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5831","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5831\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5831\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5831\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5831","id":1701813835,"node_id":"I_kwDODunzps5lb55L","number":5831,"title":"[Bug]504 Server Error when loading dataset which was already cached","user":{"login":"SingL3","id":20473466,"node_id":"MDQ6VXNlcjIwNDczNDY2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20473466?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/SingL3","html_url":"https:\/\/github.com\/SingL3","followers_url":"https:\/\/api.github.com\/users\/SingL3\/followers","following_url":"https:\/\/api.github.com\/users\/SingL3\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/SingL3\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/SingL3\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/SingL3\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/SingL3\/orgs","repos_url":"https:\/\/api.github.com\/users\/SingL3\/repos","events_url":"https:\/\/api.github.com\/users\/SingL3\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/SingL3\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-05-09T10:31:07Z","updated_at":"2023-05-10T01:48:20Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI have already cached the dataset using:\r\n```\r\ndataset = load_dataset(\"databricks\/databricks-dolly-15k\",\r\n                        cache_dir=\"\/mnt\/data\/llm\/datasets\/databricks-dolly-15k\")\r\n```\r\nAfter that, I tried to load it again using the same machine, I got this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/mnt\/home\/llm\/pythia\/train.py\", line 16, in <module>\r\n    dataset = load_dataset(\"databricks\/databricks-dolly-15k\",\r\n  File \"\/mnt\/data\/conda\/envs\/pythia_ft\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1773, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"\/mnt\/data\/conda\/envs\/pythia_ft\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1502, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"\/mnt\/data\/conda\/envs\/pythia_ft\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1219, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"\/mnt\/data\/conda\/envs\/pythia_ft\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1186, in dataset_module_factory\r\n    raise e\r\n  File \"\/mnt\/data\/conda\/envs\/pythia_ft\/lib\/python3.9\/site-packages\/datasets\/load.py\", line 1160, in dataset_module_factory\r\n    dataset_info = hf_api.dataset_info(\r\n  File \"\/mnt\/data\/conda\/envs\/pythia_ft\/lib\/python3.9\/site-packages\/huggingface_hub\/utils\/_validators.py\", line 120, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"\/mnt\/data\/conda\/envs\/pythia_ft\/lib\/python3.9\/site-packages\/huggingface_hub\/hf_api.py\", line 1667, in dataset_info\r\n    hf_raise_for_status(r)\r\n  File \"\/mnt\/data\/conda\/envs\/pythia_ft\/lib\/python3.9\/site-packages\/huggingface_hub\/utils\/_errors.py\", line 301, in hf_raise_for_status\r\n    raise HfHubHTTPError(str(e), response=response) from e\r\nhuggingface_hub.utils._errors.HfHubHTTPError: 504 Server Error: Gateway Time-out for url: https:\/\/huggingface.co\/api\/datasets\/databricks\/databricks-dolly-15k\r\n```\n\n### Steps to reproduce the bug\n\n1. cache the databrick-dolly-15k dataset using load_dataset, setting a cache_dir\r\n2. use load_dataset again, setting the same cache_dir\n\n### Expected behavior\n\nDataset loaded succuessfully.\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Linux-4.18.0-372.16.1.el8_6.x86_64-x86_64-with-glibc2.27\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5831\/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5831\/timeline","performed_via_github_app":null,"state_reason":"reopened","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5830","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5830\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5830\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5830\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5830","id":1701451399,"node_id":"PR_kwDODunzps5QEFEi","number":5830,"title":"Debug windows #2","user":{"login":"HyukjinKwon","id":6477701,"node_id":"MDQ6VXNlcjY0Nzc3MDE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6477701?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/HyukjinKwon","html_url":"https:\/\/github.com\/HyukjinKwon","followers_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/followers","following_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/orgs","repos_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/repos","events_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-05-09T06:40:34Z","updated_at":"2023-05-09T06:40:47Z","closed_at":"2023-05-09T06:40:47Z","author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5830\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5830\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5830","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5830","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5830.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5830.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5829","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5829\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5829\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5829\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5829","id":1699958189,"node_id":"I_kwDODunzps5lU02t","number":5829,"title":"(mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))","user":{"login":"elcolie","id":18206728,"node_id":"MDQ6VXNlcjE4MjA2NzI4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/18206728?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/elcolie","html_url":"https:\/\/github.com\/elcolie","followers_url":"https:\/\/api.github.com\/users\/elcolie\/followers","following_url":"https:\/\/api.github.com\/users\/elcolie\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/elcolie\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/elcolie\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/elcolie\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/elcolie\/orgs","repos_url":"https:\/\/api.github.com\/users\/elcolie\/repos","events_url":"https:\/\/api.github.com\/users\/elcolie\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/elcolie\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-08T10:07:14Z","updated_at":"2023-06-30T11:39:14Z","closed_at":"2023-05-09T00:46:42Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nM2 MBP can't run\r\n```python\r\nfrom datasets import load_dataset\r\n\r\njazzy = load_dataset(\"nomic-ai\/gpt4all-j-prompt-generations\", revision='v1.2-jazzy')\r\n```\n\n### Steps to reproduce the bug\n\n1. Use M2 MBP\r\n2. Python 3.10.10 from pyenv\r\n3. Run \r\n```\r\nfrom datasets import load_dataset\r\n\r\njazzy = load_dataset(\"nomic-ai\/gpt4all-j-prompt-generations\", revision='v1.2-jazzy')\r\n```\n\n### Expected behavior\n\nBe able to run normally\n\n### Environment info\n\n```\r\nfrom datasets import load_dataset\r\n\r\njazzy = load_dataset(\"nomic-ai\/gpt4all-j-prompt-generations\", revision='v1.2-jazzy')\r\n```\r\nOSX: 13.2\r\nCPU: M2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5829\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5829\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5828","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5828\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5828\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5828\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5828","id":1699235739,"node_id":"I_kwDODunzps5lSEeb","number":5828,"title":"Stream data concatenation issue","user":{"login":"krishnapriya-18","id":48817796,"node_id":"MDQ6VXNlcjQ4ODE3Nzk2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/48817796?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/krishnapriya-18","html_url":"https:\/\/github.com\/krishnapriya-18","followers_url":"https:\/\/api.github.com\/users\/krishnapriya-18\/followers","following_url":"https:\/\/api.github.com\/users\/krishnapriya-18\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/krishnapriya-18\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/krishnapriya-18\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/krishnapriya-18\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/krishnapriya-18\/orgs","repos_url":"https:\/\/api.github.com\/users\/krishnapriya-18\/repos","events_url":"https:\/\/api.github.com\/users\/krishnapriya-18\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/krishnapriya-18\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-07T21:02:54Z","updated_at":"2023-06-29T20:07:56Z","closed_at":"2023-05-10T05:05:47Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI am not able to concatenate the augmentation of the stream data. I am using the latest version of dataset.\r\n\r\nValueError: The features can't be aligned because the key audio of features {'audio_id': Value(dtype='string', \r\nid=None), 'audio': {'array': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'path': \r\nValue(dtype='null', id=None), 'sampling_rate': Value(dtype='int64', id=None)}, 'transcript': Value(dtype='string', \r\nid=None)} has unexpected type - {'array': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), \r\n'path': Value(dtype='null', id=None), 'sampling_rate': Value(dtype='int64', id=None)} (expected either \r\nAudio(sampling_rate=16000, mono=True, decode=True, id=None) or Value(\"null\").\r\n\r\n### Steps to reproduce the bug\r\n\r\ndataset = load_dataset(\"tobiolatunji\/afrispeech-200\", \"all\", streaming=True).shuffle(seed=42)\r\ndataset_cln = dataset.remove_columns(['speaker_id', 'path', 'age_group', 'gender', 'accent', 'domain', 'country', 'duration'])\r\ndataset_cln = dataset_cln.cast_column(\"audio\", Audio(sampling_rate=16000))\r\nfrom audiomentations import AddGaussianNoise,Compose,Gain,OneOf,PitchShift,PolarityInversion,TimeStretch\r\n\r\naugmentation = Compose([\r\n    AddGaussianNoise(min_amplitude=0.005, max_amplitude=0.015, p=0.2)\r\n])\r\n\r\ndef augment_dataset(batch):\r\n    audio = batch[\"audio\"]\r\n    audio[\"array\"] = augmentation(audio[\"array\"], sample_rate=audio[\"sampling_rate\"])    \r\n    return batch\r\n\r\naugmented_dataset_cln = dataset_cln['train'].map(augment_dataset)\r\ndataset_cln['train'] = interleave_datasets([dataset_cln['train'], augmented_dataset_cln])\r\ndataset_cln['train'] = dataset_cln['train'].shuffle(seed=42)\r\n\r\n### Expected behavior\r\n\r\nI should be able to merge as sampling rate is same.\r\n\r\n### Environment info\r\n\r\nimport datasets\r\nimport transformers\r\nimport accelerate\r\nprint(datasets.__version__)\r\nprint(transformers.__version__)\r\nprint(torch.__version__)\r\nprint(evaluate.__version__)\r\nprint(accelerate.__version__)\r\n\r\n2.12.0\r\n4.28.1\r\n2.0.0\r\n0.4.0\r\n0.18.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5828\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5828\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5827","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5827\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5827\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5827\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5827","id":1698891246,"node_id":"I_kwDODunzps5lQwXu","number":5827,"title":"load json dataset interrupt when dtype cast problem occured","user":{"login":"1014661165","id":46060451,"node_id":"MDQ6VXNlcjQ2MDYwNDUx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/46060451?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/1014661165","html_url":"https:\/\/github.com\/1014661165","followers_url":"https:\/\/api.github.com\/users\/1014661165\/followers","following_url":"https:\/\/api.github.com\/users\/1014661165\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/1014661165\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/1014661165\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/1014661165\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/1014661165\/orgs","repos_url":"https:\/\/api.github.com\/users\/1014661165\/repos","events_url":"https:\/\/api.github.com\/users\/1014661165\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/1014661165\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-07T04:52:09Z","updated_at":"2023-05-10T12:32:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\ni have a json like this:\r\n[\r\n    {\"id\": 1, \"name\": 1},\r\n    {\"id\": 2, \"name\": \"Nan\"},\r\n    {\"id\": 3, \"name\": 3}, \r\n    ....\r\n]\r\n\uff0cwhich have several problematic rows data like row 2, then i load it with datasets.load_dataset('json', data_files=['xx.json'], split='train'), it will report like this:\r\nGenerating train split: 0 examples [00:00, ? examples\/s]Failed to read file 'C:\\Users\\gawinjunwu\\Downloads\\test\\data\\a.json' with error <class 'pyarrow.lib.ArrowInvalid'>: Could not convert '2' with type str: tried to convert to int64\r\nTraceback (most recent call last):\r\n  File \"D:\\Python3.9\\lib\\site-packages\\datasets\\builder.py\", line 1858, in _prepare_split_single\r\n    for _, table in generator:\r\n  File \"D:\\Python3.9\\lib\\site-packages\\datasets\\packaged_modules\\json\\json.py\", line 146, in _generate_tables\r\n    raise ValueError(f\"Not able to read records in the JSON file at {file}.\") from None\r\nValueError: Not able to read records in the JSON file at C:\\Users\\gawinjunwu\\Downloads\\test\\data\\a.json.     \r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\gawinjunwu\\Downloads\\test\\scripts\\a.py\", line 4, in <module>\r\n    ds = load_dataset('json', data_dir='data', split='train')\r\n  File \"D:\\Python3.9\\lib\\site-packages\\datasets\\load.py\", line 1797, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"D:\\Python3.9\\lib\\site-packages\\datasets\\builder.py\", line 890, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"D:\\Python3.9\\lib\\site-packages\\datasets\\builder.py\", line 985, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"D:\\Python3.9\\lib\\site-packages\\datasets\\builder.py\", line 1746, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"D:\\Python3.9\\lib\\site-packages\\datasets\\builder.py\", line 1891, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset.\r\nCould datasets skip those problematic data row? \n\n### Steps to reproduce the bug\n\nprepare a json file like this:\r\n[\r\n    {\"id\": 1, \"name\": 1},\r\n    {\"id\": 2, \"name\": \"Nan\"},\r\n    {\"id\": 3, \"name\": 3}\r\n]\r\nthen use datasets.load_dataset('json', dir_files=['xxx.json']) to load the json file\n\n### Expected behavior\n\nskip the problematic data row and load row1 and row3\n\n### Environment info\n\npython3.9","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5827\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5827\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5826","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5826\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5826\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5826\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5826","id":1698155751,"node_id":"PR_kwDODunzps5P5FYZ","number":5826,"title":"Support working_dir in from_spark","user":{"login":"maddiedawson","id":106995444,"node_id":"U_kgDOBmCe9A","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/106995444?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/maddiedawson","html_url":"https:\/\/github.com\/maddiedawson","followers_url":"https:\/\/api.github.com\/users\/maddiedawson\/followers","following_url":"https:\/\/api.github.com\/users\/maddiedawson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/maddiedawson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/maddiedawson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/maddiedawson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/maddiedawson\/orgs","repos_url":"https:\/\/api.github.com\/users\/maddiedawson\/repos","events_url":"https:\/\/api.github.com\/users\/maddiedawson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/maddiedawson\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-05-05T20:22:40Z","updated_at":"2023-05-25T17:45:54Z","closed_at":"2023-05-25T08:46:15Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Accept `working_dir` as an argument to `Dataset.from_spark`. Setting a non-NFS working directory for Spark workers to materialize to will improve write performance.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5826\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5826\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5826","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5826","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5826.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5826.patch","merged_at":"2023-05-25T08:46:15Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5825","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5825\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5825\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5825\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5825","id":1697327483,"node_id":"I_kwDODunzps5lKyl7","number":5825,"title":"FileNotFound even though exists","user":{"login":"Muennighoff","id":62820084,"node_id":"MDQ6VXNlcjYyODIwMDg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/62820084?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Muennighoff","html_url":"https:\/\/github.com\/Muennighoff","followers_url":"https:\/\/api.github.com\/users\/Muennighoff\/followers","following_url":"https:\/\/api.github.com\/users\/Muennighoff\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Muennighoff\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Muennighoff\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Muennighoff\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Muennighoff\/orgs","repos_url":"https:\/\/api.github.com\/users\/Muennighoff\/repos","events_url":"https:\/\/api.github.com\/users\/Muennighoff\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Muennighoff\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-05-05T09:49:55Z","updated_at":"2023-08-16T10:02:01Z","closed_at":"2023-08-16T10:02:01Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nI'm trying to download https:\/\/huggingface.co\/datasets\/bigscience\/xP3\/resolve\/main\/ur\/xp3_facebook_flores_spa_Latn-urd_Arab_devtest_ab-spa_Latn-urd_Arab.jsonl which works fine in my webbrowser, but somehow not with datasets. Am I doing sth wrong?\r\n\r\n```\r\nDownloading builder script: 100%\r\n2.82k\/2.82k [00:00<00:00, 64.2kB\/s]\r\nDownloading readme: 100%\r\n12.6k\/12.6k [00:00<00:00, 585kB\/s]\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n[<ipython-input-2-4b45446a91d5>](https:\/\/localhost:8080\/#) in <cell line: 4>()\r\n      2 lang = \"ur\"\r\n      3 fname = \"xp3_facebook_flores_spa_Latn-urd_Arab_devtest_ab-spa_Latn-urd_Arab.jsonl\"\r\n----> 4 dataset = load_dataset(\"bigscience\/xP3\", data_files=f\"{lang}\/{fname}\")\r\n\r\n6 frames\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/datasets\/data_files.py](https:\/\/localhost:8080\/#) in _resolve_single_pattern_locally(base_path, pattern, allowed_extensions)\r\n    291         if allowed_extensions is not None:\r\n    292             error_msg += f\" with any supported extension {list(allowed_extensions)}\"\r\n--> 293         raise FileNotFoundError(error_msg)\r\n    294     return sorted(out)\r\n    295 \r\n\r\nFileNotFoundError: Unable to find 'https:\/\/huggingface.co\/datasets\/bigscience\/xP3\/resolve\/main\/ur\/xp3_facebook_flores_spa_Latn-urd_Arab_devtest_ab-spa_Latn-urd_Arab.jsonl' at \/content\/https:\/huggingface.co\/datasets\/bigscience\/xP3\/resolve\/main\r\n```\n\n### Steps to reproduce the bug\n\n```\r\n!pip install -q datasets\r\nfrom datasets import load_dataset\r\nlang = \"ur\"\r\nfname = \"xp3_facebook_flores_spa_Latn-urd_Arab_devtest_ab-spa_Latn-urd_Arab.jsonl\"\r\ndataset = load_dataset(\"bigscience\/xP3\", data_files=f\"{lang}\/{fname}\")\r\n```\n\n### Expected behavior\n\nCorrectly downloads\n\n### Environment info\n\nlatest versions","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5825\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5825\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5824","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5824\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5824\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5824\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5824","id":1697152148,"node_id":"PR_kwDODunzps5P1rIZ","number":5824,"title":"Fix incomplete docstring for `BuilderConfig`","user":{"login":"Laurent2916","id":21087104,"node_id":"MDQ6VXNlcjIxMDg3MTA0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/21087104?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Laurent2916","html_url":"https:\/\/github.com\/Laurent2916","followers_url":"https:\/\/api.github.com\/users\/Laurent2916\/followers","following_url":"https:\/\/api.github.com\/users\/Laurent2916\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Laurent2916\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Laurent2916\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Laurent2916\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Laurent2916\/orgs","repos_url":"https:\/\/api.github.com\/users\/Laurent2916\/repos","events_url":"https:\/\/api.github.com\/users\/Laurent2916\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Laurent2916\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-05T07:34:28Z","updated_at":"2023-05-05T12:39:14Z","closed_at":"2023-05-05T12:31:54Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fixes #5820\r\nAlso fixed a couple of typos I spotted","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5824\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5824\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5824","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5824","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5824.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5824.patch","merged_at":"2023-05-05T12:31:54Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5823","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5823\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5823\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5823\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5823","id":1697024789,"node_id":"I_kwDODunzps5lJosV","number":5823,"title":"[2.12.0] DatasetDict.save_to_disk not saving to S3","user":{"login":"thejamesmarq","id":5233185,"node_id":"MDQ6VXNlcjUyMzMxODU=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5233185?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thejamesmarq","html_url":"https:\/\/github.com\/thejamesmarq","followers_url":"https:\/\/api.github.com\/users\/thejamesmarq\/followers","following_url":"https:\/\/api.github.com\/users\/thejamesmarq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thejamesmarq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thejamesmarq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thejamesmarq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thejamesmarq\/orgs","repos_url":"https:\/\/api.github.com\/users\/thejamesmarq\/repos","events_url":"https:\/\/api.github.com\/users\/thejamesmarq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thejamesmarq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-05T05:22:59Z","updated_at":"2023-05-05T15:01:18Z","closed_at":"2023-05-05T15:01:17Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen trying to save a `DatasetDict` to a private S3 bucket using `save_to_disk`, the artifacts are instead saved locally, and not in the S3 bucket.\r\n\r\nI have tried using the deprecated `fs` as well as the `storage_options` arguments and I get the same results.\n\n### Steps to reproduce the bug\n\n1. Create a DatsetDict `dataset`\r\n2. Create a S3FileSystem object\r\n`s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)`\r\n3. Save using `dataset_dict.save_to_disk(f\"{s3_bucket}\/{s3_dir}\/{dataset_name}\", storage_options=s3.storage_options)` or `dataset_dict.save_to_disk(f\"{s3_bucket}\/{s3_dir}\/{dataset_name}\", fs=s3)`\r\n4. Check the corresponding S3 bucket and verify nothing has been uploaded\r\n5. Check the path at f\"{s3_bucket}\/{s3_dir}\/{dataset_name}\" and verify that files have been saved there\n\n### Expected behavior\n\nArtifacts are uploaded at the f\"{s3_bucket}\/{s3_dir}\/{dataset_name}\" S3 location.\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: macOS-13.3.1-x86_64-i386-64bit\r\n- Python version: 3.11.2\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5823\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5823\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5822","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5822\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5822\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5822\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5822","id":1696627308,"node_id":"I_kwDODunzps5lIHps","number":5822,"title":"Audio Dataset with_format torch problem","user":{"login":"paulbauriegel","id":20282916,"node_id":"MDQ6VXNlcjIwMjgyOTE2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/20282916?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/paulbauriegel","html_url":"https:\/\/github.com\/paulbauriegel","followers_url":"https:\/\/api.github.com\/users\/paulbauriegel\/followers","following_url":"https:\/\/api.github.com\/users\/paulbauriegel\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/paulbauriegel\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/paulbauriegel\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/paulbauriegel\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/paulbauriegel\/orgs","repos_url":"https:\/\/api.github.com\/users\/paulbauriegel\/repos","events_url":"https:\/\/api.github.com\/users\/paulbauriegel\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/paulbauriegel\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-04T20:07:51Z","updated_at":"2023-05-11T20:45:53Z","closed_at":"2023-05-11T20:45:53Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nCommon Voice v10 Delta (German) Dataset from here https:\/\/commonvoice.mozilla.org\/de\/datasets\r\n\r\n```\r\naudio_dataset = \\\r\n   (Dataset\r\n    .from_dict({\"audio\": ('\/tmp\/cv-corpus-10.0-delta-2022-07-04\/de\/clips\/' + df.path).to_list()})\r\n    .cast_column(\"audio\", Audio(sampling_rate=16_000))\r\n    .with_format('numpy'))\r\naudio_dataset[0][\"audio\"]\r\n```\r\n\r\nworks, but\r\n\r\n```\r\naudio_dataset = \\\r\n   (Dataset\r\n    .from_dict({\"audio\": ('\/tmp\/cv-corpus-10.0-delta-2022-07-04\/de\/clips\/' + df.path).to_list()})\r\n    .cast_column(\"audio\", Audio(sampling_rate=16_000))\r\n    .with_format('torch'))\r\naudio_dataset[0][\"audio\"]\r\n```\r\n\r\ndoes not instead I get\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[54], line 1\r\n----> 1 audio_dataset[0][\"audio\"]\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py:2154, in Dataset.__getitem__(self, key)\r\n   2152 def __getitem__(self, key):  # noqa: F811\r\n   2153     \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 2154     return self._getitem(\r\n   2155         key,\r\n   2156     )\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py:2139, in Dataset._getitem(self, key, decoded, **kwargs)\r\n   2137 formatter = get_formatter(format_type, features=self.features, decoded=decoded, **format_kwargs)\r\n   2138 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n-> 2139 formatted_output = format_table(\r\n   2140     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n   2141 )\r\n   2142 return formatted_output\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py:532, in format_table(table, key, formatter, format_columns, output_all_columns)\r\n    530 python_formatter = PythonFormatter(features=None)\r\n    531 if format_columns is None:\r\n--> 532     return formatter(pa_table, query_type=query_type)\r\n    533 elif query_type == \"column\":\r\n    534     if key in format_columns:\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/formatting\/formatting.py:281, in Formatter.__call__(self, pa_table, query_type)\r\n    279 def __call__(self, pa_table: pa.Table, query_type: str) -> Union[RowFormat, ColumnFormat, BatchFormat]:\r\n    280     if query_type == \"row\":\r\n--> 281         return self.format_row(pa_table)\r\n    282     elif query_type == \"column\":\r\n    283         return self.format_column(pa_table)\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/formatting\/torch_formatter.py:58, in TorchFormatter.format_row(self, pa_table)\r\n     56 def format_row(self, pa_table: pa.Table) -> dict:\r\n     57     row = self.numpy_arrow_extractor().extract_row(pa_table)\r\n---> 58     return self.recursive_tensorize(row)\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/formatting\/torch_formatter.py:54, in TorchFormatter.recursive_tensorize(self, data_struct)\r\n     53 def recursive_tensorize(self, data_struct: dict):\r\n---> 54     return map_nested(self._recursive_tensorize, data_struct, map_list=False)\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py:356, in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types, disable_tqdm, desc)\r\n    354     num_proc = 1\r\n    355 if num_proc <= 1 or len(iterable) <= num_proc:\r\n--> 356     mapped = [\r\n    357         _single_map_nested((function, obj, types, None, True, None))\r\n    358         for obj in logging.tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n    359     ]\r\n    360 else:\r\n    361     split_kwds = []  # We organize the splits ourselve (contiguous splits)\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py:357, in <listcomp>(.0)\r\n    354     num_proc = 1\r\n    355 if num_proc <= 1 or len(iterable) <= num_proc:\r\n    356     mapped = [\r\n--> 357         _single_map_nested((function, obj, types, None, True, None))\r\n    358         for obj in logging.tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n    359     ]\r\n    360 else:\r\n    361     split_kwds = []  # We organize the splits ourselve (contiguous splits)\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py:309, in _single_map_nested(args)\r\n    306 pbar = logging.tqdm(pbar_iterable, disable=disable_tqdm, position=rank, unit=\"obj\", desc=pbar_desc)\r\n    308 if isinstance(data_struct, dict):\r\n--> 309     return {k: _single_map_nested((function, v, types, None, True, None)) for k, v in pbar}\r\n    310 else:\r\n    311     mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py:309, in <dictcomp>(.0)\r\n    306 pbar = logging.tqdm(pbar_iterable, disable=disable_tqdm, position=rank, unit=\"obj\", desc=pbar_desc)\r\n    308 if isinstance(data_struct, dict):\r\n--> 309     return {k: _single_map_nested((function, v, types, None, True, None)) for k, v in pbar}\r\n    310 else:\r\n    311     mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py:293, in _single_map_nested(args)\r\n    291 # Singleton first to spare some computation\r\n    292 if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 293     return function(data_struct)\r\n    295 # Reduce logging to keep things readable in multiprocessing with tqdm\r\n    296 if rank is not None and logging.get_verbosity() < logging.WARNING:\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/formatting\/torch_formatter.py:51, in TorchFormatter._recursive_tensorize(self, data_struct)\r\n     49     if data_struct.dtype == np.object:  # pytorch tensors cannot be instantied from an array of objects\r\n     50         return [self.recursive_tensorize(substruct) for substruct in data_struct]\r\n---> 51 return self._tensorize(data_struct)\r\n\r\nFile \/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages\/datasets\/formatting\/torch_formatter.py:38, in TorchFormatter._tensorize(self, value)\r\n     35 import torch\r\n     37 default_dtype = {}\r\n---> 38 if np.issubdtype(value.dtype, np.integer):\r\n     39     default_dtype = {\"dtype\": torch.int64}\r\n     40 elif np.issubdtype(value.dtype, np.floating):\r\n\r\nAttributeError: 'NoneType' object has no attribute 'dtype'\r\n```\n\n### Steps to reproduce the bug\n\n1. Download some audio dataset in this case I used Common Voice v10 Delta (German) Dataset from here https:\/\/commonvoice.mozilla.org\/de\/datasets\r\n2. Try the Code from above\n\n### Expected behavior\n\nIt should work for torch\n\n### Environment info\n\npytorch: 2.0.0\r\ndatasets: 2.3.2\r\nnumpy: 1.21.6\r\n\r\nPython: 3.8\r\nLinux","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5822\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5822\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5821","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5821\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5821\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5821\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5821","id":1696400343,"node_id":"PR_kwDODunzps5PzHLU","number":5821,"title":"IterableDataset Arrow formatting","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2023-05-04T17:23:43Z","updated_at":"2023-05-31T09:43:26Z","closed_at":"2023-05-31T09:36:18Z","author_association":"MEMBER","active_lock_reason":null,"body":"Adding an optional `.iter_arrow` to examples iterable. This allows to use Arrow formatting in map\/filter.\r\n\r\nThis will also be useful for torch formatting, since we can reuse the TorchFormatter that converts Arrow data to torch tensors\r\n\r\nRelated to https:\/\/github.com\/huggingface\/datasets\/issues\/5793 and https:\/\/github.com\/huggingface\/datasets\/issues\/3444\r\n\r\nRequired for https:\/\/github.com\/huggingface\/datasets\/pull\/5852\r\n\r\n### Example: \r\n\r\nSpeed x10 in map\r\n\r\n```python\r\nfrom datasets import Dataset\r\nimport pyarrow.compute as pc\r\nimport time\r\n\r\n\r\nds = Dataset.from_dict({\"a\": range(100_000)})\r\n\r\n\r\nids = ds.to_iterable_dataset()\r\nids = ids.map(lambda x: {\"a\": [a + 10 for a in x[\"a\"]]}, batched=True)\r\n\r\n_start = time.time()\r\nprint(f\"Python ({sum(1 for _ in ids)} items):\\t{(time.time() - _start) * 1000:.1f}ms\")\r\n# Python (100000 items):  695.7ms\r\n\r\nids = ds.to_iterable_dataset().with_format(\"arrow\")\r\nids = ids.map(lambda t: t.set_column(0, \"a\", pc.add(t[0], 10)), batched=True)\r\nids = ids.with_format(None)\r\n\r\n_start = time.time()\r\nprint(f\"Arrow ({sum(1 for _ in ids)} items):\\t{(time.time() - _start) * 1000:.1f}ms)\")\r\n# Arrow (100000 items):   81.0ms)\r\n```\r\n\r\n\r\n### Implementation details\r\n\r\nI added an optional `iter_arrow` method to examples iterable. If an example iterable has this method, then it can be used to iterate on the examples by batch of arrow tables.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5821\/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":1,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5821\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5821","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5821","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5821.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5821.patch","merged_at":"2023-05-31T09:36:18Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5820","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5820\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5820\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5820\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5820","id":1695892811,"node_id":"I_kwDODunzps5lFUVL","number":5820,"title":"Incomplete docstring for `BuilderConfig`","user":{"login":"Laurent2916","id":21087104,"node_id":"MDQ6VXNlcjIxMDg3MTA0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/21087104?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Laurent2916","html_url":"https:\/\/github.com\/Laurent2916","followers_url":"https:\/\/api.github.com\/users\/Laurent2916\/followers","following_url":"https:\/\/api.github.com\/users\/Laurent2916\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Laurent2916\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Laurent2916\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Laurent2916\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Laurent2916\/orgs","repos_url":"https:\/\/api.github.com\/users\/Laurent2916\/repos","events_url":"https:\/\/api.github.com\/users\/Laurent2916\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Laurent2916\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892877,"node_id":"MDU6TGFiZWwxOTM1ODkyODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/good%20first%20issue","name":"good first issue","color":"7057ff","default":true,"description":"Good for newcomers"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-04T12:14:34Z","updated_at":"2023-05-05T12:31:56Z","closed_at":"2023-05-05T12:31:56Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Hi guys !\r\nI stumbled upon this docstring while working on a project.\r\nSome of the attributes have missing descriptions.\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/bc5fef5b6d91f009e4101684adcb374df2c170f6\/src\/datasets\/builder.py#L104-L117","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5820\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5820\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5819","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5819\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5819\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5819\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5819","id":1695536738,"node_id":"I_kwDODunzps5lD9Zi","number":5819,"title":"Cannot pickle error in Dataset.from_generator()","user":{"login":"xinghaow99","id":50691954,"node_id":"MDQ6VXNlcjUwNjkxOTU0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/50691954?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/xinghaow99","html_url":"https:\/\/github.com\/xinghaow99","followers_url":"https:\/\/api.github.com\/users\/xinghaow99\/followers","following_url":"https:\/\/api.github.com\/users\/xinghaow99\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/xinghaow99\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/xinghaow99\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/xinghaow99\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/xinghaow99\/orgs","repos_url":"https:\/\/api.github.com\/users\/xinghaow99\/repos","events_url":"https:\/\/api.github.com\/users\/xinghaow99\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/xinghaow99\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-05-04T08:39:09Z","updated_at":"2023-05-05T19:20:59Z","closed_at":"2023-05-05T19:20:58Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm trying to use Dataset.from_generator() to generate a large dataset. \n\n### Steps to reproduce the bug\n\nCode to reproduce:\r\n```\r\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, GenerationConfig\r\nimport torch\r\nfrom tqdm import tqdm\r\nfrom datasets import load_dataset\r\n\r\ntokenizer = T5Tokenizer.from_pretrained(\"google\/flan-t5-small\")\r\nmodel = T5ForConditionalGeneration.from_pretrained(\"google\/flan-t5-small\", device_map=\"auto\")\r\nmodel = torch.compile(model)\r\n\r\ndef generate_data(data_loader):\r\n    model.eval()\r\n    for batch in tqdm(data_loader):\r\n        input_ids = tokenizer(batch['instruction'], return_tensors='pt', padding=True, truncation=True).input_ids.to(\"cuda:0\")\r\n        with torch.no_grad():\r\n            outputs = model.generate(input_ids, generation_config=generation_config)\r\n        decoder_hidden_states = outputs.decoder_hidden_states\r\n        for i, h in zip(batch['instruction'], decoder_hidden_states):\r\n            yield {\"instruction\": i, \"decoder_hidden_states\": h}\r\n\r\ngeneration_config = GenerationConfig(\r\n    temperature=1,\r\n    max_new_tokens=1024,\r\n    do_sample=False,\r\n    num_return_sequences=1,\r\n    return_dict_in_generate=True,\r\n    output_scores=True,\r\n    output_hidden_states=True,\r\n)\r\nfrom datasets import Dataset, load_dataset\r\nfrom torch.utils.data import DataLoader\r\ndataset = load_dataset(\"HuggingFaceH4\/databricks_dolly_15k\")\r\ntrain_loader = DataLoader(dataset['train'], batch_size=2, shuffle=True)\r\ndataset = Dataset.from_generator(generator=generate_data, gen_kwargs={\"data_loader\": train_loader})\r\ndataset.save_to_disk(\"data\/flant5_small_generation\")\r\n    \r\n```\r\n\n\n### Expected behavior\n\nThe dataset should be generated and saved.\r\nBut the following error occurred:\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/remote-home\/xhwang\/alpaca-lora\/data_collection_t5.py\", line 46, in <module>\r\n    dataset = Dataset.from_generator(generator=generate_data, gen_kwargs={\"data_loader\": train_loader})\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 1035, in from_generator\r\n    return GeneratorDatasetInputStream(\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/io\/generator.py\", line 28, in __init__\r\n    self.builder = Generator(\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 336, in __init__\r\n    self.config, self.config_id = self._create_builder_config(\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 505, in _create_builder_config\r\n    config_id = builder_config.create_config_id(\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 179, in create_config_id\r\n    suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/fingerprint.py\", line 236, in hash\r\n    return cls.hash_default(value)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/fingerprint.py\", line 229, in hash_default\r\n    return cls.hash_bytes(dumps(value))\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 726, in dumps\r\n    dump(obj, file)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 701, in dump\r\n    Pickler(file, recurse=True).dump(obj)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 394, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 487, in dump\r\n    self.save(obj)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 1186, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 972, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 998, in _batch_setitems\r\n    save(v)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 1311, in save_function\r\n    dill._dill._save_with_postproc(\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 1084, in _save_with_postproc\r\n    pickler._batch_setitems(iter(source.items()))\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 998, in _batch_setitems\r\n    save(v)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 603, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 717, in save_reduce\r\n    save(state)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 1186, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 972, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 998, in _batch_setitems\r\n    save(v)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 603, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 717, in save_reduce\r\n    save(state)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 1186, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 972, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 998, in _batch_setitems\r\n    save(v)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 1311, in save_function\r\n    dill._dill._save_with_postproc(\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 1070, in _save_with_postproc\r\n    pickler.save_reduce(*reduction, obj=obj)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 717, in save_reduce\r\n    save(state)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 887, in save_tuple\r\n    save(element)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 1186, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 972, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 998, in _batch_setitems\r\n    save(v)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 1311, in save_function\r\n    dill._dill._save_with_postproc(\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 1070, in _save_with_postproc\r\n    pickler.save_reduce(*reduction, obj=obj)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 717, in save_reduce\r\n    save(state)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 887, in save_tuple\r\n    save(element)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 1186, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 972, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 1003, in _batch_setitems\r\n    save(v)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 1311, in save_function\r\n    dill._dill._save_with_postproc(\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 1084, in _save_with_postproc\r\n    pickler._batch_setitems(iter(source.items()))\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 998, in _batch_setitems\r\n    save(v)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 691, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/site-packages\/dill\/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"\/remote-home\/xhwang\/anaconda3\/envs\/alpaca-lora\/lib\/python3.10\/pickle.py\", line 578, in save\r\n    rv = reduce(self.proto)\r\nTypeError: cannot pickle 'ConfigModuleInstance' object\r\n```\n\n### Environment info\n\n- `datasets` version: 2.11.0\r\n- Platform: Linux-4.15.0-156-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.10\r\n- Huggingface_hub version: 0.13.2\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5819\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5819\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5818","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5818\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5818\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5818\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5818","id":1695052555,"node_id":"I_kwDODunzps5lCHML","number":5818,"title":"Ability to update a dataset","user":{"login":"davidgilbertson","id":4443482,"node_id":"MDQ6VXNlcjQ0NDM0ODI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/4443482?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/davidgilbertson","html_url":"https:\/\/github.com\/davidgilbertson","followers_url":"https:\/\/api.github.com\/users\/davidgilbertson\/followers","following_url":"https:\/\/api.github.com\/users\/davidgilbertson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/davidgilbertson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/davidgilbertson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/davidgilbertson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/davidgilbertson\/orgs","repos_url":"https:\/\/api.github.com\/users\/davidgilbertson\/repos","events_url":"https:\/\/api.github.com\/users\/davidgilbertson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/davidgilbertson\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-04T01:08:13Z","updated_at":"2023-05-04T20:43:39Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nThe ability to load a dataset, add or change something, and save it back to disk.\r\n\r\nMaybe it's possible, but I can't work out how to do it, e.g. this fails:\r\n\r\n```py\r\nimport datasets\r\n\r\ndataset = datasets.load_from_disk(\"data\/test1\")\r\ndataset = dataset.add_item({\"text\": \"A new item\"})\r\ndataset.save_to_disk(\"data\/test1\")\r\n```\r\n\r\nWith the error:\r\n```\r\nPermissionError: Tried to overwrite \/mnt\/c\/Users\/david\/py\/learning\/mini_projects\/data_sorting_and_filtering\/data\/test1 but a dataset can't overwrite itself.\r\n```\r\n\n\n### Motivation\n\nMy use case is that I want to process a dataset in a particular way but it doesn't fit in memory if I do it in one go. So I want to perform a loop and at each step in the loop, process one shard and append it to an ever-growing dataset. The code in the loop will load a dataset, add some rows, then save it again.\r\n\r\nMaybe I'm just thinking about things incorrectly and there's a better approach. FWIW I can't use `dataset.map()` to do the task because that doesn't work with `num_proc` when adding rows, so is confined to a single process which is too slow.\r\n\r\nThe only other way I can think of is to create a new file each time, but surely that's not how people do this sort of thing.\n\n### Your contribution\n\nna","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5818\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5818\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5817","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5817\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5817\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5817\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5817","id":1694891866,"node_id":"I_kwDODunzps5lBf9a","number":5817,"title":"Setting `num_proc` errors when `.map` returns additional items.","user":{"login":"davidgilbertson","id":4443482,"node_id":"MDQ6VXNlcjQ0NDM0ODI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/4443482?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/davidgilbertson","html_url":"https:\/\/github.com\/davidgilbertson","followers_url":"https:\/\/api.github.com\/users\/davidgilbertson\/followers","following_url":"https:\/\/api.github.com\/users\/davidgilbertson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/davidgilbertson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/davidgilbertson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/davidgilbertson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/davidgilbertson\/orgs","repos_url":"https:\/\/api.github.com\/users\/davidgilbertson\/repos","events_url":"https:\/\/api.github.com\/users\/davidgilbertson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/davidgilbertson\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-03T21:46:53Z","updated_at":"2023-05-04T21:14:21Z","closed_at":"2023-05-04T20:22:25Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm using a map function that returns more rows than are passed in.\r\n\r\nIf I try to use `num_proc` I get:\r\n```\r\n  File \"\/home\/davidg\/.virtualenvs\/learning\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 563, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/davidg\/.virtualenvs\/learning\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 528, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"\/home\/davidg\/.virtualenvs\/learning\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 3097, in map\r\n    for rank, done, content in iflatmap_unordered(\r\n  File \"\/home\/davidg\/.virtualenvs\/learning\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 1372, in iflatmap_unordered\r\n    yield queue.get(timeout=0.05)\r\n  File \"<string>\", line 2, in get\r\n  File \"\/home\/davidg\/.virtualenvs\/learning\/lib\/python3.10\/site-packages\/multiprocess\/managers.py\", line 818, in _callmethod\r\n    kind, result = conn.recv()\r\n  File \"\/home\/davidg\/.virtualenvs\/learning\/lib\/python3.10\/site-packages\/multiprocess\/connection.py\", line 258, in recv\r\n    buf = self._recv_bytes()\r\n  File \"\/home\/davidg\/.virtualenvs\/learning\/lib\/python3.10\/site-packages\/multiprocess\/connection.py\", line 422, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"\/home\/davidg\/.virtualenvs\/learning\/lib\/python3.10\/site-packages\/multiprocess\/connection.py\", line 391, in _recv\r\n    raise EOFError\r\nEOFError\r\n```\n\n### Steps to reproduce the bug\n\nThis is copied from the [Datasets docs](https:\/\/huggingface.co\/docs\/datasets\/v2.12.0\/en\/process#batch-processing), with `num_proc` added, and will error.\r\n\r\n```py\r\nimport datasets\r\n\r\ndataset = ... # any old dataset\r\n\r\n\r\ndef chunk_examples(examples):\r\n    chunks = []\r\n    for sentence in examples[\"text\"]:\r\n        chunks += [sentence[i : i + 50] for i in range(0, len(sentence), 50)]\r\n    return {\"chunks\": chunks}\r\n\r\n\r\nchunked_dataset = dataset.map(\r\n    chunk_examples,\r\n    batched=True,\r\n    remove_columns=dataset.column_names,\r\n    num_proc=2,  # Remove and it works\r\n)\r\n```\n\n### Expected behavior\n\nShould work fine. On a related note, multi-processing also fails if there is a Meta class anywhere in scope (and there are plenty in the standard library). This is the fault of `dill` and is a long standing issue.\r\n\r\nHave you considered using Loky for multiprocessing? I've found that the built-in `datasets` multi-processing breaks more than it works so have written my own function using `loky`, for reference:\r\n```py\r\nimport datasets\r\nimport loky\r\n\r\ndef fast_loop(dataset: datasets.Dataset, func, num_proc=None):\r\n    if num_proc is None:\r\n        import os\r\n\r\n        num_proc = len(os.sched_getaffinity(0))\r\n\r\n    shards = [\r\n        dataset.shard(num_shards=num_proc, index=i, contiguous=True)\r\n        for i in range(num_proc)\r\n    ]\r\n\r\n    executor = loky.get_reusable_executor(max_workers=num_proc)\r\n    results = executor.map(func, shards)\r\n    return datasets.combine.concatenate_datasets(list(results))\r\n```\n\n### Environment info\n\n- `datasets` version: 2.11.0\r\n- Platform: Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\r\n- Python version: 3.10.8\r\n- Huggingface_hub version: 0.12.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5817\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5817\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5816","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5816\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5816\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5816\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5816","id":1694590856,"node_id":"PR_kwDODunzps5Ps4t9","number":5816,"title":"Preserve `stopping_strategy` of shuffled interleaved dataset (random cycling case)","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-05-03T18:34:18Z","updated_at":"2023-05-04T14:31:55Z","closed_at":"2023-05-04T14:24:49Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Preserve the `stopping_strategy` in the `RandomlyCyclingMultiSourcesExamplesIterable.shard_data_sources` to fix shuffling a dataset interleaved (from multiple sources) with probabilities. \r\n\r\nFix #5812 \r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5816\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5816\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5816","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5816","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5816.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5816.patch","merged_at":"2023-05-04T14:24:49Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5814","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5814\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5814\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5814\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5814","id":1693216778,"node_id":"PR_kwDODunzps5PoOQ9","number":5814,"title":"Repro windows crash","user":{"login":"maddiedawson","id":106995444,"node_id":"U_kgDOBmCe9A","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/106995444?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/maddiedawson","html_url":"https:\/\/github.com\/maddiedawson","followers_url":"https:\/\/api.github.com\/users\/maddiedawson\/followers","following_url":"https:\/\/api.github.com\/users\/maddiedawson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/maddiedawson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/maddiedawson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/maddiedawson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/maddiedawson\/orgs","repos_url":"https:\/\/api.github.com\/users\/maddiedawson\/repos","events_url":"https:\/\/api.github.com\/users\/maddiedawson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/maddiedawson\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-05-02T23:30:18Z","updated_at":"2023-05-02T23:47:07Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5814\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5814\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5814","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5814","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5814.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5814.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5815","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5815\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5815\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5815\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5815","id":1693701743,"node_id":"I_kwDODunzps5k89Zv","number":5815,"title":"Easy way to create a Kaggle dataset from a Huggingface dataset?","user":{"login":"hrbigelow","id":5355286,"node_id":"MDQ6VXNlcjUzNTUyODY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5355286?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/hrbigelow","html_url":"https:\/\/github.com\/hrbigelow","followers_url":"https:\/\/api.github.com\/users\/hrbigelow\/followers","following_url":"https:\/\/api.github.com\/users\/hrbigelow\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/hrbigelow\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/hrbigelow\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/hrbigelow\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/hrbigelow\/orgs","repos_url":"https:\/\/api.github.com\/users\/hrbigelow\/repos","events_url":"https:\/\/api.github.com\/users\/hrbigelow\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/hrbigelow\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-05-02T21:43:33Z","updated_at":"2023-07-26T16:13:31Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I'm not sure whether this is more appropriately addressed with HuggingFace or Kaggle.  I would like to somehow directly create a Kaggle dataset from a HuggingFace Dataset.\r\n\r\nWhile Kaggle does provide the option to create a dataset from a URI, that URI must point to a single file.  For example:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/5355286\/235792394-7c559d07-4aff-45b7-ad2b-9c5280c88415.png)\r\n\r\n\r\nIs there some mechanism from huggingface to represent a dataset (such as that from `load_dataset('wmt14', 'de-en', split='train')` as a single file?  Or, some other way to get that into a Kaggle dataset so that I can use the huggingface `datasets` module to process and consume it inside of a Kaggle notebook?\r\n\r\nThanks in advance!\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5815\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5815\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5813","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5813\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5813\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5813\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5813","id":1691908535,"node_id":"PR_kwDODunzps5Pj0_E","number":5813,"title":"[DO-NOT-MERGE] Debug Windows issue at #3","user":{"login":"HyukjinKwon","id":6477701,"node_id":"MDQ6VXNlcjY0Nzc3MDE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/6477701?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/HyukjinKwon","html_url":"https:\/\/github.com\/HyukjinKwon","followers_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/followers","following_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/orgs","repos_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/repos","events_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/HyukjinKwon\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-05-02T07:19:34Z","updated_at":"2023-05-02T07:21:30Z","closed_at":"2023-05-02T07:21:30Z","author_association":"NONE","active_lock_reason":null,"body":"TBD","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5813\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5813\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5813","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5813","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5813.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5813.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5812","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5812\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5812\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5812\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5812","id":1691798169,"node_id":"I_kwDODunzps5k1sqZ","number":5812,"title":"Cannot shuffle interleaved IterableDataset with \"all_exhausted\" stopping strategy","user":{"login":"off99555","id":15215732,"node_id":"MDQ6VXNlcjE1MjE1NzMy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/15215732?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/off99555","html_url":"https:\/\/github.com\/off99555","followers_url":"https:\/\/api.github.com\/users\/off99555\/followers","following_url":"https:\/\/api.github.com\/users\/off99555\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/off99555\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/off99555\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/off99555\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/off99555\/orgs","repos_url":"https:\/\/api.github.com\/users\/off99555\/repos","events_url":"https:\/\/api.github.com\/users\/off99555\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/off99555\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":3287858981,"node_id":"MDU6TGFiZWwzMjg3ODU4OTgx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/streaming","name":"streaming","color":"fef2c0","default":false,"description":""}],"state":"closed","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-05-02T05:26:17Z","updated_at":"2023-05-04T14:24:51Z","closed_at":"2023-05-04T14:24:51Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nShuffling interleaved `IterableDataset` with \"all_exhausted\" strategy yields non-exhaustive sampling.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```py\r\nfrom datasets import IterableDataset, interleave_datasets\r\n\r\ndef gen(bias, length):\r\n  for i in range(length):\r\n    yield dict(a=bias+i)\r\n\r\nseed = 42\r\nprobabilities = [0.2, 0.6, 0.2]\r\nd1 = IterableDataset.from_generator(lambda: gen(0, 3))\r\nd2 = IterableDataset.from_generator(lambda: gen(10, 4))\r\nd3 = IterableDataset.from_generator(lambda: gen(20, 3))\r\nds = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed, stopping_strategy='all_exhausted')\r\nds = ds.shuffle(buffer_size=1000)\r\nfor x in ds:\r\n  print(x)\r\n```\r\n\r\nThis code produces \r\n```\r\n{'a': 0}\r\n{'a': 22}\r\n{'a': 20}\r\n{'a': 21}\r\n{'a': 10}\r\n{'a': 1}\r\n```\r\n\r\n### Expected behavior\r\n\r\nIt should produce a longer list of examples to exhaust all the datasets.\r\nIf you comment out the shuffle line, it will exhaust all the datasets properly.\r\nHere is the output if you comment out shuffling:\r\n```\r\n{'a': 10}\r\n{'a': 11}\r\n{'a': 20}\r\n{'a': 12}\r\n{'a': 0}\r\n{'a': 21}\r\n{'a': 13}\r\n{'a': 10}\r\n{'a': 1}\r\n{'a': 11}\r\n{'a': 12}\r\n{'a': 22}\r\n{'a': 13}\r\n{'a': 20}\r\n{'a': 10}\r\n{'a': 11}\r\n{'a': 12}\r\n{'a': 2}\r\n```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.10.147+-x86_64-with-glibc2.31\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3\r\n\r\nThis was run on Google Colab.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5812\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5812\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5811","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5811\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5811\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5811\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5811","id":1689919046,"node_id":"I_kwDODunzps5kuh5G","number":5811,"title":"load_dataset: TypeError: 'NoneType' object is not callable, on local dataset filename changes","user":{"login":"durapensa","id":50685483,"node_id":"MDQ6VXNlcjUwNjg1NDgz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/50685483?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/durapensa","html_url":"https:\/\/github.com\/durapensa","followers_url":"https:\/\/api.github.com\/users\/durapensa\/followers","following_url":"https:\/\/api.github.com\/users\/durapensa\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/durapensa\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/durapensa\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/durapensa\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/durapensa\/orgs","repos_url":"https:\/\/api.github.com\/users\/durapensa\/repos","events_url":"https:\/\/api.github.com\/users\/durapensa\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/durapensa\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-30T13:27:17Z","updated_at":"2023-05-05T17:44:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI've adapted Databrick's [train_dolly.py](\/databrickslabs\/dolly\/blob\/master\/train_dolly.py) to train using a local dataset, which has been working. Upon changing the filenames of the `.json` & `.py` files in my local dataset directory, `dataset = load_dataset(path_or_dataset)[\"train\"]` throws the error:\r\n\r\n```python\r\n2023-04-30 09:10:52 INFO [training.trainer] Loading dataset from dushowxa-characters\r\nTraceback (most recent call last):\r\n  File \"\/data\/dushowxa-dolly\/train_dushowxa.py\", line 26, in <module>\r\n    load_training_dataset()\r\n  File \"\/data\/dushowxa-dolly\/training\/trainer.py\", line 89, in load_training_dataset\r\n    dataset = load_dataset(path_or_dataset)[\"train\"]\r\n  File \"\/data\/dushowxa-dolly\/.venv\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1773, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"\/data\/dushowxa-dolly\/.venv\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1528, in load_dataset_builder\r\n    builder_instance: DatasetBuilder = builder_cls(\r\nTypeError: 'NoneType' object is not callable\r\n```\r\nThe local dataset filenames were of the form `dushowxa-characters\/expanse-dushowxa-characters.json` and are now of the form `dushowxa-characters\/dushowxa-characters.json` (the word `expanse-` was removed from the filenames). Is this perhaps a dataset caching issue?\r\n\r\nI have attempted to manually clear caches, but to no effect:\r\n\r\n```sh\r\nrm -rfv ~\/.cache\/huggingface\/datasets\/*\r\nrm -rfv ~\/.cache\/huggingface\/modules\/* \r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nRun `python3 train_dushowxa.py` (adapted from Databrick's [train_dolly.py](\/databrickslabs\/dolly\/blob\/master\/train_dolly.py)).\r\n\r\n### Expected behavior\r\n\r\nTraining succeeds as before local dataset filenames were changed.\r\n\r\n### Environment info\r\n\r\nUbuntu 22.04, Python 3.10.6, venv\r\n```python\r\naccelerate>=0.16.0,<1\r\nclick>=8.0.4,<9\r\ndatasets>=2.10.0,<3\r\ndeepspeed>=0.9.0,<1\r\ntransformers[torch]>=4.28.1,<5\r\nlangchain>=0.0.139\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5811\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5811\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5810","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5810\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5810\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5810\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5810","id":1689917822,"node_id":"PR_kwDODunzps5PdJHI","number":5810,"title":"Add `fn_kwargs` to `map` and `filter` of `IterableDataset` and `IterableDatasetDict`","user":{"login":"yuukicammy","id":3927621,"node_id":"MDQ6VXNlcjM5Mjc2MjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3927621?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yuukicammy","html_url":"https:\/\/github.com\/yuukicammy","followers_url":"https:\/\/api.github.com\/users\/yuukicammy\/followers","following_url":"https:\/\/api.github.com\/users\/yuukicammy\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yuukicammy\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yuukicammy\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yuukicammy\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yuukicammy\/orgs","repos_url":"https:\/\/api.github.com\/users\/yuukicammy\/repos","events_url":"https:\/\/api.github.com\/users\/yuukicammy\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yuukicammy\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-04-30T13:23:01Z","updated_at":"2023-05-22T08:12:39Z","closed_at":"2023-05-22T08:05:31Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"# Overview\r\nI've added an argument`fn_kwargs` for map and filter methods of `IterableDataset` and `IterableDatasetDict` classes.\r\n\r\n# Details\r\nCurrently, the map and filter methods of some classes related to `IterableDataset` do not allow specifing the arguments passed to the function. This pull request adds `fn_kwargs` to pass arguments to the mapping function. This allows users to preprocess data more flexibly.\r\n\r\nAdded `fn_kwargs` to the following classes and methods (description of the argument is also added).\r\n1. class `FilteredExamplesIterable`\r\n2. method `filter` of class `IterableDataset`\r\n3. method `map` of class `IterableDatasetDict`\r\n4. method `filter` of class `IterableDatasetDict`\r\n\r\n# Example of changes\r\nHere's an example of how to use the new functionality:\r\n```python\r\nfrom datasets import IterableDatasetDict\r\n\r\ndef preprocess_function(example, a=None, b=None):\r\n    # do something\r\n    return example\r\n\r\ndataset = IterableDatasetDict(...)\r\ndataset = dataset.map(preprocess_function, fn_kwargs={\"a\": 1, \"b\": 2})\r\n```\r\n# Related Issues\r\nThis pull request is related to the following issue:\r\nhttps:\/\/github.com\/huggingface\/datasets\/issues\/3444 .\r\n\r\n# Testing\r\nI have added unit tests to test the new functionality.\r\n\r\nIn test_iterable_dataset.py  \r\n- Added `test_filtered_examples_iterable_with_fn_kwargs` for [1](#details).\r\n- Added `test_iterable_dataset_filter` for [2](#details).\r\n- Added `test_iterable_dataset_map_with_fn_kwargs`. This is not a newly added feature, but was added because it was not tested.\r\n\r\nIn test_dataset_dict.py  \r\n- Added `_create_dummy_iterable_dataset` for [3](#details) and [4](#details).\r\n- Added `_create_dummy_iterable_dataset_dict` for [3](#details) and [4](#details).\r\n- Added `test_iterable_map` for [3](#details).\r\n- Added `test_iterable_filter` for [4](#details).\r\n\r\nNote that, there is no test for `IterableDatasetDict` at the current main branch. I thought about writing tests for `IterableDatasetDict` in a new file, but I decided to add them in the test file for `DatasetDict` (test_dataset_dict.py).\r\n\r\n# Checklist\r\n- [x] Format the code.\r\n- [x] Added tests.\r\n- [x] Passed tests locally.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5810\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5810\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5810","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5810","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5810.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5810.patch","merged_at":"2023-05-22T08:05:31Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5809","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5809\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5809\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5809\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5809","id":1689797293,"node_id":"I_kwDODunzps5kuEKt","number":5809,"title":"wiki_dpr details for Open Domain Question Answering tasks","user":{"login":"yulgok22","id":64122846,"node_id":"MDQ6VXNlcjY0MTIyODQ2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/64122846?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yulgok22","html_url":"https:\/\/github.com\/yulgok22","followers_url":"https:\/\/api.github.com\/users\/yulgok22\/followers","following_url":"https:\/\/api.github.com\/users\/yulgok22\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yulgok22\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yulgok22\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yulgok22\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yulgok22\/orgs","repos_url":"https:\/\/api.github.com\/users\/yulgok22\/repos","events_url":"https:\/\/api.github.com\/users\/yulgok22\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yulgok22\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-30T06:12:04Z","updated_at":"2023-07-21T14:11:00Z","closed_at":"2023-07-21T14:11:00Z","author_association":"NONE","active_lock_reason":null,"body":"Hey guys!\r\n\r\nThanks for creating the wiki_dpr dataset!\r\n\r\nI am currently trying to combine wiki_dpr and my own datasets. but I don't know how to make the embedding value the same way as wiki_dpr.\r\n\r\nAs an experiment, I embeds the text of id=\"7\" of wiki_dpr, but this result was very different from wiki_dpr.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5809\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5809\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5807","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5807\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5807\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5807\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5807","id":1688977237,"node_id":"PR_kwDODunzps5PaKRE","number":5807,"title":"Support parallelized downloading in load_dataset with Spark","user":{"login":"es94129","id":12763339,"node_id":"MDQ6VXNlcjEyNzYzMzM5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12763339?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/es94129","html_url":"https:\/\/github.com\/es94129","followers_url":"https:\/\/api.github.com\/users\/es94129\/followers","following_url":"https:\/\/api.github.com\/users\/es94129\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/es94129\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/es94129\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/es94129\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/es94129\/orgs","repos_url":"https:\/\/api.github.com\/users\/es94129\/repos","events_url":"https:\/\/api.github.com\/users\/es94129\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/es94129\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-28T18:34:32Z","updated_at":"2023-05-25T16:54:14Z","closed_at":"2023-05-25T16:54:14Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"As proposed in https:\/\/github.com\/huggingface\/datasets\/issues\/5798, this adds support to parallelized downloading in `load_dataset` with Spark, which can speed up the process by distributing the workload to worker nodes.\r\n\r\nParallelizing dataset processing is not supported in this PR.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5807\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5807\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5807","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5807","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5807.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5807.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5806","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5806\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5806\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5806\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5806","id":1688598095,"node_id":"I_kwDODunzps5kpfZP","number":5806,"title":"Return the name of the currently loaded file in the load_dataset function.","user":{"login":"s-JoL","id":16948304,"node_id":"MDQ6VXNlcjE2OTQ4MzA0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16948304?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/s-JoL","html_url":"https:\/\/github.com\/s-JoL","followers_url":"https:\/\/api.github.com\/users\/s-JoL\/followers","following_url":"https:\/\/api.github.com\/users\/s-JoL\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/s-JoL\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/s-JoL\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/s-JoL\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/s-JoL\/orgs","repos_url":"https:\/\/api.github.com\/users\/s-JoL\/repos","events_url":"https:\/\/api.github.com\/users\/s-JoL\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/s-JoL\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":1935892877,"node_id":"MDU6TGFiZWwxOTM1ODkyODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/good%20first%20issue","name":"good first issue","color":"7057ff","default":true,"description":"Good for newcomers"}],"state":"open","locked":false,"assignee":{"login":"tsabbir96","id":49894149,"node_id":"MDQ6VXNlcjQ5ODk0MTQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49894149?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/tsabbir96","html_url":"https:\/\/github.com\/tsabbir96","followers_url":"https:\/\/api.github.com\/users\/tsabbir96\/followers","following_url":"https:\/\/api.github.com\/users\/tsabbir96\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/tsabbir96\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/tsabbir96\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/tsabbir96\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/tsabbir96\/orgs","repos_url":"https:\/\/api.github.com\/users\/tsabbir96\/repos","events_url":"https:\/\/api.github.com\/users\/tsabbir96\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/tsabbir96\/received_events","type":"User","site_admin":false},"assignees":[{"login":"tsabbir96","id":49894149,"node_id":"MDQ6VXNlcjQ5ODk0MTQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49894149?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/tsabbir96","html_url":"https:\/\/github.com\/tsabbir96","followers_url":"https:\/\/api.github.com\/users\/tsabbir96\/followers","following_url":"https:\/\/api.github.com\/users\/tsabbir96\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/tsabbir96\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/tsabbir96\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/tsabbir96\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/tsabbir96\/orgs","repos_url":"https:\/\/api.github.com\/users\/tsabbir96\/repos","events_url":"https:\/\/api.github.com\/users\/tsabbir96\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/tsabbir96\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":11,"created_at":"2023-04-28T13:50:15Z","updated_at":"2023-09-29T17:49:53Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nAdd an optional parameter return_file_name in the load_dataset function. When it is set to True, the function will include the name of the file corresponding to the current line as a feature in the returned output.\n\n### Motivation\n\nWhen training large language models, machine problems may interrupt the training process. In such cases, it is common to load a previously saved checkpoint to resume training. I would like to be able to obtain the names of the previously trained data shards, so that I can skip these parts of the data during continued training to avoid overfitting and redundant training time.\n\n### Your contribution\n\nI currently use a dataset in jsonl format, so I am primarily interested in the json format. I suggest adding the file name to the returned table here https:\/\/github.com\/huggingface\/datasets\/blob\/main\/src\/datasets\/packaged_modules\/json\/json.py#L92.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5806\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5806\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5805","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5805\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5805\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5805\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5805","id":1688558577,"node_id":"I_kwDODunzps5kpVvx","number":5805,"title":"Improve `Create a dataset` tutorial ","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-28T13:26:22Z","updated_at":"2023-06-23T14:58:44Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Our [tutorial on how to create a dataset](https:\/\/huggingface.co\/docs\/datasets\/create_dataset) is a bit misleading. \r\n1. In **Folder-based builders** section it says that we have two folder-based builders as standard builders, but we also have similar builders (that can be created from directory with data of required format) for `csv`, `json\/jsonl`, `parquet` and `txt` files. We have info about these loaders in separate [guide for loading](https:\/\/huggingface.co\/docs\/datasets\/loading#local-and-remote-files) but it's worth briefly mentioning them in the beginning tutorial because they are more common and for consistency. Would be helpful to add the link to the full guide.\r\n2. **From local files** section lists methods for creating a dataset from in-memory data which are also described in [loading guide](https:\/\/huggingface.co\/docs\/datasets\/loading#inmemory-data).  \r\n\r\nMaybe we should actually rethink and restructure this tutorial somehow.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5805\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5805\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5804","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5804\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5804\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5804\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5804","id":1688285666,"node_id":"PR_kwDODunzps5PX0Dk","number":5804,"title":"Set dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-28T10:10:01Z","updated_at":"2023-04-28T10:18:51Z","closed_at":"2023-04-28T10:10:29Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5804\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5804\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5804","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5804","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5804.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5804.patch","merged_at":"2023-04-28T10:10:29Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5803","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5803\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5803\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5803\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5803","id":1688256290,"node_id":"PR_kwDODunzps5PXtte","number":5803,"title":"Release: 2.12.0","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-28T09:52:11Z","updated_at":"2023-04-28T10:18:56Z","closed_at":"2023-04-28T09:54:43Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5803\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5803\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5803","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5803","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5803.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5803.patch","merged_at":"2023-04-28T09:54:43Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5802","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5802\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5802\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5802\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5802","id":1686509799,"node_id":"PR_kwDODunzps5PR199","number":5802,"title":"Validate non-empty data_files","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-27T09:51:36Z","updated_at":"2023-04-27T14:59:47Z","closed_at":"2023-04-27T14:51:40Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR adds validation of `data_files`, so that they are non-empty (str, list, or dict) or `None` (default).\r\n\r\nSee: https:\/\/github.com\/huggingface\/datasets\/pull\/5787#discussion_r1178862327","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5802\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5802\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5802","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5802","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5802.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5802.patch","merged_at":"2023-04-27T14:51:40Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5800","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5800\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5800\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5800\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5800","id":1686348096,"node_id":"PR_kwDODunzps5PRTRh","number":5800,"title":"Change downloaded file permission based on umask","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-27T08:13:30Z","updated_at":"2023-04-27T09:33:05Z","closed_at":"2023-04-27T09:30:16Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR changes the permission of downloaded files to cache, so that the umask is taken into account.\r\n\r\nRelated to:\r\n- #2157\r\n\r\nFix #5799.\r\n\r\nCC: @stas00 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5800\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5800\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5800","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5800","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5800.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5800.patch","merged_at":"2023-04-27T09:30:16Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5799","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5799\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5799\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5799\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5799","id":1686334572,"node_id":"I_kwDODunzps5kg2xs","number":5799,"title":"Files downloaded to cache do not respect umask","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-04-27T08:06:05Z","updated_at":"2023-04-27T09:30:17Z","closed_at":"2023-04-27T09:30:17Z","author_association":"MEMBER","active_lock_reason":null,"body":"As reported by @stas00, files downloaded to the cache do not respect umask:\r\n```bash\r\n$ ls -l \/path\/to\/cache\/datasets\/downloads\/\r\n-rw------- 1 uername    username    150M Apr 25 16:41 5e646c1d600f065adaeb134e536f6f2f296a6d804bd1f0e1fdcd20ee28c185c6 \r\n```\r\n\r\nRelated to:\r\n- #2065","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5799\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5799\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5798","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5798\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5798\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5798\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5798","id":1685904526,"node_id":"I_kwDODunzps5kfNyO","number":5798,"title":"Support parallelized downloading and processing in load_dataset with Spark","user":{"login":"es94129","id":12763339,"node_id":"MDQ6VXNlcjEyNzYzMzM5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/12763339?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/es94129","html_url":"https:\/\/github.com\/es94129","followers_url":"https:\/\/api.github.com\/users\/es94129\/followers","following_url":"https:\/\/api.github.com\/users\/es94129\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/es94129\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/es94129\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/es94129\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/es94129\/orgs","repos_url":"https:\/\/api.github.com\/users\/es94129\/repos","events_url":"https:\/\/api.github.com\/users\/es94129\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/es94129\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":17,"created_at":"2023-04-27T00:16:11Z","updated_at":"2023-05-25T14:11:41Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nWhen calling `load_dataset` for datasets that have multiple files, support using Spark to distribute the downloading and processing job to worker nodes when `cache_dir` is a cloud file system shared among nodes.\r\n\r\n```python\r\nload_dataset(..., use_spark=True)\r\n```\n\n### Motivation\n\nFurther speed up `dl_manager.download` and `_prepare_split` by distributing the workloads to worker nodes. \n\n### Your contribution\n\nI can submit a PR to support this.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5798\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5798\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5797","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5797\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5797\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5797\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5797","id":1685501199,"node_id":"I_kwDODunzps5kdrUP","number":5797,"title":"load_dataset is case sentitive?","user":{"login":"haonan-li","id":34729065,"node_id":"MDQ6VXNlcjM0NzI5MDY1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/34729065?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/haonan-li","html_url":"https:\/\/github.com\/haonan-li","followers_url":"https:\/\/api.github.com\/users\/haonan-li\/followers","following_url":"https:\/\/api.github.com\/users\/haonan-li\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/haonan-li\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/haonan-li\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/haonan-li\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/haonan-li\/orgs","repos_url":"https:\/\/api.github.com\/users\/haonan-li\/repos","events_url":"https:\/\/api.github.com\/users\/haonan-li\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/haonan-li\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-26T18:19:04Z","updated_at":"2023-04-27T11:56:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nload_dataset() function is case sensitive?\n\n### Steps to reproduce the bug\n\nThe following two code, get totally different behavior.\r\n\r\n1. load_dataset('mbzuai\/bactrian-x','en')\r\n\r\n2. load_dataset('MBZUAI\/Bactrian-X','en')\n\n### Expected behavior\n\nCompare 1 and 2.\r\n1 will download all 52 subsets, shell output:\r\n```Downloading and preparing dataset json\/MBZUAI--bactrian-X to xxx```\r\n2 will only download single subset, shell output\r\n```Downloading and preparing dataset bactrian-x\/en to xxx```\r\n\n\n### Environment info\n\nPython 3.10.11\r\ndatasets Version: 2.11.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5797\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5797\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5796","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5796\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5796\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5796\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5796","id":1685451919,"node_id":"PR_kwDODunzps5PORm-","number":5796,"title":"Spark docs","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-26T17:39:43Z","updated_at":"2023-04-27T16:41:50Z","closed_at":"2023-04-27T16:34:45Z","author_association":"MEMBER","active_lock_reason":null,"body":"Added a \"Use with Spark\" doc page to document `Dataset.from_spark` following https:\/\/github.com\/huggingface\/datasets\/pull\/5701\r\n\r\ncc @maddiedawson ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5796\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5796\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5796","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5796","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5796.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5796.patch","merged_at":"2023-04-27T16:34:45Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5795","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5795\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5795\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5795\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5795","id":1685414505,"node_id":"PR_kwDODunzps5POJo8","number":5795,"title":"Fix spark imports","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-26T17:09:32Z","updated_at":"2023-04-26T17:49:03Z","closed_at":"2023-04-26T17:39:12Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5795\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5795\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5795","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5795","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5795.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5795.patch","merged_at":"2023-04-26T17:39:12Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5794","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5794\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5794\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5794\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5794","id":1685196061,"node_id":"I_kwDODunzps5kcg0d","number":5794,"title":"CI ZeroDivisionError","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-04-26T14:55:23Z","updated_at":"2023-04-26T14:55:23Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Sometimes when running our CI on Windows, we get a ZeroDivisionError:\r\n```\r\nFAILED tests\/test_metric_common.py::LocalMetricTest::test_load_metric_frugalscore - ZeroDivisionError: float division by zero\r\n```\r\n\r\nSee for example:\r\n- https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/4809358266\/jobs\/8560513110\r\n- https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/4798359836\/jobs\/8536573688\r\n\r\n```\r\n _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nsplit = 'test', start_time = 1682516718.8236516, num_samples = 2, num_steps = 1\r\n\r\n    def speed_metrics(split, start_time, num_samples=None, num_steps=None):\r\n        \"\"\"\r\n        Measure and return speed performance metrics.\r\n    \r\n        This function requires a time snapshot `start_time` before the operation to be measured starts and this function\r\n        should be run immediately after the operation to be measured has completed.\r\n    \r\n        Args:\r\n        - split: name to prefix metric (like train, eval, test...)\r\n        - start_time: operation start time\r\n        - num_samples: number of samples processed\r\n        \"\"\"\r\n        runtime = time.time() - start_time\r\n        result = {f\"{split}_runtime\": round(runtime, 4)}\r\n        if num_samples is not None:\r\n>           samples_per_second = num_samples \/ runtime\r\nE           ZeroDivisionError: float division by zero\r\n\r\nC:\\hostedtoolcache\\windows\\Python\\3.7.9\\x64\\lib\\site-packages\\transformers\\trainer_utils.py:354: ZeroDivisionError\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5794\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5794\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5793","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5793\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5793\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5793\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5793","id":1684777320,"node_id":"I_kwDODunzps5ka6lo","number":5793,"title":"IterableDataset.with_format(\"torch\") not working","user":{"login":"jiangwangyi","id":39762734,"node_id":"MDQ6VXNlcjM5NzYyNzM0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/39762734?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jiangwangyi","html_url":"https:\/\/github.com\/jiangwangyi","followers_url":"https:\/\/api.github.com\/users\/jiangwangyi\/followers","following_url":"https:\/\/api.github.com\/users\/jiangwangyi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jiangwangyi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jiangwangyi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jiangwangyi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jiangwangyi\/orgs","repos_url":"https:\/\/api.github.com\/users\/jiangwangyi\/repos","events_url":"https:\/\/api.github.com\/users\/jiangwangyi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jiangwangyi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":3287858981,"node_id":"MDU6TGFiZWwzMjg3ODU4OTgx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/streaming","name":"streaming","color":"fef2c0","default":false,"description":""}],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-04-26T10:50:23Z","updated_at":"2023-06-13T15:57:06Z","closed_at":"2023-06-13T15:57:06Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nAfter calling the with_format(\"torch\") method on an IterableDataset instance, the data format is unchanged.\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import IterableDataset\r\ndef gen():\r\n    for i in range(4):\r\n        yield {\"a\": [i] * 4}\r\ndataset = IterableDataset.from_generator(gen).with_format(\"torch\")\r\nnext(iter(dataset))\r\n```\n\n### Expected behavior\n\n`{\"a\": torch.tensor([0, 0, 0, 0])}` is expected, but `{\"a\": [0, 0, 0, 0]}` is observed.\n\n### Environment info\n\n```bash\r\nplatform==ubuntu 22.04.01\r\npython==3.10.9\r\ndatasets==2.11.0\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5793\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5793\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5791","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5791\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5791\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5791\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5791","id":1683473943,"node_id":"I_kwDODunzps5kV8YX","number":5791,"title":"TIFF\/TIF support ","user":{"login":"sebasmos","id":31293221,"node_id":"MDQ6VXNlcjMxMjkzMjIx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/31293221?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sebasmos","html_url":"https:\/\/github.com\/sebasmos","followers_url":"https:\/\/api.github.com\/users\/sebasmos\/followers","following_url":"https:\/\/api.github.com\/users\/sebasmos\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sebasmos\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sebasmos\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sebasmos\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sebasmos\/orgs","repos_url":"https:\/\/api.github.com\/users\/sebasmos\/repos","events_url":"https:\/\/api.github.com\/users\/sebasmos\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sebasmos\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-25T16:14:18Z","updated_at":"2023-05-05T16:22:50Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nI currently have a dataset (with tiff and json files) where I have to do this:\r\n\r\n`wget path_to_data\/images.zip && unzip images.zip`\r\n\r\n`wget path_to_data\/annotations.zip && unzip annotations.zip`\r\n \r\n Would it make sense a contribution that supports these type of files? \n\n### Motivation\n\ninstead of using `load_dataset` have to use wget as these files are not supported for annotations with JSON and images with TIFF files. \r\n\r\n\r\nAdditionally to this, the PIL formatting from datasets does not read correctly the image channels with TIFF format, besides multichannel adaptation might be necessary as well (as my data e.g has more than 3 channels)\n\n### Your contribution\n\n1. Support TIFF images over multi channel format\r\n2. Support JSON annotations","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5791\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5791\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5790","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5790\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5790\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5790\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5790","id":1683229126,"node_id":"PR_kwDODunzps5PG0mJ","number":5790,"title":"Allow to run CI on push to ci-branch","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-25T13:57:26Z","updated_at":"2023-04-26T13:43:08Z","closed_at":"2023-04-26T13:35:47Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR allows to run the CI on push to a branch named \"ci-*\", without needing to open a PR.\r\n- This will allow to make CI tests without opening a PR, e.g., for future `huggingface-hub` releases, future dependency releases (like `fsspec`, `pandas`,...)\r\n\r\nNote that to build the documentation, we already allow it on push to a branch named \"doc-builder*\".\r\n\r\nSee:\r\n- #5788\r\n\r\nCC: @Wauplin  ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5790\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5790\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5790","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5790","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5790.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5790.patch","merged_at":"2023-04-26T13:35:47Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5789","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5789\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5789\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5789\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5789","id":1682611179,"node_id":"I_kwDODunzps5kSpvr","number":5789,"title":"Support streaming datasets that use jsonlines","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-04-25T07:40:02Z","updated_at":"2023-04-25T07:40:03Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Extend support for streaming datasets that use `jsonlines.open`.\r\n\r\nCurrently, if `jsonlines` is installed, `datasets` raises a `FileNotFoundError`:\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: 'https:\/\/...'\r\n```\r\n\r\nSee:\r\n- https:\/\/huggingface.co\/datasets\/masakhane\/afriqa\/discussions\/1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5789\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5789\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5788","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5788\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5788\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5788\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5788","id":1681136256,"node_id":"PR_kwDODunzps5O_v4B","number":5788,"title":"Prepare tests for hfh 0.14","user":{"login":"Wauplin","id":11801849,"node_id":"MDQ6VXNlcjExODAxODQ5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/11801849?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Wauplin","html_url":"https:\/\/github.com\/Wauplin","followers_url":"https:\/\/api.github.com\/users\/Wauplin\/followers","following_url":"https:\/\/api.github.com\/users\/Wauplin\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Wauplin\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Wauplin\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Wauplin\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Wauplin\/orgs","repos_url":"https:\/\/api.github.com\/users\/Wauplin\/repos","events_url":"https:\/\/api.github.com\/users\/Wauplin\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Wauplin\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-04-24T12:13:03Z","updated_at":"2023-04-25T14:32:56Z","closed_at":"2023-04-25T14:25:30Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Related to the coming release of `huggingface_hub==0.14.0`. It will break some internal tests. The PR fixes these tests. Let's double-check the CI but I expect the fixed tests to be running fine with both `hfh<=0.13.4` and `hfh==0.14`. Worth case scenario, existing PRs will have to be rebased once this fix is merged.\r\n\r\nSee related [discussion](https:\/\/huggingface.slack.com\/archives\/C02V5EA0A95\/p1682337463368609?thread_ts=1681994202.635609&cid=C02V5EA0A95) (private slack).\r\n\r\ncc @lhoestq ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5788\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5788\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5788","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5788","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5788.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5788.patch","merged_at":"2023-04-25T14:25:30Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5787","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5787\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5787\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5787\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5787","id":1680965959,"node_id":"PR_kwDODunzps5O_KNU","number":5787,"title":"Fix inferring module for unsupported data files","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-24T10:44:50Z","updated_at":"2023-04-27T13:06:01Z","closed_at":"2023-04-27T12:57:28Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR raises a FileNotFoundError instead:\r\n```\r\nFileNotFoundError: No (supported) data files or dataset script found in <dataset_name>\r\n```\r\n\r\nFix #5785.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5787\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5787\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5787","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5787","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5787.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5787.patch","merged_at":"2023-04-27T12:57:28Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5786","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5786\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5786\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5786\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5786","id":1680957070,"node_id":"I_kwDODunzps5kMV6O","number":5786,"title":"Multiprocessing in a `filter` or `map` function with a Pytorch model","user":{"login":"HugoLaurencon","id":44556846,"node_id":"MDQ6VXNlcjQ0NTU2ODQ2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/44556846?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/HugoLaurencon","html_url":"https:\/\/github.com\/HugoLaurencon","followers_url":"https:\/\/api.github.com\/users\/HugoLaurencon\/followers","following_url":"https:\/\/api.github.com\/users\/HugoLaurencon\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/HugoLaurencon\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/HugoLaurencon\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/HugoLaurencon\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/HugoLaurencon\/orgs","repos_url":"https:\/\/api.github.com\/users\/HugoLaurencon\/repos","events_url":"https:\/\/api.github.com\/users\/HugoLaurencon\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/HugoLaurencon\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-04-24T10:38:07Z","updated_at":"2023-05-30T09:56:30Z","closed_at":"2023-04-24T10:43:58Z","author_association":"MEMBER","active_lock_reason":null,"body":"### Describe the bug\n\nI am trying to use a Pytorch model loaded on CPUs with multiple processes with a `.map` or a `.filter` method.\r\n\r\nUsually, when dealing with models that are non-pickable, creating a class such that the `map` function is the method `__call__`, and adding `reduce` helps to solve the problem.\r\n\r\nHowever, here, the command hangs without throwing an error.\r\n\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import Dataset\r\nimport torch\r\nfrom torch import nn\r\nfrom torchvision import models\r\n\u200b\r\n\u200b\r\nclass FilterFunction:\r\n    #__slots__ = (\"path_model\", \"model\")  # Doesn't change anything uncommented\r\n    def __init__(self, path_model):\r\n        self.path_model = path_model\r\n        model = models.resnet50()\r\n        model.fc = nn.Sequential(\r\n            nn.Linear(2048, 512),\r\n            nn.ReLU(),\r\n            nn.Dropout(0.2),\r\n            nn.Linear(512, 10),\r\n            nn.LogSoftmax(dim=1)\r\n        )\r\n        model.load_state_dict(torch.load(path_model, map_location=torch.device(\"cpu\")))\r\n        model.eval()\r\n        self.model = model\r\n    def __call__(self, batch):\r\n        return [True] * len(batch[\"id\"])\r\n    # Comment this to have an error\r\n    def __reduce__(self):\r\n        return (self.__class__, (self.path_model,))\r\n\u200b\r\n\u200b\r\ndataset = Dataset.from_dict({\"id\": [0, 1, 2, 4]})\r\n\u200b\r\n# Download (100 MB) at https:\/\/github.com\/emiliantolo\/pytorch_nsfw_model\/raw\/master\/ResNet50_nsfw_model.pth\r\npath_model = \"\/fsx\/hugo\/nsfw_image\/ResNet50_nsfw_model.pth\"\r\n\u200b\r\nfilter_function = FilterFunction(path_model=path_model)\r\n\u200b\r\n# Works\r\nfiltered_dataset = dataset.filter(filter_function, num_proc=1, batched=True, batch_size=2)\r\n# Doesn't work\r\nfiltered_dataset = dataset.filter(filter_function, num_proc=2, batched=True, batch_size=2)\r\n```\n\n### Expected behavior\n\nThe command `filtered_dataset = dataset.filter(filter_function, num_proc=2, batched=True, batch_size=2)` should work and not hang.\n\n### Environment info\n\nDatasets: 2.11.0\r\nPyarrow: 11.0.0\r\nUbuntu","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5786\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5786\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5785","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5785\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5785\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5785\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5785","id":1680956964,"node_id":"I_kwDODunzps5kMV4k","number":5785,"title":"Unsupported data files raise TypeError: 'NoneType' object is not iterable","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-04-24T10:38:03Z","updated_at":"2023-04-27T12:57:30Z","closed_at":"2023-04-27T12:57:30Z","author_association":"MEMBER","active_lock_reason":null,"body":"Currently, we raise a TypeError for unsupported data files:\r\n```\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\nSee:\r\n- https:\/\/github.com\/huggingface\/datasets-server\/issues\/1073\r\n\r\nWe should give a more informative error message.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5785\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5785\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5784","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5784\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5784\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5784\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5784","id":1680950726,"node_id":"PR_kwDODunzps5O_G9S","number":5784,"title":"Raise subprocesses traceback when interrupting","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-24T10:34:03Z","updated_at":"2023-04-26T16:04:42Z","closed_at":"2023-04-26T15:54:44Z","author_association":"MEMBER","active_lock_reason":null,"body":"When a subprocess hangs in `filter` or `map`, one should be able to get the subprocess' traceback when interrupting the main process. Right now it shows nothing.\r\n\r\nTo do so I `.get()` the subprocesses async results even the main process is stopped with e.g. `KeyboardInterrupt`. I added a timeout in case the subprocess is hanging or crashed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5784\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5784\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5784","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5784","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5784.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5784.patch","merged_at":"2023-04-26T15:54:44Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5783","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5783\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5783\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5783\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5783","id":1679664393,"node_id":"I_kwDODunzps5kHaUJ","number":5783,"title":"Offset overflow while doing regex on a text column","user":{"login":"nishanthcgit","id":5066268,"node_id":"MDQ6VXNlcjUwNjYyNjg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5066268?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/nishanthcgit","html_url":"https:\/\/github.com\/nishanthcgit","followers_url":"https:\/\/api.github.com\/users\/nishanthcgit\/followers","following_url":"https:\/\/api.github.com\/users\/nishanthcgit\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/nishanthcgit\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/nishanthcgit\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/nishanthcgit\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/nishanthcgit\/orgs","repos_url":"https:\/\/api.github.com\/users\/nishanthcgit\/repos","events_url":"https:\/\/api.github.com\/users\/nishanthcgit\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/nishanthcgit\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-04-22T19:12:03Z","updated_at":"2023-09-22T06:44:07Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\n`ArrowInvalid: offset overflow while concatenating arrays`\r\n\r\nSame error as [here](https:\/\/github.com\/huggingface\/datasets\/issues\/615)\r\n\r\n### Steps to reproduce the bug\r\n\r\n\r\nSteps to reproduce: (dataset is a few GB big so try in colab maybe)\r\n\r\n```\r\nimport datasets\r\nimport re\r\n\r\nds = datasets.load_dataset('nishanthc\/dnd_map_dataset_v0.1', split = 'train')\r\n\r\ndef get_text_caption(example):\r\n    regex_pattern = r'\\s\\d+x\\d+|,\\sLQ|,\\sgrid|\\.\\w+$'\r\n    example['text_caption'] = re.sub(regex_pattern, '', example['picture_text'])\r\n    return example\r\n\r\nds = ds.map(get_text_caption)\r\n```\r\n\r\nI am trying to apply a regex to remove certain patterns from a text column. Not sure why this error is showing up.\r\n\r\n### Expected behavior\r\n\r\nDataset should have a new column with processed text\r\n\r\n### Environment info\r\n\r\nDatasets version - 2.11.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5783\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5783\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5782","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5782\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5782\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5782\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5782","id":1679622367,"node_id":"I_kwDODunzps5kHQDf","number":5782,"title":"Support for various audio-loading backends instead of always relying on SoundFile","user":{"login":"BoringDonut","id":129098876,"node_id":"U_kgDOB7HkfA","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/129098876?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/BoringDonut","html_url":"https:\/\/github.com\/BoringDonut","followers_url":"https:\/\/api.github.com\/users\/BoringDonut\/followers","following_url":"https:\/\/api.github.com\/users\/BoringDonut\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/BoringDonut\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/BoringDonut\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/BoringDonut\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/BoringDonut\/orgs","repos_url":"https:\/\/api.github.com\/users\/BoringDonut\/repos","events_url":"https:\/\/api.github.com\/users\/BoringDonut\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/BoringDonut\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-22T17:09:25Z","updated_at":"2023-05-10T20:23:04Z","closed_at":"2023-05-10T20:23:04Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nIntroduce an option to select from a variety of audio-loading backends rather than solely relying on the SoundFile library. For instance, if the ffmpeg library is installed, it can serve as a fallback loading option.\n\n### Motivation\n\n- The SoundFile library, used in [features\/audio.py](https:\/\/github.com\/huggingface\/datasets\/blob\/649d5a3315f9e7666713b6affe318ee00c7163a0\/src\/datasets\/features\/audio.py#L185), supports only a [limited number of audio formats](https:\/\/pysoundfile.readthedocs.io\/en\/latest\/index.html?highlight=supported#soundfile.available_formats). \r\n- However, current methods for creating audio datasets permit the inclusion of audio files in formats not supported by SoundFile. \r\n- As a result, developers may potentially create a dataset they cannot read back.\r\n\r\nIn my most recent project, I dealt with phone call recordings in `.amr` or `.gsm` formats and was genuinely surprised when I couldn't read the dataset I had just packaged a minute prior. Nonetheless, I can still accurately read these files using the librosa library, which employs the audioread library that internally leverages ffmpeg to read such files.\r\n\r\nExample:\r\n```python\r\naudio_dataset_amr = Dataset.from_dict({\"audio\": [\"audio_samples\/audio.amr\"]}).cast_column(\"audio\", Audio())\r\naudio_dataset_amr.save_to_disk(\"audio_dataset_amr\")\r\naudio_dataset_amr = Dataset.load_from_disk(\"audio_dataset_amr\")\r\nprint(audio_dataset_amr[0])\r\n```\r\nResults in:\r\n```\r\nTraceback (most recent call last):\r\n...\r\n    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\r\nsoundfile.LibsndfileError: Error opening <_io.BytesIO object at 0x7f316323e4d0>: Format not recognised.\r\n```\r\n\r\nWhile I acknowledge that support for these rare file types may not be a priority, I believe it's quite unfortunate that it's possible to create an unreadable dataset in this manner.\n\n### Your contribution\n\nI've created a [simple demo repository](https:\/\/github.com\/BoringDonut\/hf-datasets-ffmpeg-audio) that highlights the mentioned issue. It demonstrates how to create an .amr dataset that results in an error when attempting to read it just a few lines later.\r\n\r\nAdditionally, I've made a [fork with a rudimentary solution](https:\/\/github.com\/BoringDonut\/datasets\/blob\/fea73a8fbbc8876467c7e6422c9360546c6372d8\/src\/datasets\/features\/audio.py#L189) that utilizes ffmpeg to load files not supported by SoundFile.\r\n\r\n\r\nHere you may see github actions fails to read `.amr` dataset using the version of the current dataset, but will work with the patched version:\r\n- https:\/\/github.com\/BoringDonut\/hf-datasets-ffmpeg-audio\/actions\/runs\/4773780420\/jobs\/8487063785\r\n- https:\/\/github.com\/BoringDonut\/hf-datasets-ffmpeg-audio\/actions\/runs\/4773780420\/jobs\/8487063829\r\n\r\nAs evident from the GitHub action above, this solution resolves the previously mentioned problem.\r\n\r\nI'd be happy to create a proper pull request, provide runtime benchmarks and tests if you could offer some guidance on the following:\r\n- Where should I incorporate the ffmpeg (or other backends) code? For example, should I create a new file or simply add a function within the Audio class?\r\n- Is it feasible to pass the audio-loading function as an argument within the current architecture? This would be useful if I know in advance that I'll be reading files not supported by SoundFile.\r\n\r\nA few more notes:\r\n- In theory, it's possible to load audio using librosa\/audioread since librosa is already expected to be installed. However, librosa [will soon discontinue audioread support](https:\/\/github.com\/librosa\/librosa\/blob\/aacb4c134002903ae56bbd4b4a330519a5abacc0\/librosa\/core\/audio.py#L227). Moreover, using audioread on its own seems inconvenient because it requires a file [path as input](https:\/\/github.com\/beetbox\/audioread\/blob\/ff9535df934c48038af7be9617fdebb12078cc07\/audioread\/__init__.py#L108) and cannot work with bytes already loaded into memory or an open file descriptor (as mentioned in [librosa docs](https:\/\/librosa.org\/doc\/main\/generated\/librosa.load.html#librosa.load), only SoundFile backend supports an open file descriptor as an input).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5782\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5782\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5781","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5781\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5781\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5781\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5781","id":1679580460,"node_id":"I_kwDODunzps5kHF0s","number":5781,"title":"Error using `load_datasets`","user":{"login":"gjyoungjr","id":61463108,"node_id":"MDQ6VXNlcjYxNDYzMTA4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/61463108?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/gjyoungjr","html_url":"https:\/\/github.com\/gjyoungjr","followers_url":"https:\/\/api.github.com\/users\/gjyoungjr\/followers","following_url":"https:\/\/api.github.com\/users\/gjyoungjr\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/gjyoungjr\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/gjyoungjr\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/gjyoungjr\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/gjyoungjr\/orgs","repos_url":"https:\/\/api.github.com\/users\/gjyoungjr\/repos","events_url":"https:\/\/api.github.com\/users\/gjyoungjr\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/gjyoungjr\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-22T15:10:44Z","updated_at":"2023-05-02T23:41:25Z","closed_at":"2023-05-02T23:41:25Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI tried to load a dataset using the `datasets` library in a conda jupyter notebook and got the below error. \r\n\r\n```\r\nImportError: dlopen(\/Users\/gilbertyoung\/miniforge3\/envs\/review_sense\/lib\/python3.8\/site-packages\/scipy\/sparse\/linalg\/_isolve\/_iterative.cpython-38-darwin.so, 0x0002): Library not loaded: @rpath\/liblapack.3.dylib\r\n  Referenced from: <65B094A2-59D7-31AC-A966-4DB9E11D2A15> \/Users\/gilbertyoung\/miniforge3\/envs\/review_sense\/lib\/python3.8\/site-packages\/scipy\/sparse\/linalg\/_isolve\/_iterative.cpython-38-darwin.so\r\n  Reason: tried: '\/Users\/gilbertyoung\/miniforge3\/envs\/review_sense\/lib\/python3.8\/site-packages\/scipy\/sparse\/linalg\/_isolve\/liblapack.3.dylib' (no such file), '\/Users\/gilbertyoung\/miniforge3\/envs\/review_sense\/lib\/python3.8\/site-packages\/scipy\/sparse\/linalg\/_isolve\/..\/..\/..\/..\/..\/..\/liblapack.3.dylib' (no such file), '\/Users\/gilbertyoung\/miniforge3\/envs\/review_sense\/lib\/python3.8\/site-packages\/scipy\/sparse\/linalg\/_isolve\/liblapack.3.dylib' (no such file), '\/Users\/gilbertyoung\/miniforge3\/envs\/review_sense\/lib\/python3.8\/site-packages\/scipy\/sparse\/linalg\/_isolve\/..\/..\/..\/..\/..\/..\/liblapack.3.dylib' (no such file), '\/Users\/gilbertyoung\/miniforge3\/envs\/review_sense\/bin\/..\/lib\/liblapack.3.dylib' (no such file), '\/Users\/gilbertyoung\/miniforge3\/envs\/review_sense\/bin\/..\/lib\/liblapack.3.dylib' (no such file), '\/usr\/local\/lib\/liblapack.3.dylib' (no such file), '\/usr\/lib\/liblapack.3.dylib' (no such file, not in dyld cache)\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nRun the `load_datasets` function\r\n\r\n### Expected behavior\r\n\r\nI expected the dataset to be loaded into my notebook.\r\n\r\n### Environment info\r\n\r\nname: review_sense\r\nchannels:\r\n  - apple\r\n  - conda-forge\r\ndependencies:\r\n  - python=3.8\r\n  - pip>=19.0\r\n  - jupyter\r\n  - tensorflow-deps\r\n  #- scikit-learn\r\n  #- scipy\r\n  - pandas\r\n  - pandas-datareader\r\n  - matplotlib\r\n  - pillow\r\n  - tqdm\r\n  - requests\r\n  - h5py\r\n  - pyyaml\r\n  - flask\r\n  - boto3\r\n  - ipykernel\r\n  - seaborn\r\n  - pip:\r\n      - tensorflow-macos==2.9\r\n      - tensorflow-metal==0.5.0\r\n      - bayesian-optimization\r\n      - gym\r\n      - kaggle\r\n      - huggingface_hub\r\n      - datasets\r\n      - numpy\r\n      - huggingface\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5781\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5781\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5780","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5780\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5780\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5780\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5780","id":1679367149,"node_id":"I_kwDODunzps5kGRvt","number":5780,"title":"TypeError: 'NoneType' object does not support item assignment","user":{"login":"v-yunbin","id":38179632,"node_id":"MDQ6VXNlcjM4MTc5NjMy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/38179632?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/v-yunbin","html_url":"https:\/\/github.com\/v-yunbin","followers_url":"https:\/\/api.github.com\/users\/v-yunbin\/followers","following_url":"https:\/\/api.github.com\/users\/v-yunbin\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/v-yunbin\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/v-yunbin\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/v-yunbin\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/v-yunbin\/orgs","repos_url":"https:\/\/api.github.com\/users\/v-yunbin\/repos","events_url":"https:\/\/api.github.com\/users\/v-yunbin\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/v-yunbin\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-04-22T06:22:43Z","updated_at":"2023-04-23T08:49:18Z","closed_at":"2023-04-23T08:49:18Z","author_association":"NONE","active_lock_reason":null,"body":"command\uff1a\r\n```\r\ndef load_datasets(formats, data_dir=datadir, data_files=datafile\uff09\uff1a\r\n      dataset = load_dataset(formats, data_dir=datadir, data_files=datafile, split=split, streaming=True, **kwargs)\r\nreturn dataset\r\nraw_datasets = DatasetDict()\r\nraw_datasets[\"train\"] = load_datasets(\u201ccsv\u201d,  args.datadir, \"train.csv\", split=train_split)\r\nraw_datasets[\"test\"] = load_datasets(\u201ccsv\u201d, args.datadir, \"dev.csv\", split=test_split)\r\nraw_datasets = raw_datasets.cast_column(\"audio\", Audio(sampling_rate=16000))\r\n```\r\n\r\nerror\uff1a\r\n```\r\n    main()\r\n  File \"peft_adalora_whisper_large_training.py\", line 502, in main\r\n    raw_datasets = raw_datasets.cast_column(\"audio\", Audio(sampling_rate=16000))\r\n  File \"\/home\/ybZhang\/miniconda3\/envs\/whister\/lib\/python3.8\/site-packages\/datasets\/dataset_dict.py\", line 2015, in cast_column\r\n    info.features[column] = feature\r\nTypeError: 'NoneType' object does not support item assignment\r\n```\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5780\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5780\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5779","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5779\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5779\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5779\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5779","id":1678669865,"node_id":"PR_kwDODunzps5O3sHp","number":5779,"title":"Call fs.makedirs in save_to_disk","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-21T15:04:28Z","updated_at":"2023-04-26T12:20:01Z","closed_at":"2023-04-26T12:11:15Z","author_association":"MEMBER","active_lock_reason":null,"body":"We need to call `fs.makedirs` when saving a dataset using `save_to_disk`, because some fs implementations have actual directories (S3 and others don't)\r\n\r\nClose https:\/\/github.com\/huggingface\/datasets\/issues\/5775","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5779\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5779\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5779","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5779","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5779.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5779.patch","merged_at":"2023-04-26T12:11:15Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5778","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5778\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5778\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5778\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5778","id":1678125951,"node_id":"I_kwDODunzps5kBit_","number":5778,"title":"Schr\u00f6dinger's dataset_dict","user":{"login":"liujuncn","id":902005,"node_id":"MDQ6VXNlcjkwMjAwNQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/902005?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/liujuncn","html_url":"https:\/\/github.com\/liujuncn","followers_url":"https:\/\/api.github.com\/users\/liujuncn\/followers","following_url":"https:\/\/api.github.com\/users\/liujuncn\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/liujuncn\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/liujuncn\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/liujuncn\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/liujuncn\/orgs","repos_url":"https:\/\/api.github.com\/users\/liujuncn\/repos","events_url":"https:\/\/api.github.com\/users\/liujuncn\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/liujuncn\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-21T08:38:12Z","updated_at":"2023-07-24T15:15:14Z","closed_at":"2023-07-24T15:15:14Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nIf you use load_dataset('json', data_files=\"path\/test.json\"), it will return DatasetDict({train:...}).\r\n\r\nAnd if you use load_dataset(\"path\"), it will return DatasetDict({test:...}).\r\n\r\nWhy can't the output behavior be unified?\n\n### Steps to reproduce the bug\n\nas description above.\n\n### Expected behavior\n\nconsistent predictable output.\n\n### Environment info\n\n'2.11.0'","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5778\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5778\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5777","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5777\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5777\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5777\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5777","id":1677655969,"node_id":"I_kwDODunzps5j_v-h","number":5777,"title":"datasets.load_dataset(\"code_search_net\", \"python\") : NotADirectoryError: [Errno 20] Not a directory","user":{"login":"jason-brian-anderson","id":34688597,"node_id":"MDQ6VXNlcjM0Njg4NTk3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/34688597?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jason-brian-anderson","html_url":"https:\/\/github.com\/jason-brian-anderson","followers_url":"https:\/\/api.github.com\/users\/jason-brian-anderson\/followers","following_url":"https:\/\/api.github.com\/users\/jason-brian-anderson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jason-brian-anderson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jason-brian-anderson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jason-brian-anderson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jason-brian-anderson\/orgs","repos_url":"https:\/\/api.github.com\/users\/jason-brian-anderson\/repos","events_url":"https:\/\/api.github.com\/users\/jason-brian-anderson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jason-brian-anderson\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":6,"created_at":"2023-04-21T02:08:07Z","updated_at":"2023-06-05T05:49:52Z","closed_at":"2023-05-11T11:51:56Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhile checking out the [tokenizer tutorial](https:\/\/huggingface.co\/course\/chapter6\/2?fw=pt), i noticed getting an error while initially downloading the python dataset used in the examples.\r\n\r\nThe [collab with the error is here](https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/master\/course\/en\/chapter6\/section2.ipynb#scrollTo=hGb69Yo3eV8S)\r\n\r\n```\r\nfrom datasets import load_dataset\r\nimport os\r\n\r\nos.environ[\"HF_DATASETS_CACHE\"] = \"\/workspace\"\r\n\r\n# This can take a few minutes to load, so grab a coffee or tea while you wait!\r\nraw_datasets = load_dataset(\"code_search_net\", \"python\")\r\n```\r\n\r\nyeilds:\r\n\r\n```\r\nile \/opt\/conda\/lib\/python3.10\/site-packages\/datasets\/download\/streaming_download_manager.py:524, in xlistdir(path, use_auth_token)\r\n    522 main_hop, *rest_hops = _as_str(path).split(\"::\")\r\n    523 if is_local_path(main_hop):\r\n--> 524     return os.listdir(path)\r\n    525 else:\r\n    526     # globbing inside a zip in a private repo requires authentication\r\n    527     if not rest_hops and (main_hop.startswith(\"http:\/\/\") or main_hop.startswith(\"https:\/\/\")):\r\n\r\nNotADirectoryError: [Errno 20] Not a directory: '\/workspace\/downloads\/25ceeb4c25ab737d688bd56ea92bfbb1f199fe572470456cf2d675479f342ac7\/python\/final\/jsonl\/train'\r\n```\r\n\r\nI was able to reproduce this erro both in the collab and on my own pytorch\/pytorch container pulled from the dockerhub official pytorch image, so i think it may be a server side thing. \n\n### Steps to reproduce the bug\n\nSteps to reproduce the issue:\r\n\r\n1. run `raw_datasets = load_dataset(\"code_search_net\", \"python\")`\n\n### Expected behavior\n\nexpect the code to not exception during dataset pull.\n\n### Environment info\n\ni tried both the default HF_DATASETS_CACHE on Collab, and on my local container.  i then pointed to the HF_DATASETS_CACHE to a  large capacity local storage and the problem was consisten across all 3 scenarios.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5777\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5777\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5776","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5776\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5776\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5776\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5776","id":1677116100,"node_id":"I_kwDODunzps5j9sLE","number":5776,"title":"Use Pandas' `read_json` in the JSON builder","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-04-20T17:15:49Z","updated_at":"2023-04-20T17:15:49Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Instead of PyArrow's `read_json`, we should use `pd.read_json` in the JSON builder for consistency with the CSV and SQL builders (e.g., to address https:\/\/github.com\/huggingface\/datasets\/issues\/5725).\r\n\r\nIn Pandas2.0, to get the same performance, we can set the `engine` to \"pyarrow\". The issue is that Colab still doesn't install Pandas 2.0 by default, so I think it's best to wait for this to be resolved on their side to avoid downgrading decoding performance in scenarios when Pandas 2.0 is not installed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5776\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5776\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5775","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5775\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5775\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5775\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5775","id":1677089901,"node_id":"I_kwDODunzps5j9lxt","number":5775,"title":"ArrowDataset.save_to_disk lost some logic of remote","user":{"login":"Zoupers","id":29817738,"node_id":"MDQ6VXNlcjI5ODE3NzM4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/29817738?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Zoupers","html_url":"https:\/\/github.com\/Zoupers","followers_url":"https:\/\/api.github.com\/users\/Zoupers\/followers","following_url":"https:\/\/api.github.com\/users\/Zoupers\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Zoupers\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Zoupers\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Zoupers\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Zoupers\/orgs","repos_url":"https:\/\/api.github.com\/users\/Zoupers\/repos","events_url":"https:\/\/api.github.com\/users\/Zoupers\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Zoupers\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-04-20T16:58:01Z","updated_at":"2023-04-26T12:11:36Z","closed_at":"2023-04-26T12:11:17Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/e7ce0ac60c7efc10886471932854903a7c19f172\/src\/datasets\/arrow_dataset.py#L1371\r\n\r\nHere is the bug point, when I want to save from a `DatasetDict` class and the items of the instance is like `[('train', Dataset({features: ..., num_rows: ...}))]` , there is no guarantee that there exists a directory name `train` under `dataset_dict_path`.\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Mock a DatasetDict with items like what I said.\r\n2. using save_to_disk with storage_options, u can use local sftp. code may like below\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(...)\r\ndataset.save_to_disk('sftp:\/\/\/tmp', storage_options={'host': 'localhost', 'username': 'admin'})\r\n```\r\nI suppose u can reproduce the bug by these steps. \r\n\r\n### Expected behavior\r\n\r\nShould create the folder if it does not exists, just like we do locally.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.11.0\r\n- Platform: Linux-6.2.10-arch1-1-x86_64-with-glibc2.35\r\n- Python version: 3.10.9\r\n- Huggingface_hub version: 0.13.2\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5775\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5775\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5774","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5774\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5774\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5774\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5774","id":1676716662,"node_id":"PR_kwDODunzps5OxIMe","number":5774,"title":"Fix style","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-20T13:21:32Z","updated_at":"2023-04-20T13:34:26Z","closed_at":"2023-04-20T13:24:28Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix C419 issues","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5774\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5774\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5774","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5774","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5774.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5774.patch","merged_at":"2023-04-20T13:24:28Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5773","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5773\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5773\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5773\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5773","id":1675984633,"node_id":"I_kwDODunzps5j5X75","number":5773,"title":"train_dataset does not implement __len__","user":{"login":"v-yunbin","id":38179632,"node_id":"MDQ6VXNlcjM4MTc5NjMy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/38179632?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/v-yunbin","html_url":"https:\/\/github.com\/v-yunbin","followers_url":"https:\/\/api.github.com\/users\/v-yunbin\/followers","following_url":"https:\/\/api.github.com\/users\/v-yunbin\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/v-yunbin\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/v-yunbin\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/v-yunbin\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/v-yunbin\/orgs","repos_url":"https:\/\/api.github.com\/users\/v-yunbin\/repos","events_url":"https:\/\/api.github.com\/users\/v-yunbin\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/v-yunbin\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2023-04-20T04:37:05Z","updated_at":"2023-07-19T20:33:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"when train using data  precessored by the datasets, I get follow warning and it leads to that I can not set epoch numbers:\r\n`ValueError: The train_dataset does not implement __len__, max_steps has to be specified. The number of steps needs to be known in advance for the learning rate scheduler.`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5773\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5773\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5772","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5772\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5772\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5772\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5772","id":1675033510,"node_id":"PR_kwDODunzps5OreXV","number":5772,"title":"Fix JSON builder when missing keys in first row ","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-19T14:32:57Z","updated_at":"2023-04-21T06:45:13Z","closed_at":"2023-04-21T06:35:27Z","author_association":"MEMBER","active_lock_reason":null,"body":"Until now, the JSON builder only considered the keys present in the first element of the list:\r\n- Either explicitly: by passing index 0 in `dataset[0].keys()`\r\n- Or implicitly: `pa.Table.from_pylist(dataset)`, where \"schema (default None): If not passed, will be inferred from the first row of the mapping values\"\r\n\r\nThis PR fixes the bug by considering the union of the keys present in all the rows.\r\n\r\n\r\nFix #5726.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5772\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5772\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5772","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5772","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5772.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5772.patch","merged_at":"2023-04-21T06:35:27Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5771","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5771\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5771\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5771\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5771","id":1674828380,"node_id":"I_kwDODunzps5j09pc","number":5771,"title":"Support cloud storage for loading datasets","user":{"login":"eli-osherovich","id":2437102,"node_id":"MDQ6VXNlcjI0MzcxMDI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2437102?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/eli-osherovich","html_url":"https:\/\/github.com\/eli-osherovich","followers_url":"https:\/\/api.github.com\/users\/eli-osherovich\/followers","following_url":"https:\/\/api.github.com\/users\/eli-osherovich\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/eli-osherovich\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/eli-osherovich\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/eli-osherovich\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/eli-osherovich\/orgs","repos_url":"https:\/\/api.github.com\/users\/eli-osherovich\/repos","events_url":"https:\/\/api.github.com\/users\/eli-osherovich\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/eli-osherovich\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892865,"node_id":"MDU6TGFiZWwxOTM1ODkyODY1","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/duplicate","name":"duplicate","color":"cfd3d7","default":true,"description":"This issue or pull request already exists"},{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-19T12:43:53Z","updated_at":"2023-05-07T17:47:41Z","closed_at":"2023-05-07T17:47:41Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\r\n\r\nIt seems that the the current implementation supports cloud storage only for `load_from_disk`. It would be nice if a similar functionality existed in `load_dataset`.\r\n\r\n### Motivation\r\n\r\nMotivation is pretty clear -- let users work with datasets located in the cloud. \r\n\r\n### Your contribution\r\n\r\nI can help implementing this. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5771\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5771\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5770","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5770\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5770\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5770\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5770","id":1673581555,"node_id":"PR_kwDODunzps5OmntV","number":5770,"title":"Add IterableDataset.from_spark","user":{"login":"maddiedawson","id":106995444,"node_id":"U_kgDOBmCe9A","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/106995444?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/maddiedawson","html_url":"https:\/\/github.com\/maddiedawson","followers_url":"https:\/\/api.github.com\/users\/maddiedawson\/followers","following_url":"https:\/\/api.github.com\/users\/maddiedawson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/maddiedawson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/maddiedawson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/maddiedawson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/maddiedawson\/orgs","repos_url":"https:\/\/api.github.com\/users\/maddiedawson\/repos","events_url":"https:\/\/api.github.com\/users\/maddiedawson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/maddiedawson\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2023-04-18T17:47:53Z","updated_at":"2023-05-17T14:07:32Z","closed_at":"2023-05-17T14:00:38Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Follow-up from https:\/\/github.com\/huggingface\/datasets\/pull\/5701\r\n\r\nRelated issue: https:\/\/github.com\/huggingface\/datasets\/issues\/5678","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5770\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5770\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5770","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5770","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5770.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5770.patch","merged_at":"2023-05-17T14:00:38Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5769","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5769\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5769\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5769\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5769","id":1673441182,"node_id":"I_kwDODunzps5jvq-e","number":5769,"title":"Tiktoken tokenizers are not pickable ","user":{"login":"markovalexander","id":22663468,"node_id":"MDQ6VXNlcjIyNjYzNDY4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22663468?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/markovalexander","html_url":"https:\/\/github.com\/markovalexander","followers_url":"https:\/\/api.github.com\/users\/markovalexander\/followers","following_url":"https:\/\/api.github.com\/users\/markovalexander\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/markovalexander\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/markovalexander\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/markovalexander\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/markovalexander\/orgs","repos_url":"https:\/\/api.github.com\/users\/markovalexander\/repos","events_url":"https:\/\/api.github.com\/users\/markovalexander\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/markovalexander\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-18T16:07:40Z","updated_at":"2023-05-04T18:55:57Z","closed_at":"2023-05-04T18:55:57Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nSince tiktoken tokenizer is not pickable, it is not possible to use it inside `dataset.map()` with multiprocessing enabled. However, you [made](https:\/\/github.com\/huggingface\/datasets\/issues\/5536) tiktoken's tokenizers pickable in `datasets==2.10.0` for caching. For some reason, this logic does not work in dataset processing and raises `TypeError: cannot pickle 'builtins.CoreBPE' object`\r\n\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\nimport tiktoken\r\n\r\ndataset = load_dataset(\"stas\/openwebtext-10k\")\r\n\r\nenc = tiktoken.get_encoding(\"gpt2\")\r\n\r\ntokenized = dataset.map(\r\n    process,\r\n    remove_columns=['text'],\r\n    desc=\"tokenizing the OWT splits\",\r\n    num_proc=2,\r\n)\r\n\r\ndef process(example):\r\n        ids = enc.encode(example['text'])\r\n        ids.append(enc.eot_token)\r\n        out = {'ids': ids, 'len': len(ids)}\r\n        return out\r\n```\n\n### Expected behavior\n\nstarts processing dataset \n\n### Environment info\n\n- `datasets` version: 2.11.0\r\n- Platform: Linux-5.15.0-1021-oracle-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.13.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 2.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5769\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5769\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5768","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5768\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5768\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5768\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5768","id":1672494561,"node_id":"I_kwDODunzps5jsD3h","number":5768,"title":"load_dataset(\"squad\") doesn't work in 2.7.1 and 2.10.1","user":{"login":"yaseen157","id":57412770,"node_id":"MDQ6VXNlcjU3NDEyNzcw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/57412770?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/yaseen157","html_url":"https:\/\/github.com\/yaseen157","followers_url":"https:\/\/api.github.com\/users\/yaseen157\/followers","following_url":"https:\/\/api.github.com\/users\/yaseen157\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/yaseen157\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/yaseen157\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/yaseen157\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/yaseen157\/orgs","repos_url":"https:\/\/api.github.com\/users\/yaseen157\/repos","events_url":"https:\/\/api.github.com\/users\/yaseen157\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/yaseen157\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":8,"created_at":"2023-04-18T07:10:56Z","updated_at":"2023-04-20T10:27:23Z","closed_at":"2023-04-20T10:27:22Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nThere is an issue that seems to be unique to the \"squad\" dataset, in which it cannot be loaded using standard methods. This issue is most quickly reproduced from the command line, using the HF examples to verify a dataset is loaded properly.\r\n\r\nThis is not a problem with \"squad_v2\" dataset for example.\r\n\r\n### Steps to reproduce the bug\r\n\r\ncmd line\r\n> $ python -c \"from datasets import load_dataset; print(load_dataset('squad', split='train')[0])\"\r\n\r\nOR\r\n\r\nPython IDE\r\n> from datasets import load_dataset\r\n> load_dataset(\"squad\")\r\n\r\n### Expected behavior\r\n\r\nI expected to either see the output described here from running the very same command in command line ([https:\/\/huggingface.co\/docs\/datasets\/installation]), or any output that does not raise Python's TypeError.\r\n\r\nThere is some funky behaviour in the dataset builder portion of the codebase that means it is trying to import the squad dataset with an incorrect path, or the squad dataset couldn't be downloaded. I'm not really sure what the problem is beyond that. Messing around with caching I did manage to get it to load the dataset once, and then couldn't repeat this.\r\n\r\n### Environment info\r\n\r\ndatasets=2.7.1 **or** 2.10.1, python=3.10.8, Linux 3.10.0-1160.36.2.el7.x86_64 **or** Windows 10-64\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5768\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5768\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5767","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5767\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5767\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5767\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5767","id":1672433979,"node_id":"I_kwDODunzps5jr1E7","number":5767,"title":"How to use Distill-BERT with different datasets?","user":{"login":"sauravtii","id":109907638,"node_id":"U_kgDOBo0Otg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/109907638?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sauravtii","html_url":"https:\/\/github.com\/sauravtii","followers_url":"https:\/\/api.github.com\/users\/sauravtii\/followers","following_url":"https:\/\/api.github.com\/users\/sauravtii\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sauravtii\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sauravtii\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sauravtii\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sauravtii\/orgs","repos_url":"https:\/\/api.github.com\/users\/sauravtii\/repos","events_url":"https:\/\/api.github.com\/users\/sauravtii\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sauravtii\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-18T06:25:12Z","updated_at":"2023-04-20T16:52:05Z","closed_at":"2023-04-20T16:52:05Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n- `transformers` version: 4.11.3\r\n- Platform: Linux-5.4.0-58-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyTorch version (GPU?): 1.12.0+cu102 (True)\r\n- Tensorflow version (GPU?): 2.10.0 (True)\r\n- Flax version (CPU?\/GPU?\/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\n\n### Steps to reproduce the bug\n\nI recently read [this](https:\/\/huggingface.co\/docs\/transformers\/quicktour#train-with-tensorflow:~:text=The%20most%20important%20thing%20to%20remember%20is%20you%20need%20to%20instantiate%20a%20tokenizer%20with%20the%20same%20model%20name%20to%20ensure%20you%E2%80%99re%20using%20the%20same%20tokenization%20rules%20a%20model%20was%20pretrained%20with.) and was wondering how to use distill-BERT (which is pre-trained with imdb dataset) with a different dataset (for eg. [this](https:\/\/huggingface.co\/datasets\/yhavinga\/imdb_dutch) dataset)?\n\n### Expected behavior\n\nDistill-BERT should work with different datasets.\n\n### Environment info\n\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.4.0-58-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 11.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5767\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5767\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5766","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5766\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5766\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5766\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5766","id":1671485882,"node_id":"I_kwDODunzps5joNm6","number":5766,"title":"Support custom feature types","user":{"login":"jmontalt","id":37540982,"node_id":"MDQ6VXNlcjM3NTQwOTgy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/37540982?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jmontalt","html_url":"https:\/\/github.com\/jmontalt","followers_url":"https:\/\/api.github.com\/users\/jmontalt\/followers","following_url":"https:\/\/api.github.com\/users\/jmontalt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jmontalt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jmontalt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jmontalt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jmontalt\/orgs","repos_url":"https:\/\/api.github.com\/users\/jmontalt\/repos","events_url":"https:\/\/api.github.com\/users\/jmontalt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jmontalt\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-17T15:46:41Z","updated_at":"2023-05-03T21:58:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nI think it would be nice to allow registering custom feature types with the \ud83e\udd17 Datasets library. For example, allow to do something along the following lines:\r\n\r\n```\r\nfrom datasets.features import register_feature_type  # this would be a new function\r\n\r\n@register_feature_type\r\nclass CustomFeatureType:\r\n    def encode_example(self, value):\r\n        \"\"\"User-provided logic to encode an example of this feature.\"\"\"\r\n        pass\r\n\r\n    def decode_example(self, value, token_per_repo_id=None):\r\n        \"\"\"User-provided logic to decode an example of this feature.\"\"\"\r\n        pass\r\n```\r\n\r\n### Motivation\r\n\r\nUsers of \ud83e\udd17 Datasets, such as myself, may want to use the library to load datasets with unsupported feature types (i.e., beyond `ClassLabel`, `Image`, or `Audio`). This would be useful for prototyping new feature types and for feature types that aren't used widely enough to warrant inclusion in \ud83e\udd17 Datasets.\r\n\r\nAt the moment, this is only possible by monkey-patching \ud83e\udd17 Datasets, which obfuscates the code and is prone to breaking with library updates. It also requires the user to write some custom code which could be easily avoided.\r\n\r\n### Your contribution\r\n\r\nI would be happy to contribute this feature. My proposed solution would involve changing the following call to `globals()` to an explicit feature type registry, which a user-facing `register_feature_type` decorator could update.\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/fd893098627230cc734f6009ad04cf885c979ac4\/src\/datasets\/features\/features.py#L1329\r\n\r\nI would also provide an abstract base class for custom feature types which users could inherit. This would have at least an `encode_example` method and a `decode_example` method, similar to `Image` or `Audio`.\r\n\r\nThe existing `encode_nested_example` and `decode_nested_example` functions would also need to be updated to correctly call the corresponding functions for the new type.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5766\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5766\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5765","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5765\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5765\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5765\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5765","id":1671388824,"node_id":"I_kwDODunzps5jn16Y","number":5765,"title":"ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['text']","user":{"login":"sauravtii","id":109907638,"node_id":"U_kgDOBo0Otg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/109907638?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sauravtii","html_url":"https:\/\/github.com\/sauravtii","followers_url":"https:\/\/api.github.com\/users\/sauravtii\/followers","following_url":"https:\/\/api.github.com\/users\/sauravtii\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sauravtii\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sauravtii\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sauravtii\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sauravtii\/orgs","repos_url":"https:\/\/api.github.com\/users\/sauravtii\/repos","events_url":"https:\/\/api.github.com\/users\/sauravtii\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sauravtii\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-17T15:00:50Z","updated_at":"2023-04-25T13:50:45Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nFollowing is my code that I am trying to run, but facing an error (have attached the whole error below):\r\n\r\nMy code:\r\n\r\n```\r\nfrom collections import OrderedDict\r\nimport warnings\r\n\r\nimport flwr as fl\r\nimport torch\r\nimport numpy as np\r\n\r\nimport random\r\nfrom torch.utils.data import DataLoader\r\n\r\nfrom datasets import load_dataset, load_metric\r\n\r\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\r\nfrom transformers import AutoModelForSequenceClassification\r\nfrom transformers import AdamW\r\n#from transformers import tokenized_datasets\r\n\r\n\r\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\r\n# DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n\r\nDEVICE = \"cpu\"\r\n\r\nCHECKPOINT = \"distilbert-base-uncased\"  # transformer model checkpoint\r\n\r\n\r\ndef load_data():\r\n    \"\"\"Load IMDB data (training and eval)\"\"\"\r\n    raw_datasets = load_dataset(\"yhavinga\/imdb_dutch\")\r\n    raw_datasets = raw_datasets.shuffle(seed=42)\r\n\r\n    # remove unnecessary data split\r\n    del raw_datasets[\"unsupervised\"]\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\r\n\r\n    def tokenize_function(examples):\r\n        return tokenizer(examples[\"text\"], truncation=True)\r\n\r\n    # random 100 samples\r\n    population = random.sample(range(len(raw_datasets[\"train\"])), 100)\r\n\r\n    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\r\n    tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].select(population)\r\n    tokenized_datasets[\"test\"] = tokenized_datasets[\"test\"].select(population)\r\n\r\n    # tokenized_datasets = tokenized_datasets.remove_columns(\"text\")\r\n    # tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\r\n\r\n    tokenized_datasets = tokenized_datasets.remove_columns(\"attention_mask\")\r\n    tokenized_datasets = tokenized_datasets.remove_columns(\"input_ids\")\r\n    tokenized_datasets = tokenized_datasets.remove_columns(\"label\")\r\n    tokenized_datasets = tokenized_datasets.remove_columns(\"text_en\")\r\n\r\n    # tokenized_datasets = tokenized_datasets.remove_columns(raw_datasets[\"train\"].column_names)\r\n    \r\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\r\n    trainloader = DataLoader(\r\n        tokenized_datasets[\"train\"],\r\n        shuffle=True,\r\n        batch_size=32,\r\n        collate_fn=data_collator,\r\n    )\r\n\r\n    testloader = DataLoader(\r\n        tokenized_datasets[\"test\"], batch_size=32, collate_fn=data_collator\r\n    )\r\n\r\n    return trainloader, testloader\r\n\r\n\r\ndef train(net, trainloader, epochs):\r\n    optimizer = AdamW(net.parameters(), lr=5e-4)\r\n    net.train()\r\n    for _ in range(epochs):\r\n        for batch in trainloader:\r\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\r\n            outputs = net(**batch)\r\n            loss = outputs.loss\r\n            loss.backward()\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n\r\n\r\ndef test(net, testloader):\r\n    metric = load_metric(\"accuracy\")\r\n    loss = 0\r\n    net.eval()\r\n    for batch in testloader:\r\n        batch = {k: v.to(DEVICE) for k, v in batch.items()}\r\n        with torch.no_grad():\r\n            outputs = net(**batch)\r\n        logits = outputs.logits\r\n        loss += outputs.loss.item()\r\n        predictions = torch.argmax(logits, dim=-1)\r\n        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\r\n    loss \/= len(testloader.dataset)\r\n    accuracy = metric.compute()[\"accuracy\"]\r\n    return loss, accuracy\r\n\r\n\r\ndef main():\r\n    net = AutoModelForSequenceClassification.from_pretrained(\r\n        CHECKPOINT, num_labels=2\r\n    ).to(DEVICE)\r\n\r\n    trainloader, testloader = load_data()\r\n\r\n    # Flower client\r\n    class IMDBClient(fl.client.NumPyClient):\r\n        def get_parameters(self, config):\r\n            return [val.cpu().numpy() for _, val in net.state_dict().items()]\r\n\r\n        def set_parameters(self, parameters):\r\n            params_dict = zip(net.state_dict().keys(), parameters)\r\n            state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\r\n            net.load_state_dict(state_dict, strict=True)\r\n\r\n        def fit(self, parameters, config):\r\n            self.set_parameters(parameters)\r\n            print(\"Training Started...\")\r\n            train(net, trainloader, epochs=1)\r\n            print(\"Training Finished.\")\r\n            return self.get_parameters(config={}), len(trainloader), {}\r\n\r\n        def evaluate(self, parameters, config):\r\n            self.set_parameters(parameters)\r\n            loss, accuracy = test(net, testloader)\r\n            return float(loss), len(testloader), {\"accuracy\": float(accuracy)}\r\n\r\n    # Start client\r\n    fl.client.start_numpy_client(server_address=\"localhost:8080\", client=IMDBClient())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"client_2.py\", line 136, in <module>\r\n    main()\r\n  File \"client_2.py\", line 132, in main\r\n    fl.client.start_numpy_client(server_address=\"localhost:8080\", client=IMDBClient())\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/flwr\/client\/app.py\", line 208, in start_numpy_client\r\n    start_client(\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/flwr\/client\/app.py\", line 142, in start_client\r\n    client_message, sleep_duration, keep_going = handle(\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/flwr\/client\/grpc_client\/message_handler.py\", line 68, in handle\r\n    return _fit(client, server_msg.fit_ins), 0, True\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/flwr\/client\/grpc_client\/message_handler.py\", line 157, in _fit\r\n    fit_res = client.fit(fit_ins)\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/flwr\/client\/app.py\", line 252, in _fit\r\n    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\r\n  File \"client_2.py\", line 122, in fit\r\n    train(net, trainloader, epochs=1)\r\n  File \"client_2.py\", line 76, in train\r\n    for batch in trainloader:\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/torch\/utils\/data\/dataloader.py\", line 652, in __next__\r\n    data = self._next_data()\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/torch\/utils\/data\/dataloader.py\", line 692, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/torch\/utils\/data\/_utils\/fetch.py\", line 52, in fetch\r\n    return self.collate_fn(data)\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/transformers\/data\/data_collator.py\", line 221, in __call__\r\n    batch = self.tokenizer.pad(\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/transformers\/tokenization_utils_base.py\", line 2713, in pad\r\n    raise ValueError(\r\nValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['text']\r\n```\n\n### Steps to reproduce the bug\n\nRun the above code.\n\n### Expected behavior\n\nDon't know, doing it for the first time.\n\n### Environment info\n\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.4.0-58-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 11.0.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5765\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5765\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5764","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5764\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5764\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5764\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5764","id":1670740198,"node_id":"I_kwDODunzps5jlXjm","number":5764,"title":"ConnectionError: Couldn't reach https:\/\/www.dropbox.com\/s\/zts98j4vkqtsns6\/aclImdb_v2.tar?dl=1","user":{"login":"sauravtii","id":109907638,"node_id":"U_kgDOBo0Otg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/109907638?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sauravtii","html_url":"https:\/\/github.com\/sauravtii","followers_url":"https:\/\/api.github.com\/users\/sauravtii\/followers","following_url":"https:\/\/api.github.com\/users\/sauravtii\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sauravtii\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sauravtii\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sauravtii\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sauravtii\/orgs","repos_url":"https:\/\/api.github.com\/users\/sauravtii\/repos","events_url":"https:\/\/api.github.com\/users\/sauravtii\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sauravtii\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":7,"created_at":"2023-04-17T09:08:18Z","updated_at":"2023-04-18T07:18:20Z","closed_at":"2023-04-18T07:18:20Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI want to use this (https:\/\/huggingface.co\/datasets\/josianem\/imdb) dataset therefore I am trying to load it using the following code:\r\n\r\n```\r\ndataset = load_dataset(\"josianem\/imdb\")\r\n```\r\nThe dataset is not getting loaded and gives the error message as the following:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"sample.py\", line 3, in <module>\r\n    dataset = load_dataset(\"josianem\/imdb\")\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/datasets\/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 636, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 704, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"\/home\/saurav\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/imdb\/cc6ab4acab2799be15d5d217c24548b856156dafdc850165fdc4f2031f27ff2f\/imdb.py\", line 79, in _split_generators\r\n    archive = dl_manager.download(_DOWNLOAD_URL)\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/datasets\/utils\/download_manager.py\", line 196, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/datasets\/utils\/py_utils.py\", line 197, in map_nested\r\n    return function(data_struct)\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/datasets\/utils\/download_manager.py\", line 217, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/datasets\/utils\/file_utils.py\", line 289, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"\/home\/saurav\/.local\/lib\/python3.8\/site-packages\/datasets\/utils\/file_utils.py\", line 606, in get_from_cache\r\n    raise ConnectionError(\"Couldn't reach {}\".format(url))\r\nConnectionError: Couldn't reach https:\/\/www.dropbox.com\/s\/zts98j4vkqtsns6\/aclImdb_v2.tar?dl=1\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nYou can reproduce the error by using the following code:\r\n\r\n```\r\nfrom datasets import load_dataset, load_metric\r\n\r\ndataset = load_dataset(\"josianem\/imdb\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe dataset should get loaded (I am using this dataset for the first time so not much aware of the exact behavior).\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.4.0-58-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 11.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5764\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5764\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5763","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5763\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5763\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5763\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5763","id":1670476302,"node_id":"PR_kwDODunzps5OcMI7","number":5763,"title":"fix typo: \"mow\" -> \"now\"","user":{"login":"csris","id":1967608,"node_id":"MDQ6VXNlcjE5Njc2MDg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1967608?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/csris","html_url":"https:\/\/github.com\/csris","followers_url":"https:\/\/api.github.com\/users\/csris\/followers","following_url":"https:\/\/api.github.com\/users\/csris\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/csris\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/csris\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/csris\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/csris\/orgs","repos_url":"https:\/\/api.github.com\/users\/csris\/repos","events_url":"https:\/\/api.github.com\/users\/csris\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/csris\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-17T06:03:44Z","updated_at":"2023-04-17T15:01:53Z","closed_at":"2023-04-17T14:54:46Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I noticed a typo as I was reading the datasets documentation. This PR contains a trivial fix changing \"mow\" to \"now.\"","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5763\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5763\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5763","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5763","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5763.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5763.patch","merged_at":"2023-04-17T14:54:46Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5762","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5762\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5762\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5762\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5762","id":1670326470,"node_id":"I_kwDODunzps5jjyjG","number":5762,"title":"Not able to load the pile","user":{"login":"surya-narayanan","id":17240858,"node_id":"MDQ6VXNlcjE3MjQwODU4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/17240858?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/surya-narayanan","html_url":"https:\/\/github.com\/surya-narayanan","followers_url":"https:\/\/api.github.com\/users\/surya-narayanan\/followers","following_url":"https:\/\/api.github.com\/users\/surya-narayanan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/surya-narayanan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/surya-narayanan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/surya-narayanan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/surya-narayanan\/orgs","repos_url":"https:\/\/api.github.com\/users\/surya-narayanan\/repos","events_url":"https:\/\/api.github.com\/users\/surya-narayanan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/surya-narayanan\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-04-17T03:09:10Z","updated_at":"2023-04-17T09:37:27Z","closed_at":"2023-04-17T09:37:27Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nGot this error when I am trying to load the pile dataset \r\n\r\n```\r\nTypeError: Couldn't cast array of type\r\nstruct<file: string, id: string>\r\nto\r\n{'id': Value(dtype='string', id=None)}\r\n\r\n```\r\n\n\n### Steps to reproduce the bug\n\nPlease visit the following sample notebook \r\n\r\nhttps:\/\/colab.research.google.com\/drive\/1JHcjawcHL6QHhi5VcqYd07W2QCEj2nWK#scrollTo=ulJP3eJCI-tB\n\n### Expected behavior\n\nThe pile should work\n\n### Environment info\n\n-  `datasets` version: 2.11.0\r\n- Platform: Linux-5.10.147+-x86_64-with-glibc2.31\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.13.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5762\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5762\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5761","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5761\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5761\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5761\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5761","id":1670034582,"node_id":"I_kwDODunzps5jirSW","number":5761,"title":"One or several metadata.jsonl were found, but not in the same directory or in a parent directory","user":{"login":"blghtr","id":69686152,"node_id":"MDQ6VXNlcjY5Njg2MTUy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/69686152?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/blghtr","html_url":"https:\/\/github.com\/blghtr","followers_url":"https:\/\/api.github.com\/users\/blghtr\/followers","following_url":"https:\/\/api.github.com\/users\/blghtr\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/blghtr\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/blghtr\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/blghtr\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/blghtr\/orgs","repos_url":"https:\/\/api.github.com\/users\/blghtr\/repos","events_url":"https:\/\/api.github.com\/users\/blghtr\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/blghtr\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2023-04-16T16:21:55Z","updated_at":"2023-04-19T11:53:24Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nAn attempt to generate a dataset from a zip archive using imagefolder and metadata.jsonl does not lead to the expected result. Tried all possible locations of the json file: the file in the archive is ignored (generated dataset contains only images), the file next to the archive like [here](https:\/\/huggingface.co\/docs\/datasets\/image_dataset#imagefolder) leads to an error:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nFile ~\\PycharmProjects\\testproj\\venv\\lib\\site-packages\\datasets\\builder.py:1610, in GeneratorBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\r\n   1609 _time = time.time()\r\n-> 1610 for key, record in generator:\r\n   1611     if max_shard_size is not None and writer._num_bytes > max_shard_size:\r\n\r\nFile ~\\PycharmProjects\\testproj\\venv\\lib\\site-packages\\datasets\\packaged_modules\\folder_based_builder\\folder_based_builder.py:370, in FolderBasedBuilder._generate_examples(self, files, metadata_files, split_name, add_metadata, add_labels)\r\n    369     else:\r\n--> 370         raise ValueError(\r\n    371             f\"One or several metadata.{metadata_ext} were found, but not in the same directory or in a parent directory of {downloaded_dir_file}.\"\r\n    372         )\r\n    373 if metadata_dir is not None and downloaded_metadata_file is not None:\r\n\r\nValueError: One or several metadata.jsonl were found, but not in the same directory or in a parent directory of C:\\Users\\User\\.cache\\huggingface\\datasets\\downloads\\extracted\\f7fb7de25fb28ae63089974524f2d271a39d83888bc456d04aa3b3d45f33e6a6\\ff0745a0-a741-4d9e-b228-a93b851adf61.png.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\nCell In[3], line 1\r\n----> 1 dataset = load_dataset(\"imagefolder\", data_dir=r'C:\\Users\\User\\data')\r\n\r\nFile ~\\PycharmProjects\\testproj\\venv\\lib\\site-packages\\datasets\\load.py:1791, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   1788 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\r\n   1790 # Download and prepare data\r\n-> 1791 builder_instance.download_and_prepare(\r\n   1792     download_config=download_config,\r\n   1793     download_mode=download_mode,\r\n   1794     verification_mode=verification_mode,\r\n   1795     try_from_hf_gcs=try_from_hf_gcs,\r\n   1796     num_proc=num_proc,\r\n   1797     storage_options=storage_options,\r\n   1798 )\r\n   1800 # Build dataset for splits\r\n   1801 keep_in_memory = (\r\n   1802     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   1803 )\r\n\r\nFile ~\\PycharmProjects\\testproj\\venv\\lib\\site-packages\\datasets\\builder.py:891, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    889     if num_proc is not None:\r\n    890         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 891     self._download_and_prepare(\r\n    892         dl_manager=dl_manager,\r\n    893         verification_mode=verification_mode,\r\n    894         **prepare_split_kwargs,\r\n    895         **download_and_prepare_kwargs,\r\n    896     )\r\n    897 # Sync info\r\n    898 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~\\PycharmProjects\\testproj\\venv\\lib\\site-packages\\datasets\\builder.py:1651, in GeneratorBasedBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_splits_kwargs)\r\n   1650 def _download_and_prepare(self, dl_manager, verification_mode, **prepare_splits_kwargs):\r\n-> 1651     super()._download_and_prepare(\r\n   1652         dl_manager,\r\n   1653         verification_mode,\r\n   1654         check_duplicate_keys=verification_mode == VerificationMode.BASIC_CHECKS\r\n   1655         or verification_mode == VerificationMode.ALL_CHECKS,\r\n   1656         **prepare_splits_kwargs,\r\n   1657     )\r\n\r\nFile ~\\PycharmProjects\\testproj\\venv\\lib\\site-packages\\datasets\\builder.py:986, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n    982 split_dict.add(split_generator.split_info)\r\n    984 try:\r\n    985     # Prepare split will record examples associated to the split\r\n--> 986     self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    987 except OSError as e:\r\n    988     raise OSError(\r\n    989         \"Cannot find data file. \"\r\n    990         + (self.manual_download_instructions or \"\")\r\n    991         + \"\\nOriginal error:\\n\"\r\n    992         + str(e)\r\n    993     ) from None\r\n\r\nFile ~\\PycharmProjects\\testproj\\venv\\lib\\site-packages\\datasets\\builder.py:1490, in GeneratorBasedBuilder._prepare_split(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\r\n   1488 gen_kwargs = split_generator.gen_kwargs\r\n   1489 job_id = 0\r\n-> 1490 for job_id, done, content in self._prepare_split_single(\r\n   1491     gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n   1492 ):\r\n   1493     if done:\r\n   1494         result = content\r\n\r\nFile ~\\PycharmProjects\\testproj\\venv\\lib\\site-packages\\datasets\\builder.py:1646, in GeneratorBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\r\n   1644     if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\r\n   1645         e = e.__context__\r\n-> 1646     raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1648 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Organize directory structure like in the docs:\r\nfolder\/metadata.jsonl\r\nfolder\/train.zip\r\n\r\n2. Run load_dataset(\"imagefolder\", data_dir='folder\/metadata.jsonl', split='train')\r\n\r\n### Expected behavior\r\n\r\nDataset generated with all additional features from metadata.jsonl\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.11.0\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.9.0\r\n- Huggingface_hub version: 0.13.4\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5761\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5761\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5760","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5760\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5760\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5760\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5760","id":1670028072,"node_id":"I_kwDODunzps5jipso","number":5760,"title":"Multi-image loading in Imagefolder dataset","user":{"login":"vvvm23","id":44398246,"node_id":"MDQ6VXNlcjQ0Mzk4MjQ2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/44398246?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/vvvm23","html_url":"https:\/\/github.com\/vvvm23","followers_url":"https:\/\/api.github.com\/users\/vvvm23\/followers","following_url":"https:\/\/api.github.com\/users\/vvvm23\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/vvvm23\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/vvvm23\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/vvvm23\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/vvvm23\/orgs","repos_url":"https:\/\/api.github.com\/users\/vvvm23\/repos","events_url":"https:\/\/api.github.com\/users\/vvvm23\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/vvvm23\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2023-04-16T16:01:05Z","updated_at":"2023-11-30T12:06:20Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nExtend the `imagefolder` dataloading script to support loading multiple images per dataset entry.\r\n\r\nThis only really makes sense if a metadata file is present.\r\n\r\nCurrently you can use the following format (example `metadata.jsonl`:\r\n```\r\n{'file_name': 'path_to_image.png', 'metadata': ...}\r\n...\r\n```\r\nwhich will return a batch with key `image` and any other metadata.\r\n\r\nI would propose extending `file_name` to also accept a list of files, which would return a batch with key `images` and any other metadata.\n\n### Motivation\n\nThis is useful for example in segmentation tasks in computer vision models, or in text-to-image models that also accept conditioning signals such as another image, feature map, or similar. Currently if I want to do this, I would need to write a custom dataset, rather than just use `imagefolder`.\n\n### Your contribution\n\nWould be open to doing a PR, but also happy for someone else to take it as I am not familiar with the datasets library.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5760\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5760\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5759","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5759\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5759\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5759\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5759","id":1669977848,"node_id":"I_kwDODunzps5jidb4","number":5759,"title":"Can I load in list of list of dict format? ","user":{"login":"LZY-the-boys","id":72137647,"node_id":"MDQ6VXNlcjcyMTM3NjQ3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/72137647?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/LZY-the-boys","html_url":"https:\/\/github.com\/LZY-the-boys","followers_url":"https:\/\/api.github.com\/users\/LZY-the-boys\/followers","following_url":"https:\/\/api.github.com\/users\/LZY-the-boys\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/LZY-the-boys\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/LZY-the-boys\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/LZY-the-boys\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/LZY-the-boys\/orgs","repos_url":"https:\/\/api.github.com\/users\/LZY-the-boys\/repos","events_url":"https:\/\/api.github.com\/users\/LZY-the-boys\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/LZY-the-boys\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-04-16T13:50:14Z","updated_at":"2023-04-19T12:04:36Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nmy jsonl dataset has following format:\r\n```\r\n[{'input':xxx, 'output':xxx},{'input:xxx,'output':xxx},...]\r\n[{'input':xxx, 'output':xxx},{'input:xxx,'output':xxx},...]\r\n```\r\n\r\nI try to use `datasets.load_dataset('json', data_files=path)` or `datasets.Dataset.from_json`, it raises\r\n```\r\n  File \"site-packages\/datasets\/arrow_dataset.py\", line 1078, in from_json\r\n    ).read()\r\n  File \"site-packages\/datasets\/io\/json.py\", line 59, in read\r\n    self.builder.download_and_prepare(\r\n  File \"site-packages\/datasets\/builder.py\", line 872, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"site-packages\/datasets\/builder.py\", line 967, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"site-packages\/datasets\/builder.py\", line 1749, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"site-packages\/datasets\/builder.py\", line 1892, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\n\n### Motivation\n\nI wanna use features like `Datasets.map` or `Datasets.shuffle`, so i need the dataset in memory to be `arrow_dataset.Datasets` format\n\n### Your contribution\n\nPR","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5759\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5759\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5758","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5758\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5758\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5758\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5758","id":1669920923,"node_id":"PR_kwDODunzps5OaY9S","number":5758,"title":"Fixes #5757","user":{"login":"eli-osherovich","id":2437102,"node_id":"MDQ6VXNlcjI0MzcxMDI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2437102?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/eli-osherovich","html_url":"https:\/\/github.com\/eli-osherovich","followers_url":"https:\/\/api.github.com\/users\/eli-osherovich\/followers","following_url":"https:\/\/api.github.com\/users\/eli-osherovich\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/eli-osherovich\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/eli-osherovich\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/eli-osherovich\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/eli-osherovich\/orgs","repos_url":"https:\/\/api.github.com\/users\/eli-osherovich\/repos","events_url":"https:\/\/api.github.com\/users\/eli-osherovich\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/eli-osherovich\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-04-16T11:56:01Z","updated_at":"2023-04-20T15:37:49Z","closed_at":"2023-04-20T15:30:48Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fixes the bug #5757","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5758\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5758\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5758","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5758","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5758.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5758.patch","merged_at":"2023-04-20T15:30:48Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5757","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5757\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5757\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5757\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5757","id":1669910503,"node_id":"I_kwDODunzps5jiM_n","number":5757,"title":"Tilde (~) is not supported","user":{"login":"eli-osherovich","id":2437102,"node_id":"MDQ6VXNlcjI0MzcxMDI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2437102?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/eli-osherovich","html_url":"https:\/\/github.com\/eli-osherovich","followers_url":"https:\/\/api.github.com\/users\/eli-osherovich\/followers","following_url":"https:\/\/api.github.com\/users\/eli-osherovich\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/eli-osherovich\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/eli-osherovich\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/eli-osherovich\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/eli-osherovich\/orgs","repos_url":"https:\/\/api.github.com\/users\/eli-osherovich\/repos","events_url":"https:\/\/api.github.com\/users\/eli-osherovich\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/eli-osherovich\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-04-16T11:48:10Z","updated_at":"2023-04-20T15:30:51Z","closed_at":"2023-04-20T15:30:51Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nIt seems that `~` is not recognized correctly in local paths. Whenever I try to use it I get an exception\n\n### Steps to reproduce the bug\n\n```python\r\n\r\nload_dataset(\"imagefolder\", data_dir=\"~\/data\/my_dataset\")\r\n\r\n```\r\n\r\nWill generate the following error:\r\n``` \r\nEmptyDatasetError: The directory at \/path\/to\/cwd\/~\/data\/datasets\/clementine_tagged_per_cam doesn't contain any data files\r\n```\n\n### Expected behavior\n\nLoad the dataset.\n\n### Environment info\n\ndatasets==2.11.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5757\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5757\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5756","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5756\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5756\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5756\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5756","id":1669678080,"node_id":"I_kwDODunzps5jhUQA","number":5756,"title":"Calling shuffle on a IterableDataset with streaming=True, gives \"ValueError: cannot reshape array\"","user":{"login":"rohfle","id":21077341,"node_id":"MDQ6VXNlcjIxMDc3MzQx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/21077341?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rohfle","html_url":"https:\/\/github.com\/rohfle","followers_url":"https:\/\/api.github.com\/users\/rohfle\/followers","following_url":"https:\/\/api.github.com\/users\/rohfle\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rohfle\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rohfle\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rohfle\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rohfle\/orgs","repos_url":"https:\/\/api.github.com\/users\/rohfle\/repos","events_url":"https:\/\/api.github.com\/users\/rohfle\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rohfle\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-16T04:59:47Z","updated_at":"2023-04-18T03:40:56Z","closed_at":"2023-04-18T03:40:56Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen calling shuffle on a IterableDataset with streaming=True, I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/administrator\/Documents\/Projects\/huggingface\/jax-diffusers-sprint-consistency-models\/virtualenv\/lib\/python3.10\/site-packages\/datasets\/iterable_dataset.py\", line 937, in __iter__\r\n    for key, example in ex_iterable:\r\n  File \"\/home\/administrator\/Documents\/Projects\/huggingface\/jax-diffusers-sprint-consistency-models\/virtualenv\/lib\/python3.10\/site-packages\/datasets\/iterable_dataset.py\", line 627, in __iter__\r\n    for x in self.ex_iterable:\r\n  File \"\/home\/administrator\/Documents\/Projects\/huggingface\/jax-diffusers-sprint-consistency-models\/virtualenv\/lib\/python3.10\/site-packages\/datasets\/iterable_dataset.py\", line 138, in __iter__\r\n    yield from self.generate_examples_fn(**kwargs_with_shuffled_shards)\r\n  File \"\/home\/administrator\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/mnist\/fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4\/mnist.py\", line 111, in _generate_examples\r\n    images = np.frombuffer(f.read(), dtype=np.uint8).reshape(size, 28, 28)\r\nValueError: cannot reshape array of size 59992 into shape (60000,28,28)\r\n\r\n```\r\nTested with the fashion_mnist and mnist datasets\r\n\r\n### Steps to reproduce the bug\r\n\r\nCode to reproduce\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nSHUFFLE_SEED = 42\r\nSHUFFLE_BUFFER_SIZE = 10_000\r\n\r\ndataset = load_dataset('fashion_mnist', streaming=True).shuffle(seed=SHUFFLE_SEED, buffer_size=SHUFFLE_BUFFER_SIZE)\r\nnext(iter(dataset['train']))\r\n```\r\n\r\n### Expected behavior\r\n\r\nA random item from the dataset and no error\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.11.0\r\n- Platform: Linux-5.15.0-69-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.13.4\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5756\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5756\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5755","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5755\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5755\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5755\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5755","id":1669048438,"node_id":"I_kwDODunzps5je6h2","number":5755,"title":"ImportError: cannot import name 'DeprecatedEnum' from 'datasets.utils.deprecation_utils' ","user":{"login":"fivejjs","id":1405491,"node_id":"MDQ6VXNlcjE0MDU0OTE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1405491?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/fivejjs","html_url":"https:\/\/github.com\/fivejjs","followers_url":"https:\/\/api.github.com\/users\/fivejjs\/followers","following_url":"https:\/\/api.github.com\/users\/fivejjs\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/fivejjs\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/fivejjs\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/fivejjs\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/fivejjs\/orgs","repos_url":"https:\/\/api.github.com\/users\/fivejjs\/repos","events_url":"https:\/\/api.github.com\/users\/fivejjs\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/fivejjs\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-14T23:28:54Z","updated_at":"2023-04-14T23:36:19Z","closed_at":"2023-04-14T23:36:19Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThe module moved to new place?\n\n### Steps to reproduce the bug\n\nin the import step,\r\n\r\n```python\r\nfrom datasets.utils.deprecation_utils import DeprecatedEnum\r\n```\r\n\r\nerror:\r\n\r\n```\r\nImportError: cannot import name 'DeprecatedEnum' from 'datasets.utils.deprecation_utils' \r\n```\n\n### Expected behavior\n\nimport successfully\n\n### Environment info\n\npython==3.9.16\r\ndatasets==1.18.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5755\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5755\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5754","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5754\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5754\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5754\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5754","id":1668755035,"node_id":"PR_kwDODunzps5OWozh","number":5754,"title":"Minor tqdm fixes","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-14T18:15:14Z","updated_at":"2023-04-20T15:27:58Z","closed_at":"2023-04-20T15:21:00Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"`GeneratorBasedBuilder`'s TQDM bars were not used as context managers. This PR fixes that (missed these bars in https:\/\/github.com\/huggingface\/datasets\/pull\/5560). \r\n\r\nAlso, this PR modifies the single-proc `save_to_disk` to fix the issue with the TQDM bar not accumulating the progress in the multi-shard setting (again, this bug was introduced by me in the linked PR \ud83d\ude0e)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5754\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5754\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5754","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5754","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5754.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5754.patch","merged_at":"2023-04-20T15:21:00Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5753","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5753\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5753\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5753\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5753","id":1668659536,"node_id":"I_kwDODunzps5jdblQ","number":5753,"title":"[IterableDatasets] Add column followed by interleave datasets gives bogus outputs","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-14T17:32:31Z","updated_at":"2023-04-14T17:45:52Z","closed_at":"2023-04-14T17:36:37Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nIf we add a new column to our iterable dataset using the hack described in #5752, when we then interleave datasets the new column is pinned to one value.\r\n\r\n### Steps to reproduce the bug\r\n\r\nWhat we're going to do here is:\r\n1. Load an iterable dataset in streaming mode (`original_dataset`)\r\n2. Add a new column to this dataset using the hack in #5752 (`modified_dataset_1`)\r\n3. Create another new dataset by adding a column with the same key but different values (`modified_dataset_2`)\r\n4. Interleave our new datasets (`modified_dataset_1` + `modified_dataset_2`)\r\n5. Check the value of our newly added column (`new_column`)\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# load an iterable dataset\r\noriginal_dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", streaming=True)\r\n\r\n# now add a new column to our streaming dataset using our hack from 5752\r\nname = \"new_column\"\r\ncolumn = [f\"new dataset 1, row {i}\" for i in range(50)]\r\n\r\nnew_features = original_dataset.features.copy()\r\nnew_features[name] = new_features[\"file\"]  #\u00a0I know that \"file\" has the right column type to match our new feature\r\n\r\ndef add_column_fn(example, idx):\r\n    if name in example:\r\n        raise ValueError(f\"Error when adding {name}: column {name} is already in the dataset.\")\r\n    return {name: column[idx]}\r\n\r\nmodified_dataset_1 = original_dataset.map(add_column_fn, with_indices=True, features=new_features)\r\n\r\n# now create a second modified dataset using the same trick\r\ncolumn = [f\"new dataset 2, row {i}\" for i in range(50)]\r\n\r\ndef add_column_fn(example, idx):\r\n    if name in example:\r\n        raise ValueError(f\"Error when adding {name}: column {name} is already in the dataset.\")\r\n    return {name: column[idx]}\r\n\r\nmodified_dataset_2 = original_dataset.map(add_column_fn, with_indices=True, features=new_features)\r\n\r\n# interleave these datasets\r\ninterleaved_dataset = interleave_datasets([modified_dataset_1, modified_dataset_2])\r\n\r\n# now check what the value of the added column is\r\nfor i, sample in enumerate(interleaved_dataset):\r\n    print(sample[\"new_column\"])\r\n    if i == 10:\r\n        break\r\n```\r\n**Print Output:**\r\n```\r\nnew dataset 2, row 0\r\nnew dataset 2, row 0\r\nnew dataset 2, row 1\r\nnew dataset 2, row 1\r\nnew dataset 2, row 2\r\nnew dataset 2, row 2\r\nnew dataset 2, row 3\r\nnew dataset 2, row 3\r\nnew dataset 2, row 4\r\nnew dataset 2, row 4\r\nnew dataset 2, row 5\r\n```\r\nWe see that we only get outputs from our second dataset.\r\n\r\n### Expected behavior\r\n\r\nWe should interleave between dataset 1 and 2 and increase in row value:\r\n```\r\nnew dataset 1, row 0\r\nnew dataset 2, row 0\r\nnew dataset 1, row 1\r\nnew dataset 2, row 1\r\nnew dataset 1, row 2\r\nnew dataset 2, row 2\r\n...\r\n```\r\n\r\n### Environment info\r\n\r\n- datasets version: 2.10.2.dev0\r\n- Platform: Linux-4.19.0-23-cloud-amd64-x86_64-with-glibc2.28\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.13.3\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5753\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5753\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5752","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5752\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5752\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5752\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5752","id":1668574209,"node_id":"I_kwDODunzps5jdGwB","number":5752,"title":"Streaming dataset looses `.feature` method after `.add_column`","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-14T16:39:50Z","updated_at":"2023-04-14T17:46:54Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nAfter appending a new column to a streaming dataset using `.add_column`, we can no longer access the list of dataset features using the `.feature` method.\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import load_dataset\r\n\r\noriginal_dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", streaming=True)\r\nprint(original_dataset.features.keys())\r\n\r\n# now add a new column to our streaming dataset\r\nmodified_dataset = original_dataset.add_column(\"new_column\", [\"some random text\" for _ in range(50)])\r\nprint(modified_dataset.features.keys())\r\n```\r\n**Print Output:**\r\n```\r\ndict_keys(['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'])\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[1], line 8\r\n      6 # now add a new column to our streaming dataset\r\n      7 modified_dataset = original_dataset.add_column(\"new_column\", [\"some random text\" for _ in range(50)])\r\n----> 8 print(modified_dataset.features.keys())\r\n\r\nAttributeError: 'NoneType' object has no attribute 'keys'\r\n```\r\n\r\nWe see that we get the features for the original dataset, but not the modified one with the added column.\r\n\n\n### Expected behavior\n\nFeatures should be persevered after adding a new column, i.e. calling:\r\n```python\r\nprint(modified_dataset.features.keys())\r\n```\r\n\r\nShould return:\r\n```\r\ndict_keys(['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id', 'new_column'])\r\n```\n\n### Environment info\n\n- `datasets` version: 2.10.2.dev0\r\n- Platform: Linux-4.19.0-23-cloud-amd64-x86_64-with-glibc2.28\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.13.3\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.2","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5752\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5752\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5751","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5751\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5751\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5751\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5751","id":1668333316,"node_id":"PR_kwDODunzps5OVMuT","number":5751,"title":"Consistent ArrayXD Python formatting + better NumPy\/Pandas formatting","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-14T14:13:59Z","updated_at":"2023-04-20T14:43:20Z","closed_at":"2023-04-20T14:40:34Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Return a list of lists instead of a list of NumPy arrays when converting the variable-shaped `ArrayXD` to Python. Additionally, improve the NumPy conversion by returning a numeric NumPy array when the offsets are equal or a NumPy object array when they aren't, and allow converting the variable-shaped `ArrayXD` to Pandas.\r\n\r\n(Reported in https:\/\/github.com\/huggingface\/datasets\/issues\/5719#issuecomment-1507579671)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5751\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5751\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5751","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5751","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5751.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5751.patch","merged_at":"2023-04-20T14:40:34Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5750","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5750\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5750\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5750\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5750","id":1668289067,"node_id":"I_kwDODunzps5jcBIr","number":5750,"title":"Fail to create datasets from a generator when using Google Big Query","user":{"login":"ivanprado","id":895720,"node_id":"MDQ6VXNlcjg5NTcyMA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/895720?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ivanprado","html_url":"https:\/\/github.com\/ivanprado","followers_url":"https:\/\/api.github.com\/users\/ivanprado\/followers","following_url":"https:\/\/api.github.com\/users\/ivanprado\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ivanprado\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ivanprado\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ivanprado\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ivanprado\/orgs","repos_url":"https:\/\/api.github.com\/users\/ivanprado\/repos","events_url":"https:\/\/api.github.com\/users\/ivanprado\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ivanprado\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-14T13:50:59Z","updated_at":"2023-04-17T12:20:43Z","closed_at":"2023-04-17T12:20:43Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nCreating a dataset from a generator using `Dataset.from_generator()` fails if the generator is the [Google Big Query Python client](https:\/\/cloud.google.com\/python\/docs\/reference\/bigquery\/latest). The problem is that the Big Query client is not pickable. And the function `create_config_id` tries to get a hash of the generator by pickling it. So the following error is generated:\r\n\r\n```\r\n_pickle.PicklingError: Pickling client objects is explicitly not supported.\r\nClients have non-trivial state that is local and unpickleable.\r\n```\r\n\n\n### Steps to reproduce the bug\n\n1. Install the big query client and datasets `pip install google-cloud-bigquery datasets`\r\n2. Run the following code:\r\n\r\n```py\r\nfrom datasets import Dataset\r\nfrom google.cloud import bigquery\r\n\r\nclient = bigquery.Client()\r\n\r\n# Perform a query.\r\nQUERY = (\r\n    'SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013` '\r\n    'WHERE state = \"TX\" '\r\n    'LIMIT 100')\r\nquery_job = client.query(QUERY)  # API request\r\nrows = query_job.result()  # Waits for query to finish\r\n\r\nds = Dataset.from_generator(rows)\r\n\r\nfor r in ds:\r\n    print(r)\r\n```\n\n### Expected behavior\n\nTwo options:\r\n1. Ignore the pickle errors when computing the hash\r\n2. Provide a scape hutch so that we can avoid calculating the hash for the generator. For example, allowing to provide a hash from the user. \n\n### Environment info\n\npython 3.9\r\ngoogle-cloud-bigquery         3.9.0\r\ndatasets                      2.11.0\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5750\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5750\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5749","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5749\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5749\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5749\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5749","id":1668016321,"node_id":"I_kwDODunzps5ja-jB","number":5749,"title":"AttributeError: 'Version' object has no attribute 'match'","user":{"login":"gulnaz-zh","id":54584290,"node_id":"MDQ6VXNlcjU0NTg0Mjkw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/54584290?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/gulnaz-zh","html_url":"https:\/\/github.com\/gulnaz-zh","followers_url":"https:\/\/api.github.com\/users\/gulnaz-zh\/followers","following_url":"https:\/\/api.github.com\/users\/gulnaz-zh\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/gulnaz-zh\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/gulnaz-zh\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/gulnaz-zh\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/gulnaz-zh\/orgs","repos_url":"https:\/\/api.github.com\/users\/gulnaz-zh\/repos","events_url":"https:\/\/api.github.com\/users\/gulnaz-zh\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/gulnaz-zh\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":8,"created_at":"2023-04-14T10:48:06Z","updated_at":"2023-06-30T11:31:17Z","closed_at":"2023-04-18T12:57:08Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen I run \r\nfrom datasets import load_dataset\r\ndata = load_dataset(\"visual_genome\", 'region_descriptions_v1.2.0')\r\n\r\nAttributeError: 'Version' object has no attribute 'match'\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\r\ndata = load_dataset(\"visual_genome\", 'region_descriptions_v1.2.0')\n\n### Expected behavior\n\nThis is error trace:\r\nDownloading and preparing dataset visual_genome\/region_descriptions_v1.2.0 to C:\/Users\/Acer\/.cache\/huggingface\/datasets\/visual_genome\/region_descriptions_v1.2.0\/1.2.0\/136fe5b83f6691884566c5530313288171e053a3b33bfe3ea2e4c8b39abaf7f3...\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[6], line 1\r\n----> 1 data = load_dataset(\"visual_genome\", 'region_descriptions_v1.2.0')\r\n\r\nFile ~\\.conda\\envs\\aai\\Lib\\site-packages\\datasets\\load.py:1791, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   1788 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\r\n   1790 # Download and prepare data\r\n-> 1791 builder_instance.download_and_prepare(\r\n   1792     download_config=download_config,\r\n   1793     download_mode=download_mode,\r\n   1794     verification_mode=verification_mode,\r\n   1795     try_from_hf_gcs=try_from_hf_gcs,\r\n   1796     num_proc=num_proc,\r\n   1797     storage_options=storage_options,\r\n   1798 )\r\n   1800 # Build dataset for splits\r\n   1801 keep_in_memory = (\r\n   1802     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   1803 )\r\n\r\nFile ~\\.conda\\envs\\aai\\Lib\\site-packages\\datasets\\builder.py:891, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    889     if num_proc is not None:\r\n    890         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 891     self._download_and_prepare(\r\n    892         dl_manager=dl_manager,\r\n    893         verification_mode=verification_mode,\r\n    894         **prepare_split_kwargs,\r\n    895         **download_and_prepare_kwargs,\r\n    896     )\r\n    897 # Sync info\r\n    898 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~\\.conda\\envs\\aai\\Lib\\site-packages\\datasets\\builder.py:1651, in GeneratorBasedBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_splits_kwargs)\r\n   1650 def _download_and_prepare(self, dl_manager, verification_mode, **prepare_splits_kwargs):\r\n-> 1651     super()._download_and_prepare(\r\n   1652         dl_manager,\r\n   1653         verification_mode,\r\n   1654         check_duplicate_keys=verification_mode == VerificationMode.BASIC_CHECKS\r\n   1655         or verification_mode == VerificationMode.ALL_CHECKS,\r\n   1656         **prepare_splits_kwargs,\r\n   1657     )\r\n\r\nFile ~\\.conda\\envs\\aai\\Lib\\site-packages\\datasets\\builder.py:964, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n    962 split_dict = SplitDict(dataset_name=self.name)\r\n    963 split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 964 split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    966 # Checksums verification\r\n    967 if verification_mode == VerificationMode.ALL_CHECKS and dl_manager.record_checksums:\r\n\r\nFile ~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\visual_genome\\136fe5b83f6691884566c5530313288171e053a3b33bfe3ea2e4c8b39abaf7f3\\visual_genome.py:377, in VisualGenome._split_generators(self, dl_manager)\r\n    375 def _split_generators(self, dl_manager):\r\n    376     # Download image meta datas.\r\n--> 377     image_metadatas_dir = dl_manager.download_and_extract(self.config.image_metadata_url)\r\n    378     image_metadatas_file = os.path.join(\r\n    379         image_metadatas_dir, _get_decompressed_filename_from_url(self.config.image_metadata_url)\r\n    380     )\r\n    382     # Download annotations\r\n\r\nFile ~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\visual_genome\\136fe5b83f6691884566c5530313288171e053a3b33bfe3ea2e4c8b39abaf7f3\\visual_genome.py:328, in VisualGenomeConfig.image_metadata_url(self)\r\n    326 @property\r\n    327 def image_metadata_url(self):\r\n--> 328     if not self.version.match(_LATEST_VERSIONS[\"image_metadata\"]):\r\n    329         logger.warning(\r\n    330             f\"Latest image metadata version is {_LATEST_VERSIONS['image_metadata']}. Trying to generate a dataset of version: {self.version}. Please double check that image data are unchanged between the two versions.\"\r\n    331         )\r\n    332     return f\"{_BASE_ANNOTATION_URL}\/image_data.json.zip\"\n\n### Environment info\n\ndatasets                  2.11.0\r\npython                    3.11.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5749\/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5749\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5748","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5748\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5748\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5748\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5748","id":1667517024,"node_id":"PR_kwDODunzps5OSgNH","number":5748,"title":"[BUG FIX] Issue 5739","user":{"login":"ericxsun","id":1772912,"node_id":"MDQ6VXNlcjE3NzI5MTI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1772912?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ericxsun","html_url":"https:\/\/github.com\/ericxsun","followers_url":"https:\/\/api.github.com\/users\/ericxsun\/followers","following_url":"https:\/\/api.github.com\/users\/ericxsun\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ericxsun\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ericxsun\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ericxsun\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ericxsun\/orgs","repos_url":"https:\/\/api.github.com\/users\/ericxsun\/repos","events_url":"https:\/\/api.github.com\/users\/ericxsun\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ericxsun\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-04-14T05:07:31Z","updated_at":"2023-04-14T05:07:31Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"A fix for https:\/\/github.com\/huggingface\/datasets\/issues\/5739","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5748\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5748\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5748","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5748","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5748.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5748.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5747","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5747\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5747\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5747\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5747","id":1667270412,"node_id":"PR_kwDODunzps5ORtBF","number":5747,"title":"[WIP] Add Dataset.to_spark","user":{"login":"maddiedawson","id":106995444,"node_id":"U_kgDOBmCe9A","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/106995444?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/maddiedawson","html_url":"https:\/\/github.com\/maddiedawson","followers_url":"https:\/\/api.github.com\/users\/maddiedawson\/followers","following_url":"https:\/\/api.github.com\/users\/maddiedawson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/maddiedawson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/maddiedawson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/maddiedawson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/maddiedawson\/orgs","repos_url":"https:\/\/api.github.com\/users\/maddiedawson\/repos","events_url":"https:\/\/api.github.com\/users\/maddiedawson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/maddiedawson\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-04-13T23:20:03Z","updated_at":"2023-05-05T12:31:10Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5747\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5747\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5747","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5747","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5747.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5747.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5746","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5746\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5746\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5746\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5746","id":1667102459,"node_id":"PR_kwDODunzps5ORIUU","number":5746,"title":"Fix link in docs","user":{"login":"bbbxyz","id":7485661,"node_id":"MDQ6VXNlcjc0ODU2NjE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7485661?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/bbbxyz","html_url":"https:\/\/github.com\/bbbxyz","followers_url":"https:\/\/api.github.com\/users\/bbbxyz\/followers","following_url":"https:\/\/api.github.com\/users\/bbbxyz\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/bbbxyz\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/bbbxyz\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/bbbxyz\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/bbbxyz\/orgs","repos_url":"https:\/\/api.github.com\/users\/bbbxyz\/repos","events_url":"https:\/\/api.github.com\/users\/bbbxyz\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/bbbxyz\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-13T20:45:19Z","updated_at":"2023-04-14T13:15:38Z","closed_at":"2023-04-14T13:08:42Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fixes a broken link in the use_with_pytorch docs","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5746\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5746\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5746","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5746","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5746.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5746.patch","merged_at":"2023-04-14T13:08:42Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5745","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5745\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5745\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5745\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5745","id":1667086143,"node_id":"PR_kwDODunzps5ORE2n","number":5745,"title":"[BUG FIX] Issue 5744","user":{"login":"keyboardAnt","id":15572698,"node_id":"MDQ6VXNlcjE1NTcyNjk4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/15572698?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/keyboardAnt","html_url":"https:\/\/github.com\/keyboardAnt","followers_url":"https:\/\/api.github.com\/users\/keyboardAnt\/followers","following_url":"https:\/\/api.github.com\/users\/keyboardAnt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/keyboardAnt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/keyboardAnt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/keyboardAnt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/keyboardAnt\/orgs","repos_url":"https:\/\/api.github.com\/users\/keyboardAnt\/repos","events_url":"https:\/\/api.github.com\/users\/keyboardAnt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/keyboardAnt\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-13T20:29:55Z","updated_at":"2023-04-21T15:22:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"A temporal fix for https:\/\/github.com\/huggingface\/datasets\/issues\/5744.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5745\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5745\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5745","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5745","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5745.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5745.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5744","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5744\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5744\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5744\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5744","id":1667076620,"node_id":"I_kwDODunzps5jXZIM","number":5744,"title":"[BUG] With Pandas 2.0.0, `load_dataset` raises `TypeError: read_csv() got an unexpected keyword argument 'mangle_dupe_cols'`","user":{"login":"keyboardAnt","id":15572698,"node_id":"MDQ6VXNlcjE1NTcyNjk4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/15572698?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/keyboardAnt","html_url":"https:\/\/github.com\/keyboardAnt","followers_url":"https:\/\/api.github.com\/users\/keyboardAnt\/followers","following_url":"https:\/\/api.github.com\/users\/keyboardAnt\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/keyboardAnt\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/keyboardAnt\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/keyboardAnt\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/keyboardAnt\/orgs","repos_url":"https:\/\/api.github.com\/users\/keyboardAnt\/repos","events_url":"https:\/\/api.github.com\/users\/keyboardAnt\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/keyboardAnt\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-04-13T20:21:28Z","updated_at":"2023-07-06T17:01:59Z","closed_at":"2023-07-06T17:01:59Z","author_association":"NONE","active_lock_reason":null,"body":"The `load_dataset` function with Pandas `1.5.3` has no issue (just a FutureWarning) but crashes with Pandas `2.0.0`.\r\nFor your convenience, I opened a draft Pull Request to fix it quickly: https:\/\/github.com\/huggingface\/datasets\/pull\/5745\r\n\r\n---\r\n\r\n* The FutureWarning mentioned above: \r\n```\r\nFutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5744\/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5744\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5743","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5743\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5743\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5743\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5743","id":1666843832,"node_id":"I_kwDODunzps5jWgS4","number":5743,"title":"dataclass.py in virtual environment is   overriding the stdlib module \"dataclasses\"","user":{"login":"syedabdullahhassan","id":71216295,"node_id":"MDQ6VXNlcjcxMjE2Mjk1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/71216295?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/syedabdullahhassan","html_url":"https:\/\/github.com\/syedabdullahhassan","followers_url":"https:\/\/api.github.com\/users\/syedabdullahhassan\/followers","following_url":"https:\/\/api.github.com\/users\/syedabdullahhassan\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/syedabdullahhassan\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/syedabdullahhassan\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/syedabdullahhassan\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/syedabdullahhassan\/orgs","repos_url":"https:\/\/api.github.com\/users\/syedabdullahhassan\/repos","events_url":"https:\/\/api.github.com\/users\/syedabdullahhassan\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/syedabdullahhassan\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-13T17:28:33Z","updated_at":"2023-04-17T12:23:18Z","closed_at":"2023-04-17T12:23:18Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n\"e:\\Krish_naik\\FSDSRegression\\venv\\Lib\\dataclasses.py\" is overriding the stdlib module \"dataclasses\"\n\n### Steps to reproduce the bug\n\nmodule issue\n\n### Expected behavior\n\n overriding the stdlib module \"dataclasses\"\n\n### Environment info\n\nVS code","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5743\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5743\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5742","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5742\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5742\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5742\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5742","id":1666209738,"node_id":"PR_kwDODunzps5OOH-W","number":5742,"title":"Warning specifying future change in to_tf_dataset behaviour","user":{"login":"amyeroberts","id":22614925,"node_id":"MDQ6VXNlcjIyNjE0OTI1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22614925?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/amyeroberts","html_url":"https:\/\/github.com\/amyeroberts","followers_url":"https:\/\/api.github.com\/users\/amyeroberts\/followers","following_url":"https:\/\/api.github.com\/users\/amyeroberts\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/amyeroberts\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/amyeroberts\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/amyeroberts\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/amyeroberts\/orgs","repos_url":"https:\/\/api.github.com\/users\/amyeroberts\/repos","events_url":"https:\/\/api.github.com\/users\/amyeroberts\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/amyeroberts\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-13T11:10:00Z","updated_at":"2023-04-21T13:18:14Z","closed_at":"2023-04-21T13:11:09Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Warning specifying future changes happening to `to_tf_dataset` behaviour when #5602 is merged in","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5742\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5742\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5742","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5742","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5742.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5742.patch","merged_at":"2023-04-21T13:11:09Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5741","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5741\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5741\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5741\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5741","id":1665860919,"node_id":"PR_kwDODunzps5OM9nZ","number":5741,"title":"Fix CI warnings","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-13T07:17:02Z","updated_at":"2023-04-13T09:48:10Z","closed_at":"2023-04-13T09:40:50Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix warnings in our CI tests.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5741\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5741\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5741","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5741","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5741.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5741.patch","merged_at":"2023-04-13T09:40:50Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5740","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5740\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5740\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5740\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5740","id":1664132130,"node_id":"PR_kwDODunzps5OHI08","number":5740,"title":"Fix CI mock filesystem fixtures","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-04-12T08:52:35Z","updated_at":"2023-04-13T11:01:24Z","closed_at":"2023-04-13T10:54:13Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR fixes the fixtures of our CI mock filesystems.\r\n\r\nBefore, we had to pass `clobber=True` to `fsspec.register_implementation` to overwrite the still present previously added \"mock\" filesystem. That meant that the mock filesystem fixture was not working properly, because the previously added \"mock\" filesystem, should have been deleted by the fixture.\r\n\r\nThis PR fixes the mock filesystem fixtures, so that the \"mock\" filesystem is properly deleted from the inner `fsspec` registry.\r\n\r\nTests were added to check the correct behavior of the mock filesystem fixtures.\r\n\r\nRelated to:\r\n- #5733","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5740\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5740\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5740","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5740","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5740.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5740.patch","merged_at":"2023-04-13T10:54:13Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5739","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5739\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5739\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5739\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5739","id":1663762901,"node_id":"I_kwDODunzps5jKwHV","number":5739,"title":"weird result during dataset split when data path starts with `\/data`","user":{"login":"ericxsun","id":1772912,"node_id":"MDQ6VXNlcjE3NzI5MTI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1772912?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ericxsun","html_url":"https:\/\/github.com\/ericxsun","followers_url":"https:\/\/api.github.com\/users\/ericxsun\/followers","following_url":"https:\/\/api.github.com\/users\/ericxsun\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ericxsun\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ericxsun\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ericxsun\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ericxsun\/orgs","repos_url":"https:\/\/api.github.com\/users\/ericxsun\/repos","events_url":"https:\/\/api.github.com\/users\/ericxsun\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ericxsun\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-12T04:51:35Z","updated_at":"2023-04-21T14:20:59Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nThe regex defined here https:\/\/github.com\/huggingface\/datasets\/blob\/f2607935c4e45c70c44fcb698db0363ca7ba83d4\/src\/datasets\/utils\/py_utils.py#L158 \r\n\r\nwill cause a weird result during dataset split when data path starts with `\/data`\n\n### Steps to reproduce the bug\n\n1.  clone dataset into local path\r\n```\r\ncd \/data\/train\/raw\/\r\ngit lfs clone  https:\/\/huggingface.co\/datasets\/deepmind\/code_contests.git\r\n\r\nls \/data\/train\/raw\/code_contests\r\n# README.md  data  dataset_infos.json\r\n\r\nls \/data\/train\/raw\/code_contests\/data\r\n# test-00000-of-00001-9c49eeff30aacaa8.parquet\r\n# train-[0-9]+-of-[0-9]+-xx.parquet\r\n# valid-00000-of-00001-5e672c5751f060d3.parquet\r\n\r\n```\r\n\r\n2. loading data from local \r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('\/data\/train\/raw\/code_contests')\r\n\r\nFileNotFoundError: Unable to resolve any data file that matches '['data\/train\/raw\/code_contests\/data\/train-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9]*.*']' at \/data\/train\/raw\/code_contests with any supported extension\r\n```\r\n\r\nweird path `data\/train\/raw\/code_contests\/data\/train-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9]*.*`\r\n\r\nWhile dive deep into `LocalDatasetModuleFactoryWithoutScript` defined in [load.py](https:\/\/github.com\/huggingface\/datasets\/blob\/f2607935c4e45c70c44fcb698db0363ca7ba83d4\/src\/datasets\/load.py#L627) and _get_data_files_patterns https:\/\/github.com\/huggingface\/datasets\/blob\/f2607935c4e45c70c44fcb698db0363ca7ba83d4\/src\/datasets\/data_files.py#L228. I found the weird behavior caused by `string_to_dict`\r\n\r\n3. check `string_to_dict`\r\n```\r\np = '\/data\/train\/raw\/code_contests\/data\/test-00000-of-00001-9c49eeff30aacaa8.parquet'\r\nsplit_pattern = 'data\/{split}-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9]*.*'\r\n\r\nstring_to_dict(p, split_pattern)\r\n# {'split': 'train\/raw\/code_contests\/data\/test'}\r\n\r\np = '\/data2\/train\/raw\/code_contests\/data\/test-00000-of-00001-9c49eeff30aacaa8.parquet'\r\nstring_to_dict(p, split_pattern)\r\n{'split': 'test'}\r\n```\r\n\r\ngo deep into string_to_dict https:\/\/github.com\/huggingface\/datasets\/blob\/f2607935c4e45c70c44fcb698db0363ca7ba83d4\/src\/datasets\/utils\/py_utils.py#L158. \r\n\r\n4. test the regex:\r\n<img width=\"680\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/1772912\/231351129-75179f01-fb9f-4f12-8fa9-0dfcc3d5f3bd.png\">\r\n\r\n<img width=\"679\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/1772912\/231351025-009f3d83-2cf3-4e15-9ed4-6b9663dcb2ee.png\">\r\n\r\n\r\n\n\n### Expected behavior\n\nstatement in `steps to reproduce the bug`\r\n\r\n3. check `string_to_dict`\r\n```\r\np = '\/data\/train\/raw\/code_contests\/data\/test-00000-of-00001-9c49eeff30aacaa8.parquet'\r\nsplit_pattern = 'data\/{split}-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9]*.*'\r\n\r\nstring_to_dict(p, split_pattern)\r\n# {'split': 'train\/raw\/code_contests\/data\/test'}\r\n\r\np = '\/data2\/train\/raw\/code_contests\/data\/test-00000-of-00001-9c49eeff30aacaa8.parquet'\r\nstring_to_dict(p, split_pattern)\r\n{'split': 'test'}\r\n```\r\n\n\n### Environment info\n\n- linux(debian)\r\n- python 3.7\r\n- datasets 2.8.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5739\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5739\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5738","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5738\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5738\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5738\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5738","id":1663477690,"node_id":"I_kwDODunzps5jJqe6","number":5738,"title":"load_dataset(\"text\",\"dataset.txt\") loads the wrong dataset!","user":{"login":"Tylersuard","id":41713505,"node_id":"MDQ6VXNlcjQxNzEzNTA1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/41713505?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Tylersuard","html_url":"https:\/\/github.com\/Tylersuard","followers_url":"https:\/\/api.github.com\/users\/Tylersuard\/followers","following_url":"https:\/\/api.github.com\/users\/Tylersuard\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Tylersuard\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Tylersuard\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Tylersuard\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Tylersuard\/orgs","repos_url":"https:\/\/api.github.com\/users\/Tylersuard\/repos","events_url":"https:\/\/api.github.com\/users\/Tylersuard\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Tylersuard\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-12T01:07:46Z","updated_at":"2023-04-19T12:08:27Z","closed_at":"2023-04-19T12:08:27Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am trying to load my own custom text dataset using the load_dataset function.  My dataset is a bunch of ordered text, think along the lines of shakespeare plays.  However, after I load the dataset and I inspect it, the dataset is a table with a bunch of latitude and longitude values!  What in the world??\n\n### Steps to reproduce the bug\n\nmy_dataset = load_dataset(\"text\",\"TextFile.txt\")\r\nmy_dataset\n\n### Expected behavior\n\nI expected the dataset to contain the actual data from the text document that I used.\n\n### Environment info\n\nGoogle Colab","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5738\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5738\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5737","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5737\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5737\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5737\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5737","id":1662919811,"node_id":"I_kwDODunzps5jHiSD","number":5737,"title":"ClassLabel Error ","user":{"login":"mrcaelumn","id":10896776,"node_id":"MDQ6VXNlcjEwODk2Nzc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10896776?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mrcaelumn","html_url":"https:\/\/github.com\/mrcaelumn","followers_url":"https:\/\/api.github.com\/users\/mrcaelumn\/followers","following_url":"https:\/\/api.github.com\/users\/mrcaelumn\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mrcaelumn\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mrcaelumn\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mrcaelumn\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mrcaelumn\/orgs","repos_url":"https:\/\/api.github.com\/users\/mrcaelumn\/repos","events_url":"https:\/\/api.github.com\/users\/mrcaelumn\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mrcaelumn\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-11T17:14:13Z","updated_at":"2023-04-13T16:49:57Z","closed_at":"2023-04-13T16:49:57Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI still getting the error \"call() takes 1 positional argument but 2 were given\" even after ensuring that the value being passed to the label object is a single value and that the ClassLabel object has been created with the correct number of label classes\r\n\r\n### Steps to reproduce the bug\r\n\r\nfrom datasets import ClassLabel, Dataset\r\n\r\n1. Create the ClassLabel object with 3 label values and their corresponding names\r\nlabel_test = ClassLabel(num_classes=3, names=[\"label_1\", \"label_2\", \"label_3\"])\r\n\r\n2. Define a dictionary with text and label fields\r\ndata = {\r\n    'text': ['text_1', 'text_2', 'text_3'],\r\n    'label': [1, 2, 3],\r\n}\r\n\r\n3. Create a Hugging Face dataset from the dictionary\r\ndataset = Dataset.from_dict(data)\r\nprint(dataset.features)\r\n4. Map the label values to their corresponding label names using the label object\r\ndataset = dataset.map(lambda example: {'text': example['text'], 'label': label_test(example['label'])})\r\n\r\n5. Print the resulting dataset\r\nprint(dataset)\r\n\r\n### Expected behavior\r\n\r\nI hope my label type is class label instead int. \r\n\r\n### Environment info\r\n\r\npython 3.9\r\ngoogle colab","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5737\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5737\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5736","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5736\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5736\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5736\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5736","id":1662286061,"node_id":"I_kwDODunzps5jFHjt","number":5736,"title":"FORCE_REDOWNLOAD raises \"Directory not empty\" exception on second run","user":{"login":"rcasero","id":1219084,"node_id":"MDQ6VXNlcjEyMTkwODQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1219084?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rcasero","html_url":"https:\/\/github.com\/rcasero","followers_url":"https:\/\/api.github.com\/users\/rcasero\/followers","following_url":"https:\/\/api.github.com\/users\/rcasero\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rcasero\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rcasero\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rcasero\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rcasero\/orgs","repos_url":"https:\/\/api.github.com\/users\/rcasero\/repos","events_url":"https:\/\/api.github.com\/users\/rcasero\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rcasero\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-11T11:29:15Z","updated_at":"2023-11-30T07:16:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nRunning `load_dataset(..., download_mode=datasets.DownloadMode.FORCE_REDOWNLOAD)` twice raises a `Directory not empty` exception on the second run.\n\n### Steps to reproduce the bug\n\nI cannot test this on datasets v2.11.0 due to #5711, but this happens in v2.10.1.\r\n\r\n1. Set up a script `my_dataset.py` to generate and load an offline dataset.\r\n2. Load it with \r\n    ```python\r\n            ds = datasets.load_dataset(path=\/path\/to\/my_dataset.py,\r\n                                       name='toy',\r\n                                       data_dir=\/path\/to\/my_dataset.py,\r\n                                       cache_dir=cache_dir,\r\n                                       download_mode=datasets.DownloadMode.FORCE_REDOWNLOAD,\r\n                                       )\r\n    ```\r\n    It loads fine\r\n    ```\r\n    Dataset my_dataset downloaded and prepared to \/path\/to\/cache\/toy-..e05e\/1.0.0\/...5b4c. Subsequent calls will reuse this data.\r\n    ```\r\n3. Try to load it again with the same snippet and the splits are generated, but at the end of the loading process it raises the error\r\n    ```\r\n    2023-04-11 12:10:19,965: DEBUG:\topen file: \/path\/to\/cache\/toy-..e05e\/1.0.0\/...5b4c.incomplete\/dataset_info.json\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 2, in <module>\r\n      File \"\/path\/to\/conda\/environment\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1782, in load_dataset\r\n        builder_instance.download_and_prepare(\r\n      File \"\/path\/to\/conda\/environment\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 852, in download_and_prepare\r\n        with incomplete_dir(self._output_dir) as tmp_output_dir:\r\n      File \"\/path\/to\/conda\/environment\/lib\/python3.10\/contextlib.py\", line 142, in __exit__\r\n        next(self.gen)\r\n      File \"\/path\/to\/conda\/environment\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 826, in incomplete_dir\r\n        shutil.rmtree(dirname)\r\n      File \"\/path\/to\/conda\/environment\/lib\/python3.10\/shutil.py\", line 730, in rmtree\r\n        onerror(os.rmdir, path, sys.exc_info())\r\n      File \"\/path\/to\/conda\/environment\/lib\/python3.10\/shutil.py\", line 728, in rmtree\r\n        os.rmdir(path)\r\n    OSError: [Errno 39] Directory not empty: '\/path\/to\/cache\/toy-..e05e\/1.0.0\/...5b4c'\r\n    ```\r\n\n\n### Expected behavior\n\nRegenerate the dataset from scratch and reload it.\n\n### Environment info\n\n- `datasets` version: 2.10.1\r\n- Platform: Linux-4.18.0-483.el8.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.10.8\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.2\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5736\/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5736\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5735","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5735\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5735\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5735\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5735","id":1662150903,"node_id":"PR_kwDODunzps5OAY3A","number":5735,"title":"Implement sharding on merged iterable datasets","user":{"login":"Hubert-Bonisseur","id":48770768,"node_id":"MDQ6VXNlcjQ4NzcwNzY4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/48770768?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur","html_url":"https:\/\/github.com\/Hubert-Bonisseur","followers_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/followers","following_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/orgs","repos_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/repos","events_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Hubert-Bonisseur\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2023-04-11T10:02:25Z","updated_at":"2023-04-27T16:39:04Z","closed_at":"2023-04-27T16:32:09Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This PR allows sharding of merged iterable datasets.\r\n\r\nMerged iterable datasets with for instance the `interleave_datasets` command are comprised of multiple sub-iterable, one for each dataset that has been merged.\r\n\r\nWith this PR, sharding a merged iterable will result in multiple merged datasets each comprised of sharded sub-iterable, ensuring that there is no duplication of data.\r\n\r\nAs a result it is now possible to set any amount of workers in the dataloader as long as it is lower or equal to the lowest amount of shards amongst the datasets. Before it had to be set to 0.\r\n\r\nI previously talked about this issue on the forum [here](https:\/\/discuss.huggingface.co\/t\/interleaving-iterable-dataset-with-num-workers-0\/35801) ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5735\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5735\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5735","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5735","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5735.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5735.patch","merged_at":"2023-04-27T16:32:09Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5734","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5734\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5734\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5734\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5734","id":1662058028,"node_id":"I_kwDODunzps5jEP4s","number":5734,"title":"Remove temporary pin of fsspec","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-04-11T09:04:17Z","updated_at":"2023-04-11T11:04:52Z","closed_at":"2023-04-11T11:04:52Z","author_association":"MEMBER","active_lock_reason":null,"body":"Once root cause is found and fixed, remove the temporary pin introduced by:\r\n- #5731","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5734\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5734\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5733","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5733\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5733\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5733\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5733","id":1662039191,"node_id":"PR_kwDODunzps5OAA04","number":5733,"title":"Unpin fsspec","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-11T08:52:12Z","updated_at":"2023-04-11T11:11:45Z","closed_at":"2023-04-11T11:04:51Z","author_association":"MEMBER","active_lock_reason":null,"body":"In `fsspec--2023.4.0` default value for clobber when registering an implementation was changed from True to False. See:\r\n- https:\/\/github.com\/fsspec\/filesystem_spec\/pull\/1237\r\n\r\nThis PR recovers previous behavior by passing clobber True when registering mock implementations.\r\n\r\nThis PR also removes the temporary pin introduced by:\r\n- #5731\r\n\r\nFix #5734.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5733\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5733\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5733","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5733","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5733.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5733.patch","merged_at":"2023-04-11T11:04:51Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5732","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5732\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5732\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5732\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5732","id":1662020571,"node_id":"I_kwDODunzps5jEGvb","number":5732,"title":"Enwik8 should support the standard split","user":{"login":"lucaslingle","id":10287371,"node_id":"MDQ6VXNlcjEwMjg3Mzcx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10287371?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lucaslingle","html_url":"https:\/\/github.com\/lucaslingle","followers_url":"https:\/\/api.github.com\/users\/lucaslingle\/followers","following_url":"https:\/\/api.github.com\/users\/lucaslingle\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lucaslingle\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lucaslingle\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lucaslingle\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lucaslingle\/orgs","repos_url":"https:\/\/api.github.com\/users\/lucaslingle\/repos","events_url":"https:\/\/api.github.com\/users\/lucaslingle\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lucaslingle\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":{"login":"lucaslingle","id":10287371,"node_id":"MDQ6VXNlcjEwMjg3Mzcx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10287371?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lucaslingle","html_url":"https:\/\/github.com\/lucaslingle","followers_url":"https:\/\/api.github.com\/users\/lucaslingle\/followers","following_url":"https:\/\/api.github.com\/users\/lucaslingle\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lucaslingle\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lucaslingle\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lucaslingle\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lucaslingle\/orgs","repos_url":"https:\/\/api.github.com\/users\/lucaslingle\/repos","events_url":"https:\/\/api.github.com\/users\/lucaslingle\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lucaslingle\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lucaslingle","id":10287371,"node_id":"MDQ6VXNlcjEwMjg3Mzcx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10287371?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lucaslingle","html_url":"https:\/\/github.com\/lucaslingle","followers_url":"https:\/\/api.github.com\/users\/lucaslingle\/followers","following_url":"https:\/\/api.github.com\/users\/lucaslingle\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lucaslingle\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lucaslingle\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lucaslingle\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lucaslingle\/orgs","repos_url":"https:\/\/api.github.com\/users\/lucaslingle\/repos","events_url":"https:\/\/api.github.com\/users\/lucaslingle\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lucaslingle\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-04-11T08:38:53Z","updated_at":"2023-04-11T09:28:17Z","closed_at":"2023-04-11T09:28:16Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nThe HuggingFace Datasets library currently supports two BuilderConfigs for Enwik8. One config yields individual lines as examples, while the other config yields the entire dataset as a single example. Both support only a monolithic split: it is all grouped as \"train\". \r\n\r\nThe HuggingFace Datasets library should include a BuilderConfig for Enwik8 with train, validation, and test sets derived from the first 90 million bytes, next 5 million bytes, and last 5 million bytes, respectively. This Enwik8 split is standard practice in LM papers, as elaborated and motivated below. \n\n### Motivation\n\nEnwik8 is commonly split into 90M, 5M, 5M consecutive bytes. This is done in the Transformer-XL [codebase](https:\/\/github.com\/kimiyoung\/transformer-xl\/blob\/44781ed21dbaec88b280f74d9ae2877f52b492a5\/getdata.sh#L34), and is additionally mentioned in the Sparse Transformers [paper](https:\/\/arxiv.org\/abs\/1904.10509) and the Compressive Transformers [paper](https:\/\/arxiv.org\/abs\/1911.05507). This split is pretty much universal among language modeling papers. \r\n\r\nOne may obtain the splits by manual wrangling, using the data yielded by the ```enwik8-raw``` BuilderConfig. However, this undermines the seamless functionality of the library: one must slice the single raw example, extract it into three tensors, and wrap each in a separate dataset. \r\n\r\nThis becomes even more of a nuisance if using the current Enwik8 HuggingFace dataset as a TfdsDataSource with [SeqIO](https:\/\/github.com\/google\/seqio), where a pipeline of preprocessors is typically included in a SeqIO Task definition, to be applied immediately after loading the data with TFDS. \n\n### Your contribution\n\nSupporting this functionality in HuggingFace Datasets will only require an additional BuilderConfig for Enwik8 and a few additional lines of code. I will submit a PR. ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5732\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5732\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5731","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5731\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5731\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5731\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5731","id":1662012913,"node_id":"PR_kwDODunzps5N_7Un","number":5731,"title":"Temporarily pin fsspec","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-11T08:33:15Z","updated_at":"2023-04-11T08:57:45Z","closed_at":"2023-04-11T08:47:55Z","author_association":"MEMBER","active_lock_reason":null,"body":"Fix #5730.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5731\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5731\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5731","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5731","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5731.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5731.patch","merged_at":"2023-04-11T08:47:55Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5730","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5730\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5730\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5730\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5730","id":1662007926,"node_id":"I_kwDODunzps5jEDp2","number":5730,"title":"CI is broken: ValueError: Name (mock) already in the registry and clobber is False","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-04-11T08:29:46Z","updated_at":"2023-04-11T08:47:56Z","closed_at":"2023-04-11T08:47:56Z","author_association":"MEMBER","active_lock_reason":null,"body":"CI is broken for `test_py310`.\r\n\r\nSee: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/4665326892\/jobs\/8258580948\r\n```\r\n=========================== short test summary info ============================\r\nERROR tests\/test_builder.py::test_builder_with_filesystem_download_and_prepare - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_builder.py::test_builder_with_filesystem_download_and_prepare_reload - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_dataset_dict.py::test_dummy_datasetdict_serialize_fs - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_file_utils.py::test_get_from_cache_fsspec - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_filesystem.py::test_is_remote_filesystem - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xexists[tmp_path\/file.txt-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xexists[tmp_path\/file_that_doesnt_exist.txt-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xexists[mock:\/\/top_level\/second_level\/date=2019-10-01\/a.parquet-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xexists[mock:\/\/top_level\/second_level\/date=2019-10-01\/file_that_doesnt_exist.parquet-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xlistdir[tmp_path-expected_paths0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xlistdir[mock:\/\/-expected_paths1] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xlistdir[mock:\/\/top_level-expected_paths2] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xlistdir[mock:\/\/top_level\/second_level\/date=2019-10-01-expected_paths3] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xisdir[tmp_path-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xisdir[tmp_path\/file.txt-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xisdir[mock:\/\/-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xisdir[mock:\/\/top_level-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xisdir[mock:\/\/dir_that_doesnt_exist-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xisfile[tmp_path\/file.txt-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xisfile[tmp_path\/file_that_doesnt_exist.txt-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xisfile[mock:\/\/-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xisfile[mock:\/\/top_level\/second_level\/date=2019-10-01\/a.parquet-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xgetsize[tmp_path\/file.txt-100] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xgetsize[mock:\/\/-0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xgetsize[mock:\/\/top_level\/second_level\/date=2019-10-01\/a.parquet-100] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xglob[tmp_path\/*.txt-expected_paths0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xglob[mock:\/\/*-expected_paths1] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xglob[mock:\/\/top_*-expected_paths2] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xglob[mock:\/\/top_level\/second_level\/date=2019-10-0[1-4]-expected_paths3] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xglob[mock:\/\/top_level\/second_level\/date=2019-10-0[1-4]\/*-expected_paths4] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xwalk[tmp_path-expected_outputs0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::test_xwalk[mock:\/\/top_level\/second_level-expected_outputs1] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_exists[tmp_path\/file.txt-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_exists[tmp_path\/file_that_doesnt_exist.txt-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_exists[mock:\/\/top_level\/second_level\/date=2019-10-01\/a.parquet-True] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_exists[mock:\/\/top_level\/second_level\/date=2019-10-01\/file_that_doesnt_exist.parquet-False] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_glob[tmp_path-*.txt-expected_paths0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_glob[mock:\/\/-*-expected_paths1] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_glob[mock:\/\/-top_*-expected_paths2] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_glob[mock:\/\/top_level\/second_level-date=2019-10-0[1-4]-expected_paths3] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_glob[mock:\/\/top_level\/second_level-date=2019-10-0[1-4]\/*-expected_paths4] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_rglob[tmp_path-*.txt-expected_paths0] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_rglob[mock:\/\/-date=2019-10-0[1-4]-expected_paths1] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_rglob[mock:\/\/top_level-date=2019-10-0[1-4]-expected_paths2] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_rglob[mock:\/\/-date=2019-10-0[1-4]\/*-expected_paths3] - ValueError: Name (mock) already in the registry and clobber is False\r\nERROR tests\/test_streaming_download_manager.py::TestxPath::test_xpath_rglob[mock:\/\/top_level-date=2019-10-0[1-4]\/*-expected_paths4] - ValueError: Name (mock) already in the registry and clobber is False\r\n===== 2105 passed, 18 skipped, 38 warnings, 46 errors in 236.22s (0:03:56) =====\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5730\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5730\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5729","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5729\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5729\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5729\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5729","id":1661929923,"node_id":"PR_kwDODunzps5N_pvI","number":5729,"title":"Fix nondeterministic sharded data split order","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-11T07:34:20Z","updated_at":"2023-04-26T15:12:25Z","closed_at":"2023-04-26T15:05:12Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR makes the order of the split names deterministic. Before it was nondeterministic because we were iterating over `set` elements.\r\n\r\nFix #5728.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5729\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5729\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5729","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5729","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5729.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5729.patch","merged_at":"2023-04-26T15:05:12Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5728","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5728\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5728\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5728\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5728","id":1661925932,"node_id":"I_kwDODunzps5jDvos","number":5728,"title":"The order of data split names is nondeterministic","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-04-11T07:31:25Z","updated_at":"2023-04-26T15:05:13Z","closed_at":"2023-04-26T15:05:13Z","author_association":"MEMBER","active_lock_reason":null,"body":"After this CI error: https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/4639528358\/jobs\/8210492953?pr=5718\r\n```\r\nFAILED tests\/test_data_files.py::test_get_data_files_patterns[data_file_per_split4] - AssertionError: assert ['random', 'train'] == ['train', 'random']\r\n  At index 0 diff: 'random' != 'train'\r\n  Full diff:\r\n  - ['train', 'random']\r\n  + ['random', 'train']\r\n```\r\nI have checked locally and found out that the data split order is nondeterministic.\r\n\r\nThis is caused by the use of `set` for sharded splits.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5728\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5728\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5727","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5727\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5727\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5727\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5727","id":1661536363,"node_id":"I_kwDODunzps5jCQhr","number":5727,"title":"load_dataset fails with FileNotFound error on Windows","user":{"login":"joelkowalewski","id":122648572,"node_id":"U_kgDOB093_A","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/122648572?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/joelkowalewski","html_url":"https:\/\/github.com\/joelkowalewski","followers_url":"https:\/\/api.github.com\/users\/joelkowalewski\/followers","following_url":"https:\/\/api.github.com\/users\/joelkowalewski\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/joelkowalewski\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/joelkowalewski\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/joelkowalewski\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/joelkowalewski\/orgs","repos_url":"https:\/\/api.github.com\/users\/joelkowalewski\/repos","events_url":"https:\/\/api.github.com\/users\/joelkowalewski\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/joelkowalewski\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-10T23:21:12Z","updated_at":"2023-07-21T14:08:20Z","closed_at":"2023-07-21T14:08:19Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nAlthough I can import and run the datasets library in a Colab environment, I cannot successfully load any data on my own machine (Windows 10) despite following the install steps:\r\n\r\n(1) create conda environment \r\n(2) activate environment\r\n(3) install with: ``conda` install -c huggingface -c conda-forge datasets`\r\n\r\nThen\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\n# this or any other example from the website fails with the FileNotFoundError\r\n glue = load_dataset(\"glue\", \"ax\")\r\n```\r\n\r\n**Below I have pasted the error omitting the full path**:\r\n\r\n```\r\nraise FileNotFoundError(\r\nFileNotFoundError: Couldn't find a dataset script at C:\\Users\\...\\glue\\glue.py or any data file in the same directory. Couldn't find 'glue' on the Hugging Face Hub either: FileNotFoundError: [WinError 3] The system cannot find the path specified: \r\n'C:\\\\Users\\\\...\\\\.cache\\\\huggingface'\r\n```\r\n\n\n### Steps to reproduce the bug\n\nOn Windows 10 \r\n\r\n1) create a minimal conda environment (with just Python)\r\n(2) activate environment\r\n(3) install datasets with: ``conda` install -c huggingface -c conda-forge datasets`\r\n(4) import load_dataset and follow example usage from any dataset card. \n\n### Expected behavior\n\nThe expected behavior is to load the file into the Python session running on my machine without error. \n\n### Environment info\n\n```\r\n# Name                    Version                   Build  Channel\r\naiohttp                   3.8.4           py311ha68e1ae_0    conda-forge\r\naiosignal                 1.3.1              pyhd8ed1ab_0    conda-forge\r\narrow-cpp                 11.0.0          h57928b3_13_cpu    conda-forge\r\nasync-timeout             4.0.2              pyhd8ed1ab_0    conda-forge\r\nattrs                     22.2.0             pyh71513ae_0    conda-forge\r\naws-c-auth                0.6.26               h1262f0c_1    conda-forge\r\naws-c-cal                 0.5.21               h7cda486_2    conda-forge\r\naws-c-common              0.8.14               hcfcfb64_0    conda-forge\r\naws-c-compression         0.2.16               h8a79959_5    conda-forge\r\naws-c-event-stream        0.2.20               h5f78564_4    conda-forge\r\naws-c-http                0.7.6                h2545be9_0    conda-forge\r\naws-c-io                  0.13.19              h0d2781e_3    conda-forge\r\naws-c-mqtt                0.8.6               hd211e0c_12    conda-forge\r\naws-c-s3                  0.2.7                h8113e7b_1    conda-forge\r\naws-c-sdkutils            0.1.8                h8a79959_0    conda-forge\r\naws-checksums             0.1.14               h8a79959_5    conda-forge\r\naws-crt-cpp               0.19.8              he6d3b81_12    conda-forge\r\naws-sdk-cpp               1.10.57              h64004b3_8    conda-forge\r\nbrotlipy                  0.7.0           py311ha68e1ae_1005    conda-forge\r\nbzip2                     1.0.8                h8ffe710_4    conda-forge\r\nc-ares                    1.19.0               h2bbff1b_0\r\nca-certificates           2023.01.10           haa95532_0\r\ncertifi                   2022.12.7          pyhd8ed1ab_0    conda-forge\r\ncffi                      1.15.1          py311h7d9ee11_3    conda-forge\r\ncharset-normalizer        2.1.1              pyhd8ed1ab_0    conda-forge\r\ncolorama                  0.4.6              pyhd8ed1ab_0    conda-forge\r\ncryptography              40.0.1          py311h28e9c30_0    conda-forge\r\ndataclasses               0.8                pyhc8e2a94_3    conda-forge\r\ndatasets                  2.11.0                     py_0    huggingface\r\ndill                      0.3.6              pyhd8ed1ab_1    conda-forge\r\nfilelock                  3.11.0             pyhd8ed1ab_0    conda-forge\r\nfrozenlist                1.3.3           py311ha68e1ae_0    conda-forge\r\nfsspec                    2023.4.0           pyh1a96a4e_0    conda-forge\r\ngflags                    2.2.2             ha925a31_1004    conda-forge\r\nglog                      0.6.0                h4797de2_0    conda-forge\r\nhuggingface_hub           0.13.4                     py_0    huggingface\r\nidna                      3.4                pyhd8ed1ab_0    conda-forge\r\nimportlib-metadata        6.3.0              pyha770c72_0    conda-forge\r\nimportlib_metadata        6.3.0                hd8ed1ab_0    conda-forge\r\nintel-openmp              2023.0.0         h57928b3_25922    conda-forge\r\nkrb5                      1.20.1               heb0366b_0    conda-forge\r\nlibabseil                 20230125.0      cxx17_h63175ca_1    conda-forge\r\nlibarrow                  11.0.0          h04c43f8_13_cpu    conda-forge\r\nlibblas                   3.9.0              16_win64_mkl    conda-forge\r\nlibbrotlicommon           1.0.9                hcfcfb64_8    conda-forge\r\nlibbrotlidec              1.0.9                hcfcfb64_8    conda-forge\r\nlibbrotlienc              1.0.9                hcfcfb64_8    conda-forge\r\nlibcblas                  3.9.0              16_win64_mkl    conda-forge\r\nlibcrc32c                 1.1.2                h0e60522_0    conda-forge\r\nlibcurl                   7.88.1               h68f0423_1    conda-forge\r\nlibexpat                  2.5.0                h63175ca_1    conda-forge\r\nlibffi                    3.4.2                h8ffe710_5    conda-forge\r\nlibgoogle-cloud           2.8.0                hf2ff781_1    conda-forge\r\nlibgrpc                   1.52.1               h32da247_1    conda-forge\r\nlibhwloc                  2.9.0                h51c2c0f_0    conda-forge\r\nlibiconv                  1.17                 h8ffe710_0    conda-forge\r\nliblapack                 3.9.0              16_win64_mkl    conda-forge\r\nlibprotobuf               3.21.12              h12be248_0    conda-forge\r\nlibsqlite                 3.40.0               hcfcfb64_0    conda-forge\r\nlibssh2                   1.10.0               h9a1e1f7_3    conda-forge\r\nlibthrift                 0.18.1               h9ce19ad_0    conda-forge\r\nlibutf8proc               2.8.0                h82a8f57_0    conda-forge\r\nlibxml2                   2.10.3               hc3477c8_6    conda-forge\r\nlibzlib                   1.2.13               hcfcfb64_4    conda-forge\r\nlz4-c                     1.9.4                hcfcfb64_0    conda-forge\r\nmkl                       2022.1.0           h6a75c08_874    conda-forge\r\nmultidict                 6.0.4           py311ha68e1ae_0    conda-forge\r\nmultiprocess              0.70.14         py311ha68e1ae_3    conda-forge\r\nnumpy                     1.24.2          py311h0b4df5a_0    conda-forge\r\nopenssl                   3.1.0                hcfcfb64_0    conda-forge\r\norc                       1.8.3                hada7b9e_0    conda-forge\r\npackaging                 23.0               pyhd8ed1ab_0    conda-forge\r\npandas                    2.0.0           py311hf63dbb6_0    conda-forge\r\nparquet-cpp               1.5.1                         2    conda-forge\r\npip                       23.0.1             pyhd8ed1ab_0    conda-forge\r\npthreads-win32            2.9.1                hfa6e2cd_3    conda-forge\r\npyarrow                   11.0.0          py311h6a6099b_13_cpu    conda-forge\r\npycparser                 2.21               pyhd8ed1ab_0    conda-forge\r\npyopenssl                 23.1.1             pyhd8ed1ab_0    conda-forge\r\npysocks                   1.7.1              pyh0701188_6    conda-forge\r\npython                    3.11.3          h2628c8c_0_cpython    conda-forge\r\npython-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge\r\npython-tzdata             2023.3             pyhd8ed1ab_0    conda-forge\r\npython-xxhash             3.2.0           py311ha68e1ae_0    conda-forge\r\npython_abi                3.11                    3_cp311    conda-forge\r\npytz                      2023.3             pyhd8ed1ab_0    conda-forge\r\npyyaml                    6.0             py311ha68e1ae_5    conda-forge\r\nre2                       2023.02.02           h63175ca_0    conda-forge\r\nrequests                  2.28.2             pyhd8ed1ab_1    conda-forge\r\nsetuptools                67.6.1             pyhd8ed1ab_0    conda-forge\r\nsix                       1.16.0             pyh6c4a22f_0    conda-forge\r\nsnappy                    1.1.10               hfb803bf_0    conda-forge\r\ntbb                       2021.8.0             h91493d7_0    conda-forge\r\ntk                        8.6.12               h8ffe710_0    conda-forge\r\ntqdm                      4.65.0             pyhd8ed1ab_1    conda-forge\r\ntyping-extensions         4.5.0                hd8ed1ab_0    conda-forge\r\ntyping_extensions         4.5.0              pyha770c72_0    conda-forge\r\ntzdata                    2023c                h71feb2d_0    conda-forge\r\nucrt                      10.0.22621.0         h57928b3_0    conda-forge\r\nurllib3                   1.26.15            pyhd8ed1ab_0    conda-forge\r\nvc                        14.3                hb6edc58_10    conda-forge\r\nvs2015_runtime            14.34.31931         h4c5c07a_10    conda-forge\r\nwheel                     0.40.0             pyhd8ed1ab_0    conda-forge\r\nwin_inet_pton             1.1.0              pyhd8ed1ab_6    conda-forge\r\nxxhash                    0.8.1                hcfcfb64_0    conda-forge\r\nxz                        5.2.10               h8cc25b3_1\r\nyaml                      0.2.5                h8ffe710_2    conda-forge\r\nyarl                      1.8.2           py311ha68e1ae_0    conda-forge\r\nzipp                      3.15.0             pyhd8ed1ab_0    conda-forge\r\nzlib                      1.2.13               hcfcfb64_4    conda-forge\r\nzstd                      1.5.4                hd43e919_0\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5727\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5727\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5726","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5726\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5726\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5726\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5726","id":1660944807,"node_id":"I_kwDODunzps5jAAGn","number":5726,"title":"Fallback JSON Dataset loading does not load all values when features specified manually","user":{"login":"myluki2000","id":3610788,"node_id":"MDQ6VXNlcjM2MTA3ODg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3610788?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/myluki2000","html_url":"https:\/\/github.com\/myluki2000","followers_url":"https:\/\/api.github.com\/users\/myluki2000\/followers","following_url":"https:\/\/api.github.com\/users\/myluki2000\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/myluki2000\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/myluki2000\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/myluki2000\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/myluki2000\/orgs","repos_url":"https:\/\/api.github.com\/users\/myluki2000\/repos","events_url":"https:\/\/api.github.com\/users\/myluki2000\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/myluki2000\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-04-10T15:22:14Z","updated_at":"2023-04-21T06:35:28Z","closed_at":"2023-04-21T06:35:28Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nThe fallback JSON dataset loader located here:\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/1c4ec00511868bd881e84a6f7e0333648d833b8e\/src\/datasets\/packaged_modules\/json\/json.py#L130-L153\r\n\r\ndoes not load the values of features correctly when features are specified manually and not all features have a value in the first entry of the dataset. I'm pretty sure this is not supposed to be expected bahavior?\r\n\r\nTo fix this you'd have to change this line:\r\n\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/1c4ec00511868bd881e84a6f7e0333648d833b8e\/src\/datasets\/packaged_modules\/json\/json.py#L140\r\n\r\nTo pass a schema to pyarrow which has the same structure as the features argument passed to the load_dataset() method.\r\n\r\n### Steps to reproduce the bug\r\n\r\nConsider a dataset JSON like this:\r\n\r\n```\r\n[\r\n{\r\n\t\"instruction\": \"Do stuff\",\r\n\t\"output\": \"Answer stuff\"\r\n},\r\n{\r\n\t\"instruction\": \"Do stuff2\",\r\n\t\"input\": \"Additional Input2\",\r\n\t\"output\": \"Answer stuff2\"\r\n}\r\n]\r\n```\r\n\r\nUsing this code to load the dataset:\r\n\r\n```\r\nfrom datasets import load_dataset, Features, Value\r\n\r\nfeatures = {\r\n    \"instruction\": Value(\"string\"),\r\n    \"input\": Value(\"string\"),\r\n    \"output\": Value(\"string\")\r\n}\r\nfeatures = Features(features)\r\n\r\nds = load_dataset(\"json\", data_files=\".\/ds.json\", features=features)\r\n\r\nfor row in ds[\"train\"]:\r\n    print(row)\r\n```\r\n\r\nwe get a dataset that looks like this:\r\n\r\n| **Instruction** | **Input**          | **Output**      |\r\n|-----------------|--------------------|-----------------|\r\n| \"Do stuff\"      | None               | \"Answer Stuff\"  |\r\n| \"Do stuff2\"     | None               | \"Answer Stuff2\" |\r\n\r\n### Expected behavior\r\n\r\nThe input column should contain values other than None for dataset entries that have the \"input\" attribute set:\r\n\r\n| **Instruction** | **Input**          | **Output**      |\r\n|-----------------|--------------------|-----------------|\r\n| \"Do stuff\"      | None               | \"Answer Stuff\"  |\r\n| \"Do stuff2\"     | \"Additional Input2\" | \"Answer Stuff2\" |\r\n\r\n### Environment info\r\n\r\nPython 3.10.10\r\nDatasets 2.11.0\r\nWindows 10","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5726\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5726\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5725","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5725\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5725\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5725\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5725","id":1660455202,"node_id":"I_kwDODunzps5i-Iki","number":5725,"title":"How to limit the number of examples in dataset, for testing?","user":{"login":"ndvbd","id":845175,"node_id":"MDQ6VXNlcjg0NTE3NQ==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/845175?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ndvbd","html_url":"https:\/\/github.com\/ndvbd","followers_url":"https:\/\/api.github.com\/users\/ndvbd\/followers","following_url":"https:\/\/api.github.com\/users\/ndvbd\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ndvbd\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ndvbd\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ndvbd\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ndvbd\/orgs","repos_url":"https:\/\/api.github.com\/users\/ndvbd\/repos","events_url":"https:\/\/api.github.com\/users\/ndvbd\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ndvbd\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-10T08:41:43Z","updated_at":"2023-04-21T06:16:24Z","closed_at":"2023-04-21T06:16:24Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI am using this command:\r\n`data = load_dataset(\"json\", data_files=data_path)`\r\nHowever, I want to add a parameter, to limit the number of loaded examples to be 10, for development purposes, but can't find this simple parameter.\n\n### Steps to reproduce the bug\n\nIn the description.\n\n### Expected behavior\n\nTo be able to limit the number of examples\n\n### Environment info\n\nNothing special","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5725\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5725\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5724","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5724\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5724\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5724\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5724","id":1659938135,"node_id":"I_kwDODunzps5i8KVX","number":5724,"title":"Error after shuffling streaming IterableDatasets with downloaded dataset","user":{"login":"szxiangjn","id":41177966,"node_id":"MDQ6VXNlcjQxMTc3OTY2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/41177966?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/szxiangjn","html_url":"https:\/\/github.com\/szxiangjn","followers_url":"https:\/\/api.github.com\/users\/szxiangjn\/followers","following_url":"https:\/\/api.github.com\/users\/szxiangjn\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/szxiangjn\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/szxiangjn\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/szxiangjn\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/szxiangjn\/orgs","repos_url":"https:\/\/api.github.com\/users\/szxiangjn\/repos","events_url":"https:\/\/api.github.com\/users\/szxiangjn\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/szxiangjn\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-09T16:58:44Z","updated_at":"2023-04-20T20:37:30Z","closed_at":"2023-04-20T20:37:30Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI downloaded the C4 dataset, and used streaming IterableDatasets to read it. Everything went normal until I used `dataset = dataset.shuffle(seed=42, buffer_size=10_000)` to shuffle the dataset. Shuffled dataset will throw the following error when it is used by `next(iter(dataset))`:\r\n```\r\nFile \"\/data\/miniconda3\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 937, in __iter__                                     \r\n    for key, example in ex_iterable:                              \r\n  File \"\/data\/miniconda3\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 627, in __iter__                                     \r\n    for x in self.ex_iterable:                                               \r\n  File \"\/data\/miniconda3\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 138, in __iter__                                     \r\n    yield from self.generate_examples_fn(**kwargs_with_shuffled_shards)\r\n  File \"\/data\/miniconda3\/lib\/python3.9\/site-packages\/datasets\/iterable_dataset.py\", line 763, in wrapper                                      \r\n    for key, table in generate_tables_fn(**kwargs):                            \r\n  File \"\/data\/miniconda3\/lib\/python3.9\/site-packages\/datasets\/packaged_modules\/json\/json.py\", line 101, in _generate_tables                   \r\n    batch = f.read(self.config.chunksize)   \r\n  File \"\/data\/miniconda3\/lib\/python3.9\/site-packages\/datasets\/download\/streaming_download_manager.py\", line 372, in read_with_retries         \r\n    out = read(*args, **kwargs)\r\n  File \"\/data\/miniconda3\/lib\/python3.9\/gzip.py\", line 300, in read\r\n    return self._buffer.read(size)\r\n  File \"\/data\/miniconda3\/lib\/python3.9\/_compression.py\", line 68, in readinto                                                                 \r\n    data = self.read(len(byte_view))       \r\n  File \"\/data\/miniconda3\/lib\/python3.9\/gzip.py\", line 487, in read\r\n    if not self._read_gzip_header():                                  \r\n  File \"\/data\/miniconda3\/lib\/python3.9\/gzip.py\", line 435, in _read_gzip_header                                                               \r\n    raise BadGzipFile('Not a gzipped file (%r)' % magic)\r\ngzip.BadGzipFile: Not a gzipped file (b've')\r\n```\r\nI found that there is no problem to use the dataset in this way without shuffling. Also, use `dataset = datasets.load_dataset('c4', 'en', split='train', streaming=True)`, which will download the dataset on-the-fly instead of loading from the local file, will also not have problems even after shuffle.\n\n### Steps to reproduce the bug\n\n1. Download C4 dataset from https:\/\/huggingface.co\/datasets\/allenai\/c4\r\n2. \r\n```\r\nimport datasets\r\ndataset = datasets.load_dataset('\/path\/to\/your\/data\/dir', 'en', streaming=True, split='train')\r\ndataset = dataset.shuffle(buffer_size=10_000, seed=42)\r\nnext(iter(dataset))\r\n```\n\n### Expected behavior\n\n`next(iter(dataset))` should give me a sample from the dataset\n\n### Environment info\n\n- `datasets` version: 2.11.0\r\n- Platform: Linux-5.4.32-1-tlinux4-0001-x86_64-with-glibc2.28\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.13.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5724\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5724\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5722","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5722\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5722\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5722\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5722","id":1659837510,"node_id":"I_kwDODunzps5i7xxG","number":5722,"title":"Distributed Training Error on Customized Dataset","user":{"login":"wlhgtc","id":16603773,"node_id":"MDQ6VXNlcjE2NjAzNzcz","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16603773?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wlhgtc","html_url":"https:\/\/github.com\/wlhgtc","followers_url":"https:\/\/api.github.com\/users\/wlhgtc\/followers","following_url":"https:\/\/api.github.com\/users\/wlhgtc\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wlhgtc\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wlhgtc\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wlhgtc\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wlhgtc\/orgs","repos_url":"https:\/\/api.github.com\/users\/wlhgtc\/repos","events_url":"https:\/\/api.github.com\/users\/wlhgtc\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wlhgtc\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-09T11:04:59Z","updated_at":"2023-07-24T14:50:46Z","closed_at":"2023-07-24T14:50:46Z","author_association":"NONE","active_lock_reason":null,"body":"Hi guys, recently I tried to use `datasets` to train a dual encoder.\r\nI finish my own datasets according to the nice [tutorial](https:\/\/huggingface.co\/docs\/datasets\/v2.11.0\/en\/dataset_script) \r\nHere are my code:\r\n```python\r\nclass RetrivalDataset(datasets.GeneratorBasedBuilder):\r\n    \"\"\"CrossEncoder dataset.\"\"\"\r\n\r\n    BUILDER_CONFIGS = [RetrivalConfig(name=\"DuReader\")]\r\n\r\n    # DEFAULT_CONFIG_NAME = \"DuReader\"\r\n\r\n    def _info(self):\r\n        return datasets.DatasetInfo(\r\n            features=datasets.Features(\r\n                {\r\n                    \"id\": datasets.Value(\"string\"),\r\n                    \"question\": datasets.Value(\"string\"),\r\n                    \"documents\": Sequence(datasets.Value(\"string\")),\r\n                }\r\n            ),\r\n            supervised_keys=None,\r\n        )\r\n\r\n    def _split_generators(self, dl_manager):\r\n        \"\"\"Returns SplitGenerators.\"\"\"\r\n\r\n        train_file = self.config.data_dir + self.config.train_file\r\n        valid_file = self.config.data_dir + self.config.valid_file\r\n\r\n        logger.info(f\"Training  on {self.config.train_file}\")\r\n        logger.info(f\"Evaluating on {self.config.valid_file}\")\r\n\r\n        return [\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TRAIN, gen_kwargs={\"file_path\": train_file}\r\n            ),\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.VALIDATION, gen_kwargs={\"file_path\": valid_file}\r\n            ),\r\n        ]\r\n\r\n    def _generate_examples(self, file_path):\r\n        with jsonlines.open(file_path, \"r\") as f:\r\n            for record in f:\r\n                label = record[\"label\"]\r\n                question = record[\"question\"]\r\n\r\n                # dual encoder\r\n                all_documents = record[\"all_documents\"]\r\n                positive_paragraph = all_documents.pop(label)\r\n                all_documents = [positive_paragraph] + all_documents\r\n\r\n                u_id = \"{}_#_{}\".format(\r\n                    md5_hash(question + \"\".join(all_documents)),\r\n                    \"\".join(random.sample(string.ascii_letters + string.digits, 7)),\r\n                )\r\n                item = {\r\n                    \"question\": question,\r\n                    \"documents\": all_documents,\r\n                    \"id\": u_id,\r\n                }\r\n                yield u_id, item\r\n```\r\n\r\nIt works well on single GPU, but got errors as follows when used DDP:\r\n```python\r\nDetected mismatch between collectives on ranks. Rank 1 is running collective: CollectiveFingerPrint(OpType=BARRIER), but Rank 0 is running collective: CollectiveFingerPrint(OpType=ALLGATHER_COALESCED)\r\n```\r\n\r\nHere are my train script on a two A100 mechine:\r\n```bash\r\nexport TORCH_DISTRIBUTED_DEBUG=DETAIL\r\nexport TORCH_SHOW_CPP_STACKTRACES=1\r\nexport NCCL_DEBUG=INFO\r\nexport NCCL_DEBUG_SUBSYS=INIT,COLL,ENV\r\nnohup torchrun --nproc_per_node 2 train.py experiments\/de-big.json >logs\/de-big.log 2>&1&\r\n```\r\n\r\nI am not sure if this error below related to my dataset code when use DDP. And I notice the PR(#5369 ), but I don't know when and where should I used the function(`split_dataset_by_node`) .\r\n\r\n@lhoestq  hope you could help me?\r\n\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5722\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5722\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5721","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5721\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5721\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5721\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5721","id":1659680682,"node_id":"I_kwDODunzps5i7Leq","number":5721,"title":"Calling datasets.load_dataset(\"text\" ...) results in a wrong split.","user":{"login":"cyrilzakka","id":1841186,"node_id":"MDQ6VXNlcjE4NDExODY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1841186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/cyrilzakka","html_url":"https:\/\/github.com\/cyrilzakka","followers_url":"https:\/\/api.github.com\/users\/cyrilzakka\/followers","following_url":"https:\/\/api.github.com\/users\/cyrilzakka\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/cyrilzakka\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/cyrilzakka\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/cyrilzakka\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/cyrilzakka\/orgs","repos_url":"https:\/\/api.github.com\/users\/cyrilzakka\/repos","events_url":"https:\/\/api.github.com\/users\/cyrilzakka\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/cyrilzakka\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-04-08T23:55:12Z","updated_at":"2023-04-08T23:55:12Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen creating a text dataset, the training split should have the bulk of the examples by default. Currently, testing does.\n\n### Steps to reproduce the bug\n\nI have a folder with 18K text files in it. Each text file essentially consists in a document or article scraped from online. Calling the following codeL\r\n```\r\nfolder_path = \"\/home\/cyril\/Downloads\/llama_dataset\"\r\ndata = datasets.load_dataset(\"text\", data_dir=folder_path)\r\ndata.save_to_disk(\"\/home\/cyril\/Downloads\/data.hf\")\r\n\r\ndata = datasets.load_from_disk(\"\/home\/cyril\/Downloads\/data.hf\")\r\nprint(data)\r\n```\r\n\r\nResults in the following split:\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['text'],\r\n        num_rows: 2114\r\n    })\r\n    test: Dataset({\r\n        features: ['text'],\r\n        num_rows: 200882\r\n    })\r\n    validation: Dataset({\r\n        features: ['text'],\r\n        num_rows: 152\r\n    })\r\n})\r\n```\r\n\r\nIt seems to me like the train\/test\/validation splits are in the wrong order since test split >>>> train_split\n\n### Expected behavior\n\nTrain split should have the bulk of the training examples.\n\n### Environment info\n\ndatasets 2.11.0, python 3.10.6","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5721\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5721\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5720","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5720\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5720\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5720\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5720","id":1659610705,"node_id":"I_kwDODunzps5i66ZR","number":5720,"title":"Streaming IterableDatasets do not work with torch DataLoaders","user":{"login":"jlehrer1","id":29244648,"node_id":"MDQ6VXNlcjI5MjQ0NjQ4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/29244648?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jlehrer1","html_url":"https:\/\/github.com\/jlehrer1","followers_url":"https:\/\/api.github.com\/users\/jlehrer1\/followers","following_url":"https:\/\/api.github.com\/users\/jlehrer1\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jlehrer1\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jlehrer1\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jlehrer1\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jlehrer1\/orgs","repos_url":"https:\/\/api.github.com\/users\/jlehrer1\/repos","events_url":"https:\/\/api.github.com\/users\/jlehrer1\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jlehrer1\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-04-08T18:45:48Z","updated_at":"2023-05-27T12:57:08Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nWhen using streaming datasets set up with train\/val split using `.skip()` and `.take()`, the following error occurs when iterating over a torch dataloader:\r\n```\r\n  File \"\/Users\/julian\/miniconda3\/envs\/sims\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 363, in __iter__\r\n    self._iterator = self._get_iterator()\r\n  File \"\/Users\/julian\/miniconda3\/envs\/sims\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 314, in _get_iterator\r\n    return _MultiProcessingDataLoaderIter(self)\r\n  File \"\/Users\/julian\/miniconda3\/envs\/sims\/lib\/python3.9\/site-packages\/torch\/utils\/data\/dataloader.py\", line 927, in __init__\r\n    w.start()\r\n  File \"\/Users\/julian\/miniconda3\/envs\/sims\/lib\/python3.9\/multiprocessing\/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"\/Users\/julian\/miniconda3\/envs\/sims\/lib\/python3.9\/multiprocessing\/context.py\", line 224, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"\/Users\/julian\/miniconda3\/envs\/sims\/lib\/python3.9\/multiprocessing\/context.py\", line 284, in _Popen\r\n    return Popen(process_obj)\r\n  File \"\/Users\/julian\/miniconda3\/envs\/sims\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"\/Users\/julian\/miniconda3\/envs\/sims\/lib\/python3.9\/multiprocessing\/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"\/Users\/julian\/miniconda3\/envs\/sims\/lib\/python3.9\/multiprocessing\/popen_spawn_posix.py\", line 47, in _launch\r\n    reduction.dump(process_obj, fp)\r\n  File \"\/Users\/julian\/miniconda3\/envs\/sims\/lib\/python3.9\/multiprocessing\/reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object '_generate_examples_from_tables_wrapper.<locals>.wrapper'\r\n```\r\n\r\nTo reproduce, run the code \r\n```\r\nfrom datasets import load_dataset\r\ndata = load_dataset(args.dataset_name, split=\"train\", streaming=True)\r\ntrain_len = 5000\r\nval_len = 100\r\n\r\ntrain, val = data.take(train_len), data.skip(train_len).take(val_len)\r\ntraindata = IterableClipDataset(data, context_length=args.max_len, tokenizer=tokenizer, image_key=\"url\", text_key=\"text\")\r\n\r\ntraindata = DataLoader(traindata, batch_size=args.batch_size, num_workers=args.num_workers, persistent_workers=True)\r\n```\r\n\r\nWhere the class IterableClipDataset is a simple wrapper to cast the dataset to a torch iterabledataset, defined via \r\n```\r\nfrom torch.utils.data import Dataset, IterableDataset\r\nfrom torchvision.transforms import Compose, Resize, ToTensor\r\nfrom transformers import AutoTokenizer\r\nimport requests\r\nfrom PIL import Image\r\n\r\nclass IterableClipDataset(IterableDataset):\r\n    def __init__(self, dataset, context_length: int, image_transform=None, tokenizer=None, image_key=\"image\", text_key=\"text\"):\r\n        self.dataset = dataset\r\n        self.context_length = context_length\r\n        self.image_transform = Compose([Resize((224, 224)), ToTensor()]) if image_transform is None else image_transform\r\n        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") if tokenizer is None else tokenizer\r\n        self.image_key = image_key\r\n        self.text_key = text_key\r\n\r\n    def read_image(self, url: str):\r\n        try: # Try to read the image\r\n            image = Image.open(requests.get(url, stream=True).raw)\r\n        except:\r\n            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\r\n        return image\r\n    \r\n    def process_sample(self, image, text):\r\n        if isinstance(image, str):\r\n            image = self.read_image(image)\r\n        if self.image_transform is not None:\r\n            image = self.image_transform(image)\r\n        text = self.tokenizer.encode(\r\n            text, add_special_tokens=True, max_length=self.context_length, truncation=True, padding=\"max_length\"\r\n        )\r\n        text = torch.tensor(text, dtype=torch.long)\r\n        return image, text\r\n\r\n    def __iter__(self):\r\n        for sample in self.dataset:\r\n            image, text = sample[self.image_key], sample[self.text_key]\r\n            yield self.process_sample(image, text)\r\n```\r\n\r\n\n\n### Steps to reproduce the bug\n\nSteps to reproduce\r\n1. Install `datasets`, `torch`, and `PIL` (if you want to reproduce exactly)\r\n2. Run the code above\n\n### Expected behavior\n\nBatched data is produced from the dataloader \n\n### Environment info\n\n```\r\ndatasets == 2.9.0\r\npython == 3.9.12\r\ntorch == 1.11.0\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5720\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5720\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5719","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5719\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5719\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5719\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5719","id":1659203222,"node_id":"I_kwDODunzps5i5W6W","number":5719,"title":"Array2D feature creates a list of list instead of a numpy array","user":{"login":"off99555","id":15215732,"node_id":"MDQ6VXNlcjE1MjE1NzMy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/15215732?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/off99555","html_url":"https:\/\/github.com\/off99555","followers_url":"https:\/\/api.github.com\/users\/off99555\/followers","following_url":"https:\/\/api.github.com\/users\/off99555\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/off99555\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/off99555\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/off99555\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/off99555\/orgs","repos_url":"https:\/\/api.github.com\/users\/off99555\/repos","events_url":"https:\/\/api.github.com\/users\/off99555\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/off99555\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-07T21:04:08Z","updated_at":"2023-04-20T15:34:41Z","closed_at":"2023-04-20T15:34:41Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI'm not sure if this is expected behavior or not. When I create a 2D array using `Array2D`, the data has list type instead of numpy array. I think it should not be the expected behavior especially when I feed a numpy array as input to the data creation function. Why is it converting my array into a list?\r\nAlso if I change the first dimension of the `Array2D` shape to None, it's returning array correctly.\r\n\r\n### Steps to reproduce the bug\r\n\r\nRun this code:\r\n```py\r\nfrom datasets import Dataset, Features, Array2D\r\nimport numpy as np\r\n\r\n# you have to change the first dimension of the shape to None to make it return an array\r\nfeatures = Features(dict(seq=Array2D((2,2), 'float32')))  \r\nds = Dataset.from_dict(dict(seq=[np.random.rand(2,2)]), features=features)\r\na = ds[0]['seq']\r\nprint(a)\r\nprint(type(a))\r\n```\r\n\r\nThe following will be printed in stdout:\r\n```\r\n[[0.8127174377441406, 0.3760348856449127], [0.7510159611701965, 0.4322739541530609]]\r\n<class 'list'>\r\n```\r\n\r\n### Expected behavior\r\n\r\nEach indexed item should be a list or numpy array.  Currently, `Array((2,2))` yields a list but `Array((None,2))` yields an array.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.11.0\r\n- Platform: Windows-10-10.0.19045-SP0\r\n- Python version: 3.9.13\r\n- Huggingface_hub version: 0.13.4\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.4.4\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5719\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5719\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5718","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5718\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5718\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5718\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5718","id":1658958406,"node_id":"PR_kwDODunzps5N2IZC","number":5718,"title":"Reorder default data splits to have validation before test","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-07T16:01:26Z","updated_at":"2023-04-27T14:43:13Z","closed_at":"2023-04-27T14:35:52Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR reorders data splits, so that by default validation appears before test.\r\n\r\nThe default order becomes: [train, validation, test] instead of [train, test, validation].","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5718\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5718\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5718","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5718","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5718.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5718.patch","merged_at":"2023-04-27T14:35:52Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5717","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5717\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5717\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5717\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5717","id":1658729866,"node_id":"I_kwDODunzps5i3jWK","number":5717,"title":"Errror when saving to disk a dataset of images","user":{"login":"jplu","id":959590,"node_id":"MDQ6VXNlcjk1OTU5MA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/959590?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jplu","html_url":"https:\/\/github.com\/jplu","followers_url":"https:\/\/api.github.com\/users\/jplu\/followers","following_url":"https:\/\/api.github.com\/users\/jplu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jplu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jplu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jplu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jplu\/orgs","repos_url":"https:\/\/api.github.com\/users\/jplu\/repos","events_url":"https:\/\/api.github.com\/users\/jplu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jplu\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":8,"created_at":"2023-04-07T11:59:17Z","updated_at":"2023-11-08T11:08:18Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nHello!\r\n\r\nI have an issue when I try to save on disk my dataset of images. The error I get is:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 1442, in save_to_disk\r\n    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/arrow_dataset.py\", line 1473, in _save_to_disk_single\r\n    writer.write_table(pa_table)\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 570, in write_table\r\n    pa_table = embed_table_storage(pa_table)\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 2268, in embed_table_storage\r\n    arrays = [\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 2269, in <listcomp>\r\n    embed_array_storage(table[name], feature) if require_storage_embed(feature) else table[name]\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1817, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1817, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 2142, in embed_array_storage\r\n    return feature.embed_storage(array)\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/features\/image.py\", line 269, in embed_storage\r\n    storage = pa.StructArray.from_arrays([bytes_array, path_array], [\"bytes\", \"path\"], mask=bytes_array.is_null())\r\n  File \"pyarrow\/array.pxi\", line 2766, in pyarrow.lib.StructArray.from_arrays\r\n  File \"pyarrow\/array.pxi\", line 2961, in pyarrow.lib.c_mask_inverted_from_obj\r\nTypeError: Mask must be a pyarrow.Array of type boolean\r\n```\r\n\r\nMy dataset is around 50K images, is this error might be due to a bad image?\r\n\r\nThanks for the help.\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"\/path\/to\/dataset\")\r\ndataset[\"train\"].save_to_disk(\".\/myds\", num_shards=40)\r\n```\n\n### Expected behavior\n\nHaving my dataset properly saved to disk.\n\n### Environment info\n\n- `datasets` version: 2.11.0\r\n- Platform: Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\n- Python version: 3.10.10\r\n- Huggingface_hub version: 0.13.3\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5717\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5717\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5716","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5716\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5716\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5716\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5716","id":1658613092,"node_id":"I_kwDODunzps5i3G1k","number":5716,"title":"Handle empty audio","user":{"login":"v-yunbin","id":38179632,"node_id":"MDQ6VXNlcjM4MTc5NjMy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/38179632?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/v-yunbin","html_url":"https:\/\/github.com\/v-yunbin","followers_url":"https:\/\/api.github.com\/users\/v-yunbin\/followers","following_url":"https:\/\/api.github.com\/users\/v-yunbin\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/v-yunbin\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/v-yunbin\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/v-yunbin\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/v-yunbin\/orgs","repos_url":"https:\/\/api.github.com\/users\/v-yunbin\/repos","events_url":"https:\/\/api.github.com\/users\/v-yunbin\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/v-yunbin\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-07T09:51:40Z","updated_at":"2023-09-27T17:47:08Z","closed_at":"2023-09-27T17:47:08Z","author_association":"NONE","active_lock_reason":null,"body":"Some audio paths exist, but they are empty, and an error will be reported when reading the audio path.How to use the filter function to avoid the empty audio path?\r\nwhen a audio is empty, when do resample , it will break:\r\n`array, sampling_rate = sf.read(f) array = librosa.resample(array, orig_sr=sampling_rate, target_sr=self.sampling_rate)`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5716\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5716\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5715","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5715\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5715\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5715\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5715","id":1657479788,"node_id":"I_kwDODunzps5iyyJs","number":5715,"title":"Return Numpy Array (fixed length) Mode, in __get_item__, Instead of List","user":{"login":"jungbaepark","id":34066771,"node_id":"MDQ6VXNlcjM0MDY2Nzcx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/34066771?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jungbaepark","html_url":"https:\/\/github.com\/jungbaepark","followers_url":"https:\/\/api.github.com\/users\/jungbaepark\/followers","following_url":"https:\/\/api.github.com\/users\/jungbaepark\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jungbaepark\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jungbaepark\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jungbaepark\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jungbaepark\/orgs","repos_url":"https:\/\/api.github.com\/users\/jungbaepark\/repos","events_url":"https:\/\/api.github.com\/users\/jungbaepark\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jungbaepark\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-06T13:57:48Z","updated_at":"2023-04-20T17:16:26Z","closed_at":"2023-04-20T17:16:26Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nThere are old known issues, but they can be easily forgettable problems in multiprocessing with pytorch-dataloader:\r\n\r\nToo high usage of RAM or shared-memory in pytorch when we set num workers > 1 and returning type of dataset or dataloader is \"List\" or \"Dict\".\r\nhttps:\/\/github.com\/pytorch\/pytorch\/issues\/13246 \r\n\r\nWith huggingface datasets, unfortunately, the default return type is the list, so the problem is raised too often if we do not set anything for the issue.\r\n\r\n\r\nHowever, this issue can be released when the returning output is fixed in length.\r\nTherefore, I request the mode, returning outputs with fixed length (e.g. numpy array) rather than list.\r\n\r\nThe design would be good when we load datasets as\r\n```python\r\nload_dataset(..., with_return_as_fixed_tensor=True)\r\n```\r\n\r\n### Motivation\r\n\r\nThe general solution for this issue is already in the comments:  https:\/\/github.com\/pytorch\/pytorch\/issues\/13246#issuecomment-905703662 \r\n\r\n: Numpy or Pandas seems not to have problems, while both have the string type.\r\n(I'm not sure that the sequence of huggingface datasets can solve this problem as well)\r\n\r\n\r\n### Your contribution\r\n\r\n I'll read it ! thanks","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5715\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5715\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5714","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5714\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5714\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5714\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5714","id":1657388033,"node_id":"PR_kwDODunzps5NxIOc","number":5714,"title":"Fix xnumpy_load for .npz files","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-06T13:01:45Z","updated_at":"2023-04-07T09:23:54Z","closed_at":"2023-04-07T09:16:57Z","author_association":"MEMBER","active_lock_reason":null,"body":"PR:\r\n- #5626 \r\n\r\nimplemented support for streaming `.npy` files by using `numpy.load`.\r\n\r\nHowever, it introduced a bug when used with `.npz` files, within a context manager:\r\n```\r\nValueError: seek of closed file\r\n```\r\nor in streaming mode:\r\n```\r\nValueError: I\/O operation on closed file.\r\n```\r\n\r\nThis PR fixes the bug and tests for both `.npy` and `.npz` files.\r\n\r\nFix #5711.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5714\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5714\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5714","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5714","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5714.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5714.patch","merged_at":"2023-04-07T09:16:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5713","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5713\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5713\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5713\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5713","id":1657141251,"node_id":"I_kwDODunzps5ixfgD","number":5713,"title":"ArrowNotImplementedError when loading dataset from the hub","user":{"login":"jplu","id":959590,"node_id":"MDQ6VXNlcjk1OTU5MA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/959590?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jplu","html_url":"https:\/\/github.com\/jplu","followers_url":"https:\/\/api.github.com\/users\/jplu\/followers","following_url":"https:\/\/api.github.com\/users\/jplu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jplu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jplu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jplu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jplu\/orgs","repos_url":"https:\/\/api.github.com\/users\/jplu\/repos","events_url":"https:\/\/api.github.com\/users\/jplu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jplu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-06T10:27:22Z","updated_at":"2023-04-06T13:06:22Z","closed_at":"2023-04-06T13:06:21Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nHello,\r\n\r\nI have created a dataset by using the image loader. Once the dataset is created I try to download it and I get the error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1860, in _prepare_split_single\r\n    for _, table in generator:\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/parquet\/parquet.py\", line 69, in _generate_tables\r\n    for batch_idx, record_batch in enumerate(\r\n  File \"pyarrow\/_parquet.pyx\", line 1323, in iter_batches\r\n  File \"pyarrow\/error.pxi\", line 121, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowNotImplementedError: Nested data conversions not implemented for chunked array outputs\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1791, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 891, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 986, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1748, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"\/home\/jplu\/miniconda3\/envs\/image-xp\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1893, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\n\n### Steps to reproduce the bug\n\nCreate the dataset and push it to the hub:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"\/path\/to\/dataset\")\r\ndataset.push_to_hub(\"org\/dataset-name\", private=True, max_shard_size=\"1GB\")\r\n```\r\n\r\nThen use it:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"org\/dataset-name\")\r\n```\n\n### Expected behavior\n\nTo properly download and use the pushed dataset.\r\n\r\nSomething else to note is that I specified to have shards of 1GB max, but at the end, for the train set, it is an almost 7GB single file that is pushed.\n\n### Environment info\n\n- `datasets` version: 2.11.0\r\n- Platform: Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\n- Python version: 3.10.10\r\n- Huggingface_hub version: 0.13.3\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5713\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5713\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5712","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5712\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5712\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5712\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5712","id":1655972106,"node_id":"I_kwDODunzps5itCEK","number":5712,"title":"load_dataset in v2.11.0 raises \"ValueError: seek of closed file\" in np.load()","user":{"login":"rcasero","id":1219084,"node_id":"MDQ6VXNlcjEyMTkwODQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1219084?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rcasero","html_url":"https:\/\/github.com\/rcasero","followers_url":"https:\/\/api.github.com\/users\/rcasero\/followers","following_url":"https:\/\/api.github.com\/users\/rcasero\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rcasero\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rcasero\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rcasero\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rcasero\/orgs","repos_url":"https:\/\/api.github.com\/users\/rcasero\/repos","events_url":"https:\/\/api.github.com\/users\/rcasero\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rcasero\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-05T16:47:10Z","updated_at":"2023-04-06T08:32:37Z","closed_at":"2023-04-05T17:17:44Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nHi,\r\n\r\nI have some `dataset_load()` code of a custom offline dataset that works with datasets v2.10.1. \r\n\r\n```python\r\n        ds = datasets.load_dataset(path=dataset_dir,\r\n                                   name=configuration,\r\n                                   data_dir=dataset_dir,\r\n                                   cache_dir=cache_dir,\r\n                                   aux_dir=aux_dir,\r\n                                #    download_mode=datasets.DownloadMode.FORCE_REDOWNLOAD,\r\n                                   num_proc=18)\r\n```\r\n\r\nWhen upgrading datasets to 2.11.0, it fails with error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 2, in <module>\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1791, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 891, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1651, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 964, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"\/home\/ramon.casero\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/71f67f69e6e00e139903a121f96b71f39b65a6b6aaeb0862e6a5da3a3f565b4c\/mydataset.py\", line 682, in _split_generators\r\n    self.some_function()\r\n  File \"\/home\/ramon.casero\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/71f67f69e6e00e139903a121f96b71f39b65a6b6aaeb0862e6a5da3a3f565b4c\/mydataset.py\", line 1314, in some_function()\r\n    x_df = pd.DataFrame({'cell_type_descriptor': fp['x'].tolist()})\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/site-packages\/numpy\/lib\/npyio.py\", line 248, in __getitem__\r\n    bytes = self.zip.open(key)\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/zipfile.py\", line 1530, in open\r\n    fheader = zef_file.read(sizeFileHeader)\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/zipfile.py\", line 744, in read\r\n    self._file.seek(self._pos)\r\nValueError: seek of closed file\r\n```\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nSorry, I cannot share the data or code because they are not mine to share, but the point of failure is a call in `some_function()`\r\n\r\n```python\r\n        with np.load(filename) as fp:\r\n            x_df = pd.DataFrame({'feature': fp['x'].tolist()})\r\n```\r\n\r\nI'll try to generate a short snippet that reproduces the error.\r\n\r\n### Expected behavior\r\n\r\nI would expect that `load_dataset` works on the custom datasets generation script for v2.11.0 the same way it works for 2.10.1, without making `np.load()` give a `ValueError: seek of closed file` error.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.11.0\r\n- Platform: Linux-4.18.0-483.el8.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.10.8\r\n- Huggingface_hub version: 0.12.0\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.2\r\n- numpy: 1.24.2\r\n- This is an offline dataset that uses `datasets.config.HF_DATASETS_OFFLINE = True` in the generation script.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5712\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5712\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5711","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5711\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5711\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5711\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5711","id":1655971647,"node_id":"I_kwDODunzps5itB8_","number":5711,"title":"load_dataset in v2.11.0 raises \"ValueError: seek of closed file\" in np.load()","user":{"login":"rcasero","id":1219084,"node_id":"MDQ6VXNlcjEyMTkwODQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1219084?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/rcasero","html_url":"https:\/\/github.com\/rcasero","followers_url":"https:\/\/api.github.com\/users\/rcasero\/followers","following_url":"https:\/\/api.github.com\/users\/rcasero\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/rcasero\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/rcasero\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/rcasero\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/rcasero\/orgs","repos_url":"https:\/\/api.github.com\/users\/rcasero\/repos","events_url":"https:\/\/api.github.com\/users\/rcasero\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/rcasero\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-04-05T16:46:49Z","updated_at":"2023-04-07T09:16:59Z","closed_at":"2023-04-07T09:16:59Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi,\r\n\r\nI have some `dataset_load()` code of a custom offline dataset that works with datasets v2.10.1. \r\n\r\n```python\r\n        ds = datasets.load_dataset(path=dataset_dir,\r\n                                   name=configuration,\r\n                                   data_dir=dataset_dir,\r\n                                   cache_dir=cache_dir,\r\n                                   aux_dir=aux_dir,\r\n                                #    download_mode=datasets.DownloadMode.FORCE_REDOWNLOAD,\r\n                                   num_proc=18)\r\n```\r\n\r\nWhen upgrading datasets to 2.11.0, it fails with error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 2, in <module>\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1791, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 891, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1651, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 964, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"\/home\/ramon.casero\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/71f67f69e6e00e139903a121f96b71f39b65a6b6aaeb0862e6a5da3a3f565b4c\/mydataset.py\", line 682, in _split_generators\r\n    self.some_function()\r\n  File \"\/home\/ramon.casero\/.cache\/huggingface\/modules\/datasets_modules\/datasets\/71f67f69e6e00e139903a121f96b71f39b65a6b6aaeb0862e6a5da3a3f565b4c\/mydataset.py\", line 1314, in some_function()\r\n    x_df = pd.DataFrame({'cell_type_descriptor': fp['x'].tolist()})\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/site-packages\/numpy\/lib\/npyio.py\", line 248, in __getitem__\r\n    bytes = self.zip.open(key)\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/zipfile.py\", line 1530, in open\r\n    fheader = zef_file.read(sizeFileHeader)\r\n  File \"\/home\/ramon.casero\/opt\/miniconda3\/envs\/myenv\/lib\/python3.10\/zipfile.py\", line 744, in read\r\n    self._file.seek(self._pos)\r\nValueError: seek of closed file\r\n```\r\n\n\n### Steps to reproduce the bug\n\nSorry, I cannot share the data or code because they are not mine to share, but the point of failure is a call in `some_function()`\r\n\r\n```python\r\n        with np.load(embedding_filename) as fp:\r\n            x_df = pd.DataFrame({'feature': fp['x'].tolist()})\r\n```\r\n\r\nI'll try to generate a short snippet that reproduces the error.\n\n### Expected behavior\n\nI would expect that `load_dataset` works on the custom datasets generation script for v2.11.0 the same way it works for 2.10.1, without making `np.load()` give a `ValueError: seek of closed file` error.\n\n### Environment info\n\n- `datasets` version: 2.11.0\r\n- Platform: Linux-4.18.0-483.el8.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.10.8\r\n- Huggingface_hub version: 0.12.0\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.2\r\n- numpy: 1.24.2\r\n- This is an offline dataset that uses `datasets.config.HF_DATASETS_OFFLINE = True` in the generation script.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5711\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5711\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5710","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5710\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5710\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5710\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5710","id":1655703534,"node_id":"I_kwDODunzps5isAfu","number":5710,"title":"OSError: Memory mapping file failed: Cannot allocate memory","user":{"login":"Saibo-creator","id":53392976,"node_id":"MDQ6VXNlcjUzMzkyOTc2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/53392976?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Saibo-creator","html_url":"https:\/\/github.com\/Saibo-creator","followers_url":"https:\/\/api.github.com\/users\/Saibo-creator\/followers","following_url":"https:\/\/api.github.com\/users\/Saibo-creator\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Saibo-creator\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Saibo-creator\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Saibo-creator\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Saibo-creator\/orgs","repos_url":"https:\/\/api.github.com\/users\/Saibo-creator\/repos","events_url":"https:\/\/api.github.com\/users\/Saibo-creator\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Saibo-creator\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-05T14:11:26Z","updated_at":"2023-04-20T17:16:40Z","closed_at":"2023-04-20T17:16:40Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nHello, I have a series of datasets each of 5 GB, 600 datasets in total. So together this makes 3TB. \r\n\r\nWhen I trying to load all the 600 datasets into memory, I get the above error message. \r\n\r\nIs this normal because I'm hitting the max size of memory mapping of the OS? \r\n\r\n\r\nThank you\r\n\r\n```terminal\r\n0_21\/cache-e9c42499f65b1881.arrow\r\nload_hf_datasets_from_disk:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                     | 494\/600 [07:26<01:35,  1.11it\/s]\r\nTraceback (most recent call last):\r\n  File \"example_load_genkalm_dataset.py\", line 35, in <module>\r\n    multi_ds.post_process(max_node_num=args.max_node_num,max_seq_length=args.max_seq_length,delay=args.delay)\r\n  File \"\/home\/geng\/GenKaLM\/src\/dataloader\/dataset.py\", line 142, in post_process\r\n    genkalm_dataset = GenKaLM_Dataset.from_hf_dataset(path_or_name=ds_path, max_seq_length=self.max_seq_length,\r\n  File \"\/home\/geng\/GenKaLM\/src\/dataloader\/dataset.py\", line 47, in from_hf_dataset\r\n    hf_ds = load_from_disk(path_or_name)\r\n  File \"\/home\/geng\/.conda\/envs\/genkalm\/lib\/python3.8\/site-packages\/datasets\/load.py\", line 1848, in load_from_disk\r\n    return Dataset.load_from_disk(dataset_path, keep_in_memory=keep_in_memory, storage_options=storage_options)\r\n  File \"\/home\/geng\/.conda\/envs\/genkalm\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 1549, in load_from_disk\r\n    arrow_table = concat_tables(\r\n  File \"\/home\/geng\/.conda\/envs\/genkalm\/lib\/python3.8\/site-packages\/datasets\/table.py\", line 1805, in concat_tables\r\n    tables = list(tables)\r\n  File \"\/home\/geng\/.conda\/envs\/genkalm\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 1550, in <genexpr>\r\n    table_cls.from_file(Path(dataset_path, data_file[\"filename\"]).as_posix())\r\n  File \"\/home\/geng\/.conda\/envs\/genkalm\/lib\/python3.8\/site-packages\/datasets\/table.py\", line 1065, in from_file\r\n    table = _memory_mapped_arrow_table_from_file(filename)\r\n  File \"\/home\/geng\/.conda\/envs\/genkalm\/lib\/python3.8\/site-packages\/datasets\/table.py\", line 50, in _memory_mapped_arrow_table_from_file\r\n    memory_mapped_stream = pa.memory_map(filename)\r\n  File \"pyarrow\/io.pxi\", line 950, in pyarrow.lib.memory_map\r\n  File \"pyarrow\/io.pxi\", line 911, in pyarrow.lib.MemoryMappedFile._open\r\n  File \"pyarrow\/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 115, in pyarrow.lib.check_status\r\nOSError: Memory mapping file failed: Cannot allocate memory\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nSorry I can not provide a reproducible code as the data is stored on my server and it's too large to share.\r\n\r\n### Expected behavior\r\n\r\nI expect the 3TB of data can be fully mapped to memory\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.9.0\r\n- Platform: Linux-4.15.0-204-generic-x86_64-with-debian-buster-sid\r\n- Python version: 3.7.6\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.0.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5710\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5710\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5709","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5709\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5709\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5709\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5709","id":1655423503,"node_id":"I_kwDODunzps5iq8IP","number":5709,"title":"Manually dataset info made not taken into account","user":{"login":"jplu","id":959590,"node_id":"MDQ6VXNlcjk1OTU5MA==","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/959590?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jplu","html_url":"https:\/\/github.com\/jplu","followers_url":"https:\/\/api.github.com\/users\/jplu\/followers","following_url":"https:\/\/api.github.com\/users\/jplu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jplu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jplu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jplu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jplu\/orgs","repos_url":"https:\/\/api.github.com\/users\/jplu\/repos","events_url":"https:\/\/api.github.com\/users\/jplu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jplu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-05T11:15:17Z","updated_at":"2023-04-06T08:52:20Z","closed_at":"2023-04-06T08:52:19Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nHello,\r\n\r\nI'm manually building an image dataset with the `from_dict` approach. I also build the features with the `cast_features` methods. Once the dataset is created I push it on the hub, and a default `dataset_infos.json` file seems to have been automatically added to the repo in same time. Hence I update it manually with all the missing info, but when I download the dataset the info are never updated.\r\n\r\nFormer `dataset_infos.json` file:\r\n```\r\n{\"default\": {\r\n    \"description\": \"\",\r\n    \"citation\": \"\",\r\n    \"homepage\": \"\",\r\n    \"license\": \"\",\r\n    \"features\": {\r\n        \"image\": {\r\n            \"_type\": \"Image\"\r\n        },\r\n        \"labels\": {\r\n            \"names\": [\r\n                \"Fake\",\r\n                \"Real\"\r\n            ],\r\n            \"_type\": \"ClassLabel\"\r\n        }\r\n    },\r\n    \"splits\": {\r\n        \"validation\": {\r\n            \"name\": \"validation\",\r\n            \"num_bytes\": 901010094.0,\r\n            \"num_examples\": 3200,\r\n            \"dataset_name\": null\r\n        },\r\n        \"train\": {\r\n            \"name\": \"train\",\r\n            \"num_bytes\": 901010094.0,\r\n            \"num_examples\": 3200,\r\n            \"dataset_name\": null\r\n        }\r\n    },\r\n    \"download_size\": 1802008414,\r\n    \"dataset_size\": 1802020188.0,\r\n    \"size_in_bytes\": 3604028602.0\r\n}}\r\n```\r\n\r\nAfter I update it manually it looks like:\r\n```\r\n{\r\n   \"bstrai--deepfake-detection\":{\r\n      \"description\":\"\",\r\n      \"citation\":\"\",\r\n      \"homepage\":\"\",\r\n      \"license\":\"\",\r\n      \"features\":{\r\n         \"image\":{\r\n            \"decode\":true,\r\n            \"id\":null,\r\n            \"_type\":\"Image\"\r\n         },\r\n         \"labels\":{\r\n            \"num_classes\":2,\r\n            \"names\":[\r\n               \"Fake\",\r\n               \"Real\"\r\n            ],\r\n            \"id\":null,\r\n            \"_type\":\"ClassLabel\"\r\n         }\r\n      },\r\n      \"supervised_keys\":{\r\n         \"input\":\"image\",\r\n         \"output\":\"labels\"\r\n      },\r\n      \"task_templates\":[\r\n         {\r\n            \"task\":\"image-classification\",\r\n            \"image_column\":\"image\",\r\n            \"label_column\":\"labels\"\r\n         }\r\n      ],\r\n      \"config_name\":null,\r\n      \"splits\":{\r\n         \"validation\":{\r\n            \"name\":\"validation\",\r\n            \"num_bytes\":36627822,\r\n            \"num_examples\":123,\r\n            \"dataset_name\":\"deepfake-detection\"\r\n         },\r\n         \"train\":{\r\n            \"name\":\"train\",\r\n            \"num_bytes\":901023694,\r\n            \"num_examples\":3200,\r\n            \"dataset_name\":\"deepfake-detection\"\r\n         }\r\n      },\r\n      \"download_checksums\":null,\r\n      \"download_size\":937562209,\r\n      \"dataset_size\":937651516,\r\n      \"size_in_bytes\":1875213725\r\n   }\r\n}\r\n```\r\n\r\nAnything I should do to have the new infos in the `dataset_infos.json` to be taken into account? Or it is not possible yet?\r\n\r\nThanks!\n\n### Steps to reproduce the bug\n\n-\n\n### Expected behavior\n\n-\n\n### Environment info\n\n- `datasets` version: 2.11.0\r\n- Platform: Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\n- Python version: 3.10.10\r\n- Huggingface_hub version: 0.13.3\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5709\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5709\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5708","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5708\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5708\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5708\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5708","id":1655023642,"node_id":"I_kwDODunzps5ipaga","number":5708,"title":"Dataset sizes are in MiB instead of MB in dataset cards","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":3470211881,"node_id":"LA_kwDODunzps7O1zsp","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/dataset-viewer","name":"dataset-viewer","color":"E5583E","default":false,"description":"Related to the dataset viewer on huggingface.co"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":12,"created_at":"2023-04-05T06:36:03Z","updated_at":"2023-12-21T10:20:28Z","closed_at":"2023-12-21T10:20:27Z","author_association":"MEMBER","active_lock_reason":null,"body":"As @severo reported in an internal discussion (https:\/\/github.com\/huggingface\/moon-landing\/issues\/5929):\r\n\r\nNow we show the dataset size:\r\n- from the dataset card (in the side column)\r\n- from the datasets-server (in the viewer)\r\n\r\nBut, even if the size is the same, we see a mismatch because the viewer shows MB, while the info from the README generally shows MiB (even if it's written MB -> https:\/\/huggingface.co\/datasets\/blimp\/blob\/main\/README.md?code=true#L1932)\r\n\r\n<img width=\"664\" alt=\"Capture d\u2019e\u0301cran 2023-04-04 a\u0300 10 16 01\" src=\"https:\/\/user-images.githubusercontent.com\/1676121\/229730887-0bd8fa6e-9462-46c6-bd4e-4d2c5784cabb.png\">\r\n\r\nTODO: Values to be fixed in: `Size of downloaded dataset files:`, `Size of the generated dataset:` and `Total amount of disk used:`\r\n- [x] Bulk edit on the Hub to fix this in all canonical datasets\r\n- [x] Bulk PR on the Hub to fix ancient canonical datasets that were moved to organizations","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5708\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5708\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5706","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5706\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5706\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5706\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5706","id":1653545835,"node_id":"I_kwDODunzps5ijxtr","number":5706,"title":"Support categorical data types for Parquet","user":{"login":"kklemon","id":1430243,"node_id":"MDQ6VXNlcjE0MzAyNDM=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1430243?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kklemon","html_url":"https:\/\/github.com\/kklemon","followers_url":"https:\/\/api.github.com\/users\/kklemon\/followers","following_url":"https:\/\/api.github.com\/users\/kklemon\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kklemon\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kklemon\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kklemon\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kklemon\/orgs","repos_url":"https:\/\/api.github.com\/users\/kklemon\/repos","events_url":"https:\/\/api.github.com\/users\/kklemon\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kklemon\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":{"login":"mhattingpete","id":22622299,"node_id":"MDQ6VXNlcjIyNjIyMjk5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22622299?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mhattingpete","html_url":"https:\/\/github.com\/mhattingpete","followers_url":"https:\/\/api.github.com\/users\/mhattingpete\/followers","following_url":"https:\/\/api.github.com\/users\/mhattingpete\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mhattingpete\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mhattingpete\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mhattingpete\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mhattingpete\/orgs","repos_url":"https:\/\/api.github.com\/users\/mhattingpete\/repos","events_url":"https:\/\/api.github.com\/users\/mhattingpete\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mhattingpete\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mhattingpete","id":22622299,"node_id":"MDQ6VXNlcjIyNjIyMjk5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22622299?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mhattingpete","html_url":"https:\/\/github.com\/mhattingpete","followers_url":"https:\/\/api.github.com\/users\/mhattingpete\/followers","following_url":"https:\/\/api.github.com\/users\/mhattingpete\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mhattingpete\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mhattingpete\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mhattingpete\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mhattingpete\/orgs","repos_url":"https:\/\/api.github.com\/users\/mhattingpete\/repos","events_url":"https:\/\/api.github.com\/users\/mhattingpete\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mhattingpete\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":17,"created_at":"2023-04-04T09:45:35Z","updated_at":"2023-09-22T16:53:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\r\n\r\nHuggingface datasets does not seem to support categorical \/ dictionary data types for Parquet as of now. There seems to be a `TODO` in the code for this feature but no implementation yet. Below you can find sample code to reproduce the error that is currently thrown when attempting to read a Parquet file with categorical columns:\r\n\r\n```python\r\nimport pandas as pd\r\nimport pyarrow.parquet as pq\r\n\r\nfrom datasets import load_dataset\r\n\r\n# Create categorical sample DataFrame\r\ndf = pd.DataFrame({'type': ['foo', 'bar']}).astype('category')\r\ndf.to_parquet('data.parquet')\r\n\r\n# Read back as pyarrow table\r\ntable = pq.read_table('data.parquet')\r\nprint(table.schema)\r\n# type: dictionary<values=string, indices=int32, ordered=0>\r\n\r\n# Load with huggingface datasets\r\nload_dataset('parquet', data_files='data.parquet')\r\n```\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):                                                                                                                                                                                                        \r\n  File \".venv\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1875, in _prepare_split_single\r\n    writer.write_table(table)\r\n  File \".venv\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 566, in write_table\r\n    self._build_writer(inferred_schema=pa_table.schema)\r\n  File \".venv\/lib\/python3.10\/site-packages\/datasets\/arrow_writer.py\", line 379, in _build_writer\r\n    inferred_features = Features.from_arrow_schema(inferred_schema)                                                                                     \r\n  File \".venv\/lib\/python3.10\/site-packages\/datasets\/features\/features.py\", line 1622, in from_arrow_schema\r\n    obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\n  File \".venv\/lib\/python3.10\/site-packages\/datasets\/features\/features.py\", line 1622, in <dictcomp>\r\n    obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\n  File \".venv\/lib\/python3.10\/site-packages\/datasets\/features\/features.py\", line 1361, in generate_from_arrow_type\r\n    raise NotImplementedError  # TODO(thom) this will need access to the dictionary as well (for labels). I.e. to the py_table\r\nNotImplementedError\r\n```\r\n\r\n### Motivation\r\n\r\nCategorical data types, as offered by Pandas and implemented with the `DictionaryType` dtype in `pyarrow` can significantly reduce dataset size and are a handy way to turn textual features into numerical representations and back. Lack of support in Huggingface datasets greatly reduces compatibility with a common Pandas \/ Parquet feature.\r\n\r\n### Your contribution\r\n\r\nI could provide a PR. However, it would be nice to have an initial complexity estimate from one of the core developers first.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5706\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5706\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5705","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5705\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5705\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5705\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5705","id":1653500383,"node_id":"I_kwDODunzps5ijmnf","number":5705,"title":"Getting next item from IterableDataset took forever.","user":{"login":"HongtaoYang","id":16588434,"node_id":"MDQ6VXNlcjE2NTg4NDM0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16588434?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/HongtaoYang","html_url":"https:\/\/github.com\/HongtaoYang","followers_url":"https:\/\/api.github.com\/users\/HongtaoYang\/followers","following_url":"https:\/\/api.github.com\/users\/HongtaoYang\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/HongtaoYang\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/HongtaoYang\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/HongtaoYang\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/HongtaoYang\/orgs","repos_url":"https:\/\/api.github.com\/users\/HongtaoYang\/repos","events_url":"https:\/\/api.github.com\/users\/HongtaoYang\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/HongtaoYang\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-04-04T09:16:17Z","updated_at":"2023-04-05T23:35:41Z","closed_at":"2023-04-05T23:35:41Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI have a large dataset, about 500GB. The format of the dataset is parquet. \r\n\r\nI then load the dataset and try to get the first item\r\n```python\r\ndef get_one_item():\r\n    dataset = load_dataset(\"path\/to\/datafiles\", split=\"train\", cache_dir=\".\", streaming=True)\r\n    dataset = dataset.filter(lambda example: example['text'].startswith('Ar'))\r\n    print(next(iter(dataset)))\r\n```\r\n\r\nHowever, this function never finish. I waited ~10mins, the function was still running so I killed the process. I'm now using `line_profiler` to profile how long it would take to return one item. I'll be patient and wait for as long as it needs. \r\n\r\nI suspect the filter operation is the reason why it took so long. Can I get some possible reasons behind this?\r\n\r\n### Steps to reproduce the bug\r\n\r\nUnfortunately without my data files, there is no way to reproduce this bug.\r\n\r\n### Expected behavior\r\n\r\nWith `IteralbeDataset`, I expect the first item to be returned instantly.\r\n\r\n### Environment info\r\n\r\n- datasets version: 2.11.0 \r\n- python: 3.7.12","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5705\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5705\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5704","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5704\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5704\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5704\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5704","id":1653471356,"node_id":"PR_kwDODunzps5NkEvJ","number":5704,"title":"5537 speedup load","user":{"login":"semajyllek","id":35013374,"node_id":"MDQ6VXNlcjM1MDEzMzc0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35013374?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/semajyllek","html_url":"https:\/\/github.com\/semajyllek","followers_url":"https:\/\/api.github.com\/users\/semajyllek\/followers","following_url":"https:\/\/api.github.com\/users\/semajyllek\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/semajyllek\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/semajyllek\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/semajyllek\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/semajyllek\/orgs","repos_url":"https:\/\/api.github.com\/users\/semajyllek\/repos","events_url":"https:\/\/api.github.com\/users\/semajyllek\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/semajyllek\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-04T08:58:14Z","updated_at":"2023-04-07T16:10:55Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I reimplemented fsspec.spec.glob() in `hffilesystem.py` as `_glob`,  used it in `_resolve_single_pattern_in_dataset_repository` only, and saw a 20% speedup in times to load the config, on average. \r\n\r\nThat's not much when usually this step takes only 2-3 seconds for most datasets, but in this particular case,  `bigcode\/the-stack-dedup` , the loading time to get the config (not download the entire 6tb dataset, of course), went from ~170 secs to ~20 secs. \r\n\r\nWhat makes this work is this code in `_glob`:\r\n\r\n```  \r\n      if self.dir_cache is not None:\r\n            allpaths = self.dir_cache\r\n        else:\r\n            allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)\r\n```\r\n\r\nI also had to `import glob.has_magic( )` for `_glob()` (confusing, I know).\r\n\r\nI hope there is no issue with copying most of the code from `fsspec.spec.glob`, as it is a BSD 3-Clause License, \r\nand I left a comment about this in the docstring of` _glob()`, that we may want to delete.\r\n\r\nAs mentioned, I evaluated the speedup across a random selection of about 1000 datasets (not all 27k+), and verified that old_config.eq(new_method_config) with the build in method, but deleted this test and related code changes on the subsequent commit. It's in the commit history if anyone wants to see it. (Note this does not include the outlier of  `bigcode\/the-stack-dedup`\r\n\r\n|   |   old_time     |    new _time |     diff   |  pct_diff |\r\n| -- | -- | -- | -- | -- |\r\n| mean  |    3.340 |    2.642 |    0.698 |   18.404 |\r\n| min   |    2.024   |  1.976   |  -0.840  | -37.634 |\r\n| max   |  66.582  |   41.517 |   30.927  |  85.538 |","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5704\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5704\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5704","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5704","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5704.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5704.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5703","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5703\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5703\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5703\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5703","id":1653158955,"node_id":"PR_kwDODunzps5NjCCV","number":5703,"title":"[WIP][Test, Please ignore] Investigate performance impact of using multiprocessing only","user":{"login":"hvaara","id":1535968,"node_id":"MDQ6VXNlcjE1MzU5Njg=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/1535968?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/hvaara","html_url":"https:\/\/github.com\/hvaara","followers_url":"https:\/\/api.github.com\/users\/hvaara\/followers","following_url":"https:\/\/api.github.com\/users\/hvaara\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/hvaara\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/hvaara\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/hvaara\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/hvaara\/orgs","repos_url":"https:\/\/api.github.com\/users\/hvaara\/repos","events_url":"https:\/\/api.github.com\/users\/hvaara\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/hvaara\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-04T04:37:49Z","updated_at":"2023-04-20T03:17:37Z","closed_at":"2023-04-20T03:17:32Z","author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5703\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5703\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5703","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5703","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5703.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5703.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5702","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5702\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5702\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5702\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5702","id":1653104720,"node_id":"I_kwDODunzps5iiGBQ","number":5702,"title":"Is it possible or how to define a `datasets.Sequence` that could potentially be either a dict, a str, or None?","user":{"login":"gitforziio","id":10508116,"node_id":"MDQ6VXNlcjEwNTA4MTE2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/10508116?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/gitforziio","html_url":"https:\/\/github.com\/gitforziio","followers_url":"https:\/\/api.github.com\/users\/gitforziio\/followers","following_url":"https:\/\/api.github.com\/users\/gitforziio\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/gitforziio\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/gitforziio\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/gitforziio\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/gitforziio\/orgs","repos_url":"https:\/\/api.github.com\/users\/gitforziio\/repos","events_url":"https:\/\/api.github.com\/users\/gitforziio\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/gitforziio\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-04T03:20:43Z","updated_at":"2023-04-05T14:15:18Z","closed_at":"2023-04-05T14:15:17Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nHello! Apologies if my question sounds naive:\r\n\r\nI was wondering if it\u2019s possible, or how one would go about defining a 'datasets.Sequence' element in datasets.Features that could potentially be either a dict, a str, or None?\r\n\r\nSpecifically, I\u2019d like to define a feature for a list that contains 18 elements, each of which has been pre-defined as either a `dict or None` or `str or None` - as demonstrated in the slightly misaligned data provided below:\r\n\r\n```json\r\n[\r\n  [\r\n    {\"text\":\"\u8001\u5987\u4eba\",\"idxes\":[0,1,2]},null,{\"text\":\"\u8dea\",\"idxes\":[3]},null,null,null,null,{\"text\":\"\u5728\u90a3\u5751\u91cc\",\"idxes\":[4,5,6,7]},null,null,null,null,null,null,null,null,null,null],\r\n  [\r\n    {\"text\":\"\u90a3\u4e9b\u6c34\",\"idxes\":[13,14,15]},null,{\"text\":\"\u8200\",\"idxes\":[11]},null,null,null,null,null,{\"text\":\"\u5728\u90a3\u5751\u91cc\",\"idxes\":[4,5,6,7]},null,{\"text\":\"\u51fa\",\"idxes\":[12]},null,null,null,null,null,null,null],\r\n  [\r\n    {\"text\":\"\u6c34\",\"idxes\":[38]},\r\n    null,\r\n    {\"text\":\"\u8200\",\"idxes\":[40]},\r\n    \"\u5047\",  \/\/ note this is just a standalone string\r\n    null,null,null,{\"text\":\"\u5751\u91cc\",\"idxes\":[35,36]},null,null,null,null,null,null,null,null,null,null]]\r\n```\n\n### Motivation\n\nI'm currently working with a dataset of the following structure and I couldn't find a solution in the [documentation](https:\/\/huggingface.co\/docs\/datasets\/v2.11.0\/en\/package_reference\/main_classes#datasets.Features).\r\n\r\n```json\r\n{\"qid\":\"3-train-1058\",\"context\":\"\u6851\u6851\u5bb3\u6015\u4e86\u3002\u4ece\u7389\u7c73\u5730\u91cc\u8d70\u5230\u7530\u57c2\u4e0a\uff0c\u4ed6\u9065\u671b\u7740\u4ed6\u5bb6\u90a3\u5e62\u8349\u623f\u5b50\u91cc\u7684\u706f\u5149\uff0c\u77e5\u9053\u6bcd\u4eb2\u6ca1\u6709\u8ba9\u4ed6\u56de\u5bb6\u7684\u610f\u601d\uff0c\u5f88\u4f24\u611f\uff0c\u6709\u70b9\u60f3\u54ed\u3002\u4f46\u6ca1\u54ed\uff0c\u8f6c\u8eab\u671d\u963f\u6055\u5bb6\u8d70\u53bb\u3002\",\"corefs\":[[{\"text\":\"\u6851\u6851\",\"idxes\":[0,1]},{\"text\":\"\u4ed6\",\"idxes\":[17]}]],\"non_corefs\":[],\"outputs\":[[{\"text\":\"\u4ed6\",\"idxes\":[17]},null,{\"text\":\"\u8d70\",\"idxes\":[11]},null,null,null,null,null,{\"text\":\"\u4ece\u7389\u7c73\u5730\u91cc\",\"idxes\":[6,7,8,9,10]},{\"text\":\"\u5230\u7530\u57c2\u4e0a\",\"idxes\":[12,13,14,15]},null,null,null,null,null,null,null,null],[{\"text\":\"\u4ed6\",\"idxes\":[17]},null,{\"text\":\"\u8d70\",\"idxes\":[66]},null,null,null,null,null,null,null,{\"text\":\"\u8f6c\u8eab\u671d\u963f\u6055\u5bb6\u53bb\",\"idxes\":[60,61,62,63,64,65,67]},null,null,null,null,null,null,null],[{\"text\":\"\u706f\u5149\",\"idxes\":[30,31]},null,null,null,null,null,null,{\"text\":\"\u8349\u623f\u5b50\u91cc\",\"idxes\":[25,26,27,28]},null,null,null,null,null,null,null,null,null,null],[{\"text\":\"\u4ed6\",\"idxes\":[17]},{\"text\":\"\u4ed6\u5bb6\u90a3\u5e62\u8349\u623f\u5b50\",\"idxes\":[21,22,23,24,25,26,27]},null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,\"\u8fdc\"],[{\"text\":\"\u4ed6\",\"idxes\":[17]},{\"text\":\"\u963f\u6055\u5bb6\",\"idxes\":[63,64,65]},null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,\"\u53d8\u8fd1\"]]}\r\n```\n\n### Your contribution\n\nI'm going to provide the dataset at https:\/\/huggingface.co\/datasets\/2030NLP\/SpaCE2022 .","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5702\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5702\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5701","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5701\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5701\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5701\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5701","id":1652931399,"node_id":"PR_kwDODunzps5NiSCy","number":5701,"title":"Add Dataset.from_spark","user":{"login":"maddiedawson","id":106995444,"node_id":"U_kgDOBmCe9A","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/106995444?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/maddiedawson","html_url":"https:\/\/github.com\/maddiedawson","followers_url":"https:\/\/api.github.com\/users\/maddiedawson\/followers","following_url":"https:\/\/api.github.com\/users\/maddiedawson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/maddiedawson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/maddiedawson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/maddiedawson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/maddiedawson\/orgs","repos_url":"https:\/\/api.github.com\/users\/maddiedawson\/repos","events_url":"https:\/\/api.github.com\/users\/maddiedawson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/maddiedawson\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":21,"created_at":"2023-04-03T23:51:29Z","updated_at":"2023-06-16T16:39:32Z","closed_at":"2023-04-26T15:43:39Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Adds static method Dataset.from_spark to create datasets from Spark DataFrames.\r\n\r\nThis approach alleviates users of the need to materialize their dataframe---a common use case is that the user loads their dataset into a dataframe, uses Spark to apply some transformation to some of the columns, and then wants to train on the dataset.\r\n\r\nRelated issue: https:\/\/github.com\/huggingface\/datasets\/issues\/5678","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5701\/reactions","total_count":6,"+1":0,"-1":0,"laugh":0,"hooray":4,"confused":0,"heart":2,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5701\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5701","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5701","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5701.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5701.patch","merged_at":"2023-04-26T15:43:39Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5700","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5700\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5700\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5700\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5700","id":1652527530,"node_id":"PR_kwDODunzps5Ng6g_","number":5700,"title":"fix: fix wrong modification of the 'cache_file_name' -related paramet\u2026","user":{"login":"FrancoisNoyez","id":47528215,"node_id":"MDQ6VXNlcjQ3NTI4MjE1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47528215?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/FrancoisNoyez","html_url":"https:\/\/github.com\/FrancoisNoyez","followers_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/followers","following_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/orgs","repos_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/repos","events_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-04-03T18:05:26Z","updated_at":"2023-04-06T17:17:27Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\u2026ers values in 'train_test_split' + fix bad interaction between 'keep_in_memory' and 'cache_file_name' -related parameters (#5699)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5700\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5700\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5700","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5700","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5700.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5700.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5699","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5699\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5699\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5699\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5699","id":1652437419,"node_id":"I_kwDODunzps5ifjGr","number":5699,"title":"Issue when wanting to split in memory a cached dataset","user":{"login":"FrancoisNoyez","id":47528215,"node_id":"MDQ6VXNlcjQ3NTI4MjE1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47528215?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/FrancoisNoyez","html_url":"https:\/\/github.com\/FrancoisNoyez","followers_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/followers","following_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/orgs","repos_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/repos","events_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/FrancoisNoyez\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-03T17:00:07Z","updated_at":"2023-04-04T16:52:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\n**In the 'train_test_split' method of the Dataset class** (defined datasets\/arrow_dataset.py), **if 'self.cache_files' is not empty**, then, **regarding the input parameters 'train_indices_cache_file_name' and 'test_indices_cache_file_name', if they are None**, we modify them to make them not None, to see if we can just provide back \/ work from cached data. But if we can't provide cached data, we move on with the call to the method, except those two values are not None anymore, which will conflict with the use of the 'keep_in_memory' parameter down the line.\r\nIndeed, at some point we end up calling the 'select' method, **and if 'keep_in_memory' is True**, since the value of this method's parameter 'indices_cache_file_name' is now not None anymore, **an exception is raised, whose message is \"Please use either 'keep_in_memory' or 'indices_cache_file_name' but not both.\".**\r\nBecause of that, it's impossible to perform a train \/ test split of a cached dataset while requesting that the result not be cached. Which is inconvenient when one is just performing experiments, with no intention of caching the result.\r\n\r\nAside from this being inconvenient, **the code which lead up to that situation seems simply wrong** to me: the input variable should not be modified so as to change the user's intention just to perform a test, if that test can fail and respecting the user's intention is necessary to proceed in that case.\r\nTo fix this, I suggest to use other variables \/ other variable names, in order to host the value(s) needed to perform the test, so as not to change the originally input values needed by the rest of the method's code.\r\n\r\nAlso, **I don't see why an exception should be raised when the 'select' method is called with both 'keep_in_memory'=True and 'indices_cache_file_name'!=None**: should the use of 'keep_in_memory' not prevail anyway, specifying that the user does not want to perform caching, and so making irrelevant the value of 'indices_cache_file_name'? This is indeed what happens when we look further in the code, in the '\\_select_with_indices_mapping' method: when 'keep_in_memory' is True, then the value of indices_cache_file_name does not matter, the data will be written to a stream buffer anyway.\r\nHence I suggest to remove the raising of exception in those circumstances. Notably, to remove the raising of it in the 'select', '\\_select_with_indices_mapping', 'shuffle' and 'map' methods.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nimport datasets\r\n\r\ndef generate_examples():\r\n    for i in range(10):\r\n        yield {\"id\": i}\r\n\r\ndataset_ = datasets.Dataset.from_generator(\r\n    generate_examples,\r\n    keep_in_memory=False,\r\n)\r\ndataset_.train_test_split(\r\n    test_size=3,\r\n    shuffle=False,\r\n    keep_in_memory=True,\r\n    train_indices_cache_file_name=None,\r\n    test_indices_cache_file_name=None,\r\n)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe result of the above code should be a DatasetDict instance.\r\n\r\nInstead, we get the following exception stack:\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[3], line 1\r\n----> 1 dataset_.train_test_split(\r\n      2     test_size=3,\r\n      3     shuffle=False,\r\n      4     keep_in_memory=True,\r\n      5     train_indices_cache_file_name=None,\r\n      6     test_indices_cache_file_name=None,\r\n      7 )\r\n\r\nFile ~\/Work\/Developments\/datasets\/src\/datasets\/arrow_dataset.py:528, in transmit_format.<locals>.wrapper(*args, **kwargs)\r\n    521 self_format = {\r\n    522     \"type\": self._format_type,\r\n    523     \"format_kwargs\": self._format_kwargs,\r\n    524     \"columns\": self._format_columns,\r\n    525     \"output_all_columns\": self._output_all_columns,\r\n    526 }\r\n    527 # apply actual function\r\n--> 528 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    529 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    530 # re-apply format to the output\r\n\r\nFile ~\/Work\/Developments\/datasets\/src\/datasets\/fingerprint.py:511, in fingerprint_transform.<locals>._fingerprint.<locals>.wrapper(*args, **kwargs)\r\n    507             validate_fingerprint(kwargs[fingerprint_name])\r\n    509 # Call actual function\r\n--> 511 out = func(dataset, *args, **kwargs)\r\n    513 # Update fingerprint of in-place transforms + update in-place history of transforms\r\n    515 if inplace:  # update after calling func so that the fingerprint doesn't change if the function fails\r\n\r\nFile ~\/Work\/Developments\/datasets\/src\/datasets\/arrow_dataset.py:4428, in Dataset.train_test_split(self, test_size, train_size, shuffle, stratify_by_column, seed, generator, keep_in_memory, load_from_cache_file, train_indices_cache_file_name, test_indices_cache_file_name, writer_batch_size, train_new_fingerprint, test_new_fingerprint)\r\n   4425         test_indices = permutation[:n_test]\r\n   4426         train_indices = permutation[n_test : (n_test + n_train)]\r\n-> 4428 train_split = self.select(\r\n   4429     indices=train_indices,\r\n   4430     keep_in_memory=keep_in_memory,\r\n   4431     indices_cache_file_name=train_indices_cache_file_name,\r\n   4432     writer_batch_size=writer_batch_size,\r\n   4433     new_fingerprint=train_new_fingerprint,\r\n   4434 )\r\n   4435 test_split = self.select(\r\n   4436     indices=test_indices,\r\n   4437     keep_in_memory=keep_in_memory,\r\n   (...)\r\n   4440     new_fingerprint=test_new_fingerprint,\r\n   4441 )\r\n   4443 return DatasetDict({\"train\": train_split, \"test\": test_split})\r\n\r\nFile ~\/Work\/Developments\/datasets\/src\/datasets\/arrow_dataset.py:528, in transmit_format.<locals>.wrapper(*args, **kwargs)\r\n    521 self_format = {\r\n    522     \"type\": self._format_type,\r\n    523     \"format_kwargs\": self._format_kwargs,\r\n    524     \"columns\": self._format_columns,\r\n    525     \"output_all_columns\": self._output_all_columns,\r\n    526 }\r\n    527 # apply actual function\r\n--> 528 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    529 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    530 # re-apply format to the output\r\n\r\nFile ~\/Work\/Developments\/datasets\/src\/datasets\/fingerprint.py:511, in fingerprint_transform.<locals>._fingerprint.<locals>.wrapper(*args, **kwargs)\r\n    507             validate_fingerprint(kwargs[fingerprint_name])\r\n    509 # Call actual function\r\n--> 511 out = func(dataset, *args, **kwargs)\r\n    513 # Update fingerprint of in-place transforms + update in-place history of transforms\r\n    515 if inplace:  # update after calling func so that the fingerprint doesn't change if the function fails\r\n\r\nFile ~\/Work\/Developments\/datasets\/src\/datasets\/arrow_dataset.py:3679, in Dataset.select(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\r\n   3645 \"\"\"Create a new dataset with rows selected following the list\/array of indices.\r\n   3646 \r\n   3647 Args:\r\n   (...)\r\n   3676 ```\r\n   3677 \"\"\"\r\n   3678 if keep_in_memory and indices_cache_file_name is not None:\r\n-> 3679     raise ValueError(\"Please use either `keep_in_memory` or `indices_cache_file_name` but not both.\")\r\n   3681 if len(self.list_indexes()) > 0:\r\n   3682     raise DatasetTransformationNotAllowedError(\r\n   3683         \"Using `.select` on a dataset with attached indexes is not allowed. You can first run `.drop_index() to remove your index and then re-add it.\"\r\n   3684     )\r\n\r\nValueError: Please use either `keep_in_memory` or `indices_cache_file_name` but not both.\r\n```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.11.1.dev0\r\n- Platform: Linux-5.4.236-1-MANJARO-x86_64-with-glibc2.2.5\r\n- Python version: 3.8.12\r\n- Huggingface_hub version: 0.13.3\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.0\r\n\r\n***\r\n***\r\n\r\nEDIT:\r\nNow with a pull request to fix this [here](https:\/\/github.com\/huggingface\/datasets\/pull\/5700)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5699\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5699\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5698","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5698\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5698\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5698\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5698","id":1652183611,"node_id":"I_kwDODunzps5ielI7","number":5698,"title":"Add Qdrant as another search index","user":{"login":"kacperlukawski","id":2649301,"node_id":"MDQ6VXNlcjI2NDkzMDE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2649301?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/kacperlukawski","html_url":"https:\/\/github.com\/kacperlukawski","followers_url":"https:\/\/api.github.com\/users\/kacperlukawski\/followers","following_url":"https:\/\/api.github.com\/users\/kacperlukawski\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/kacperlukawski\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/kacperlukawski\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/kacperlukawski\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/kacperlukawski\/orgs","repos_url":"https:\/\/api.github.com\/users\/kacperlukawski\/repos","events_url":"https:\/\/api.github.com\/users\/kacperlukawski\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/kacperlukawski\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-04-03T14:25:19Z","updated_at":"2023-04-11T10:28:40Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nI'd suggest adding Qdrant (https:\/\/qdrant.tech) as another search index available, so users can directly build an index from a dataset. Currently, FAISS and ElasticSearch are only supported: https:\/\/huggingface.co\/docs\/datasets\/faiss_es \n\n### Motivation\n\nElasticSearch is a keyword-based search system, while FAISS is a vector search library. Vector database, such as Qdrant, is a different tool based on similarity (like FAISS) but is not limited to a single machine. It makes the vector database well-suited for bigger datasets and collaboration if several people want to access a particular dataset.\n\n### Your contribution\n\nI can provide a PR implementing that functionality on my own.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5698\/reactions","total_count":6,"+1":6,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5698\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5697","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5697\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5697\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5697\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5697","id":1651812614,"node_id":"PR_kwDODunzps5NefxZ","number":5697,"title":"Raise an error on missing distributed seed","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-04-03T10:44:58Z","updated_at":"2023-04-04T15:05:24Z","closed_at":"2023-04-04T14:58:16Z","author_association":"MEMBER","active_lock_reason":null,"body":"close https:\/\/github.com\/huggingface\/datasets\/issues\/5696","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5697\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5697\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5697","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5697","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5697.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5697.patch","merged_at":"2023-04-04T14:58:16Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5696","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5696\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5696\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5696\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5696","id":1651707008,"node_id":"I_kwDODunzps5icwyA","number":5696,"title":"Shuffle a sharded iterable dataset without seed can lead to duplicate data","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"assignees":[{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-04-03T09:40:03Z","updated_at":"2023-04-04T14:58:18Z","closed_at":"2023-04-04T14:58:18Z","author_association":"MEMBER","active_lock_reason":null,"body":"As reported in https:\/\/github.com\/huggingface\/datasets\/issues\/5360\r\n\r\nIf `seed=None` in `.shuffle()`, shuffled datasets don't use the same shuffling seed across nodes.\r\n\r\nBecause of that, the lists of shards is not shuffled the same way across nodes, and therefore some shards may be assigned to multiple nodes instead of exactly one.\r\n\r\nThis can happen only when you have a number of shards that is a factor of the number of nodes.\r\n\r\nThe current workaround is to always set a `seed` in `.shuffle()`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5696\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5696\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5695","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5695\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5695\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5695\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5695","id":1650974156,"node_id":"I_kwDODunzps5iZ93M","number":5695,"title":"Loading big dataset raises pyarrow.lib.ArrowNotImplementedError","user":{"login":"amariucaitheodor","id":32778667,"node_id":"MDQ6VXNlcjMyNzc4NjY3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32778667?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/amariucaitheodor","html_url":"https:\/\/github.com\/amariucaitheodor","followers_url":"https:\/\/api.github.com\/users\/amariucaitheodor\/followers","following_url":"https:\/\/api.github.com\/users\/amariucaitheodor\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/amariucaitheodor\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/amariucaitheodor\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/amariucaitheodor\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/amariucaitheodor\/orgs","repos_url":"https:\/\/api.github.com\/users\/amariucaitheodor\/repos","events_url":"https:\/\/api.github.com\/users\/amariucaitheodor\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/amariucaitheodor\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-04-02T14:42:44Z","updated_at":"2023-04-11T09:17:54Z","closed_at":"2023-04-10T08:04:04Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nCalling `datasets.load_dataset` to load the (publicly available) dataset `theodor1289\/wit` fails with `pyarrow.lib.ArrowNotImplementedError`.\n\n### Steps to reproduce the bug\n\nSteps to reproduce this behavior:\r\n\r\n1. `!pip install datasets`\r\n2. `!huggingface-cli login`\r\n3. This step will throw the error (it might take a while as the dataset has ~170GB):\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"theodor1289\/wit\", \"train\", use_auth_token=True)\r\n```\r\n\r\nStack trace:\r\n```\r\n(torch-multimodal) bash-4.2$ python test.py \r\nDownloading and preparing dataset None\/None to \/cluster\/work\/cotterell\/tamariucai\/HuggingfaceDatasets\/theodor1289___parquet\/theodor1289--wit-7a3e984414a86a0f\/0.0.0\/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2\/2 [00:00<00:00, 491.68it\/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2\/2 [00:00<00:00, 16.93it\/s]\r\nTraceback (most recent call last):                                                                                                                                                                                                    \r\n  File \"\/cluster\/home\/tamariucai\/.local\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1860, in _prepare_split_single\r\n    for _, table in generator:\r\n  File \"\/cluster\/home\/tamariucai\/.local\/lib\/python3.10\/site-packages\/datasets\/packaged_modules\/parquet\/parquet.py\", line 69, in _generate_tables\r\n    for batch_idx, record_batch in enumerate(\r\n  File \"pyarrow\/_parquet.pyx\", line 1323, in iter_batches\r\n  File \"pyarrow\/error.pxi\", line 121, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowNotImplementedError: Nested data conversions not implemented for chunked array outputs\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/cluster\/work\/cotterell\/tamariucai\/multimodal-mirror\/examples\/test.py\", line 2, in <module>\r\n    dataset = load_dataset(\"theodor1289\/wit\", \"train\", use_auth_token=True)\r\n  File \"\/cluster\/home\/tamariucai\/.local\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1791, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"\/cluster\/home\/tamariucai\/.local\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 891, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"\/cluster\/home\/tamariucai\/.local\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 986, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"\/cluster\/home\/tamariucai\/.local\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1748, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"\/cluster\/home\/tamariucai\/.local\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1893, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n\r\n```\n\n### Expected behavior\n\nThe dataset is loaded in variable `dataset`.\n\n### Environment info\n\n- `datasets` version: 2.11.0\r\n- Platform: Linux-3.10.0-1160.80.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.10.4\r\n- Huggingface_hub version: 0.13.3\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5695\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5695\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5694","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5694\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5694\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5694\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5694","id":1650467793,"node_id":"I_kwDODunzps5iYCPR","number":5694,"title":"Dataset configuration","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[{"id":2067400324,"node_id":"MDU6TGFiZWwyMDY3NDAwMzI0","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/generic%20discussion","name":"generic discussion","color":"c5def5","default":false,"description":"Generic discussion on the library"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-04-01T13:08:05Z","updated_at":"2023-04-04T14:54:37Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Following discussions from https:\/\/github.com\/huggingface\/datasets\/pull\/5331\r\n\r\nWe could have something like `config.json` to define the configuration of a dataset.\r\n\r\n```json\r\n{\r\n    \"data_dir\": \"data\"\r\n    \"data_files\":  {\r\n        \"train\": \"train-[0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9]*.*\"\r\n    }\r\n}\r\n```\r\n\r\nwe could also support a list for several configs with a 'config_name' field.\r\n\r\nThe alternative was to use YAML in the README.md.\r\n\r\nI think it could also support a `dataset_type` field to specify which dataset builder class to use, and the other parameters would be the builder's parameters. Some parameters exist for all builders like `data_files` and `data_dir`, but some parameters are builder specific like `sep` for csv.\r\n\r\nThis format would be used in `push_to_hub` to be able to push multiple configs.\r\n\r\ncc @huggingface\/datasets \r\n\r\nEDIT: actually we're going for the YAML approach in README.md","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5694\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5694\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5693","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5693\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5693\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5693\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5693","id":1649934749,"node_id":"PR_kwDODunzps5NYdPS","number":5693,"title":"[docs] Split pattern search order","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-31T19:51:38Z","updated_at":"2023-04-03T18:43:30Z","closed_at":"2023-04-03T18:29:58Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR addresses #5681 about the order of split patterns \ud83e\udd17 Datasets searches for when generating dataset splits.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5693\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5693\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5693","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5693","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5693.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5693.patch","merged_at":"2023-04-03T18:29:58Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5692","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5692\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5692\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5692\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5692","id":1649818644,"node_id":"I_kwDODunzps5iVjwU","number":5692,"title":"pyarrow.lib.ArrowInvalid: Unable to merge: Field <field> has incompatible types","user":{"login":"cyanic-selkie","id":32219669,"node_id":"MDQ6VXNlcjMyMjE5NjY5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/32219669?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/cyanic-selkie","html_url":"https:\/\/github.com\/cyanic-selkie","followers_url":"https:\/\/api.github.com\/users\/cyanic-selkie\/followers","following_url":"https:\/\/api.github.com\/users\/cyanic-selkie\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/cyanic-selkie\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/cyanic-selkie\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/cyanic-selkie\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/cyanic-selkie\/orgs","repos_url":"https:\/\/api.github.com\/users\/cyanic-selkie\/repos","events_url":"https:\/\/api.github.com\/users\/cyanic-selkie\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/cyanic-selkie\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-03-31T18:19:40Z","updated_at":"2023-09-07T11:42:49Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen loading the dataset [wikianc-en](https:\/\/huggingface.co\/datasets\/cyanic-selkie\/wikianc-en) which I created using [this](https:\/\/github.com\/cyanic-selkie\/wikianc) code, I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/sven\/code\/rector\/answer-detection\/train.py\", line 106, in <module>\r\n    (dataset, weights) = get_dataset(args.dataset, tokenizer, labels, args.padding)\r\n  File \"\/home\/sven\/code\/rector\/answer-detection\/dataset.py\", line 106, in get_dataset\r\n    dataset = load_dataset(\"cyanic-selkie\/wikianc-en\")\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/load.py\", line 1794, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1106, in as_dataset\r\n    datasets = map_nested(\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 443, in map_nested\r\n    mapped = [\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 444, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True, None))\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/utils\/py_utils.py\", line 346, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1136, in _build_single_dataset\r\n    ds = self._as_dataset(\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/builder.py\", line 1207, in _as_dataset\r\n    dataset_kwargs = ArrowReader(cache_dir, self.info).read(\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/arrow_reader.py\", line 239, in read\r\n    return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/arrow_reader.py\", line 260, in read_files\r\n    pa_table = self._read_files(files, in_memory=in_memory)\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/arrow_reader.py\", line 203, in _read_files\r\n    pa_table = concat_tables(pa_tables) if len(pa_tables) != 1 else pa_tables[0]\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1808, in concat_tables\r\n    return ConcatenationTable.from_tables(tables, axis=axis)\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1514, in from_tables\r\n    return cls.from_blocks(blocks)\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1427, in from_blocks\r\n    table = cls._concat_blocks(blocks, axis=0)\r\n  File \"\/home\/sven\/.cache\/pypoetry\/virtualenvs\/rector-Z2mdKRnn-py3.10\/lib\/python3.10\/site-packages\/datasets\/table.py\", line 1373, in _concat_blocks\r\n    return pa.concat_tables(pa_tables, promote=True)\r\n  File \"pyarrow\/table.pxi\", line 5224, in pyarrow.lib.concat_tables\r\n  File \"pyarrow\/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Unable to merge: Field paragraph_anchors has incompatible types: list<: struct<start: uint32 not null, end: uint32 not null, qid: uint32, pageid: uint32, title: string not null> not null> vs list<item: struct<start: uint32, end: uint32, qid: uint32, pageid: uint32, title: string>>\r\n```\r\n\r\nThis only happens when I load the `train` split, indicating that the size of the dataset is the deciding factor. \r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"cyanic-selkie\/wikianc-en\", split=\"train\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe dataset should load normally without any errors.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.10.1\r\n- Platform: Linux-6.2.8-arch1-1-x86_64-with-glibc2.37\r\n- Python version: 3.10.10\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5692\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5692\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5691","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5691\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5691\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5691\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5691","id":1649737526,"node_id":"PR_kwDODunzps5NX08d","number":5691,"title":"[docs] Compress data files","user":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-03-31T17:17:26Z","updated_at":"2023-04-19T13:37:32Z","closed_at":"2023-04-19T07:25:58Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR addresses the comments in #5687 about compressing text file extensions before uploading to the Hub. Also clarified what \"too large\" means based on the GitLFS [docs](https:\/\/docs.github.com\/en\/repositories\/working-with-files\/managing-large-files\/about-git-large-file-storage).","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5691\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5691\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5691","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5691","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5691.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5691.patch","merged_at":"2023-04-19T07:25:58Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5689","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5689\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5689\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5689\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5689","id":1648956349,"node_id":"PR_kwDODunzps5NVMuI","number":5689,"title":"Support streaming Beam datasets from HF GCS preprocessed data","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-03-31T08:44:24Z","updated_at":"2023-04-12T05:57:55Z","closed_at":"2023-04-12T05:50:31Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR implements streaming Apache Beam datasets that are already preprocessed by us and stored in the HF Google Cloud Storage:\r\n- natural_questions\r\n- wiki40b\r\n- wikipedia\r\n\r\nThis is done by streaming from the prepared Arrow files in HF Google Cloud Storage.\r\n\r\nThis will fix their corresponding dataset viewers. Related to:\r\n- https:\/\/github.com\/huggingface\/datasets-server\/pull\/988#discussion_r1150767138\r\n\r\nRelated to:\r\n- https:\/\/huggingface.co\/datasets\/natural_questions\/discussions\/4\r\n- https:\/\/huggingface.co\/datasets\/wiki40b\/discussions\/2\r\n- https:\/\/huggingface.co\/datasets\/wikipedia\/discussions\/9\r\n\r\nCC: @severo ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5689\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5689\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5689","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5689","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5689.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5689.patch","merged_at":"2023-04-12T05:50:30Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5690","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5690\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5690\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5690\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5690","id":1649289883,"node_id":"I_kwDODunzps5iTiqb","number":5690,"title":"raise AttributeError(f\"No {package_name} attribute {name}\") AttributeError: No huggingface_hub attribute hf_api","user":{"login":"wccccp","id":55964850,"node_id":"MDQ6VXNlcjU1OTY0ODUw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/55964850?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/wccccp","html_url":"https:\/\/github.com\/wccccp","followers_url":"https:\/\/api.github.com\/users\/wccccp\/followers","following_url":"https:\/\/api.github.com\/users\/wccccp\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/wccccp\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/wccccp\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/wccccp\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/wccccp\/orgs","repos_url":"https:\/\/api.github.com\/users\/wccccp\/repos","events_url":"https:\/\/api.github.com\/users\/wccccp\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/wccccp\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-03-31T08:22:22Z","updated_at":"2023-07-21T14:21:57Z","closed_at":"2023-07-21T14:21:57Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nrta.sh\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 7, in <module>\r\n    import datasets\r\n  File \"\/home\/appuser\/miniconda3\/envs\/pt2\/lib\/python3.8\/site-packages\/datasets\/__init__.py\", line 37, in <module>\r\n    from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n  File \"\/home\/appuser\/miniconda3\/envs\/pt2\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 44, in <module>\r\n    from .data_files import DataFilesDict, _sanitize_patterns\r\n  File \"\/home\/appuser\/miniconda3\/envs\/pt2\/lib\/python3.8\/site-packages\/datasets\/data_files.py\", line 120, in <module>\r\n    dataset_info: huggingface_hub.hf_api.DatasetInfo,\r\n  File \"\/home\/appuser\/miniconda3\/envs\/pt2\/lib\/python3.8\/site-packages\/huggingface_hub\/__init__.py\", line 290, in __getattr__\r\n    raise AttributeError(f\"No {package_name} attribute {name}\")\r\nAttributeError: No huggingface_hub attribute hf_api\r\n\n\n### Reproduction\n\n_No response_\n\n### Logs\n\n```shell\nTraceback (most recent call last):\r\n  File \"run.py\", line 7, in <module>\r\n    import datasets\r\n  File \"\/home\/appuser\/miniconda3\/envs\/pt2\/lib\/python3.8\/site-packages\/datasets\/__init__.py\", line 37, in <module>\r\n    from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n  File \"\/home\/appuser\/miniconda3\/envs\/pt2\/lib\/python3.8\/site-packages\/datasets\/builder.py\", line 44, in <module>\r\n    from .data_files import DataFilesDict, _sanitize_patterns\r\n  File \"\/home\/appuser\/miniconda3\/envs\/pt2\/lib\/python3.8\/site-packages\/datasets\/data_files.py\", line 120, in <module>\r\n    dataset_info: huggingface_hub.hf_api.DatasetInfo,\r\n  File \"\/home\/appuser\/miniconda3\/envs\/pt2\/lib\/python3.8\/site-packages\/huggingface_hub\/__init__.py\", line 290, in __getattr__\r\n    raise AttributeError(f\"No {package_name} attribute {name}\")\r\nAttributeError: No huggingface_hub attribute hf_api\n```\n\n\n### System info\n\n```shell\n- huggingface_hub version: 0.13.2\r\n- Platform: Linux-5.4.0-144-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- Running in iPython ?: No\r\n- Running in notebook ?: No\r\n- Running in Google Colab ?: No\r\n- Token path ?: \/home\/appuser\/.cache\/huggingface\/token\r\n- Has saved token ?: False\r\n- Configured git credential helpers: \r\n- FastAI: N\/A\r\n- Tensorflow: N\/A\r\n- Torch: 1.7.1\r\n- Jinja2: N\/A\r\n- Graphviz: N\/A\r\n- Pydot: N\/A\r\n- Pillow: 9.3.0\r\n- hf_transfer: N\/A\r\n- ENDPOINT: https:\/\/huggingface.co\r\n- HUGGINGFACE_HUB_CACHE: \/home\/appuser\/.cache\/huggingface\/hub\r\n- HUGGINGFACE_ASSETS_CACHE: \/home\/appuser\/.cache\/huggingface\/assets\r\n- HF_TOKEN_PATH: \/home\/appuser\/.cache\/huggingface\/token\r\n- HF_HUB_OFFLINE: False\r\n- HF_HUB_DISABLE_TELEMETRY: False\r\n- HF_HUB_DISABLE_PROGRESS_BARS: None\r\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\r\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\n```\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5690\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5690\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5688","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5688\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5688\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5688\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5688","id":1648463504,"node_id":"I_kwDODunzps5iQY6Q","number":5688,"title":"Wikipedia download_and_prepare for GCS","user":{"login":"adrianfagerland","id":25522531,"node_id":"MDQ6VXNlcjI1NTIyNTMx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/25522531?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/adrianfagerland","html_url":"https:\/\/github.com\/adrianfagerland","followers_url":"https:\/\/api.github.com\/users\/adrianfagerland\/followers","following_url":"https:\/\/api.github.com\/users\/adrianfagerland\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/adrianfagerland\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/adrianfagerland\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/adrianfagerland\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/adrianfagerland\/orgs","repos_url":"https:\/\/api.github.com\/users\/adrianfagerland\/repos","events_url":"https:\/\/api.github.com\/users\/adrianfagerland\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/adrianfagerland\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-03-30T23:43:22Z","updated_at":"2023-03-31T13:31:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI am unable to download the wikipedia dataset onto GCS.\r\n\r\nWhen I run the script provided the memory firstly gets eaten up, then it crashes. \r\nI tried running this on a VM with 128GB RAM and all I got was a two empty files: _data_builder.lock_, _data.incomplete\/beam-temp-wikipedia-train-1ab2039acf3611ed87a9893475de0093_\r\n\r\nI have troubleshot this for two straight days now, but I am just unable to get the dataset into storage.\r\n\r\n### Steps to reproduce the bug\r\n\r\nRun this and insert a path:\r\n```\r\nimport datasets\r\n\r\nbuilder = datasets.load_dataset_builder(\r\n    \"wikipedia\", language=\"en\", date=\"20230320\", beam_runner=\"DirectRunner\")\r\n\r\nbuilder.download_and_prepare({path}, file_format=\"parquet\")\r\n```\r\nThis is where the problem of it eating RAM occurs.\r\n\r\n\r\n\r\nI have also tried several versions of this, based on the docs:\r\n```\r\nimport gcsfs\r\nimport datasets\r\n\r\nstorage_options = {\"project\": \"tdt4310\", \"token\": \"cloud\"}\r\nfs = gcsfs.GCSFileSystem(**storage_options)\r\n\r\noutput_dir = \"gcs:\/\/wikipediadata\/\"\r\nbuilder = datasets.load_dataset_builder(\r\n    \"wikipedia\", date=\"20230320\", language=\"en\",  beam_runner=\"DirectRunner\")\r\nbuilder.download_and_prepare(\r\n    output_dir, storage_options=storage_options, file_format=\"parquet\")\r\n```\r\nThe error message that is received here is:\r\n\r\n> ValueError: Unable to get filesystem from specified path, please use the correct path or ensure the required dependency is installed, e.g., pip install apache-beam[gcp]. Path specified: gcs:\/\/wikipediadata\/wikipedia-train [while running 'train\/Save to parquet\/Write\/WriteImpl\/InitializeWrite']\r\n\r\nI have ran `pip install apache-beam[gcp]`\r\n\r\n### Expected behavior\r\n\r\nThe wikipedia data loaded into GCS\r\n\r\nEverything worked when testing with a smaller demo dataset found somewhere in the docs\r\n\r\n### Environment info\r\n\r\nNewest published version of datasets. Python 3.9. Also tested with Python 3.7. 128GB RAM Google Cloud VM instance.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5688\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5688\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5687","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5687\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5687\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5687\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5687","id":1647009018,"node_id":"I_kwDODunzps5iK1z6","number":5687,"title":"Document to compress data files before uploading","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-03-30T06:41:07Z","updated_at":"2023-04-19T07:25:59Z","closed_at":"2023-04-19T07:25:59Z","author_association":"MEMBER","active_lock_reason":null,"body":"In our docs to [Share a dataset to the Hub](https:\/\/huggingface.co\/docs\/datasets\/upload_dataset), we tell users to upload directly their data files, like CSV, JSON, JSON-Lines, text,... However, these extensions are not tracked by Git LFS by default, as they are not in the `.giattributes` file. Therefore, if they are too large, Git will fail to commit\/upload them.\r\n\r\nI think for those file extensions (.csv, .json, .jsonl, .txt), we should better recommend to **compress** their data files (using ZIP for example) before uploading them to the Hub.\r\n- Compressed files are tracked by Git LFS in our default `.gitattributes` file\r\n\r\nWhat do you think?\r\nCC: @stevhliu \r\n\r\nSee related issue:\r\n- https:\/\/huggingface.co\/datasets\/tcor0005\/langchain-docs-400-chunksize\/discussions\/1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5687\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5687\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5686","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5686\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5686\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5686\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5686","id":1646308228,"node_id":"PR_kwDODunzps5NMXdu","number":5686,"title":"set dev version","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-03-29T18:24:13Z","updated_at":"2023-03-29T18:33:49Z","closed_at":"2023-03-29T18:24:22Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5686\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5686\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5686","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5686","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5686.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5686.patch","merged_at":"2023-03-29T18:24:22Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5685","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5685\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5685\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5685\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5685","id":1646048667,"node_id":"I_kwDODunzps5iHLWb","number":5685,"title":"Broken Image render on the hub website","user":{"login":"FrancescoSaverioZuppichini","id":15908060,"node_id":"MDQ6VXNlcjE1OTA4MDYw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/15908060?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/FrancescoSaverioZuppichini","html_url":"https:\/\/github.com\/FrancescoSaverioZuppichini","followers_url":"https:\/\/api.github.com\/users\/FrancescoSaverioZuppichini\/followers","following_url":"https:\/\/api.github.com\/users\/FrancescoSaverioZuppichini\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/FrancescoSaverioZuppichini\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/FrancescoSaverioZuppichini\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/FrancescoSaverioZuppichini\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/FrancescoSaverioZuppichini\/orgs","repos_url":"https:\/\/api.github.com\/users\/FrancescoSaverioZuppichini\/repos","events_url":"https:\/\/api.github.com\/users\/FrancescoSaverioZuppichini\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/FrancescoSaverioZuppichini\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-03-29T15:25:30Z","updated_at":"2023-03-30T07:54:25Z","closed_at":"2023-03-30T07:54:25Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi :wave: \r\n\r\nNot sure if this is the right place to ask, but I am trying to load a huge amount of datasets on the hub (:partying_face: ) but I am facing a little issue with the `image` type\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/15908060\/228587875-427a37f1-3a31-4e17-8bbe-0f759003910d.png)\r\n\r\nSee this [dataset](https:\/\/huggingface.co\/datasets\/Francesco\/cell-towers), basically for some reason the first image has numerical bytes inside, not sure if that is okay, but the image render feature **doesn't work**\r\n\r\nSo the dataset is stored in the following way\r\n\r\n```python\r\n    builder.download_and_prepare(output_dir=str(output_dir))\r\n\r\n    ds = builder.as_dataset(split=\"train\")\r\n    # [NOTE] no idea how to push it from the builder folder\r\n    ds.push_to_hub(repo_id=repo_id)\r\n    builder.as_dataset(split=\"validation\").push_to_hub(repo_id=repo_id)\r\n    ds = builder.as_dataset(split=\"test\")\r\n    ds.push_to_hub(repo_id=repo_id)\r\n   ```\r\n   \r\n   The build is this class\r\n   \r\n   ```python\r\n       class COCOLikeDatasetBuilder(datasets.GeneratorBasedBuilder):\r\n\r\n        VERSION = datasets.Version(\"1.0.0\")\r\n\r\n        def _info(self):\r\n            features = datasets.Features(\r\n                {\r\n                    \"image_id\": datasets.Value(\"int64\"),\r\n                    \"image\": datasets.Image(),\r\n                    \"width\": datasets.Value(\"int32\"),\r\n                    \"height\": datasets.Value(\"int32\"),\r\n                    \"objects\": datasets.Sequence(\r\n                        {\r\n                            \"id\": datasets.Value(\"int64\"),\r\n                            \"area\": datasets.Value(\"int64\"),\r\n                            \"bbox\": datasets.Sequence(\r\n                                datasets.Value(\"float32\"), length=4\r\n                            ),\r\n                            \"category\": datasets.ClassLabel(names=categories),\r\n                        }\r\n                    ),\r\n                }\r\n            )\r\n            return datasets.DatasetInfo(\r\n                description=description,\r\n                features=features,\r\n                homepage=homepage,\r\n                license=license,\r\n                citation=citation,\r\n            )\r\n\r\n        def _split_generators(self, dl_manager):\r\n            archive = dl_manager.download(url)\r\n\r\n            return [\r\n                datasets.SplitGenerator(\r\n                    name=datasets.Split.TRAIN,\r\n                    gen_kwargs={\r\n                        \"annotation_file_path\": \"train\/_annotations.coco.json\",\r\n                        \"files\": dl_manager.iter_archive(archive),\r\n                    },\r\n                ),\r\n                datasets.SplitGenerator(\r\n                    name=datasets.Split.VALIDATION,\r\n                    gen_kwargs={\r\n                        \"annotation_file_path\": \"test\/_annotations.coco.json\",\r\n                        \"files\": dl_manager.iter_archive(archive),\r\n                    },\r\n                ),\r\n                datasets.SplitGenerator(\r\n                    name=datasets.Split.TEST,\r\n                    gen_kwargs={\r\n                        \"annotation_file_path\": \"valid\/_annotations.coco.json\",\r\n                        \"files\": dl_manager.iter_archive(archive),\r\n                    },\r\n                ),\r\n            ]\r\n\r\n        def _generate_examples(self, annotation_file_path, files):\r\n            def process_annot(annot, category_id_to_category):\r\n                return {\r\n                    \"id\": annot[\"id\"],\r\n                    \"area\": annot[\"area\"],\r\n                    \"bbox\": annot[\"bbox\"],\r\n                    \"category\": category_id_to_category[annot[\"category_id\"]],\r\n                }\r\n\r\n            image_id_to_image = {}\r\n            idx = 0\r\n\r\n            # This loop relies on the ordering of the files in the archive:\r\n            # Annotation files come first, then the images.\r\n            for path, f in files:\r\n                file_name = os.path.basename(path)\r\n                if annotation_file_path in path:\r\n                    annotations = json.load(f)\r\n                    category_id_to_category = {\r\n                        category[\"id\"]: category[\"name\"]\r\n                        for category in annotations[\"categories\"]\r\n                    }\r\n                    print(category_id_to_category)\r\n                    image_id_to_annotations = collections.defaultdict(list)\r\n                    for annot in annotations[\"annotations\"]:\r\n                        image_id_to_annotations[annot[\"image_id\"]].append(annot)\r\n                    image_id_to_image = {\r\n                        annot[\"file_name\"]: annot for annot in annotations[\"images\"]\r\n                    }\r\n                elif file_name in image_id_to_image:\r\n                    image = image_id_to_image[file_name]\r\n                    objects = [\r\n                        process_annot(annot, category_id_to_category)\r\n                        for annot in image_id_to_annotations[image[\"id\"]]\r\n                    ]\r\n                    print(file_name)\r\n                    yield idx, {\r\n                        \"image_id\": image[\"id\"],\r\n                        \"image\": {\"path\": path, \"bytes\": f.read()},\r\n                        \"width\": image[\"width\"],\r\n                        \"height\": image[\"height\"],\r\n                        \"objects\": objects,\r\n                    }\r\n                    idx += 1\r\n```\r\n\r\nBasically, I want to add to the hub every dataset I come across on coco format\r\n\r\nThanks \r\n\r\nFra\n\n### Steps to reproduce the bug\n\nIn this case, you can just navigate on the [dataset](https:\/\/huggingface.co\/datasets\/Francesco\/cell-towers)\n\n### Expected behavior\n\nI was expecting the image rendering feature to work\n\n### Environment info\n\nNot a lot to share, I am using `datasets` from a fresh venv","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5685\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5685\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5684","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5684\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5684\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5684\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5684","id":1646013226,"node_id":"PR_kwDODunzps5NLXWm","number":5684,"title":"Release: 2.11.0","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-03-29T15:06:07Z","updated_at":"2023-03-29T18:30:34Z","closed_at":"2023-03-29T18:15:54Z","author_association":"MEMBER","active_lock_reason":null,"body":null,"reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5684\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5684\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5684","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5684","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5684.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5684.patch","merged_at":"2023-03-29T18:15:54Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5683","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5683\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5683\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5683\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5683","id":1646001197,"node_id":"PR_kwDODunzps5NLUq1","number":5683,"title":"Fix verification_mode when ignore_verifications is passed","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-29T15:00:50Z","updated_at":"2023-03-29T17:36:06Z","closed_at":"2023-03-29T17:28:57Z","author_association":"MEMBER","active_lock_reason":null,"body":"This PR fixes the values assigned to `verification_mode` when passing `ignore_verifications` to `load_dataset`.\r\n\r\nRelated to:\r\n- #5303\r\n\r\nFix #5682.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5683\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5683\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5683","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5683","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5683.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5683.patch","merged_at":"2023-03-29T17:28:57Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5682","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5682\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5682\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5682\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5682","id":1646000571,"node_id":"I_kwDODunzps5iG_m7","number":5682,"title":"ValueError when passing ignore_verifications","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-03-29T15:00:30Z","updated_at":"2023-03-29T17:28:58Z","closed_at":"2023-03-29T17:28:58Z","author_association":"MEMBER","active_lock_reason":null,"body":"When passing `ignore_verifications=True` to `load_dataset`, we get a ValueError:\r\n```\r\nValueError: 'none' is not a valid VerificationMode\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5682\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5682\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5681","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5681\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5681\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5681\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5681","id":1645630784,"node_id":"I_kwDODunzps5iFlVA","number":5681,"title":"Add information about patterns search order to the doc about structuring repo","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"}],"state":"closed","locked":false,"assignee":{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false},"assignees":[{"login":"stevhliu","id":59462357,"node_id":"MDQ6VXNlcjU5NDYyMzU3","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/59462357?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/stevhliu","html_url":"https:\/\/github.com\/stevhliu","followers_url":"https:\/\/api.github.com\/users\/stevhliu\/followers","following_url":"https:\/\/api.github.com\/users\/stevhliu\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/stevhliu\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/stevhliu\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/stevhliu\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/stevhliu\/orgs","repos_url":"https:\/\/api.github.com\/users\/stevhliu\/repos","events_url":"https:\/\/api.github.com\/users\/stevhliu\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/stevhliu\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-03-29T11:44:49Z","updated_at":"2023-04-03T18:31:11Z","closed_at":"2023-04-03T18:31:11Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Following [this](https:\/\/github.com\/huggingface\/datasets\/issues\/5650) issue I think we should add a note about the order of patterns that is used to find splits, see [my comment](https:\/\/github.com\/huggingface\/datasets\/issues\/5650#issuecomment-1488412527). Also we should reference this page in pages about packaged loaders. \r\n\r\nI have a d\u00e9j\u00e0 vu that it had already been discussed as some point but I don't remember....","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5681\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5681\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5680","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5680\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5680\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5680\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5680","id":1645430103,"node_id":"PR_kwDODunzps5NJYNz","number":5680,"title":"Fix a description error for interleave_datasets.","user":{"login":"QizhiPei","id":55624066,"node_id":"MDQ6VXNlcjU1NjI0MDY2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/55624066?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/QizhiPei","html_url":"https:\/\/github.com\/QizhiPei","followers_url":"https:\/\/api.github.com\/users\/QizhiPei\/followers","following_url":"https:\/\/api.github.com\/users\/QizhiPei\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/QizhiPei\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/QizhiPei\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/QizhiPei\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/QizhiPei\/orgs","repos_url":"https:\/\/api.github.com\/users\/QizhiPei\/repos","events_url":"https:\/\/api.github.com\/users\/QizhiPei\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/QizhiPei\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-03-29T09:50:23Z","updated_at":"2023-03-30T13:14:19Z","closed_at":"2023-03-30T13:07:18Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"There is a description mistake in the annotation of interleave_dataset with \"all_exhausted\" stopping_strategy.\r\n``` python\r\nd1 = Dataset.from_dict({\"a\": [0, 1, 2]})\r\nd2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\r\nd3 = Dataset.from_dict({\"a\": [20, 21, 22, 23, 24]})\r\ndataset = interleave_datasets([d1, d2, d3], stopping_strategy=\"all_exhausted\")\r\n```\r\nAccording to the interleave way, the correct output of `dataset[\"a\"]` is `[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 23, 1, 10, 24]`, not `[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 23, 1, 0, 24]`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5680\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5680\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5680","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5680","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5680.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5680.patch","merged_at":"2023-03-30T13:07:18Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5679","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5679\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5679\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5679\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5679","id":1645184622,"node_id":"I_kwDODunzps5iD4Zu","number":5679,"title":"Allow load_dataset to take a working dir for intermediate data","user":{"login":"lu-wang-dl","id":38018689,"node_id":"MDQ6VXNlcjM4MDE4Njg5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/38018689?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lu-wang-dl","html_url":"https:\/\/github.com\/lu-wang-dl","followers_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/followers","following_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/orgs","repos_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/repos","events_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-03-29T07:21:09Z","updated_at":"2023-04-12T22:30:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nAs a user, I can set a working dir for intermediate data creation. The processed files will be moved to the cache dir, like\r\n```\r\nload_dataset(\u2026, working_dir=\u201d\/temp\/dir\u201d, cache_dir=\u201d\/cloud_dir\u201d).\r\n```\n\n### Motivation\n\nThis will help the use case for using datasets with cloud storage as cache. It will help boost the performance.\n\n### Your contribution\n\nI can provide a PR to fix this if the proposal seems reasonable.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5679\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5679\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5678","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5678\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5678\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5678\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5678","id":1645018359,"node_id":"I_kwDODunzps5iDPz3","number":5678,"title":"Add support to create a Dataset from spark dataframe","user":{"login":"lu-wang-dl","id":38018689,"node_id":"MDQ6VXNlcjM4MDE4Njg5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/38018689?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lu-wang-dl","html_url":"https:\/\/github.com\/lu-wang-dl","followers_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/followers","following_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/orgs","repos_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/repos","events_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lu-wang-dl\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-03-29T04:36:28Z","updated_at":"2023-07-21T14:15:38Z","closed_at":"2023-07-21T14:15:38Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nAdd a new API `Dataset.from_spark` to create a Dataset from Spark DataFrame.\n\n### Motivation\n\nSpark is a distributed computing framework that can handle large datasets. By supporting loading Spark DataFrames directly into Hugging Face Datasets, we enable take the advantages of spark to processing the data in parallel. \r\n\r\nBy providing a seamless integration between these two frameworks, we make it easier for data scientists and developers to work with both Spark and Hugging Face in the same workflow.\n\n### Your contribution\n\nWe can discuss about the ideas and I can help preparing a PR for this feature.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5678\/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5678\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5677","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5677\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5677\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5677\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5677","id":1644828606,"node_id":"I_kwDODunzps5iChe-","number":5677,"title":"Dataset.map() crashes when any column contains more than 1000 empty dictionaries","user":{"login":"mtoles","id":7139344,"node_id":"MDQ6VXNlcjcxMzkzNDQ=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/7139344?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mtoles","html_url":"https:\/\/github.com\/mtoles","followers_url":"https:\/\/api.github.com\/users\/mtoles\/followers","following_url":"https:\/\/api.github.com\/users\/mtoles\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mtoles\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mtoles\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mtoles\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mtoles\/orgs","repos_url":"https:\/\/api.github.com\/users\/mtoles\/repos","events_url":"https:\/\/api.github.com\/users\/mtoles\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mtoles\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-03-29T00:01:31Z","updated_at":"2023-07-07T14:01:14Z","closed_at":"2023-07-07T14:01:14Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n`Dataset.map()` crashes any time any column contains more than `writer_batch_size` (default 1000) empty dictionaries, regardless of whether the column is being operated on. The error does not occur if the dictionaries are non-empty.\n\n### Steps to reproduce the bug\n\nExample:\r\n\r\n```\r\nimport datasets\r\n\r\ndef add_one(example):\r\n    example[\"col2\"] += 1\r\n    return example\r\n\r\nn = 1001  # crashes\r\n# n = 999 # works\r\nds = datasets.Dataset.from_dict({\"col1\": [{}] * n, \"col2\": [1] * n})\r\n\r\nds = ds.map(add_one, writer_batch_size=1000)\r\n```\n\n### Expected behavior\n\nAbove code should not crash\n\n### Environment info\n\n- `datasets` version: 2.10.1\r\n- Platform: Linux-5.4.0-120-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.15\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5677\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5677\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5675","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5675\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5675\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5675\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5675","id":1641763478,"node_id":"I_kwDODunzps5h21KW","number":5675,"title":"Filter datasets by language code","user":{"login":"named-entity","id":5658496,"node_id":"MDQ6VXNlcjU2NTg0OTY=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/5658496?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/named-entity","html_url":"https:\/\/github.com\/named-entity","followers_url":"https:\/\/api.github.com\/users\/named-entity\/followers","following_url":"https:\/\/api.github.com\/users\/named-entity\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/named-entity\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/named-entity\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/named-entity\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/named-entity\/orgs","repos_url":"https:\/\/api.github.com\/users\/named-entity\/repos","events_url":"https:\/\/api.github.com\/users\/named-entity\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/named-entity\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-03-27T09:42:28Z","updated_at":"2023-03-30T08:08:15Z","closed_at":"2023-03-30T08:08:15Z","author_association":"NONE","active_lock_reason":null,"body":"Hi! I use the language search field on https:\/\/huggingface.co\/datasets\r\nHowever, some of the datasets tagged by ISO language code are not accessible by this search form.\r\nFor example, [myv_ru_2022](https:\/\/huggingface.co\/datasets\/slone\/myv_ru_2022) is has `myv` language tag but it is not included in Languages search form.\r\nI've also noticed the same problem with `mhr` (see https:\/\/huggingface.co\/datasets\/AigizK\/mari-russian-parallel-corpora)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5675\/reactions","total_count":6,"+1":6,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5675\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5674","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5674\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5674\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5674\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5674","id":1641084105,"node_id":"I_kwDODunzps5h0PTJ","number":5674,"title":"Stored XSS","user":{"login":"Fadavvi","id":21213484,"node_id":"MDQ6VXNlcjIxMjEzNDg0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/21213484?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Fadavvi","html_url":"https:\/\/github.com\/Fadavvi","followers_url":"https:\/\/api.github.com\/users\/Fadavvi\/followers","following_url":"https:\/\/api.github.com\/users\/Fadavvi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Fadavvi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Fadavvi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Fadavvi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Fadavvi\/orgs","repos_url":"https:\/\/api.github.com\/users\/Fadavvi\/repos","events_url":"https:\/\/api.github.com\/users\/Fadavvi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Fadavvi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-03-26T20:55:58Z","updated_at":"2023-03-27T21:01:55Z","closed_at":"2023-03-27T21:01:55Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI found a Stored XSS on a page that can be publicly accessible to all visitors. But I didn't find a suitable place to report. \r\nPlease guide me on this. \n\n### Steps to reproduce the bug\n\nDue to security restrictions, I don't want to publish it publicly.\n\n### Expected behavior\n\nUser inputs must be sanitized before rendering.\n\n### Environment info\n\nhttps:\/\/huggingface.co\/ Web UI","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5674\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5674\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5673","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5673\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5673\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5673\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5673","id":1641066352,"node_id":"PR_kwDODunzps5M6wc3","number":5673,"title":"Pass down storage options","user":{"login":"dwyatte","id":2512762,"node_id":"MDQ6VXNlcjI1MTI3NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2512762?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/dwyatte","html_url":"https:\/\/github.com\/dwyatte","followers_url":"https:\/\/api.github.com\/users\/dwyatte\/followers","following_url":"https:\/\/api.github.com\/users\/dwyatte\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/dwyatte\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/dwyatte\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/dwyatte\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/dwyatte\/orgs","repos_url":"https:\/\/api.github.com\/users\/dwyatte\/repos","events_url":"https:\/\/api.github.com\/users\/dwyatte\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/dwyatte\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-03-26T20:09:37Z","updated_at":"2023-03-28T15:03:38Z","closed_at":"2023-03-28T14:54:17Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Remove implementation-specific kwargs from `file_utils.fsspec_get` and `file_utils.fsspec_head`, instead allowing them to be passed down via `storage_options`. This fixes an issue where s3fs did not recognize a timeout arg as well as fixes an issue mentioned in https:\/\/github.com\/huggingface\/datasets\/issues\/5281 by allowing users to pass down `storage_options` all the way from `datasets.load_dataset` to support implementation-specific credentials\r\n\r\nSupports something like the following to provide credentials explicitly instead of relying on boto's methods of locating them\r\n\r\n```\r\nload_dataset(..., data_files=[\"s3:\/\/...\"], storage_options={\"profile\": \"...\"})\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5673\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5673\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5673","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5673","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5673.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5673.patch","merged_at":"2023-03-28T14:54:17Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5672","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5672\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5672\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5672\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5672","id":1641005322,"node_id":"I_kwDODunzps5hz8EK","number":5672,"title":"Pushing dataset to hub crash","user":{"login":"tzvc","id":14275989,"node_id":"MDQ6VXNlcjE0Mjc1OTg5","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14275989?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/tzvc","html_url":"https:\/\/github.com\/tzvc","followers_url":"https:\/\/api.github.com\/users\/tzvc\/followers","following_url":"https:\/\/api.github.com\/users\/tzvc\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/tzvc\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/tzvc\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/tzvc\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/tzvc\/orgs","repos_url":"https:\/\/api.github.com\/users\/tzvc\/repos","events_url":"https:\/\/api.github.com\/users\/tzvc\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/tzvc\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-03-26T17:42:13Z","updated_at":"2023-03-30T08:11:05Z","closed_at":"2023-03-30T08:11:05Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nUploading a dataset with `push_to_hub()` fails without error description. \n\n### Steps to reproduce the bug\n\nHey there,\r\n\r\nI've built a image dataset of 100k images + text pair as described here https:\/\/huggingface.co\/docs\/datasets\/image_dataset#imagefolder\r\n\r\nNow I'm trying to push it to the hub but I'm running into issues. First, I tried doing it via git directly, I added all the files in git lfs and pushed but I got hit with an error saying huggingface only accept up to 10k files in a folder.\r\n\r\nSo I'm now trying with the `push_to_hub()` func as follow:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nimport os\r\n\r\ndataset = load_dataset(\"imagefolder\", data_dir=\".\/data\", split=\"train\")\r\ndataset.push_to_hub(\"tzvc\/organization-logos\", token=os.environ.get('HF_TOKEN'))\r\n```\r\n\r\nBut again, this produces an error:\r\n\r\n```\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100212\/100212 [00:00<00:00, 439108.61it\/s]\r\nDownloading and preparing dataset imagefolder\/default to \/home\/contact_theochampion\/.cache\/huggingface\/datasets\/imagefolder\/default-20567ffc703aa314\/0.0.0\/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100211\/100211 [00:00<00:00, 149323.73it\/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 15947.92it\/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1\/1 [00:00<00:00, 2245.34it\/s]\r\nDataset imagefolder downloaded and prepared to \/home\/contact_theochampion\/.cache\/huggingface\/datasets\/imagefolder\/default-20567ffc703aa314\/0.0.0\/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\r\nResuming upload of the dataset shards.                                                                                                                                        \r\nPushing dataset shards to the dataset hub: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14\/14 [00:31<00:00,  2.24s\/it]\r\nDownloading metadata: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 118\/118 [00:00<00:00, 225kB\/s]\r\nTraceback (most recent call last):\r\n  File \"\/home\/contact_theochampion\/organization-logos\/push_to_hub.py\", line 5, in <module>\r\n    dataset.push_to_hub(\"tzvc\/organization-logos\", token=os.environ.get('HF_TOKEN'))\r\n  File \"\/home\/contact_theochampion\/.local\/lib\/python3.9\/site-packages\/datasets\/arrow_dataset.py\", line 5245, in push_to_hub\r\n    repo_info = dataset_infos[next(iter(dataset_infos))]\r\nStopIteration\r\n```\r\n\r\nWhat could be happening here ?\n\n### Expected behavior\n\nThe dataset is pushed to the hub\n\n### Environment info\n\n- `datasets` version: 2.10.1\r\n- Platform: Linux-5.10.0-21-cloud-amd64-x86_64-with-glibc2.31\r\n- Python version: 3.9.2\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5672\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5672\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5671","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5671\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5671\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5671\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5671","id":1640840012,"node_id":"I_kwDODunzps5hzTtM","number":5671,"title":"How to use `load_dataset('glue', 'cola')`","user":{"login":"makinzm","id":40193664,"node_id":"MDQ6VXNlcjQwMTkzNjY0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/40193664?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/makinzm","html_url":"https:\/\/github.com\/makinzm","followers_url":"https:\/\/api.github.com\/users\/makinzm\/followers","following_url":"https:\/\/api.github.com\/users\/makinzm\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/makinzm\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/makinzm\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/makinzm\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/makinzm\/orgs","repos_url":"https:\/\/api.github.com\/users\/makinzm\/repos","events_url":"https:\/\/api.github.com\/users\/makinzm\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/makinzm\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-26T09:40:34Z","updated_at":"2023-03-28T07:43:44Z","closed_at":"2023-03-28T07:43:43Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm new to use HuggingFace datasets but I cannot use `load_dataset('glue', 'cola')`.\r\n\r\n- I was stacked by the following problem:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ncola_dataset = load_dataset('glue', 'cola')\r\n\r\n---------------------------------------------------------------------------\r\nInvalidVersion                            Traceback (most recent call last)\r\nFile <timed exec>:1\r\n\r\n(Omit because of long error message)\r\n\r\nFile \/usr\/local\/lib\/python3.8\/site-packages\/packaging\/version.py:197, in Version.__init__(self, version)\r\n    195 match = self._regex.search(version)\r\n    196 if not match:\r\n--> 197     raise InvalidVersion(f\"Invalid version: '{version}'\")\r\n    199 # Store the parsed out pieces of the version\r\n    200 self._version = _Version(\r\n    201     epoch=int(match.group(\"epoch\")) if match.group(\"epoch\") else 0,\r\n    202     release=tuple(int(i) for i in match.group(\"release\").split(\".\")),\r\n   (...)\r\n    208     local=_parse_local_version(match.group(\"local\")),\r\n    209 )\r\n\r\nInvalidVersion: Invalid version: '0.10.1,<0.11'\r\n```\r\n\r\n- You can check this full error message in my repository: [MLOps-Basics\/week_0_project_setup\/experimental_notebooks\/data_exploration.ipynb](https:\/\/github.com\/makinzm\/MLOps-Basics\/blob\/eabab4b837880607d9968d3fa687c70177b2affd\/week_0_project_setup\/experimental_notebooks\/data_exploration.ipynb)\r\n\n\n### Steps to reproduce the bug\n\n- This is my repository to reproduce: [MLOps-Basics\/week_0_project_setup](https:\/\/github.com\/makinzm\/MLOps-Basics\/tree\/eabab4b837880607d9968d3fa687c70177b2affd\/week_0_project_setup)\r\n\r\n1. cd `\/DockerImage` and command `docker build . -t week0`\r\n2. cd `\/` and command `docker-compose up`\r\n3. Run `experimental_notebooks\/data_exploration.ipynb`\r\n\r\n----\r\n\r\nJust to be sure, I wrote down Dockerfile and requirements.txt\r\n\r\n- Dockerfile\r\n```Dockerfile\r\nFROM python:3.8\r\n\r\nWORKDIR \/root\/working\r\n\r\nRUN apt-get update && \\\r\n    apt-get install -y python3-dev python3-pip python3-venv && \\\r\n    apt-get clean && \\\r\n    rm -rf \/var\/lib\/apt\/lists\/*\r\n\r\nCOPY requirements.txt .\r\n\r\nRUN pip3 install --no-cache-dir jupyter notebook && pip install --no-cache-dir -r requirements.txt\r\n\r\nCMD [\"bash\"]\r\n```\r\n\r\n- requirements.txt\r\n```txt\r\npytorch-lightning==1.2.10\r\ndatasets==1.6.2\r\ntransformers==4.5.1\r\nscikit-learn==0.24.2\r\n```\r\n\n\n### Expected behavior\n\nThere is no bug to implement `load_dataset('glue', 'cola')`\n\n### Environment info\n\nI already wrote it.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5671\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5671\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5670","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5670\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5670\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5670\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5670","id":1640607045,"node_id":"I_kwDODunzps5hya1F","number":5670,"title":"Unable to load multi class classification datasets","user":{"login":"ysahil97","id":19690506,"node_id":"MDQ6VXNlcjE5NjkwNTA2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/19690506?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/ysahil97","html_url":"https:\/\/github.com\/ysahil97","followers_url":"https:\/\/api.github.com\/users\/ysahil97\/followers","following_url":"https:\/\/api.github.com\/users\/ysahil97\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/ysahil97\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/ysahil97\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/ysahil97\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/ysahil97\/orgs","repos_url":"https:\/\/api.github.com\/users\/ysahil97\/repos","events_url":"https:\/\/api.github.com\/users\/ysahil97\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/ysahil97\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-25T18:06:15Z","updated_at":"2023-03-27T22:54:56Z","closed_at":"2023-03-27T22:54:56Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI've been playing around with huggingface library, mostly with `datasets` and wanted to download the multi class classification datasets to fine tune BERT on this task. ([link](https:\/\/huggingface.co\/docs\/transformers\/training#train-with-pytorch-trainer)).\r\n\r\nWhile loading the dataset, I'm getting the following error snippet.\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[44], line 3\r\n      1 from datasets import load_dataset\r\n----> 3 imdb_dataset = load_dataset(\"yelp_review_full\")\r\n      4 imdb_dataset\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/load.py:1719, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1716 ignore_verifications = ignore_verifications or save_infos\r\n   1718 # Create a dataset builder\r\n-> 1719 builder_instance = load_dataset_builder(\r\n   1720     path=path,\r\n   1721     name=name,\r\n   1722     data_dir=data_dir,\r\n   1723     data_files=data_files,\r\n   1724     cache_dir=cache_dir,\r\n   1725     features=features,\r\n   1726     download_config=download_config,\r\n   1727     download_mode=download_mode,\r\n   1728     revision=revision,\r\n   1729     use_auth_token=use_auth_token,\r\n   1730     **config_kwargs,\r\n   1731 )\r\n   1733 # Return iterable dataset in case of streaming\r\n   1734 if streaming:\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/load.py:1523, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\r\n   1520     raise ValueError(error_msg)\r\n   1522 # Instantiate the dataset builder\r\n-> 1523 builder_instance: DatasetBuilder = builder_cls(\r\n   1524     cache_dir=cache_dir,\r\n   1525     config_name=config_name,\r\n   1526     data_dir=data_dir,\r\n   1527     data_files=data_files,\r\n   1528     hash=hash,\r\n   1529     features=features,\r\n   1530     use_auth_token=use_auth_token,\r\n   1531     **builder_kwargs,\r\n   1532     **config_kwargs,\r\n   1533 )\r\n   1535 return builder_instance\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/builder.py:1292, in GeneratorBasedBuilder.__init__(self, writer_batch_size, *args, **kwargs)\r\n   1291 def __init__(self, *args, writer_batch_size=None, **kwargs):\r\n-> 1292     super().__init__(*args, **kwargs)\r\n   1293     # Batch size used by the ArrowWriter\r\n   1294     # It defines the number of samples that are kept in memory before writing them\r\n   1295     # and also the length of the arrow chunks\r\n   1296     # None means that the ArrowWriter will use its default value\r\n   1297     self._writer_batch_size = writer_batch_size or self.DEFAULT_WRITER_BATCH_SIZE\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/builder.py:312, in DatasetBuilder.__init__(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\r\n    309 # prepare info: DatasetInfo are a standardized dataclass across all datasets\r\n    310 # Prefill datasetinfo\r\n    311 if info is None:\r\n--> 312     info = self.get_exported_dataset_info()\r\n    313     info.update(self._info())\r\n    314     info.builder_name = self.name\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/builder.py:412, in DatasetBuilder.get_exported_dataset_info(self)\r\n    400 def get_exported_dataset_info(self) -> DatasetInfo:\r\n    401     \"\"\"Empty DatasetInfo if doesn't exist\r\n    402 \r\n    403     Example:\r\n   (...)\r\n    410     ```\r\n    411     \"\"\"\r\n--> 412     return self.get_all_exported_dataset_infos().get(self.config.name, DatasetInfo())\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/builder.py:398, in DatasetBuilder.get_all_exported_dataset_infos(cls)\r\n    385 @classmethod\r\n    386 def get_all_exported_dataset_infos(cls) -> DatasetInfosDict:\r\n    387     \"\"\"Empty dict if doesn't exist\r\n    388 \r\n    389     Example:\r\n   (...)\r\n    396     ```\r\n    397     \"\"\"\r\n--> 398     return DatasetInfosDict.from_directory(cls.get_imported_module_dir())\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/info.py:370, in DatasetInfosDict.from_directory(cls, dataset_infos_dir)\r\n    368     dataset_metadata = DatasetMetadata.from_readme(Path(dataset_infos_dir) \/ \"README.md\")\r\n    369     if \"dataset_info\" in dataset_metadata:\r\n--> 370         return cls.from_metadata(dataset_metadata)\r\n    371 if os.path.exists(os.path.join(dataset_infos_dir, config.DATASETDICT_INFOS_FILENAME)):\r\n    372     # this is just to have backward compatibility with dataset_infos.json files\r\n    373     with open(os.path.join(dataset_infos_dir, config.DATASETDICT_INFOS_FILENAME), encoding=\"utf-8\") as f:\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/info.py:396, in DatasetInfosDict.from_metadata(cls, dataset_metadata)\r\n    387     return cls(\r\n    388         {\r\n    389             dataset_info_yaml_dict.get(\"config_name\", \"default\"): DatasetInfo._from_yaml_dict(\r\n   (...)\r\n    393         }\r\n    394     )\r\n    395 else:\r\n--> 396     dataset_info = DatasetInfo._from_yaml_dict(dataset_metadata[\"dataset_info\"])\r\n    397     dataset_info.config_name = dataset_metadata[\"dataset_info\"].get(\"config_name\", \"default\")\r\n    398     return cls({dataset_info.config_name: dataset_info})\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/info.py:332, in DatasetInfo._from_yaml_dict(cls, yaml_data)\r\n    330 yaml_data = copy.deepcopy(yaml_data)\r\n    331 if yaml_data.get(\"features\") is not None:\r\n--> 332     yaml_data[\"features\"] = Features._from_yaml_list(yaml_data[\"features\"])\r\n    333 if yaml_data.get(\"splits\") is not None:\r\n    334     yaml_data[\"splits\"] = SplitDict._from_yaml_list(yaml_data[\"splits\"])\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/features\/features.py:1745, in Features._from_yaml_list(cls, yaml_data)\r\n   1742     else:\r\n   1743         raise TypeError(f\"Expected a dict or a list but got {type(obj)}: {obj}\")\r\n-> 1745 return cls.from_dict(from_yaml_inner(yaml_data))\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/features\/features.py:1741, in Features._from_yaml_list.<locals>.from_yaml_inner(obj)\r\n   1739 elif isinstance(obj, list):\r\n   1740     names = [_feature.pop(\"name\") for _feature in obj]\r\n-> 1741     return {name: from_yaml_inner(_feature) for name, _feature in zip(names, obj)}\r\n   1742 else:\r\n   1743     raise TypeError(f\"Expected a dict or a list but got {type(obj)}: {obj}\")\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/features\/features.py:1741, in <dictcomp>(.0)\r\n   1739 elif isinstance(obj, list):\r\n   1740     names = [_feature.pop(\"name\") for _feature in obj]\r\n-> 1741     return {name: from_yaml_inner(_feature) for name, _feature in zip(names, obj)}\r\n   1742 else:\r\n   1743     raise TypeError(f\"Expected a dict or a list but got {type(obj)}: {obj}\")\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/features\/features.py:1736, in Features._from_yaml_list.<locals>.from_yaml_inner(obj)\r\n   1734             return {\"_type\": snakecase_to_camelcase(obj[\"dtype\"])}\r\n   1735     else:\r\n-> 1736         return from_yaml_inner(obj[\"dtype\"])\r\n   1737 else:\r\n   1738     return {\"_type\": snakecase_to_camelcase(_type), **unsimplify(obj)[_type]}\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/features\/features.py:1738, in Features._from_yaml_list.<locals>.from_yaml_inner(obj)\r\n   1736             return from_yaml_inner(obj[\"dtype\"])\r\n   1737     else:\r\n-> 1738         return {\"_type\": snakecase_to_camelcase(_type), **unsimplify(obj)[_type]}\r\n   1739 elif isinstance(obj, list):\r\n   1740     names = [_feature.pop(\"name\") for _feature in obj]\r\n\r\nFile \/work\/pi_adrozdov_umass_edu\/syerawar_umass_edu\/envs\/vadops\/lib\/python3.10\/site-packages\/datasets\/features\/features.py:1706, in Features._from_yaml_list.<locals>.unsimplify(feature)\r\n   1704 if isinstance(feature.get(\"class_label\"), dict) and isinstance(feature[\"class_label\"].get(\"names\"), dict):\r\n   1705     label_ids = sorted(feature[\"class_label\"][\"names\"])\r\n-> 1706     if label_ids and label_ids != list(range(label_ids[-1] + 1)):\r\n   1707         raise ValueError(\r\n   1708             f\"ClassLabel expected a value for all label ids [0:{label_ids[-1] + 1}] but some ids are missing.\"\r\n   1709         )\r\n   1710     feature[\"class_label\"][\"names\"] = [feature[\"class_label\"][\"names\"][label_id] for label_id in label_ids]\r\n\r\nTypeError: can only concatenate str (not \"int\") to str\r\n```\r\n\r\nThe same issue happens when I try to load `go-emotions` multi class classification dataset. Could somebody guide me on how to fix this issue?\r\n\n\n### Steps to reproduce the bug\n\nRun the following code snippet in a python script\/ notebook cell:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\nyelp_dataset = load_dataset(\"yelp_review_full\")\r\nyelp_dataset\r\n```\n\n### Expected behavior\n\nThe dataset should be loaded perfectly, which showing the train, test and unsupervised splits with the basic data statistics\n\n### Environment info\n\n- `datasets` version: 2.6.1\r\n- Platform: Linux-5.4.0-124-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.9\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.5.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5670\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5670\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5669","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5669\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5669\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5669\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5669","id":1638070046,"node_id":"I_kwDODunzps5hovce","number":5669,"title":"Almost identical datasets, huge performance difference","user":{"login":"eli-osherovich","id":2437102,"node_id":"MDQ6VXNlcjI0MzcxMDI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/2437102?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/eli-osherovich","html_url":"https:\/\/github.com\/eli-osherovich","followers_url":"https:\/\/api.github.com\/users\/eli-osherovich\/followers","following_url":"https:\/\/api.github.com\/users\/eli-osherovich\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/eli-osherovich\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/eli-osherovich\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/eli-osherovich\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/eli-osherovich\/orgs","repos_url":"https:\/\/api.github.com\/users\/eli-osherovich\/repos","events_url":"https:\/\/api.github.com\/users\/eli-osherovich\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/eli-osherovich\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-03-23T18:20:20Z","updated_at":"2023-04-09T18:56:23Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\n\nI am struggling to understand (huge) performance difference between two datasets that are almost identical. \n\n### Steps to reproduce the bug\n\n# Fast (normal) dataset speed:\r\n```python\r\nimport cv2\r\nfrom datasets import load_dataset\r\nfrom torch.utils.data import DataLoader\r\n\r\ndataset = load_dataset(\"beans\", split=\"train\")\r\n\r\nfor x in DataLoader(dataset.with_format(\"torch\"), batch_size=16, shuffle=True, num_workers=8):\r\n    pass\r\n```\r\n\r\nThe above pass over the dataset takes about 1.5 seconds on my computer. \r\nHowever, if I re-create (almost) the same dataset, the sweep takes HUGE amount of time: 15 minutes. Steps to reproduce:\r\n\r\n```python\r\ndef transform(example):\r\n    example[\"image2\"] = cv2.imread(example[\"image_file_path\"])\r\n    return example\r\n\r\ndataset2 = dataset.map(transform, remove_columns=[\"image\"])\r\n\r\nfor x in DataLoader(dataset2.with_format(\"torch\"), batch_size=16, shuffle=True, num_workers=8):\r\n    pass\r\n\r\n```\n\n### Expected behavior\n\nSame timings\n\n### Environment info\n\npython==3.10.9\r\ndatasets==2.10.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5669\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5669\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5668","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5668\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5668\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5668\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5668","id":1638018598,"node_id":"PR_kwDODunzps5MwuIp","number":5668,"title":"Support for downloading only provided split","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-23T17:53:39Z","updated_at":"2023-03-24T06:43:14Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"We can pass split to `_split_generators()`.\r\nBut I'm not sure if it's possible to solve cache issues, mostly with `dataset_info.json`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5668\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5668\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5668","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5668","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5668.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5668.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5667","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5667\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5667\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5667\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5667","id":1637789361,"node_id":"PR_kwDODunzps5Mv8Im","number":5667,"title":"Jax requires jaxlib","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-03-23T15:41:09Z","updated_at":"2023-03-23T16:23:11Z","closed_at":"2023-03-23T16:14:52Z","author_association":"MEMBER","active_lock_reason":null,"body":"close https:\/\/github.com\/huggingface\/datasets\/issues\/5666","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5667\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5667\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5667","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5667","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5667.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5667.patch","merged_at":"2023-03-23T16:14:52Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5666","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5666\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5666\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5666\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5666","id":1637675062,"node_id":"I_kwDODunzps5hnPA2","number":5666,"title":"Support tensorflow 2.12.0 in CI","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-03-23T14:37:51Z","updated_at":"2023-03-23T16:14:54Z","closed_at":"2023-03-23T16:14:54Z","author_association":"MEMBER","active_lock_reason":null,"body":"Once we find out the root cause of:\r\n- #5663\r\n\r\nwe should revert the temporary pin on tensorflow introduced by:\r\n- #5664","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5666\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5666\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5665","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5665\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5665\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5665\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5665","id":1637193648,"node_id":"I_kwDODunzps5hlZew","number":5665,"title":"Feature request: IterableDataset.push_to_hub","user":{"login":"NielsRogge","id":48327001,"node_id":"MDQ6VXNlcjQ4MzI3MDAx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/48327001?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/NielsRogge","html_url":"https:\/\/github.com\/NielsRogge","followers_url":"https:\/\/api.github.com\/users\/NielsRogge\/followers","following_url":"https:\/\/api.github.com\/users\/NielsRogge\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/NielsRogge\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/NielsRogge\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/NielsRogge\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/NielsRogge\/orgs","repos_url":"https:\/\/api.github.com\/users\/NielsRogge\/repos","events_url":"https:\/\/api.github.com\/users\/NielsRogge\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/NielsRogge\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-03-23T09:53:04Z","updated_at":"2023-03-23T09:53:16Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\r\n\r\nIt'd be great to have a lazy push to hub, similar to the lazy loading we have with `IterableDataset`.\r\n\r\nSuppose you'd like to filter [LAION](https:\/\/huggingface.co\/datasets\/laion\/laion400m) based on certain conditions, but as LAION doesn't fit into your disk, you'd like to leverage streaming:\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"laion\/laion400m\", streaming=True, split=\"train\")\r\n```\r\nThen you could filter the dataset based on certain conditions:\r\n```\r\nfiltered_dataset = dataset.filter(lambda example: example['HEIGHT'] > 400)\r\n```\r\n\r\nIn order to persist this dataset and push it back to the hub, one currently needs to first load the entire filtered dataset on disk and then push:\r\n\r\n```\r\nfrom datasets import Dataset\r\n\r\nDataset.from_generator(filtered_dataset.__iter__).push_to_hub(...)\r\n```\r\nIt would be great if we can instead lazy push to the data to the hub (basically stream the data to the hub), not being limited by our disk size:\r\n```\r\nfiltered_dataset.push_to_hub(\"my-filtered-dataset\")\r\n```\r\n\r\n### Motivation\r\n\r\nThis feature would be very useful for people that want to filter huge datasets without having to load the entire dataset or a filtered version thereof on their local disk.\r\n\r\n### Your contribution\r\n\r\nHappy to test out a PR :)","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5665\/reactions","total_count":9,"+1":9,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5665\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5664","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5664\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5664\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5664\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5664","id":1637192684,"node_id":"PR_kwDODunzps5Mt6vp","number":5664,"title":"Fix CI by temporarily pinning tensorflow < 2.12.0","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-23T09:52:26Z","updated_at":"2023-03-23T10:17:11Z","closed_at":"2023-03-23T10:09:54Z","author_association":"MEMBER","active_lock_reason":null,"body":"As a hotfix for our CI, temporarily pin `tensorflow` upper version:\r\n- In Python 3.10, tensorflow-2.12.0 also installs `jax`\r\n\r\nFix #5663\r\n\r\nUntil root cause is fixed.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5664\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5664\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5664","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5664","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5664.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5664.patch","merged_at":"2023-03-23T10:09:53Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5663","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5663\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5663\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5663\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5663","id":1637173248,"node_id":"I_kwDODunzps5hlUgA","number":5663,"title":"CI is broken: ModuleNotFoundError: jax requires jaxlib to be installed","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-03-23T09:39:43Z","updated_at":"2023-03-23T10:09:55Z","closed_at":"2023-03-23T10:09:55Z","author_association":"MEMBER","active_lock_reason":null,"body":"CI test_py310 is broken: see https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/4498945505\/jobs\/7916194236?pr=5662\r\n```\r\nFAILED tests\/test_arrow_dataset.py::BaseDatasetTest::test_map_jax_in_memory - ModuleNotFoundError: jax requires jaxlib to be installed. See https:\/\/github.com\/google\/jax#installation for installation instructions.\r\nFAILED tests\/test_arrow_dataset.py::BaseDatasetTest::test_map_jax_on_disk - ModuleNotFoundError: jax requires jaxlib to be installed. See https:\/\/github.com\/google\/jax#installation for installation instructions.\r\nFAILED tests\/test_formatting.py::FormatterTest::test_jax_formatter - ModuleNotFoundError: jax requires jaxlib to be installed. See https:\/\/github.com\/google\/jax#installation for installation instructions.\r\nFAILED tests\/test_formatting.py::FormatterTest::test_jax_formatter_audio - ModuleNotFoundError: jax requires jaxlib to be installed. See https:\/\/github.com\/google\/jax#installation for installation instructions.\r\nFAILED tests\/test_formatting.py::FormatterTest::test_jax_formatter_device - ModuleNotFoundError: jax requires jaxlib to be installed. See https:\/\/github.com\/google\/jax#installation for installation instructions.\r\nFAILED tests\/test_formatting.py::FormatterTest::test_jax_formatter_image - ModuleNotFoundError: jax requires jaxlib to be installed. See https:\/\/github.com\/google\/jax#installation for installation instructions.\r\nFAILED tests\/test_formatting.py::FormatterTest::test_jax_formatter_jnp_array_kwargs - ModuleNotFoundError: jax requires jaxlib to be installed. See https:\/\/github.com\/google\/jax#installation for installation instructions.\r\nFAILED tests\/features\/test_features.py::CastToPythonObjectsTest::test_cast_to_python_objects_jax - ModuleNotFoundError: jax requires jaxlib to be installed. See https:\/\/github.com\/google\/jax#installation for installation instructions.\r\n===== 8 failed, 2147 passed, 10 skipped, 37 warnings in 228.69s (0:03:48) ======\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5663\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5663\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5662","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5662\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5662\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5662\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5662","id":1637140813,"node_id":"PR_kwDODunzps5MtvsM","number":5662,"title":"Fix unnecessary dict comprehension","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-03-23T09:18:58Z","updated_at":"2023-03-23T09:46:59Z","closed_at":"2023-03-23T09:37:49Z","author_association":"MEMBER","active_lock_reason":null,"body":"After ruff-0.0.258 release, the C416 rule was updated with unnecessary dict comprehensions. See:\r\n- https:\/\/github.com\/charliermarsh\/ruff\/releases\/tag\/v0.0.258\r\n- https:\/\/github.com\/charliermarsh\/ruff\/pull\/3605\r\n\r\nThis PR fixes one unnecessary dict comprehension in our code: no need to unpack and re-pack the tuple values.\r\n\r\nFix #5661","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5662\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5662\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5662","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5662","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5662.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5662.patch","merged_at":"2023-03-23T09:37:49Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5661","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5661\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5661\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5661\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5661","id":1637129445,"node_id":"I_kwDODunzps5hlJzl","number":5661,"title":"CI is broken: Unnecessary `dict` comprehension","user":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2023-03-23T09:13:01Z","updated_at":"2023-03-23T09:37:51Z","closed_at":"2023-03-23T09:37:51Z","author_association":"MEMBER","active_lock_reason":null,"body":"CI check_code_quality is broken:\r\n```\r\nsrc\/datasets\/arrow_dataset.py:3267:35: C416 [*] Unnecessary `dict` comprehension (rewrite using `dict()`)\r\nFound 1 error.\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5661\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5661\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5660","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5660\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5660\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5660\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5660","id":1635543646,"node_id":"I_kwDODunzps5hfGpe","number":5660,"title":"integration with imbalanced-learn","user":{"login":"tansaku","id":30216,"node_id":"MDQ6VXNlcjMwMjE2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/30216?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/tansaku","html_url":"https:\/\/github.com\/tansaku","followers_url":"https:\/\/api.github.com\/users\/tansaku\/followers","following_url":"https:\/\/api.github.com\/users\/tansaku\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/tansaku\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/tansaku\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/tansaku\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/tansaku\/orgs","repos_url":"https:\/\/api.github.com\/users\/tansaku\/repos","events_url":"https:\/\/api.github.com\/users\/tansaku\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/tansaku\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":1935892913,"node_id":"MDU6TGFiZWwxOTM1ODkyOTEz","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/wontfix","name":"wontfix","color":"ffffff","default":true,"description":"This will not be worked on"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-03-22T11:05:17Z","updated_at":"2023-07-06T18:10:15Z","closed_at":"2023-07-06T18:10:15Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nWouldn't it be great if the various class balancing operations from imbalanced-learn were available as part of datasets?\n\n### Motivation\n\nI'm trying to use imbalanced-learn to balance a dataset, but it's not clear how to get the two to interoperate - what would be great would be some examples.  I've looked online, asked gpt-4, but so far not making much progress.\n\n### Your contribution\n\nIf I can get this working myself I can submit a PR with example code to go in the docs","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5660\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5660\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5659","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5659\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5659\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5659\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5659","id":1635447540,"node_id":"I_kwDODunzps5hevL0","number":5659,"title":"[Audio] Soundfile\/libsndfile requirements too stringent for decoding mp3 files","user":{"login":"sanchit-gandhi","id":93869735,"node_id":"U_kgDOBZhWpw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/93869735?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/sanchit-gandhi","html_url":"https:\/\/github.com\/sanchit-gandhi","followers_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/followers","following_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/orgs","repos_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/repos","events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/sanchit-gandhi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2023-03-22T10:07:33Z","updated_at":"2023-11-10T15:42:32Z","closed_at":"2023-04-07T08:51:28Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nI'm encountering several issues trying to load mp3 audio files using `datasets` on a TPU v4. \r\n\r\nThe PR https:\/\/github.com\/huggingface\/datasets\/pull\/5573 updated the audio loading logic to rely solely on the `soundfile`\/`libsndfile` libraries for loading audio samples, regardless of their file type. \r\n\r\nThe installation guide suggests that `libsndfile` is bundled in when `soundfile` is pip installed:\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/e1af108015e43f9df8734a1faeeaeb9eafce3971\/docs\/source\/installation.md?plain=1#L70-L71\r\n\r\nHowever, just pip installing `soundfile==0.12.1` throws an error that `libsndfile` is missing:\r\n\r\n```\r\npip install soundfile==0.12.1\r\n```\r\n\r\nThen:\r\n```python\r\n>>> soundfile\r\n>>> soundfile.__libsndfile_version__\r\n```\r\n\r\n<details>\r\n\r\n<summary> Traceback (most recent call last): <\/summary>\r\n\r\n```\r\n  File \"\/home\/sanchitgandhi\/hf\/lib\/python3.8\/site-packages\/soundfile.py\", line 161, in <module>\r\n    import _soundfile_data  # ImportError if this doesn't exist\r\nModuleNotFoundError: No module named '_soundfile_data'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/sanchitgandhi\/hf\/lib\/python3.8\/site-packages\/soundfile.py\", line 170, in <module>\r\n    raise OSError('sndfile library not found using ctypes.util.find_library')\r\nOSError: sndfile library not found using ctypes.util.find_library\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/sanchitgandhi\/hf\/lib\/python3.8\/site-packages\/soundfile.py\", line 192, in <module>\r\n    _snd = _ffi.dlopen(_explicit_libname)\r\nOSError: cannot load library 'libsndfile.so': libsndfile.so: cannot open shared object file: No such file or directory\r\n```\r\n<\/details>\r\n\r\nThus, I've followed the official instructions for installing the `soundfile` package from https:\/\/github.com\/bastibe\/python-soundfile#installation, which states that `libsndfile` needs to be installed separately as:\r\n```\r\npip install --upgrade soundfile\r\nsudo apt install libsndfile1\r\n```\r\n\r\nWe can now import `soundfile`:\r\n```python\r\n>>> import soundfile\r\n>>> soundfile.__version__\r\n'0.12.1'\r\n>>> soundfile.__libsndfile_version__\r\n'1.0.28'\r\n```\r\n\r\nWe see that we have `soundfile==0.12.1`, which matches the `datasets[audio]` package constraints:\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/e1af108015e43f9df8734a1faeeaeb9eafce3971\/setup.py#L144-L147\r\n\r\nBut we have `libsndfile==1.0.28`, which is too low for decoding mp3 files:\r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/e1af108015e43f9df8734a1faeeaeb9eafce3971\/src\/datasets\/config.py#L136-L138\r\n\r\nUpdating\/upgrading the `libsndfile` doesn't change this:\r\n```\r\nsudo apt-get update\r\nsudo apt-get upgrade\r\n```\r\n\r\nIs there any other suggestion for how to get a compatible `libsndfile` version? Currently, the version bundled with Ubuntu `apt-get` is too low for decoding mp3 files.\r\n\r\nMaybe we could add this under `setup.py` such that we install the correct `libsndfile` version when we do `pip install datasets[audio]`? IMO this would help circumvent such version issues.\r\n\r\n### Steps to reproduce the bug\r\n\r\nEnvironment described above. Loading mp3 files:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ncommon_voice_es = load_dataset(\"common_voice\", \"es\", split=\"validation\", streaming=True)\r\nprint(next(iter(common_voice_es)))\r\n```\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[4], line 2\r\n      1 common_voice_es = load_dataset(\"common_voice\", \"es\", split=\"validation\", streaming=True)\r\n----> 2 print(next(iter(common_voice_es)))\r\n\r\nFile ~\/datasets\/src\/datasets\/iterable_dataset.py:941, in IterableDataset.__iter__(self)\r\n    937 for key, example in ex_iterable:\r\n    938     if self.features:\r\n    939         # `IterableDataset` automatically fills missing columns with None.\r\n    940         # This is done with `_apply_feature_types_on_example`.\r\n--> 941         yield _apply_feature_types_on_example(\r\n    942             example, self.features, token_per_repo_id=self._token_per_repo_id\r\n    943         )\r\n    944     else:\r\n    945         yield example\r\n\r\nFile ~\/datasets\/src\/datasets\/iterable_dataset.py:700, in _apply_feature_types_on_example(example, features, token_per_repo_id)\r\n    698 encoded_example = features.encode_example(example)\r\n    699 # Decode example for Audio feature, e.g.\r\n--> 700 decoded_example = features.decode_example(encoded_example, token_per_repo_id=token_per_repo_id)\r\n    701 return decoded_example\r\n\r\nFile ~\/datasets\/src\/datasets\/features\/features.py:1864, in Features.decode_example(self, example, token_per_repo_id)\r\n   1850 def decode_example(self, example: dict, token_per_repo_id: Optional[Dict[str, Union[str, bool, None]]] = None):\r\n   1851     \"\"\"Decode example with custom feature decoding.\r\n   1852 \r\n   1853     Args:\r\n   (...)\r\n   1861         `dict[str, Any]`\r\n   1862     \"\"\"\r\n-> 1864     return {\r\n   1865         column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\r\n   1866         if self._column_requires_decoding[column_name]\r\n   1867         else value\r\n   1868         for column_name, (feature, value) in zip_dict(\r\n   1869             {key: value for key, value in self.items() if key in example}, example\r\n   1870         )\r\n   1871     }\r\n\r\nFile ~\/datasets\/src\/datasets\/features\/features.py:1865, in <dictcomp>(.0)\r\n   1850 def decode_example(self, example: dict, token_per_repo_id: Optional[Dict[str, Union[str, bool, None]]] = None):\r\n   1851     \"\"\"Decode example with custom feature decoding.\r\n   1852 \r\n   1853     Args:\r\n   (...)\r\n   1861         `dict[str, Any]`\r\n   1862     \"\"\"\r\n   1864     return {\r\n-> 1865         column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\r\n   1866         if self._column_requires_decoding[column_name]\r\n   1867         else value\r\n   1868         for column_name, (feature, value) in zip_dict(\r\n   1869             {key: value for key, value in self.items() if key in example}, example\r\n   1870         )\r\n   1871     }\r\n\r\nFile ~\/datasets\/src\/datasets\/features\/features.py:1308, in decode_nested_example(schema, obj, token_per_repo_id)\r\n   1305 elif isinstance(schema, (Audio, Image)):\r\n   1306     # we pass the token to read and decode files from private repositories in streaming mode\r\n   1307     if obj is not None and schema.decode:\r\n-> 1308         return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n   1309 return obj\r\n\r\nFile ~\/datasets\/src\/datasets\/features\/audio.py:167, in Audio.decode_example(self, value, token_per_repo_id)\r\n    162     raise RuntimeError(\r\n    163         \"Decoding 'opus' files requires system library 'libsndfile'>=1.0.31, \"\r\n    164         'You can try to update `soundfile` python library: `pip install \"soundfile>=0.12.1\"`. '\r\n    165     )\r\n    166 elif not config.IS_MP3_SUPPORTED and audio_format == \"mp3\":\r\n--> 167     raise RuntimeError(\r\n    168         \"Decoding 'mp3' files requires system library 'libsndfile'>=1.1.0, \"\r\n    169         'You can try to update `soundfile` python library: `pip install \"soundfile>=0.12.1\"`. '\r\n    170     )\r\n    172 if file is None:\r\n    173     token_per_repo_id = token_per_repo_id or {}\r\n\r\nRuntimeError: Decoding 'mp3' files requires system library 'libsndfile'>=1.1.0, You can try to update `soundfile` python library: `pip install \"soundfile>=0.12.1\"`.\r\n```\r\n\r\n### Expected behavior\r\n\r\nLoad mp3 files!\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.10.2.dev0\r\n- Platform: Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.13.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n- Soundfile version: 0.12.1\r\n- Libsndfile version: 1.0.28","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5659\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5659\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5658","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5658\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5658\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5658\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5658","id":1634867204,"node_id":"PR_kwDODunzps5MmJe0","number":5658,"title":"docs: Update num_shards docs to mention num_proc on Dataset and DatasetDict","user":{"login":"connor-henderson","id":78612354,"node_id":"MDQ6VXNlcjc4NjEyMzU0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/78612354?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/connor-henderson","html_url":"https:\/\/github.com\/connor-henderson","followers_url":"https:\/\/api.github.com\/users\/connor-henderson\/followers","following_url":"https:\/\/api.github.com\/users\/connor-henderson\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/connor-henderson\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/connor-henderson\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/connor-henderson\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/connor-henderson\/orgs","repos_url":"https:\/\/api.github.com\/users\/connor-henderson\/repos","events_url":"https:\/\/api.github.com\/users\/connor-henderson\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/connor-henderson\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-22T00:12:18Z","updated_at":"2023-03-24T16:43:34Z","closed_at":"2023-03-24T16:36:21Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Closes #5653\r\n\r\n@mariosasko","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5658\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5658\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5658","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5658","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5658.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5658.patch","merged_at":"2023-03-24T16:36:21Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5656","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5656\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5656\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5656\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5656","id":1634156563,"node_id":"PR_kwDODunzps5Mjxoo","number":5656,"title":"Fix `fsspec.open` when using an HTTP proxy","user":{"login":"bryant1410","id":3905501,"node_id":"MDQ6VXNlcjM5MDU1MDE=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/3905501?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/bryant1410","html_url":"https:\/\/github.com\/bryant1410","followers_url":"https:\/\/api.github.com\/users\/bryant1410\/followers","following_url":"https:\/\/api.github.com\/users\/bryant1410\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/bryant1410\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/bryant1410\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/bryant1410\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/bryant1410\/orgs","repos_url":"https:\/\/api.github.com\/users\/bryant1410\/repos","events_url":"https:\/\/api.github.com\/users\/bryant1410\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/bryant1410\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-21T15:23:29Z","updated_at":"2023-03-23T14:14:50Z","closed_at":"2023-03-23T13:15:46Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Most HTTP(S) downloads from this library support proxy automatically by reading the `HTTP_PROXY` environment variable (et al.) because `requests` is widely used. However, in some parts of the code, `fsspec` is used, which in turn uses `aiohttp` for HTTP(S) requests (as opposed to `requests`), which in turn doesn't support reading proxy env variables by default. This PR enables reading them automatically.\r\n\r\nRead [aiohttp docs on using proxies](https:\/\/docs.aiohttp.org\/en\/stable\/client_advanced.html?highlight=trust_env#proxy-support).\r\n\r\nFor context, [the Python library requests](https:\/\/requests.readthedocs.io\/en\/latest\/user\/advanced\/?highlight=http_proxy#proxies) and [the official Python library via `urllib.urlopen` support this automatically by default](https:\/\/docs.python.org\/3\/library\/urllib.request.html#urllib.request.urlopen). Many (most common ones?) programs also do the same, including cURL, APT, Wget, and many others.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5656\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5656\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5656","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5656","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5656.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5656.patch","merged_at":"2023-03-23T13:15:46Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5655","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5655\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5655\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5655\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5655","id":1634030017,"node_id":"PR_kwDODunzps5MjWYy","number":5655,"title":"Improve features decoding in to_iterable_dataset","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-03-21T14:18:09Z","updated_at":"2023-03-23T13:19:27Z","closed_at":"2023-03-23T13:12:25Z","author_association":"MEMBER","active_lock_reason":null,"body":"Following discussion at https:\/\/github.com\/huggingface\/datasets\/pull\/5589\r\n\r\nRight now `to_iterable_dataset` on images\/audio hurts iterable dataset performance a lot (e.g. x4 slower because it encodes+decodes images\/audios unnecessarily).\r\n\r\nI fixed it by providing a generator that yields undecoded examples","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5655\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5655\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5655","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5655","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5655.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5655.patch","merged_at":"2023-03-23T13:12:25Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5654","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5654\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5654\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5654\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5654","id":1633523705,"node_id":"I_kwDODunzps5hXZf5","number":5654,"title":"Offset overflow when executing Dataset.map","user":{"login":"jan-pair","id":118280608,"node_id":"U_kgDOBwzRoA","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/118280608?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/jan-pair","html_url":"https:\/\/github.com\/jan-pair","followers_url":"https:\/\/api.github.com\/users\/jan-pair\/followers","following_url":"https:\/\/api.github.com\/users\/jan-pair\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/jan-pair\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/jan-pair\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/jan-pair\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/jan-pair\/orgs","repos_url":"https:\/\/api.github.com\/users\/jan-pair\/repos","events_url":"https:\/\/api.github.com\/users\/jan-pair\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/jan-pair\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-21T09:33:27Z","updated_at":"2023-03-21T10:32:07Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi, I'm trying to use `.map` method to cache multiple random crops from the image to speed up data processing during training, as the image size is too big. \r\n\r\nThe map function executes all iterations, and then returns the following error:\r\n\r\n```bash\r\nTraceback (most recent call last):                                                                                                                                                                                                                                                                                          \r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/enhancement\/lib\/python3.8\/site-packages\/datasets\/arrow_dataset.py\", line 3353, in _map_single\r\n    writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/enhancement\/lib\/python3.8\/site-packages\/datasets\/arrow_writer.py\", line 582, in finalize\r\n    self.write_examples_on_file()\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/enhancement\/lib\/python3.8\/site-packages\/datasets\/arrow_writer.py\", line 446, in write_examples_on_file\r\n    self.write_batch(batch_examples=batch_examples)\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/enhancement\/lib\/python3.8\/site-packages\/datasets\/arrow_writer.py\", line 555, in write_batch\r\n    self.write_table(pa_table, writer_batch_size)\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/enhancement\/lib\/python3.8\/site-packages\/datasets\/arrow_writer.py\", line 567, in write_table\r\n    pa_table = pa_table.combine_chunks()\r\n  File \"pyarrow\/table.pxi\", line 3315, in pyarrow.lib.Table.combine_chunks\r\n  File \"pyarrow\/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow\/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\r\n```\r\n\r\nHere is the minimal code (`\/home\/datasets\/DIV2K_train_HR` is just a folder of images that can be replaced by any appropriate):\n\n### Steps to reproduce the bug\n\n```python\r\nfrom glob import glob\r\n\r\nimport torch\r\nfrom datasets import Dataset, Image\r\nfrom torchvision.transforms import PILToTensor, RandomCrop\r\n\r\nfile_paths = glob(\"\/home\/datasets\/DIV2K_train_HR\/*\")\r\n\r\nto_tensor = PILToTensor()\r\ncrop_transf = RandomCrop(size=256)\r\n\r\ndef prepare_data(example):\r\n    tensor = to_tensor(example[\"image\"].convert(\"RGB\"))\r\n    return {\"hr\": torch.stack([crop_transf(tensor) for _ in range(25)])}\r\n\r\ntrain_data = Dataset.from_dict({\"image\": file_paths}).cast_column(\"image\", Image())\r\ntrain_data = train_data.map(\r\n    prepare_data,\r\n    cache_file_name=\"\/home\/datasets\/DIV2K_train_HR_crops.tmp\",\r\n    desc=\"Caching multiple random crops of image\",\r\n    remove_columns=\"image\",\r\n)\r\nprint(train_data[0].keys(), train_data[0][\"hr\"].shape)\r\n```\n\n### Expected behavior\n\nCached file is stored at `\"\/home\/datasets\/DIV2K_train_HR_crops.tmp\"`, output is `dict_keys(['hr']) torch.Size([25, 3, 256, 256])`\n\n### Environment info\n\n- `datasets` version: 2.10.1\r\n- Platform: Linux-5.15.0-67-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.16\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n- Pytorch version: 2.0.0+cu117\r\n- torchvision version: 0.15.1+cu117","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5654\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5654\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5653","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5653\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5653\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5653\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5653","id":1633254159,"node_id":"I_kwDODunzps5hWXsP","number":5653,"title":"Doc: save_to_disk, `num_proc` will affect `num_shards`, but it's not documented","user":{"login":"RmZeta2718","id":42400165,"node_id":"MDQ6VXNlcjQyNDAwMTY1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42400165?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/RmZeta2718","html_url":"https:\/\/github.com\/RmZeta2718","followers_url":"https:\/\/api.github.com\/users\/RmZeta2718\/followers","following_url":"https:\/\/api.github.com\/users\/RmZeta2718\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/RmZeta2718\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/RmZeta2718\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/RmZeta2718\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/RmZeta2718\/orgs","repos_url":"https:\/\/api.github.com\/users\/RmZeta2718\/repos","events_url":"https:\/\/api.github.com\/users\/RmZeta2718\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/RmZeta2718\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892861,"node_id":"MDU6TGFiZWwxOTM1ODkyODYx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/documentation","name":"documentation","color":"0075ca","default":true,"description":"Improvements or additions to documentation"},{"id":1935892877,"node_id":"MDU6TGFiZWwxOTM1ODkyODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/good%20first%20issue","name":"good first issue","color":"7057ff","default":true,"description":"Good for newcomers"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2023-03-21T05:25:35Z","updated_at":"2023-03-24T16:36:23Z","closed_at":"2023-03-24T16:36:23Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\n[`num_proc`](https:\/\/huggingface.co\/docs\/datasets\/main\/en\/package_reference\/main_classes#datasets.DatasetDict.save_to_disk.num_proc) will affect `num_shards`, but it's not documented\r\n\n\n### Steps to reproduce the bug\n\nNothing to reproduce\n\n### Expected behavior\n\n[document of `num_shards`](https:\/\/huggingface.co\/docs\/datasets\/main\/en\/package_reference\/main_classes#datasets.DatasetDict.save_to_disk.num_shards) explicitly says that it depends on `max_shard_size`, it should also mention `num_proc`.\r\n\n\n### Environment info\n\ndatasets main document","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5653\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5653\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5652","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5652\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5652\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5652\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5652","id":1632546073,"node_id":"PR_kwDODunzps5MeVUR","number":5652,"title":"Copy features","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2023-03-20T17:17:23Z","updated_at":"2023-03-23T13:19:19Z","closed_at":"2023-03-23T13:12:08Z","author_association":"MEMBER","active_lock_reason":null,"body":"Some users (even internally at HF) are doing\r\n```python\r\ndset_features = dset.features\r\ndset_features.pop(col_to_remove)\r\ndset = dset.map(..., features=dset_features)\r\n```\r\n\r\nRight now this causes issues because it modifies the features dict in place before the map.\r\n\r\nIn this PR I modified `dset.features` to return a copy of the features, so that users can modify it if they want.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5652\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5652\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5652","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5652","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5652.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5652.patch","merged_at":"2023-03-23T13:12:08Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5651","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5651\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5651\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5651\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5651","id":1631967509,"node_id":"I_kwDODunzps5hRdkV","number":5651,"title":"expanduser in save_to_disk","user":{"login":"RmZeta2718","id":42400165,"node_id":"MDQ6VXNlcjQyNDAwMTY1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42400165?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/RmZeta2718","html_url":"https:\/\/github.com\/RmZeta2718","followers_url":"https:\/\/api.github.com\/users\/RmZeta2718\/followers","following_url":"https:\/\/api.github.com\/users\/RmZeta2718\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/RmZeta2718\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/RmZeta2718\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/RmZeta2718\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/RmZeta2718\/orgs","repos_url":"https:\/\/api.github.com\/users\/RmZeta2718\/repos","events_url":"https:\/\/api.github.com\/users\/RmZeta2718\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/RmZeta2718\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892877,"node_id":"MDU6TGFiZWwxOTM1ODkyODc3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/good%20first%20issue","name":"good first issue","color":"7057ff","default":true,"description":"Good for newcomers"}],"state":"closed","locked":false,"assignee":{"login":"benjaminbrown038","id":35114142,"node_id":"MDQ6VXNlcjM1MTE0MTQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35114142?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/benjaminbrown038","html_url":"https:\/\/github.com\/benjaminbrown038","followers_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/followers","following_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/orgs","repos_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/repos","events_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/received_events","type":"User","site_admin":false},"assignees":[{"login":"benjaminbrown038","id":35114142,"node_id":"MDQ6VXNlcjM1MTE0MTQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/35114142?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/benjaminbrown038","html_url":"https:\/\/github.com\/benjaminbrown038","followers_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/followers","following_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/orgs","repos_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/repos","events_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/benjaminbrown038\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":5,"created_at":"2023-03-20T12:02:18Z","updated_at":"2023-10-27T14:04:37Z","closed_at":"2023-10-27T14:04:37Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nsave_to_disk() does not expand `~`\r\n\r\n1. `dataset = load_datasets(\"any dataset\")`\r\n2. `dataset.save_to_disk(\"~\/data\")`\r\n3. a folder named \"~\" created in current folder\r\n4. FileNotFoundError is raised, because the expanded path does not exist (`\/home\/<user>\/data`)\r\n\r\nrelated issue https:\/\/github.com\/huggingface\/transformers\/issues\/10628\r\n\r\n### Steps to reproduce the bug\r\n\r\nAs described above.\r\n\r\n### Expected behavior\r\n\r\nexpanduser correctly\r\n\r\n### Environment info\r\n\r\n- datasets 2.10.1\r\n- python 3.10","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5651\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5651\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5650","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5650\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5650\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5650\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5650","id":1630336919,"node_id":"I_kwDODunzps5hLPeX","number":5650,"title":"load_dataset can't work correct with my image data ","user":{"login":"WiNE-iNEFF","id":41611046,"node_id":"MDQ6VXNlcjQxNjExMDQ2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/41611046?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/WiNE-iNEFF","html_url":"https:\/\/github.com\/WiNE-iNEFF","followers_url":"https:\/\/api.github.com\/users\/WiNE-iNEFF\/followers","following_url":"https:\/\/api.github.com\/users\/WiNE-iNEFF\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/WiNE-iNEFF\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/WiNE-iNEFF\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/WiNE-iNEFF\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/WiNE-iNEFF\/orgs","repos_url":"https:\/\/api.github.com\/users\/WiNE-iNEFF\/repos","events_url":"https:\/\/api.github.com\/users\/WiNE-iNEFF\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/WiNE-iNEFF\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":21,"created_at":"2023-03-18T13:59:13Z","updated_at":"2023-07-24T14:13:02Z","closed_at":"2023-07-24T14:13:01Z","author_association":"NONE","active_lock_reason":null,"body":"I have about 20000 images in my folder which divided into 4 folders with class names.\nWhen i use load_dataset(\"my_folder_name\", split=\"train\") this function create dataset in which there are only 4 images, the remaining 19000 images were not added there. What is the problem and did not understand. Tried converting images and the like but absolutely nothing worked","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5650\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5650\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5649","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5649\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5649\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5649\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5649","id":1630173460,"node_id":"I_kwDODunzps5hKnkU","number":5649,"title":"The index column created with .to_sql() is dependent on the batch_size when writing","user":{"login":"lsb","id":45281,"node_id":"MDQ6VXNlcjQ1Mjgx","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/45281?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lsb","html_url":"https:\/\/github.com\/lsb","followers_url":"https:\/\/api.github.com\/users\/lsb\/followers","following_url":"https:\/\/api.github.com\/users\/lsb\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lsb\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lsb\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lsb\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lsb\/orgs","repos_url":"https:\/\/api.github.com\/users\/lsb\/repos","events_url":"https:\/\/api.github.com\/users\/lsb\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lsb\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false},"assignees":[{"login":"albertvillanova","id":8515462,"node_id":"MDQ6VXNlcjg1MTU0NjI=","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/8515462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/albertvillanova","html_url":"https:\/\/github.com\/albertvillanova","followers_url":"https:\/\/api.github.com\/users\/albertvillanova\/followers","following_url":"https:\/\/api.github.com\/users\/albertvillanova\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/albertvillanova\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/albertvillanova\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/albertvillanova\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/albertvillanova\/orgs","repos_url":"https:\/\/api.github.com\/users\/albertvillanova\/repos","events_url":"https:\/\/api.github.com\/users\/albertvillanova\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/albertvillanova\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2023-03-18T05:25:17Z","updated_at":"2023-06-17T07:01:57Z","closed_at":"2023-06-17T07:01:57Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nIt seems like the \"index\" column is designed to be unique? The values are only unique per batch. The SQL index is not a unique index.\r\n\r\nThis can be a problem, for instance, when building a faiss index on a dataset and then trying to match up ids with a sql export.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import Dataset\r\nimport sqlite3\r\ndb = sqlite3.connect(\":memory:\")\r\nnice_numbers = Dataset.from_dict({\"nice_number\": range(101,106)})\r\nnice_numbers.to_sql(\"nice1\", db, batch_size=1)\r\nnice_numbers.to_sql(\"nice2\", db, batch_size=2)\r\nprint(db.execute(\"select * from nice1\").fetchall()) # [(0, 101), (0, 102), (0, 103), (0, 104), (0, 105)]\r\nprint(db.execute(\"select * from nice2\").fetchall()) # [(0, 101), (1, 102), (0, 103), (1, 104), (0, 105)]\r\n```\r\n\r\n### Expected behavior\r\n\r\nI expected the \"index\" column to be unique\r\n\r\n### Environment info\r\n\r\n```\r\n % datasets-cli env\r\n\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.10.1\r\n- Platform: macOS-13.2.1-arm64-arm-64bit\r\n- Python version: 3.9.6\r\n- PyArrow version: 7.0.0\r\n- Pandas version: 1.5.2\r\n\r\nzsh: segmentation fault  datasets-cli env\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5649\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5649\/timeline","performed_via_github_app":null,"state_reason":"not_planned","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5648","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5648\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5648\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5648\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5648","id":1629253719,"node_id":"I_kwDODunzps5hHHBX","number":5648,"title":"flatten_indices doesn't work with pandas format","user":{"login":"alialamiidrissi","id":14365168,"node_id":"MDQ6VXNlcjE0MzY1MTY4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14365168?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alialamiidrissi","html_url":"https:\/\/github.com\/alialamiidrissi","followers_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/followers","following_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/orgs","repos_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/repos","events_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892857,"node_id":"MDU6TGFiZWwxOTM1ODkyODU3","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"assignees":[{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2023-03-17T12:44:25Z","updated_at":"2023-03-21T13:12:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi,\r\nI noticed that `flatten_indices` throws an error when the batch format is `pandas`. This is probably due to the fact that flatten_indices uses map internally which doesn't accept dataframes as the transformation function output\n\n### Steps to reproduce the bug\n\ntabular_data = pd.DataFrame(np.random.randn(10,10))\r\ntabular_data = datasets.arrow_dataset.Dataset.from_pandas(tabular_data)\r\ntabular_data.with_format(\"pandas\").select([0,1,2,3]).flatten_indices()\n\n### Expected behavior\n\nNo error thrown\n\n### Environment info\n\n- `datasets` version: 2.10.1\r\n- Python version: 3.9.5\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.4.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5648\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5648\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5647","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5647\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5647\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5647\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5647","id":1628225544,"node_id":"I_kwDODunzps5hDMAI","number":5647,"title":"Make all print statements optional","user":{"login":"gagan3012","id":49101362,"node_id":"MDQ6VXNlcjQ5MTAxMzYy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/49101362?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/gagan3012","html_url":"https:\/\/github.com\/gagan3012","followers_url":"https:\/\/api.github.com\/users\/gagan3012\/followers","following_url":"https:\/\/api.github.com\/users\/gagan3012\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/gagan3012\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/gagan3012\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/gagan3012\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/gagan3012\/orgs","repos_url":"https:\/\/api.github.com\/users\/gagan3012\/repos","events_url":"https:\/\/api.github.com\/users\/gagan3012\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/gagan3012\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-16T20:30:07Z","updated_at":"2023-07-21T14:20:25Z","closed_at":"2023-07-21T14:20:24Z","author_association":"NONE","active_lock_reason":null,"body":"### Feature request\n\nMake all print statements optional to speed up the development\n\n### Motivation\n\nIm loading multiple tiny datasets and all the print statements make the loading slower\n\n### Your contribution\n\nI can help contribute","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5647\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5647\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5646","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5646\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5646\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5646\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5646","id":1627838762,"node_id":"PR_kwDODunzps5MOqjj","number":5646,"title":"Allow self as key in `Features`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-03-16T16:17:03Z","updated_at":"2023-03-16T17:21:58Z","closed_at":"2023-03-16T17:14:50Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Fix #5641 ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5646\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5646\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5646","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5646","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5646.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5646.patch","merged_at":"2023-03-16T17:14:50Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5645","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5645\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5645\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5645\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5645","id":1627108278,"node_id":"I_kwDODunzps5g-7O2","number":5645,"title":"Datasets map and select(range()) is giving dill error","user":{"login":"Tanya-11","id":90728105,"node_id":"MDQ6VXNlcjkwNzI4MTA1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/90728105?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Tanya-11","html_url":"https:\/\/github.com\/Tanya-11","followers_url":"https:\/\/api.github.com\/users\/Tanya-11\/followers","following_url":"https:\/\/api.github.com\/users\/Tanya-11\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Tanya-11\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Tanya-11\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Tanya-11\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Tanya-11\/orgs","repos_url":"https:\/\/api.github.com\/users\/Tanya-11\/repos","events_url":"https:\/\/api.github.com\/users\/Tanya-11\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Tanya-11\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-16T10:01:28Z","updated_at":"2023-03-17T04:24:51Z","closed_at":"2023-03-17T04:24:51Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nI'm using Huggingface Datasets library to load the dataset in google colab\r\n\r\nWhen I do, \r\n \r\n\r\n> data = train_dataset.select(range(10))\r\n\r\nor \r\n\r\n> train_datasets = train_dataset.map(\r\n>      process_data_to_model_inputs,\r\n>      batched=True,\r\n>      batch_size=batch_size,\r\n>      remove_columns=[\"article\", \"abstract\"],\r\n>  )\r\n\r\n I get following error: `module 'dill._dill' has no attribute 'log'`\r\nI've tried downgrading the dill version from latest to 0.2.8, but no luck. \r\n\r\nStack trace:\r\n\r\n> ---------------------------------------------------------------------------\r\n> ModuleNotFoundError                       Traceback (most recent call last)\r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/datasets\/utils\/py_utils.py in _no_cache_fields(obj)\r\n>     367     try:\r\n> --> 368         import transformers as tr\r\n>     369 \r\n> \r\n> ModuleNotFoundError: No module named 'transformers'\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> AttributeError                            Traceback (most recent call last)\r\n> 17 frames\r\n> <ipython-input-13-dd14813880a6> in <module>\r\n> ----> 1 test = train_dataset.select(range(10))\r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/datasets\/arrow_dataset.py in wrapper(*args, **kwargs)\r\n>     155         }\r\n>     156         # apply actual function\r\n> --> 157         out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n>     158         datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n>     159         # re-apply format to the output\r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/datasets\/fingerprint.py in wrapper(*args, **kwargs)\r\n>     155                     if kwargs.get(fingerprint_name) is None:\r\n>     156                         kwargs_for_fingerprint[\"fingerprint_name\"] = fingerprint_name\r\n> --> 157                         kwargs[fingerprint_name] = update_fingerprint(\r\n>     158                             self._fingerprint, transform, kwargs_for_fingerprint\r\n>     159                         )\r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/datasets\/fingerprint.py in update_fingerprint(fingerprint, transform, transform_args)\r\n>     103     for key in sorted(transform_args):\r\n>     104         hasher.update(key)\r\n> --> 105         hasher.update(transform_args[key])\r\n>     106     return hasher.hexdigest()\r\n>     107 \r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/datasets\/fingerprint.py in update(self, value)\r\n>      55     def update(self, value):\r\n>      56         self.m.update(f\"=={type(value)}==\".encode(\"utf8\"))\r\n> ---> 57         self.m.update(self.hash(value).encode(\"utf-8\"))\r\n>      58 \r\n>      59     def hexdigest(self):\r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/datasets\/fingerprint.py in hash(cls, value)\r\n>      51             return cls.dispatch[type(value)](cls, value)\r\n>      52         else:\r\n> ---> 53             return cls.hash_default(value)\r\n>      54 \r\n>      55     def update(self, value):\r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/datasets\/fingerprint.py in hash_default(cls, value)\r\n>      44     @classmethod\r\n>      45     def hash_default(cls, value):\r\n> ---> 46         return cls.hash_bytes(dumps(value))\r\n>      47 \r\n>      48     @classmethod\r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/datasets\/utils\/py_utils.py in dumps(obj)\r\n>     387     file = StringIO()\r\n>     388     with _no_cache_fields(obj):\r\n> --> 389         dump(obj, file)\r\n>     390     return file.getvalue()\r\n>     391 \r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/datasets\/utils\/py_utils.py in dump(obj, file)\r\n>     359 def dump(obj, file):\r\n>     360     \"\"\"pickle an object to a file\"\"\"\r\n> --> 361     Pickler(file, recurse=True).dump(obj)\r\n>     362     return\r\n>     363 \r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/dill\/_dill.py in dump(self, obj)\r\n>     392     return\r\n>     393 \r\n> --> 394 def load_session(filename='\/tmp\/session.pkl', main=None):\r\n>     395     \"\"\"update the __main__ module with the state from the session file\"\"\"\r\n>     396     if main is None: main = _main_module\r\n> \r\n> \/usr\/lib\/python3.9\/pickle.py in dump(self, obj)\r\n>     485         if self.proto >= 4:\r\n>     486             self.framer.start_framing()\r\n> --> 487         self.save(obj)\r\n>     488         self.write(STOP)\r\n>     489         self.framer.end_framing()\r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/dill\/_dill.py in save(self, obj, save_persistent_id)\r\n>     386         pickler._byref = False   # disable pickling by name reference\r\n>     387         pickler._recurse = False # disable pickling recursion for globals\r\n> --> 388         pickler._session = True  # is best indicator of when pickling a session\r\n>     389         pickler.dump(main)\r\n>     390     finally:\r\n> \r\n> \/usr\/lib\/python3.9\/pickle.py in save(self, obj, save_persistent_id)\r\n>     558             f = self.dispatch.get(t)\r\n>     559             if f is not None:\r\n> --> 560                 f(self, obj)  # Call unbound method with explicit self\r\n>     561                 return\r\n>     562 \r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/dill\/_dill.py in save_singleton(pickler, obj)\r\n> \r\n> \/usr\/lib\/python3.9\/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\r\n>     689             write(NEWOBJ)\r\n>     690         else:\r\n> --> 691             save(func)\r\n>     692             save(args)\r\n>     693             write(REDUCE)\r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/dill\/_dill.py in save(self, obj, save_persistent_id)\r\n>     386         pickler._byref = False   # disable pickling by name reference\r\n>     387         pickler._recurse = False # disable pickling recursion for globals\r\n> --> 388         pickler._session = True  # is best indicator of when pickling a session\r\n>     389         pickler.dump(main)\r\n>     390     finally:\r\n> \r\n> \/usr\/lib\/python3.9\/pickle.py in save(self, obj, save_persistent_id)\r\n>     558             f = self.dispatch.get(t)\r\n>     559             if f is not None:\r\n> --> 560                 f(self, obj)  # Call unbound method with explicit self\r\n>     561                 return\r\n>     562 \r\n> \r\n> \/usr\/local\/lib\/python3.9\/dist-packages\/datasets\/utils\/py_utils.py in save_function(pickler, obj)\r\n>     583         dill._dill.log.info(\"# F1\")\r\n>     584     else:\r\n> --> 585         dill._dill.log.info(\"F2: %s\" % obj)\r\n>     586         name = getattr(obj, \"__qualname__\", getattr(obj, \"__name__\", None))\r\n>     587         dill._dill.StockPickler.save_global(pickler, obj, name=name)\r\n> \r\n> AttributeError: module 'dill._dill' has no attribute 'log'\n\n### Steps to reproduce the bug\n\nAfter loading the dataset(eg: https:\/\/huggingface.co\/datasets\/scientific_papers) in google colab\r\n\r\ndo either \r\n> data = train_dataset.select(range(10))\r\n\r\nor \r\n\r\n> train_datasets = train_dataset.map(\r\n>      process_data_to_model_inputs,\r\n>      batched=True,\r\n>      batch_size=batch_size,\r\n>      remove_columns=[\"article\", \"abstract\"],\r\n>  )\n\n### Expected behavior\n\nThe map and select function should work\n\n### Environment info\n\ndataset: https:\/\/huggingface.co\/datasets\/scientific_papers\r\ndill = 0.3.6\r\npython= 3.9.16\r\ntransformer = 4.2.0","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5645\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5645\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5644","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5644\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5644\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5644\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5644","id":1626204046,"node_id":"PR_kwDODunzps5MJHUi","number":5644,"title":"Allow direct cast from binary to Audio\/Image","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-03-15T20:02:54Z","updated_at":"2023-03-16T14:20:44Z","closed_at":"2023-03-16T14:12:55Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"To address https:\/\/github.com\/huggingface\/datasets\/discussions\/5593.\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5644\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5644\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5644","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5644","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5644.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5644.patch","merged_at":"2023-03-16T14:12:55Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5643","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5643\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5643\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5643\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5643","id":1626160220,"node_id":"PR_kwDODunzps5MI9zO","number":5643,"title":"Support PyArrow arrays as column values in `from_dict`","user":{"login":"mariosasko","id":47462742,"node_id":"MDQ6VXNlcjQ3NDYyNzQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/47462742?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/mariosasko","html_url":"https:\/\/github.com\/mariosasko","followers_url":"https:\/\/api.github.com\/users\/mariosasko\/followers","following_url":"https:\/\/api.github.com\/users\/mariosasko\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/mariosasko\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/mariosasko\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/mariosasko\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/mariosasko\/orgs","repos_url":"https:\/\/api.github.com\/users\/mariosasko\/repos","events_url":"https:\/\/api.github.com\/users\/mariosasko\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/mariosasko\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2023-03-15T19:32:40Z","updated_at":"2023-03-16T17:23:06Z","closed_at":"2023-03-16T17:15:40Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"For consistency with `pa.Table.from_pydict`, which supports both Python lists and PyArrow arrays as column values.\r\n\r\n\"Fixes\" https:\/\/discuss.huggingface.co\/t\/pyarrow-lib-floatarray-did-not-recognize-python-value-type-when-inferring-an-arrow-data-type\/33417","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5643\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5643\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5643","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5643","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5643.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5643.patch","merged_at":"2023-03-16T17:15:39Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5642","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5642\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5642\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5642\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5642","id":1626043177,"node_id":"PR_kwDODunzps5MIjw9","number":5642,"title":"Bump hfh to 0.11.0","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-03-15T18:26:07Z","updated_at":"2023-03-20T12:34:09Z","closed_at":"2023-03-20T12:26:58Z","author_association":"MEMBER","active_lock_reason":null,"body":"to fix errors like\r\n\r\n```\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https:\/\/hub-ci.huggingface.co\/api\/datasets\/__DUMMY_TRANSFORMERS_USER__\/...\r\n```\r\n\r\n(e.g. from this [failing CI](https:\/\/github.com\/huggingface\/datasets\/actions\/runs\/4428956210\/jobs\/7769160997))\r\n\r\n0.11.0 is the current minimum version in `transformers`\r\n\r\naround 5% of users are currently using versions `<0.11.0`","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5642\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5642\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5642","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5642","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5642.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5642.patch","merged_at":"2023-03-20T12:26:58Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5641","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5641\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5641\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5641\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5641","id":1625942730,"node_id":"I_kwDODunzps5g6erK","number":5641,"title":"Features cannot be named \"self\"","user":{"login":"alialamiidrissi","id":14365168,"node_id":"MDQ6VXNlcjE0MzY1MTY4","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/14365168?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/alialamiidrissi","html_url":"https:\/\/github.com\/alialamiidrissi","followers_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/followers","following_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/orgs","repos_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/repos","events_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/alialamiidrissi\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-03-15T17:16:40Z","updated_at":"2023-03-16T17:14:51Z","closed_at":"2023-03-16T17:14:51Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nHi,\r\nI noticed that we cannot create a HuggingFace dataset from Pandas DataFrame with a column named `self`.\r\nThe error seems to be coming from arguments validation in the `Features.from_dict` function.\r\n\r\n\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\ndummy_pandas = pd.DataFrame([0,1,2,3], columns = [\"self\"])\r\ndatasets.arrow_dataset.Dataset.from_pandas(dummy_pandas)\r\n```\n\n### Expected behavior\n\nNo error thrown\n\n### Environment info\n\n- `datasets` version: 2.8.0\r\n- Python version: 3.9.5\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.4.1","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5641\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5641\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5640","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5640\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5640\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5640\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5640","id":1625896057,"node_id":"PR_kwDODunzps5MID3I","number":5640,"title":"Less zip false positives","user":{"login":"lhoestq","id":42851186,"node_id":"MDQ6VXNlcjQyODUxMTg2","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/42851186?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/lhoestq","html_url":"https:\/\/github.com\/lhoestq","followers_url":"https:\/\/api.github.com\/users\/lhoestq\/followers","following_url":"https:\/\/api.github.com\/users\/lhoestq\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/lhoestq\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/lhoestq\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/lhoestq\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/lhoestq\/orgs","repos_url":"https:\/\/api.github.com\/users\/lhoestq\/repos","events_url":"https:\/\/api.github.com\/users\/lhoestq\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/lhoestq\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2023-03-15T16:48:59Z","updated_at":"2023-03-16T13:47:37Z","closed_at":"2023-03-16T13:40:12Z","author_association":"MEMBER","active_lock_reason":null,"body":"`zipfile.is_zipfile` return false positives for some Parquet files. It causes errors when loading certain parquet datasets, where some files are considered ZIP files by `zipfile.is_zipfile`\r\n\r\nThis is a known issue: https:\/\/github.com\/python\/cpython\/issues\/72680\r\n\r\nAt first I wanted to rely only on magic numbers, but then I found that someone contributed a [fix to is_zipfile](https:\/\/github.com\/python\/cpython\/pull\/5053) - do you think we should use it @albertvillanova or not ?\r\n\r\nIMO it's ok to rely on magic numbers only for now, since in streaming mode we've had no issue checking only the magic number so far.\r\n\r\nClose https:\/\/github.com\/huggingface\/datasets\/issues\/5639","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5640\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5640\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5640","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5640","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5640.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5640.patch","merged_at":"2023-03-16T13:40:12Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5639","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5639\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5639\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5639\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5639","id":1625737098,"node_id":"I_kwDODunzps5g5seK","number":5639,"title":"Parquet file wrongly recognized as zip prevents loading a dataset","user":{"login":"clefourrier","id":22726840,"node_id":"MDQ6VXNlcjIyNzI2ODQw","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/22726840?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/clefourrier","html_url":"https:\/\/github.com\/clefourrier","followers_url":"https:\/\/api.github.com\/users\/clefourrier\/followers","following_url":"https:\/\/api.github.com\/users\/clefourrier\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/clefourrier\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/clefourrier\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/clefourrier\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/clefourrier\/orgs","repos_url":"https:\/\/api.github.com\/users\/clefourrier\/repos","events_url":"https:\/\/api.github.com\/users\/clefourrier\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/clefourrier\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2023-03-15T15:20:45Z","updated_at":"2023-03-16T13:40:14Z","closed_at":"2023-03-16T13:40:14Z","author_association":"MEMBER","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nWhen trying to `load_dataset_builder` for `HuggingFaceGECLM\/StackExchange_Mar2023`, extraction fails, because parquet file [devops-00000-of-00001-22fe902fd8702892.parquet](https:\/\/huggingface.co\/datasets\/HuggingFaceGECLM\/StackExchange_Mar2023\/resolve\/1f8c9a2ab6f7d0f9ae904b8b922e4384592ae1a5\/data\/devops-00000-of-00001-22fe902fd8702892.parquet) is wrongly identified by python as being a zip not a parquet.\r\n(Full thread on [Slack](https:\/\/huggingface.slack.com\/archives\/C02V51Q3800\/p1678890880803599))\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\n\r\nds = load_dataset_builder(\"HuggingFaceGECLM\/StackExchange_Mar2023\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nLoading the file normally.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.3.2\r\n- Platform: Linux-5.14.0-1058-oem-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.4.3","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5639\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5639\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5638","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5638\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5638\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5638\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5638","id":1625564471,"node_id":"I_kwDODunzps5g5CU3","number":5638,"title":"xPath to implement all operations for Path","user":{"login":"thomasw21","id":24695242,"node_id":"MDQ6VXNlcjI0Njk1MjQy","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/24695242?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/thomasw21","html_url":"https:\/\/github.com\/thomasw21","followers_url":"https:\/\/api.github.com\/users\/thomasw21\/followers","following_url":"https:\/\/api.github.com\/users\/thomasw21\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/thomasw21\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/thomasw21\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/thomasw21\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/thomasw21\/orgs","repos_url":"https:\/\/api.github.com\/users\/thomasw21\/repos","events_url":"https:\/\/api.github.com\/users\/thomasw21\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/thomasw21\/received_events","type":"User","site_admin":false},"labels":[{"id":1935892871,"node_id":"MDU6TGFiZWwxOTM1ODkyODcx","url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/labels\/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2023-03-15T13:47:11Z","updated_at":"2023-03-17T13:21:12Z","closed_at":"2023-03-17T13:21:12Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Feature request\n\nCurrent xPath implementation is a great extension of Path in order to work with remote objects. However some methods such as `mkdir` are not implemented correctly. It should instead rely on `fsspec` methods, instead of defaulting do `Path` methods which only work locally.\n\n### Motivation\n\nI'm using xPath to interact with remote objects.\n\n### Your contribution\n\nI could try to make a PR. I'm a bit unfamiliar with chaining right now.","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5638\/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5638\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5637","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5637\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5637\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5637\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5637","id":1625295691,"node_id":"I_kwDODunzps5g4AtL","number":5637,"title":"IterableDataset with_format does not support 'device' keyword for jax","user":{"login":"Lime-Cakes","id":91322985,"node_id":"MDQ6VXNlcjkxMzIyOTg1","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/91322985?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/Lime-Cakes","html_url":"https:\/\/github.com\/Lime-Cakes","followers_url":"https:\/\/api.github.com\/users\/Lime-Cakes\/followers","following_url":"https:\/\/api.github.com\/users\/Lime-Cakes\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/Lime-Cakes\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/Lime-Cakes\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/Lime-Cakes\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/Lime-Cakes\/orgs","repos_url":"https:\/\/api.github.com\/users\/Lime-Cakes\/repos","events_url":"https:\/\/api.github.com\/users\/Lime-Cakes\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/Lime-Cakes\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-15T11:04:12Z","updated_at":"2023-03-16T18:30:59Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nAs seen here: https:\/\/huggingface.co\/docs\/datasets\/use_with_jax dataset.with_format() supports the keyword 'device', to put data on a specific device when loaded as jax. However, when called on an IterableDataset, I got the error `TypeError: with_format() got an unexpected keyword argument 'device'`\r\n\r\nLooking over the code, it seems IterableDataset support only pytorch and no support for jax device keyword? \r\nhttps:\/\/github.com\/huggingface\/datasets\/blob\/fc5c84f36684343bff3e424cb0fd1ac5ecdd66da\/src\/datasets\/iterable_dataset.py#L1029\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Load an IterableDataset (tested in streaming mode)\r\n2. Call with_format('jax',device=device)\r\n\r\n### Expected behavior\r\n\r\nI expect to call `with_format('jax', device=device)`  as per [documentation](https:\/\/huggingface.co\/docs\/datasets\/use_with_jax) without error\r\n\r\n### Environment info\r\n\r\nTested with installing newest (dev) and also pip release (2.10.1).\r\n- `datasets` version: 2.10.2.dev0\r\n- Platform: Linux-5.15.89+-x86_64-with-debian-bullseye-sid\r\n- Python version: 3.7.12\r\n- Huggingface_hub version: 0.12.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.3.5\r\n","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5637\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5637\/timeline","performed_via_github_app":null,"state_reason":null,"draft":null,"pull_request":null}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5636","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5636\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5636\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5636\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5636","id":1623721577,"node_id":"PR_kwDODunzps5MAunR","number":5636,"title":"Fix CI: ignore C901 (\"some_func\" is to complex) in `ruff`","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-14T15:29:11Z","updated_at":"2023-03-14T16:37:06Z","closed_at":"2023-03-14T16:29:52Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"idk if I should have added this ignore to `ruff` too, but I added :) ","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5636\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5636\/timeline","performed_via_github_app":null,"state_reason":null,"draft":false,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5636","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5636","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5636.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5636.patch","merged_at":"2023-03-14T16:29:52Z"}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5635","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5635\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5635\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5635\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5635","id":1623682558,"node_id":"PR_kwDODunzps5MAmLU","number":5635,"title":"Pass custom metadata filename to Image\/Audio folders","user":{"login":"polinaeterna","id":16348744,"node_id":"MDQ6VXNlcjE2MzQ4NzQ0","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/16348744?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/polinaeterna","html_url":"https:\/\/github.com\/polinaeterna","followers_url":"https:\/\/api.github.com\/users\/polinaeterna\/followers","following_url":"https:\/\/api.github.com\/users\/polinaeterna\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/polinaeterna\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/polinaeterna\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/polinaeterna\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/polinaeterna\/orgs","repos_url":"https:\/\/api.github.com\/users\/polinaeterna\/repos","events_url":"https:\/\/api.github.com\/users\/polinaeterna\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/polinaeterna\/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2023-03-14T15:08:16Z","updated_at":"2023-03-22T17:50:31Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This is a quick fix. \r\nNow it requires to pass data via `data_files` parameters and include a required metadata file there and pass its filename as `metadata_filename` parameter.\r\nFor example, with the structure like:\r\n```\r\ndata\r\n   images_dir\/\r\n      im1.jpg\r\n      im2.jpg\r\n      ...\r\n   metadata_dir\/\r\n      meta_file1.jsonl\r\n      meta_file2.jsonl\r\n      ...\r\n```\r\nto load data with `metadata_file1.jsonl` do:\r\n```python\r\nds = load_dataset(\"imagefolder\", data_files=[\"data\/images_dir\/**\", \"data\/metadata_dir\/meta_file1.jsonl\"], metadata_filename=\"meta_file1.jsonl\")\r\n```\r\n\r\nNote that if you have multiple splits, metadata file should be specified in each of them in `data_files`, smth like:\r\n```python\r\ndata_files={\r\n\"train\": [\"data\/train\/**\", \"data\/metadata_dir\/meta_file1.jsonl\"], \r\n\"test\": [\"data\/train\/**\", \"data\/metadata_dir\/meta_file1.jsonl\"]\r\n}\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5635\/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":1,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5635\/timeline","performed_via_github_app":null,"state_reason":null,"draft":true,"pull_request":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/pulls\/5635","html_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5635","diff_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5635.diff","patch_url":"https:\/\/github.com\/huggingface\/datasets\/pull\/5635.patch","merged_at":null}}
{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5634","repository_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets","labels_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5634\/labels{\/name}","comments_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5634\/comments","events_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5634\/events","html_url":"https:\/\/github.com\/huggingface\/datasets\/issues\/5634","id":1622424174,"node_id":"I_kwDODunzps5gtDpu","number":5634,"title":"Not all progress bars are showing up when they should for downloading dataset","user":{"login":"garlandz-db","id":110427462,"node_id":"U_kgDOBpT9Rg","avatar_url":"https:\/\/avatars.githubusercontent.com\/u\/110427462?v=4","gravatar_id":"","url":"https:\/\/api.github.com\/users\/garlandz-db","html_url":"https:\/\/github.com\/garlandz-db","followers_url":"https:\/\/api.github.com\/users\/garlandz-db\/followers","following_url":"https:\/\/api.github.com\/users\/garlandz-db\/following{\/other_user}","gists_url":"https:\/\/api.github.com\/users\/garlandz-db\/gists{\/gist_id}","starred_url":"https:\/\/api.github.com\/users\/garlandz-db\/starred{\/owner}{\/repo}","subscriptions_url":"https:\/\/api.github.com\/users\/garlandz-db\/subscriptions","organizations_url":"https:\/\/api.github.com\/users\/garlandz-db\/orgs","repos_url":"https:\/\/api.github.com\/users\/garlandz-db\/repos","events_url":"https:\/\/api.github.com\/users\/garlandz-db\/events{\/privacy}","received_events_url":"https:\/\/api.github.com\/users\/garlandz-db\/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2023-03-13T23:04:18Z","updated_at":"2023-10-11T16:30:16Z","closed_at":"2023-10-11T16:30:16Z","author_association":"NONE","active_lock_reason":null,"body":"### Describe the bug\n\nDuring downloading the rotten tomatoes dataset, not all progress bars are displayed properly. This might be related to [this ticket](https:\/\/github.com\/huggingface\/datasets\/issues\/5117) as it raised the same concern but its not clear if the fix solves this issue too.\r\n\r\n\r\n\r\nipywidgets\r\n<img width=\"1243\" alt=\"image\" src=\"https:\/\/user-images.githubusercontent.com\/110427462\/224851138-13fee5b7-ab51-4883-b96f-1b9808782e3b.png\">\r\n\r\n\r\ntqdm\r\n<img width=\"1251\" alt=\"Screen Shot 2023-03-13 at 3 58 59 PM\" src=\"https:\/\/user-images.githubusercontent.com\/110427462\/224851180-5feb7825-9250-4b1e-ad0c-f3172ac1eb78.png\">\n\n### Steps to reproduce the bug\n\n1. Run this line\r\n```\r\nfrom datasets import load_dataset\r\n\r\nrotten_tomatoes = load_dataset(\"rotten_tomatoes\", split=\"train\")\r\n```\n\n### Expected behavior\n\nall progress bars for builder script, metadata, readme, training, validation, and test set\r\n\n\n### Environment info\n\nrequirements.txt\r\n```\r\naiofiles==22.1.0\r\naiohttp==3.8.4\r\naiosignal==1.3.1\r\naiosqlite==0.18.0\r\nanyio==3.6.2\r\nappnope==0.1.3\r\nargon2-cffi==21.3.0\r\nargon2-cffi-bindings==21.2.0\r\narrow==1.2.3\r\nasttokens==2.2.1\r\nasync-generator==1.10\r\nasync-timeout==4.0.2\r\nattrs==22.2.0\r\nBabel==2.12.1\r\nbackcall==0.2.0\r\nbeautifulsoup4==4.11.2\r\nbleach==6.0.0\r\nbrotlipy @ file:\/\/\/Users\/runner\/miniforge3\/conda-bld\/brotlipy_1666764961872\/work\r\ncertifi==2022.12.7\r\ncffi @ file:\/\/\/Users\/runner\/miniforge3\/conda-bld\/cffi_1671179414629\/work\r\ncfgv==3.3.1\r\ncharset-normalizer @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/charset-normalizer_1661170624537\/work\r\ncomm==0.1.2\r\nconda==22.9.0\r\nconda-package-handling @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/conda-package-handling_1669907009957\/work\r\nconda_package_streaming @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/conda-package-streaming_1669733752472\/work\r\ncoverage==7.2.1\r\ncryptography @ file:\/\/\/Users\/runner\/miniforge3\/conda-bld\/cryptography_1669592251328\/work\r\ndatasets==2.1.0\r\ndebugpy==1.6.6\r\ndecorator==5.1.1\r\ndefusedxml==0.7.1\r\ndill==0.3.6\r\ndistlib==0.3.6\r\ndistro==1.4.0\r\nentrypoints==0.4\r\nexceptiongroup==1.1.0\r\nexecuting==1.2.0\r\nfastjsonschema==2.16.3\r\nfilelock==3.9.0\r\nflaky==3.7.0\r\nfqdn==1.5.1\r\nfrozenlist==1.3.3\r\nfsspec==2023.3.0\r\nhuggingface-hub==0.10.1\r\nidentify==2.5.18\r\nidna @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/idna_1663625384323\/work\r\niniconfig==2.0.0\r\nipykernel==6.12.1\r\nipyparallel==8.4.1\r\nipython==7.32.0\r\nipython-genutils==0.2.0\r\nipywidgets==8.0.4\r\nisoduration==20.11.0\r\njedi==0.18.2\r\nJinja2==3.1.2\r\njson5==0.9.11\r\njsonpointer==2.3\r\njsonschema==4.17.3\r\njupyter-events==0.6.3\r\njupyter-ydoc==0.2.2\r\njupyter_client==8.0.3\r\njupyter_core==5.2.0\r\njupyter_server==2.4.0\r\njupyter_server_fileid==0.8.0\r\njupyter_server_terminals==0.4.4\r\njupyter_server_ydoc==0.6.1\r\njupyterlab==3.6.1\r\njupyterlab-pygments==0.2.2\r\njupyterlab-widgets==3.0.5\r\njupyterlab_server==2.20.0\r\nlibmambapy @ file:\/\/\/Users\/runner\/miniforge3\/conda-bld\/mamba-split_1671598370072\/work\/libmambapy\r\nmamba @ file:\/\/\/Users\/runner\/miniforge3\/conda-bld\/mamba-split_1671598370072\/work\/mamba\r\nMarkupSafe==2.1.2\r\nmatplotlib-inline==0.1.6\r\nmistune==2.0.5\r\nmultidict==6.0.4\r\nmultiprocess==0.70.14\r\nnbclassic==0.5.3\r\nnbclient==0.7.2\r\nnbconvert==7.2.9\r\nnbformat==5.7.3\r\nnest-asyncio==1.5.6\r\nnodeenv==1.7.0\r\nnotebook==6.5.3\r\nnotebook_shim==0.2.2\r\nnumpy==1.24.2\r\noutcome==1.2.0\r\npackaging==23.0\r\npandas==1.5.3\r\npandocfilters==1.5.0\r\nparso==0.8.3\r\npexpect==4.8.0\r\npickleshare==0.7.5\r\nplatformdirs==3.0.0\r\nplotly==5.13.1\r\npluggy==1.0.0\r\npre-commit==3.1.0\r\nprometheus-client==0.16.0\r\nprompt-toolkit==3.0.38\r\npsutil==5.9.4\r\nptyprocess==0.7.0\r\npure-eval==0.2.2\r\npyarrow==11.0.0\r\npycosat @ file:\/\/\/Users\/runner\/miniforge3\/conda-bld\/pycosat_1666836580084\/work\r\npycparser @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/pycparser_1636257122734\/work\r\nPygments==2.14.0\r\npyOpenSSL @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/pyopenssl_1665350324128\/work\r\npyrsistent==0.19.3\r\nPySocks @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/pysocks_1661604839144\/work\r\npytest==7.2.1\r\npytest-asyncio==0.20.3\r\npytest-cov==4.0.0\r\npytest-timeout==2.1.0\r\npython-dateutil==2.8.2\r\npython-json-logger==2.0.7\r\npytz==2022.7.1\r\nPyYAML==6.0\r\npyzmq==25.0.0\r\nrequests @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/requests_1661872987712\/work\r\nresponses==0.18.0\r\nrfc3339-validator==0.1.4\r\nrfc3986-validator==0.1.1\r\nruamel-yaml-conda @ file:\/\/\/Users\/runner\/miniforge3\/conda-bld\/ruamel_yaml_1666819760545\/work\r\nSend2Trash==1.8.0\r\nsimplegeneric==0.8.1\r\nsix==1.16.0\r\nsniffio==1.3.0\r\nsortedcontainers==2.4.0\r\nsoupsieve==2.4\r\nstack-data==0.6.2\r\ntenacity==8.2.2\r\nterminado==0.17.1\r\ntinycss2==1.2.1\r\ntomli==2.0.1\r\ntoolz @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/toolz_1657485559105\/work\r\ntornado==6.2\r\ntqdm==4.64.1\r\ntraitlets==5.8.1\r\ntrio==0.22.0\r\ntyping_extensions==4.5.0\r\nuri-template==1.2.0\r\nurllib3 @ file:\/\/\/home\/conda\/feedstock_root\/build_artifacts\/urllib3_1669259737463\/work\r\nvirtualenv==20.19.0\r\nwcwidth==0.2.6\r\nwebcolors==1.12\r\nwebencodings==0.5.1\r\nwebsocket-client==1.5.1\r\nwidgetsnbextension==4.0.5\r\nxxhash==3.2.0\r\ny-py==0.5.9\r\nyarl==1.8.2\r\nypy-websocket==0.8.2\r\nzstandard==0.19.0\r\n```","reactions":{"url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5634\/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https:\/\/api.github.com\/repos\/huggingface\/datasets\/issues\/5634\/timeline","performed_via_github_app":null,"state_reason":"completed","draft":null,"pull_request":null}
